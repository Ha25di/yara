[
    {
        "id": "0704.0046",
        "submitter": "Denes Petz",
        "authors": "I. Csiszar, F. Hiai and D. Petz",
        "title": "A limit relation for entropy and channel capacity per unit cost",
        "comments": "LATEX file, 11 pages",
        "journal-ref": "J. Math. Phys. 48(2007), 092102.",
        "doi": "10.1063/1.2779138",
        "report-no": null,
        "categories": "quant-ph cs.IT math.IT",
        "license": null,
        "abstract": "  In a quantum mechanical model, Diosi, Feldmann and Kosloff arrived at a\nconjecture stating that the limit of the entropy of certain mixtures is the\nrelative entropy as system size goes to infinity. The conjecture is proven in\nthis paper for density matrices. The first proof is analytic and uses the\nquantum law of large numbers. The second one clarifies the relation to channel\ncapacity per unit cost for classical-quantum channels. Both proofs lead to\ngeneralization of the conjecture.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 1 Apr 2007 16:37:36 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Csiszar",
                "I.",
                ""
            ],
            [
                "Hiai",
                "F.",
                ""
            ],
            [
                "Petz",
                "D.",
                ""
            ]
        ]
    },
    {
        "id": "0704.0047",
        "submitter": "Igor Grabec",
        "authors": "T. Kosel and I. Grabec",
        "title": "Intelligent location of simultaneously active acoustic emission sources:\n  Part I",
        "comments": "5 pages, 5 eps figures, uses IEEEtran.cls",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  The intelligent acoustic emission locator is described in Part I, while Part\nII discusses blind source separation, time delay estimation and location of two\nsimultaneously active continuous acoustic emission sources.\n  The location of acoustic emission on complicated aircraft frame structures is\na difficult problem of non-destructive testing. This article describes an\nintelligent acoustic emission source locator. The intelligent locator comprises\na sensor antenna and a general regression neural network, which solves the\nlocation problem based on learning from examples. Locator performance was\ntested on different test specimens. Tests have shown that the accuracy of\nlocation depends on sound velocity and attenuation in the specimen, the\ndimensions of the tested area, and the properties of stored data. The location\naccuracy achieved by the intelligent locator is comparable to that obtained by\nthe conventional triangulation method, while the applicability of the\nintelligent locator is more general since analysis of sonic ray paths is\navoided. This is a promising method for non-destructive testing of aircraft\nframe structures by the acoustic emission method.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 1 Apr 2007 13:06:50 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Kosel",
                "T.",
                ""
            ],
            [
                "Grabec",
                "I.",
                ""
            ]
        ]
    },
    {
        "id": "0704.0047",
        "submitter": "Igor Grabec",
        "authors": "T. Kosel and I. Grabec",
        "title": "Intelligent location of simultaneously active acoustic emission sources:\n  Part I",
        "comments": "5 pages, 5 eps figures, uses IEEEtran.cls",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  The intelligent acoustic emission locator is described in Part I, while Part\nII discusses blind source separation, time delay estimation and location of two\nsimultaneously active continuous acoustic emission sources.\n  The location of acoustic emission on complicated aircraft frame structures is\na difficult problem of non-destructive testing. This article describes an\nintelligent acoustic emission source locator. The intelligent locator comprises\na sensor antenna and a general regression neural network, which solves the\nlocation problem based on learning from examples. Locator performance was\ntested on different test specimens. Tests have shown that the accuracy of\nlocation depends on sound velocity and attenuation in the specimen, the\ndimensions of the tested area, and the properties of stored data. The location\naccuracy achieved by the intelligent locator is comparable to that obtained by\nthe conventional triangulation method, while the applicability of the\nintelligent locator is more general since analysis of sonic ray paths is\navoided. This is a promising method for non-destructive testing of aircraft\nframe structures by the acoustic emission method.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 1 Apr 2007 13:06:50 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Kosel",
                "T.",
                ""
            ],
            [
                "Grabec",
                "I.",
                ""
            ]
        ]
    },
    {
        "id": "0704.0050",
        "submitter": "Igor Grabec",
        "authors": "T. Kosel and I. Grabec",
        "title": "Intelligent location of simultaneously active acoustic emission sources:\n  Part II",
        "comments": "5 pages, 7 eps figures, uses IEEEtran.cls",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  Part I describes an intelligent acoustic emission locator, while Part II\ndiscusses blind source separation, time delay estimation and location of two\ncontinuous acoustic emission sources.\n  Acoustic emission (AE) analysis is used for characterization and location of\ndeveloping defects in materials. AE sources often generate a mixture of various\nstatistically independent signals. A difficult problem of AE analysis is\nseparation and characterization of signal components when the signals from\nvarious sources and the mode of mixing are unknown. Recently, blind source\nseparation (BSS) by independent component analysis (ICA) has been used to solve\nthese problems. The purpose of this paper is to demonstrate the applicability\nof ICA to locate two independent simultaneously active acoustic emission\nsources on an aluminum band specimen. The method is promising for\nnon-destructive testing of aircraft frame structures by acoustic emission\nanalysis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 1 Apr 2007 18:53:13 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Kosel",
                "T.",
                ""
            ],
            [
                "Grabec",
                "I.",
                ""
            ]
        ]
    },
    {
        "id": "0704.0050",
        "submitter": "Igor Grabec",
        "authors": "T. Kosel and I. Grabec",
        "title": "Intelligent location of simultaneously active acoustic emission sources:\n  Part II",
        "comments": "5 pages, 7 eps figures, uses IEEEtran.cls",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  Part I describes an intelligent acoustic emission locator, while Part II\ndiscusses blind source separation, time delay estimation and location of two\ncontinuous acoustic emission sources.\n  Acoustic emission (AE) analysis is used for characterization and location of\ndeveloping defects in materials. AE sources often generate a mixture of various\nstatistically independent signals. A difficult problem of AE analysis is\nseparation and characterization of signal components when the signals from\nvarious sources and the mode of mixing are unknown. Recently, blind source\nseparation (BSS) by independent component analysis (ICA) has been used to solve\nthese problems. The purpose of this paper is to demonstrate the applicability\nof ICA to locate two independent simultaneously active acoustic emission\nsources on an aluminum band specimen. The method is promising for\nnon-destructive testing of aircraft frame structures by acoustic emission\nanalysis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 1 Apr 2007 18:53:13 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Kosel",
                "T.",
                ""
            ],
            [
                "Grabec",
                "I.",
                ""
            ]
        ]
    },
    {
        "id": "0704.0090",
        "submitter": "Lester Ingber",
        "authors": "Lester Ingber",
        "title": "Real Options for Project Schedules (ROPS)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "Report 2007:ROPS",
        "categories": "cs.CE cond-mat.stat-mech cs.MS cs.NA physics.data-an",
        "license": null,
        "abstract": "  Real Options for Project Schedules (ROPS) has three recursive\nsampling/optimization shells. An outer Adaptive Simulated Annealing (ASA)\noptimization shell optimizes parameters of strategic Plans containing multiple\nProjects containing ordered Tasks. A middle shell samples probability\ndistributions of durations of Tasks. An inner shell samples probability\ndistributions of costs of Tasks. PATHTREE is used to develop options on\nschedules.. Algorithms used for Trading in Risk Dimensions (TRD) are applied to\ndevelop a relative risk analysis among projects.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 1 Apr 2007 14:35:40 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Ingber",
                "Lester",
                ""
            ]
        ]
    },
    {
        "id": "0704.0090",
        "submitter": "Lester Ingber",
        "authors": "Lester Ingber",
        "title": "Real Options for Project Schedules (ROPS)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "Report 2007:ROPS",
        "categories": "cs.CE cond-mat.stat-mech cs.MS cs.NA physics.data-an",
        "license": null,
        "abstract": "  Real Options for Project Schedules (ROPS) has three recursive\nsampling/optimization shells. An outer Adaptive Simulated Annealing (ASA)\noptimization shell optimizes parameters of strategic Plans containing multiple\nProjects containing ordered Tasks. A middle shell samples probability\ndistributions of durations of Tasks. An inner shell samples probability\ndistributions of costs of Tasks. PATHTREE is used to develop options on\nschedules.. Algorithms used for Trading in Risk Dimensions (TRD) are applied to\ndevelop a relative risk analysis among projects.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 1 Apr 2007 14:35:40 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Ingber",
                "Lester",
                ""
            ]
        ]
    },
    {
        "id": "0704.0098",
        "submitter": "Jack Raymond",
        "authors": "Jack Raymond, David Saad",
        "title": "Sparsely-spread CDMA - a statistical mechanics based analysis",
        "comments": "23 pages, 5 figures, figure 1 amended since published version",
        "journal-ref": "J. Phys. A: Math. Theor. 40 No 41 (12 October 2007) 12315-12333",
        "doi": "10.1088/1751-8113/40/41/004",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  Sparse Code Division Multiple Access (CDMA), a variation on the standard CDMA\nmethod in which the spreading (signature) matrix contains only a relatively\nsmall number of non-zero elements, is presented and analysed using methods of\nstatistical physics. The analysis provides results on the performance of\nmaximum likelihood decoding for sparse spreading codes in the large system\nlimit. We present results for both cases of regular and irregular spreading\nmatrices for the binary additive white Gaussian noise channel (BIAWGN) with a\ncomparison to the canonical (dense) random spreading code.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 1 Apr 2007 18:27:26 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 27 Apr 2007 12:39:14 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 1 Aug 2007 19:10:18 GMT"
            },
            {
                "version": "v4",
                "created": "Sun, 7 Oct 2007 16:50:39 GMT"
            },
            {
                "version": "v5",
                "created": "Wed, 30 Apr 2008 15:36:55 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Raymond",
                "Jack",
                ""
            ],
            [
                "Saad",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0704.0217",
        "submitter": "Wiroonsak Santipach",
        "authors": "Wiroonsak Santipach and Michael L. Honig",
        "title": "Capacity of a Multiple-Antenna Fading Channel with a Quantized Precoding\n  Matrix",
        "comments": null,
        "journal-ref": "IEEE Trans. Inf. Theory, vol. 55, no. 3, pp. 1218--1234, March\n  2009",
        "doi": "10.1109/TIT.2008.2011437",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a multiple-input multiple-output (MIMO) channel, feedback from the\nreceiver can be used to specify a transmit precoding matrix, which selectively\nactivates the strongest channel modes. Here we analyze the performance of\nRandom Vector Quantization (RVQ), in which the precoding matrix is selected\nfrom a random codebook containing independent, isotropically distributed\nentries. We assume that channel elements are i.i.d. and known to the receiver,\nwhich relays the optimal (rate-maximizing) precoder codebook index to the\ntransmitter using B bits. We first derive the large system capacity of\nbeamforming (rank-one precoding matrix) as a function of B, where large system\nrefers to the limit as B and the number of transmit and receive antennas all go\nto infinity with fixed ratios. With beamforming RVQ is asymptotically optimal,\ni.e., no other quantization scheme can achieve a larger asymptotic rate. The\nperformance of RVQ is also compared with that of a simpler reduced-rank scalar\nquantization scheme in which the beamformer is constrained to lie in a random\nsubspace. We subsequently consider a precoding matrix with arbitrary rank, and\napproximate the asymptotic RVQ performance with optimal and linear receivers\n(matched filter and Minimum Mean Squared Error (MMSE)). Numerical examples show\nthat these approximations accurately predict the performance of finite-size\nsystems of interest. Given a target spectral efficiency, numerical examples\nshow that the amount of feedback required by the linear MMSE receiver is only\nslightly more than that required by the optimal receiver, whereas the matched\nfilter can require significantly more feedback.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 2 Apr 2007 15:35:24 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 17 Feb 2009 03:49:20 GMT"
            }
        ],
        "update_date": "2010-08-27",
        "authors_parsed": [
            [
                "Santipach",
                "Wiroonsak",
                ""
            ],
            [
                "Honig",
                "Michael L.",
                ""
            ]
        ]
    },
    {
        "id": "0704.0282",
        "submitter": "Samuele Bandi",
        "authors": "Samuele Bandi, Luca Stabellini, Andrea Conti and Velio Tralli",
        "title": "On Punctured Pragmatic Space-Time Codes in Block Fading Channel",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.CC math.IT",
        "license": null,
        "abstract": "  This paper considers the use of punctured convolutional codes to obtain\npragmatic space-time trellis codes over block-fading channel. We show that good\nperformance can be achieved even when puncturation is adopted and that we can\nstill employ the same Viterbi decoder of the convolutional mother code by using\napproximated metrics without increasing the complexity of the decoding\noperations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 2 Apr 2007 22:44:17 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Bandi",
                "Samuele",
                ""
            ],
            [
                "Stabellini",
                "Luca",
                ""
            ],
            [
                "Conti",
                "Andrea",
                ""
            ],
            [
                "Tralli",
                "Velio",
                ""
            ]
        ]
    },
    {
        "id": "0704.0304",
        "submitter": "Carlos Gershenson",
        "authors": "Carlos Gershenson",
        "title": "The World as Evolving Information",
        "comments": "16 pages. Extended version, three more laws of information, two\n  classifications, and discussion added. To be published (soon) in\n  International Conference on Complex Systems 2007 Proceedings",
        "journal-ref": "Minai, A., Braha, D., and Bar-Yam, Y., eds. Unifying Themes in\n  Complex Systems VII, pp. 100-115. Springer, Berlin Heidelberg, 2012",
        "doi": "10.1007/978-3-642-18003-3_10",
        "report-no": null,
        "categories": "cs.IT cs.AI math.IT q-bio.PE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses the benefits of describing the world as information,\nespecially in the study of the evolution of life and cognition. Traditional\nstudies encounter problems because it is difficult to describe life and\ncognition in terms of matter and energy, since their laws are valid only at the\nphysical scale. However, if matter and energy, as well as life and cognition,\nare described in terms of information, evolution can be described consistently\nas information becoming more complex.\n  The paper presents eight tentative laws of information, valid at multiple\nscales, which are generalizations of Darwinian, cybernetic, thermodynamic,\npsychological, philosophical, and complexity principles. These are further used\nto discuss the notions of life, cognition and their evolution.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Apr 2007 02:08:48 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 30 Aug 2007 20:03:59 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 13 Oct 2010 19:49:16 GMT"
            }
        ],
        "update_date": "2013-04-05",
        "authors_parsed": [
            [
                "Gershenson",
                "Carlos",
                ""
            ]
        ]
    },
    {
        "id": "0704.0304",
        "submitter": "Carlos Gershenson",
        "authors": "Carlos Gershenson",
        "title": "The World as Evolving Information",
        "comments": "16 pages. Extended version, three more laws of information, two\n  classifications, and discussion added. To be published (soon) in\n  International Conference on Complex Systems 2007 Proceedings",
        "journal-ref": "Minai, A., Braha, D., and Bar-Yam, Y., eds. Unifying Themes in\n  Complex Systems VII, pp. 100-115. Springer, Berlin Heidelberg, 2012",
        "doi": "10.1007/978-3-642-18003-3_10",
        "report-no": null,
        "categories": "cs.IT cs.AI math.IT q-bio.PE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses the benefits of describing the world as information,\nespecially in the study of the evolution of life and cognition. Traditional\nstudies encounter problems because it is difficult to describe life and\ncognition in terms of matter and energy, since their laws are valid only at the\nphysical scale. However, if matter and energy, as well as life and cognition,\nare described in terms of information, evolution can be described consistently\nas information becoming more complex.\n  The paper presents eight tentative laws of information, valid at multiple\nscales, which are generalizations of Darwinian, cybernetic, thermodynamic,\npsychological, philosophical, and complexity principles. These are further used\nto discuss the notions of life, cognition and their evolution.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Apr 2007 02:08:48 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 30 Aug 2007 20:03:59 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 13 Oct 2010 19:49:16 GMT"
            }
        ],
        "update_date": "2013-04-05",
        "authors_parsed": [
            [
                "Gershenson",
                "Carlos",
                ""
            ]
        ]
    },
    {
        "id": "0704.0361",
        "submitter": "Ioannis Chatzigeorgiou",
        "authors": "Ioannis Chatzigeorgiou, Miguel R. D. Rodrigues, Ian J. Wassell and\n  Rolando Carrasco",
        "title": "Pseudo-random Puncturing: A Technique to Lower the Error Floor of Turbo\n  Codes",
        "comments": "5 pages, 1 figure, Proceedings of the 2007 IEEE International\n  Symposium on Information Theory, Nice, France, June 24-29, 2007",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2007.4557299",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  It has been observed that particular rate-1/2 partially systematic parallel\nconcatenated convolutional codes (PCCCs) can achieve a lower error floor than\nthat of their rate-1/3 parent codes. Nevertheless, good puncturing patterns can\nonly be identified by means of an exhaustive search, whilst convergence towards\nlow bit error probabilities can be problematic when the systematic output of a\nrate-1/2 partially systematic PCCC is heavily punctured. In this paper, we\npresent and study a family of rate-1/2 partially systematic PCCCs, which we\ncall pseudo-randomly punctured codes. We evaluate their bit error rate\nperformance and we show that they always yield a lower error floor than that of\ntheir rate-1/3 parent codes. Furthermore, we compare analytic results to\nsimulations and we demonstrate that their performance converges towards the\nerror floor region, owning to the moderate puncturing of their systematic\noutput. Consequently, we propose pseudo-random puncturing as a means of\nimproving the bandwidth efficiency of a PCCC and simultaneously lowering its\nerror floor.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Apr 2007 10:24:38 GMT"
            }
        ],
        "update_date": "2016-11-18",
        "authors_parsed": [
            [
                "Chatzigeorgiou",
                "Ioannis",
                ""
            ],
            [
                "Rodrigues",
                "Miguel R. D.",
                ""
            ],
            [
                "Wassell",
                "Ian J.",
                ""
            ],
            [
                "Carrasco",
                "Rolando",
                ""
            ]
        ]
    },
    {
        "id": "0704.0492",
        "submitter": "Shenghui Su",
        "authors": "Shenghui Su, and Shuwang Lu",
        "title": "Refuting the Pseudo Attack on the REESSE1+ Cryptosystem",
        "comments": "14 pages, and 2 table",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We illustrate through example 1 and 2 that the condition at theorem 1 in [8]\ndissatisfies necessity, and the converse proposition of fact 1.1 in [8] does\nnot hold, namely the condition Z/M - L/Ak < 1/(2 Ak^2) is not sufficient for\nf(i) + f(j) = f(k). Illuminate through an analysis and ex.3 that there is a\nlogic error during deduction of fact 1.2, which causes each of fact 1.2, 1.3, 4\nto be invalid. Demonstrate through ex.4 and 5 that each or the combination of\nqu+1 > qu * D at fact 4 and table 1 at fact 2.2 is not sufficient for f(i) +\nf(j) = f(k), property 1, 2, 3, 4, 5 each are invalid, and alg.1 based on fact 4\nand alg.2 based on table 1 are disordered and wrong logically. Further,\nmanifest through a repeated experiment and ex.5 that the data at table 2 is\nfalsified, and the example in [8] is woven elaborately. We explain why Cx = Ax\n* W^f(x) (% M) is changed to Cx = (Ax * W^f(x))^d (% M) in REESSE1+ v2.1. To\nthe signature fraud, we point out that [8] misunderstands the existence of T^-1\nand Q^-1 % (M-1), and forging of Q can be easily avoided through moving H.\nTherefore, the conclusion of [8] that REESSE1+ is not secure at all (which\nconnotes that [8] can extract a related private key from any public key in\nREESSE1+) is fully incorrect, and as long as the parameter Omega is fitly\nselected, REESSE1+ with Cx = Ax * W^f(x) (% M) is secure.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Apr 2007 04:31:00 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 4 Feb 2010 14:06:43 GMT"
            }
        ],
        "update_date": "2010-02-04",
        "authors_parsed": [
            [
                "Su",
                "Shenghui",
                ""
            ],
            [
                "Lu",
                "Shuwang",
                ""
            ]
        ]
    },
    {
        "id": "0704.0499",
        "submitter": "Lawrence Ong",
        "authors": "Lawrence Ong and Mehul Motani",
        "title": "Optimal Routing for Decode-and-Forward based Cooperation in Wireless\n  Networks",
        "comments": "Accepted and to be presented at Fourth Annual IEEE Communications\n  Society Conference on Sensor, Mesh, and Ad Hoc Communications and Networks\n  (SECON 2007), San Diego, California, June 18-21 2007",
        "journal-ref": "Proceedings of the 4th Annual IEEE Communications Society\n  Conference on Sensor, Mesh, and Ad Hoc Communications and Networks (SECON\n  2007), San Diego, CA, pp. 334-343, Jun. 18-21 2007.",
        "doi": "10.1109/SAHCN.2007.4292845",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  We investigate cooperative wireless relay networks in which the nodes can\nhelp each other in data transmission. We study different coding strategies in\nthe single-source single-destination network with many relay nodes. Given the\nmyriad of ways in which nodes can cooperate, there is a natural routing\nproblem, i.e., determining an ordered set of nodes to relay the data from the\nsource to the destination. We find that for a given route, the\ndecode-and-forward strategy, which is an information theoretic cooperative\ncoding strategy, achieves rates significantly higher than that achievable by\nthe usual multi-hop coding strategy, which is a point-to-point non-cooperative\ncoding strategy. We construct an algorithm to find an optimal route (in terms\nof rate maximizing) for the decode-and-forward strategy. Since the algorithm\nruns in factorial time in the worst case, we propose a heuristic algorithm that\nruns in polynomial time. The heuristic algorithm outputs an optimal route when\nthe nodes transmit independent codewords. We implement these coding strategies\nusing practical low density parity check codes to compare the performance of\nthe strategies on different routes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Apr 2007 07:22:56 GMT"
            }
        ],
        "update_date": "2010-04-15",
        "authors_parsed": [
            [
                "Ong",
                "Lawrence",
                ""
            ],
            [
                "Motani",
                "Mehul",
                ""
            ]
        ]
    },
    {
        "id": "0704.0528",
        "submitter": "Soung Liew",
        "authors": "Chi Pan Chan, Soung Chang Liew, An Chan",
        "title": "Many-to-One Throughput Capacity of IEEE 802.11 Multi-hop Wireless\n  Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.IT math.IT",
        "license": null,
        "abstract": "  This paper investigates the many-to-one throughput capacity (and by symmetry,\none-to-many throughput capacity) of IEEE 802.11 multi-hop networks. It has\ngenerally been assumed in prior studies that the many-to-one throughput\ncapacity is upper-bounded by the link capacity L. Throughput capacity L is not\nachievable under 802.11. This paper introduces the notion of \"canonical\nnetworks\", which is a class of regularly-structured networks whose capacities\ncan be analyzed more easily than unstructured networks. We show that the\nthroughput capacity of canonical networks under 802.11 has an analytical upper\nbound of 3L/4 when the source nodes are two or more hops away from the sink;\nand simulated throughputs of 0.690L (0.740L) when the source nodes are many\nhops away. We conjecture that 3L/4 is also the upper bound for general\nnetworks. When all links have equal length, 2L/3 can be shown to be the upper\nbound for general networks. Our simulations show that 802.11 networks with\nrandom topologies operated with AODV routing can only achieve throughputs far\nbelow the upper bounds. Fortunately, by properly selecting routes near the\ngateway (or by properly positioning the relay nodes leading to the gateway) to\nfashion after the structure of canonical networks, the throughput can be\nimproved significantly by more than 150%. Indeed, in a dense network, it is\nworthwhile to deactivate some of the relay nodes near the sink judiciously.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Apr 2007 09:17:38 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Chan",
                "Chi Pan",
                ""
            ],
            [
                "Liew",
                "Soung Chang",
                ""
            ],
            [
                "Chan",
                "An",
                ""
            ]
        ]
    },
    {
        "id": "0704.0528",
        "submitter": "Soung Liew",
        "authors": "Chi Pan Chan, Soung Chang Liew, An Chan",
        "title": "Many-to-One Throughput Capacity of IEEE 802.11 Multi-hop Wireless\n  Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.IT math.IT",
        "license": null,
        "abstract": "  This paper investigates the many-to-one throughput capacity (and by symmetry,\none-to-many throughput capacity) of IEEE 802.11 multi-hop networks. It has\ngenerally been assumed in prior studies that the many-to-one throughput\ncapacity is upper-bounded by the link capacity L. Throughput capacity L is not\nachievable under 802.11. This paper introduces the notion of \"canonical\nnetworks\", which is a class of regularly-structured networks whose capacities\ncan be analyzed more easily than unstructured networks. We show that the\nthroughput capacity of canonical networks under 802.11 has an analytical upper\nbound of 3L/4 when the source nodes are two or more hops away from the sink;\nand simulated throughputs of 0.690L (0.740L) when the source nodes are many\nhops away. We conjecture that 3L/4 is also the upper bound for general\nnetworks. When all links have equal length, 2L/3 can be shown to be the upper\nbound for general networks. Our simulations show that 802.11 networks with\nrandom topologies operated with AODV routing can only achieve throughputs far\nbelow the upper bounds. Fortunately, by properly selecting routes near the\ngateway (or by properly positioning the relay nodes leading to the gateway) to\nfashion after the structure of canonical networks, the throughput can be\nimproved significantly by more than 150%. Indeed, in a dense network, it is\nworthwhile to deactivate some of the relay nodes near the sink judiciously.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Apr 2007 09:17:38 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Chan",
                "Chi Pan",
                ""
            ],
            [
                "Liew",
                "Soung Chang",
                ""
            ],
            [
                "Chan",
                "An",
                ""
            ]
        ]
    },
    {
        "id": "0704.0540",
        "submitter": "Jinhua Jiang",
        "authors": "Jinhua Jiang and Xin Yan",
        "title": "On the Achievable Rate Regions for Interference Channels with Degraded\n  Message Sets",
        "comments": "22 pages, 8 figures, submitted to IEEE Trans. Inform. Theory",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  The interference channel with degraded message sets (IC-DMS) refers to a\ncommunication model in which two senders attempt to communicate with their\nrespective receivers simultaneously through a common medium, and one of the\nsenders has complete and a priori (non-causal) knowledge about the message\nbeing transmitted by the other. A coding scheme that collectively has\nadvantages of cooperative coding, collaborative coding, and dirty paper coding,\nis developed for such a channel. With resorting to this coding scheme,\nachievable rate regions of the IC-DMS in both discrete memoryless and Gaussian\ncases are derived, which, in general, include several previously known rate\nregions. Numerical examples for the Gaussian case demonstrate that in the\nhigh-interference-gain regime, the derived achievable rate regions offer\nconsiderable improvements over these existing results.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Apr 2007 10:28:16 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 5 Apr 2007 01:41:14 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Jiang",
                "Jinhua",
                ""
            ],
            [
                "Yan",
                "Xin",
                ""
            ]
        ]
    },
    {
        "id": "0704.0590",
        "submitter": "Rachit Agarwal",
        "authors": "Rachit Agarwal, Ralf Koetter and Emanuel Popovici",
        "title": "A Low Complexity Algorithm and Architecture for Systematic Encoding of\n  Hermitian Codes",
        "comments": "5 Pages, Accepted in IEEE International Symposium on Information\n  Theory ISIT 2007",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2007.4557408",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  We present an algorithm for systematic encoding of Hermitian codes. For a\nHermitian code defined over GF(q^2), the proposed algorithm achieves a run time\ncomplexity of O(q^2) and is suitable for VLSI implementation. The encoder\narchitecture uses as main blocks q varying-rate Reed-Solomon encoders and\nachieves a space complexity of O(q^2) in terms of finite field multipliers and\nmemory elements.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Apr 2007 15:06:14 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 5 Apr 2007 11:47:23 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Agarwal",
                "Rachit",
                ""
            ],
            [
                "Koetter",
                "Ralf",
                ""
            ],
            [
                "Popovici",
                "Emanuel",
                ""
            ]
        ]
    },
    {
        "id": "0704.0671",
        "submitter": "Maxim Raginsky",
        "authors": "Maxim Raginsky",
        "title": "Learning from compressed observations",
        "comments": "6 pages; submitted to the 2007 IEEE Information Theory Workshop (ITW\n  2007)",
        "journal-ref": null,
        "doi": "10.1109/ITW.2007.4313111",
        "report-no": null,
        "categories": "cs.IT cs.LG math.IT",
        "license": null,
        "abstract": "  The problem of statistical learning is to construct a predictor of a random\nvariable $Y$ as a function of a related random variable $X$ on the basis of an\ni.i.d. training sample from the joint distribution of $(X,Y)$. Allowable\npredictors are drawn from some specified class, and the goal is to approach\nasymptotically the performance (expected loss) of the best predictor in the\nclass. We consider the setting in which one has perfect observation of the\n$X$-part of the sample, while the $Y$-part has to be communicated at some\nfinite bit rate. The encoding of the $Y$-values is allowed to depend on the\n$X$-values. Under suitable regularity conditions on the admissible predictors,\nthe underlying family of probability distributions and the loss function, we\ngive an information-theoretic characterization of achievable predictor\nperformance in terms of conditional distortion-rate functions. The ideas are\nillustrated on the example of nonparametric regression in Gaussian noise.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Apr 2007 02:57:15 GMT"
            }
        ],
        "update_date": "2016-11-15",
        "authors_parsed": [
            [
                "Raginsky",
                "Maxim",
                ""
            ]
        ]
    },
    {
        "id": "0704.0671",
        "submitter": "Maxim Raginsky",
        "authors": "Maxim Raginsky",
        "title": "Learning from compressed observations",
        "comments": "6 pages; submitted to the 2007 IEEE Information Theory Workshop (ITW\n  2007)",
        "journal-ref": null,
        "doi": "10.1109/ITW.2007.4313111",
        "report-no": null,
        "categories": "cs.IT cs.LG math.IT",
        "license": null,
        "abstract": "  The problem of statistical learning is to construct a predictor of a random\nvariable $Y$ as a function of a related random variable $X$ on the basis of an\ni.i.d. training sample from the joint distribution of $(X,Y)$. Allowable\npredictors are drawn from some specified class, and the goal is to approach\nasymptotically the performance (expected loss) of the best predictor in the\nclass. We consider the setting in which one has perfect observation of the\n$X$-part of the sample, while the $Y$-part has to be communicated at some\nfinite bit rate. The encoding of the $Y$-values is allowed to depend on the\n$X$-values. Under suitable regularity conditions on the admissible predictors,\nthe underlying family of probability distributions and the loss function, we\ngive an information-theoretic characterization of achievable predictor\nperformance in terms of conditional distortion-rate functions. The ideas are\nillustrated on the example of nonparametric regression in Gaussian noise.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Apr 2007 02:57:15 GMT"
            }
        ],
        "update_date": "2016-11-15",
        "authors_parsed": [
            [
                "Raginsky",
                "Maxim",
                ""
            ]
        ]
    },
    {
        "id": "0704.0730",
        "submitter": "Hamed Haddadi MSc MIEE",
        "authors": "Hamed Haddadi, Raul Landa, Miguel Rio, Saleem Bhatti",
        "title": "Revisiting the Issues On Netflow Sample and Export Performance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.NI",
        "license": null,
        "abstract": "  The high volume of packets and packet rates of traffic on some router links\nmakes it exceedingly difficult for routers to examine every packet in order to\nkeep detailed statistics about the traffic which is traversing the router.\nSampling is commonly applied on routers in order to limit the load incurred by\nthe collection of information that the router has to undertake when evaluating\nflow information for monitoring purposes. The sampling process in nearly all\ncases is a deterministic process of choosing 1 in every N packets on a\nper-interface basis, and then forming the flow statistics based on the\ncollected sampled statistics. Even though this sampling may not be significant\nfor some statistics, such as packet rate, others can be severely distorted.\nHowever, it is important to consider the sampling techniques and their relative\naccuracy when applied to different traffic patterns. The main disadvantage of\nsampling is the loss of accuracy in the collected trace when compared to the\noriginal traffic stream. To date there has not been a detailed analysis of the\nimpact of sampling at a router in various traffic profiles and flow criteria.\nIn this paper, we assess the performance of the sampling process as used in\nNetFlow in detail, and we discuss some techniques for the compensation of loss\nof monitoring detail.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Apr 2007 14:47:25 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Haddadi",
                "Hamed",
                ""
            ],
            [
                "Landa",
                "Raul",
                ""
            ],
            [
                "Rio",
                "Miguel",
                ""
            ],
            [
                "Bhatti",
                "Saleem",
                ""
            ]
        ]
    },
    {
        "id": "0704.0730",
        "submitter": "Hamed Haddadi MSc MIEE",
        "authors": "Hamed Haddadi, Raul Landa, Miguel Rio, Saleem Bhatti",
        "title": "Revisiting the Issues On Netflow Sample and Export Performance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.NI",
        "license": null,
        "abstract": "  The high volume of packets and packet rates of traffic on some router links\nmakes it exceedingly difficult for routers to examine every packet in order to\nkeep detailed statistics about the traffic which is traversing the router.\nSampling is commonly applied on routers in order to limit the load incurred by\nthe collection of information that the router has to undertake when evaluating\nflow information for monitoring purposes. The sampling process in nearly all\ncases is a deterministic process of choosing 1 in every N packets on a\nper-interface basis, and then forming the flow statistics based on the\ncollected sampled statistics. Even though this sampling may not be significant\nfor some statistics, such as packet rate, others can be severely distorted.\nHowever, it is important to consider the sampling techniques and their relative\naccuracy when applied to different traffic patterns. The main disadvantage of\nsampling is the loss of accuracy in the collected trace when compared to the\noriginal traffic stream. To date there has not been a detailed analysis of the\nimpact of sampling at a router in various traffic profiles and flow criteria.\nIn this paper, we assess the performance of the sampling process as used in\nNetFlow in detail, and we discuss some techniques for the compensation of loss\nof monitoring detail.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Apr 2007 14:47:25 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Haddadi",
                "Hamed",
                ""
            ],
            [
                "Landa",
                "Raul",
                ""
            ],
            [
                "Rio",
                "Miguel",
                ""
            ],
            [
                "Bhatti",
                "Saleem",
                ""
            ]
        ]
    },
    {
        "id": "0704.0788",
        "submitter": "Kerry Soileau",
        "authors": "Kerry M. Soileau",
        "title": "Optimal Synthesis of Multiple Algorithms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.PF",
        "license": null,
        "abstract": "  In this paper we give a definition of \"algorithm,\" \"finite algorithm,\"\n\"equivalent algorithms,\" and what it means for a single algorithm to dominate a\nset of algorithms. We define a derived algorithm which may have a smaller mean\nexecution time than any of its component algorithms. We give an explicit\nexpression for the mean execution time (when it exists) of the derived\nalgorithm. We give several illustrative examples of derived algorithms with two\ncomponent algorithms. We include mean execution time solutions for\ntwo-algorithm processors whose joint density of execution times are of several\ngeneral forms. For the case in which the joint density for a two-algorithm\nprocessor is a step function, we give a maximum-likelihood estimation scheme\nwith which to analyze empirical processing time data.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Apr 2007 19:47:54 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Soileau",
                "Kerry M.",
                ""
            ]
        ]
    },
    {
        "id": "0704.0802",
        "submitter": "Caleb Lo",
        "authors": "Caleb K. Lo, Robert W. Heath, Jr. and Sriram Vishwanath",
        "title": "Hybrid-ARQ in Multihop Networks with Opportunistic Relay Selection",
        "comments": "4 pages, 5 figures, to appear in Proceedings of the 2007\n  International Conference on Acoustics, Speech, and Signal Processing in\n  Honolulu, HI",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  This paper develops a contention-based opportunistic feedback technique\ntowards relay selection in a dense wireless network. This technique enables the\nforwarding of additional parity information from the selected relay to the\ndestination. For a given network, the effects of varying key parameters such as\nthe feedback probability are presented and discussed. A primary advantage of\nthe proposed technique is that relay selection can be performed in a\ndistributed way. Simulation results find its performance to closely match that\nof centralized schemes that utilize GPS information, unlike the proposed\nmethod. The proposed relay selection method is also found to achieve throughput\ngains over a point-to-point transmission strategy.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Apr 2007 20:37:44 GMT"
            }
        ],
        "update_date": "2012-08-27",
        "authors_parsed": [
            [
                "Lo",
                "Caleb K.",
                ""
            ],
            [
                "Heath,",
                "Robert W.",
                "Jr."
            ],
            [
                "Vishwanath",
                "Sriram",
                ""
            ]
        ]
    },
    {
        "id": "0704.0805",
        "submitter": "Caleb Lo",
        "authors": "Caleb K. Lo, Robert W. Heath, Jr. and Sriram Vishwanath",
        "title": "Opportunistic Relay Selection with Limited Feedback",
        "comments": "5 pages, 6 figures, to appear in Proceedings of 2007 IEEE Vehicular\n  Technology Conference-Spring in Dublin, Ireland",
        "journal-ref": null,
        "doi": "10.1109/VETECS.2007.40",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  It has been shown that a decentralized relay selection protocol based on\nopportunistic feedback from the relays yields good throughput performance in\ndense wireless networks. This selection strategy supports a hybrid-ARQ\ntransmission approach where relays forward parity information to the\ndestination in the event of a decoding error. Such an approach, however,\nsuffers a loss compared to centralized strategies that select relays with the\nbest channel gain to the destination. This paper closes the performance gap by\nadding another level of channel feedback to the decentralized relay selection\nproblem. It is demonstrated that only one additional bit of feedback is\nnecessary for good throughput performance. The performance impact of varying\nkey parameters such as the number of relays and the channel feedback threshold\nis discussed. An accompanying bit error rate analysis demonstrates the\nimportance of relay selection.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Apr 2007 20:52:26 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Lo",
                "Caleb K.",
                ""
            ],
            [
                "Heath,",
                "Robert W.",
                "Jr."
            ],
            [
                "Vishwanath",
                "Sriram",
                ""
            ]
        ]
    },
    {
        "id": "0704.0831",
        "submitter": "Brooke Shrader",
        "authors": "Brooke Shrader and Anthony Ephremides",
        "title": "On packet lengths and overhead for random linear coding over the erasure\n  channel",
        "comments": "5 pages, 5 figures, submitted to the 2007 International Wireless\n  Communications and Mobile Computing Conference",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  We assess the practicality of random network coding by illuminating the issue\nof overhead and considering it in conjunction with increasingly long packets\nsent over the erasure channel. We show that the transmission of increasingly\nlong packets, consisting of either of an increasing number of symbols per\npacket or an increasing symbol alphabet size, results in a data rate\napproaching zero over the erasure channel. This result is due to an erasure\nprobability that increases with packet length. Numerical results for a\nparticular modulation scheme demonstrate a data rate of approximately zero for\na large, but finite-length packet. Our results suggest a reduction in the\nperformance gains offered by random network coding.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Apr 2007 02:25:40 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Shrader",
                "Brooke",
                ""
            ],
            [
                "Ephremides",
                "Anthony",
                ""
            ]
        ]
    },
    {
        "id": "0704.0838",
        "submitter": "Gil Shamir",
        "authors": "Gil I. Shamir",
        "title": "Universal Source Coding for Monotonic and Fast Decaying Monotonic\n  Distributions",
        "comments": "Submitted to IEEE Transactions on Information Theory",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  We study universal compression of sequences generated by monotonic\ndistributions. We show that for a monotonic distribution over an alphabet of\nsize $k$, each probability parameter costs essentially $0.5 \\log (n/k^3)$ bits,\nwhere $n$ is the coded sequence length, as long as $k = o(n^{1/3})$. Otherwise,\nfor $k = O(n)$, the total average sequence redundancy is $O(n^{1/3+\\epsilon})$\nbits overall. We then show that there exists a sub-class of monotonic\ndistributions over infinite alphabets for which redundancy of\n$O(n^{1/3+\\epsilon})$ bits overall is still achievable. This class contains\nfast decaying distributions, including many distributions over the integers and\ngeometric distributions. For some slower decays, including other distributions\nover the integers, redundancy of $o(n)$ bits overall is achievable, where a\nmethod to compute specific redundancy rates for such distributions is derived.\nThe results are specifically true for finite entropy monotonic distributions.\nFinally, we study individual sequence redundancy behavior assuming a sequence\nis governed by a monotonic distribution. We show that for sequences whose\nempirical distributions are monotonic, individual redundancy bounds similar to\nthose in the average case can be obtained. However, even if the monotonicity in\nthe empirical distribution is violated, diminishing per symbol individual\nsequence redundancies with respect to the monotonic maximum likelihood\ndescription length may still be achievable.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Apr 2007 03:12:02 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Shamir",
                "Gil I.",
                ""
            ]
        ]
    },
    {
        "id": "0704.0858",
        "submitter": "Mohamed Kaaniche",
        "authors": "Eric Alata (LAAS), Vincent Nicomette (LAAS), Mohamed Ka\\^aniche\n  (LAAS), Marc Dacier (LAAS), Matthieu Herrb (LAAS)",
        "title": "Lessons Learned from the deployment of a high-interaction honeypot",
        "comments": null,
        "journal-ref": "Proc. 6th European Dependable Computing Conference (EDCC-6),\n  Coimbra (Portugal), 18-20 octobre 2006 (18/10/2006) 39-44",
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  This paper presents an experimental study and the lessons learned from the\nobservation of the attackers when logged on a compromised machine. The results\nare based on a six months period during which a controlled experiment has been\nrun with a high interaction honeypot. We correlate our findings with those\nobtained with a worldwide distributed system of lowinteraction honeypots.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Apr 2007 08:22:50 GMT"
            }
        ],
        "update_date": "2016-08-14",
        "authors_parsed": [
            [
                "Alata",
                "Eric",
                "",
                "LAAS"
            ],
            [
                "Nicomette",
                "Vincent",
                "",
                "LAAS"
            ],
            [
                "Ka\u00e2niche",
                "Mohamed",
                "",
                "LAAS"
            ],
            [
                "Dacier",
                "Marc",
                "",
                "LAAS"
            ],
            [
                "Herrb",
                "Matthieu",
                "",
                "LAAS"
            ]
        ]
    },
    {
        "id": "0704.0860",
        "submitter": "Mohamed Kaaniche",
        "authors": "Cristina Simache (LAAS), Mohamed Kaaniche (LAAS)",
        "title": "Availability assessment of SunOS/Solaris Unix Systems based on Syslogd\n  and wtmpx logfiles : a case study",
        "comments": null,
        "journal-ref": "Proc. 2005 IEEE Pacific Rim International Symposium on Dependable\n  Computing (PRDC'2005), Changsha, Hunan (Chine), 12-14 D{\\'e}cembre 2005\n  (18/12/2005) 49-56",
        "doi": null,
        "report-no": null,
        "categories": "cs.PF",
        "license": null,
        "abstract": "  This paper presents a measurement-based availability assessment study using\nfield data collected during a 4-year period from 373 SunOS/Solaris Unix\nworkstations and servers interconnected through a local area network. We focus\non the estimation of machine uptimes, downtimes and availability based on the\nidentification of failures that caused total service loss. Data corresponds to\nsyslogd event logs that contain a large amount of information about the normal\nactivity of the studied systems as well as their behavior in the presence of\nfailures. It is widely recognized that the information contained in such event\nlogs might be incomplete or imperfect. The solution investigated in this paper\nto address this problem is based on the use of auxiliary sources of data\nobtained from wtmpx files maintained by the SunOS/Solaris Unix operating\nsystem. The results obtained suggest that the combined use of wtmpx and syslogd\nlog files provides more complete information on the state of the target systems\nthat is useful to provide availability estimations that better reflect reality.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Apr 2007 08:24:47 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Simache",
                "Cristina",
                "",
                "LAAS"
            ],
            [
                "Kaaniche",
                "Mohamed",
                "",
                "LAAS"
            ]
        ]
    },
    {
        "id": "0704.0861",
        "submitter": "Mohamed Kaaniche",
        "authors": "Mohamed Kaaniche (LAAS), Y. Deswarte (LAAS), Eric Alata (LAAS), Marc\n  Dacier (SC), Vincent Nicomette (LAAS)",
        "title": "Empirical analysis and statistical modeling of attack processes based on\n  honeypots",
        "comments": null,
        "journal-ref": "IEEE/IFIP International Conference on Dependable Systems and\n  Networks (DSN-2006) (25/06/2006) 119-124",
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.CR",
        "license": null,
        "abstract": "  Honeypots are more and more used to collect data on malicious activities on\nthe Internet and to better understand the strategies and techniques used by\nattackers to compromise target systems. Analysis and modeling methodologies are\nneeded to support the characterization of attack processes based on the data\ncollected from the honeypots. This paper presents some empirical analyses based\non the data collected from the Leurr{\\'e}.com honeypot platforms deployed on\nthe Internet and presents some preliminary modeling studies aimed at fulfilling\nsuch objectives.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Apr 2007 08:50:34 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Kaaniche",
                "Mohamed",
                "",
                "LAAS"
            ],
            [
                "Deswarte",
                "Y.",
                "",
                "LAAS"
            ],
            [
                "Alata",
                "Eric",
                "",
                "LAAS"
            ],
            [
                "Dacier",
                "Marc",
                "",
                "SC"
            ],
            [
                "Nicomette",
                "Vincent",
                "",
                "LAAS"
            ]
        ]
    },
    {
        "id": "0704.0861",
        "submitter": "Mohamed Kaaniche",
        "authors": "Mohamed Kaaniche (LAAS), Y. Deswarte (LAAS), Eric Alata (LAAS), Marc\n  Dacier (SC), Vincent Nicomette (LAAS)",
        "title": "Empirical analysis and statistical modeling of attack processes based on\n  honeypots",
        "comments": null,
        "journal-ref": "IEEE/IFIP International Conference on Dependable Systems and\n  Networks (DSN-2006) (25/06/2006) 119-124",
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.CR",
        "license": null,
        "abstract": "  Honeypots are more and more used to collect data on malicious activities on\nthe Internet and to better understand the strategies and techniques used by\nattackers to compromise target systems. Analysis and modeling methodologies are\nneeded to support the characterization of attack processes based on the data\ncollected from the honeypots. This paper presents some empirical analyses based\non the data collected from the Leurr{\\'e}.com honeypot platforms deployed on\nthe Internet and presents some preliminary modeling studies aimed at fulfilling\nsuch objectives.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Apr 2007 08:50:34 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Kaaniche",
                "Mohamed",
                "",
                "LAAS"
            ],
            [
                "Deswarte",
                "Y.",
                "",
                "LAAS"
            ],
            [
                "Alata",
                "Eric",
                "",
                "LAAS"
            ],
            [
                "Dacier",
                "Marc",
                "",
                "SC"
            ],
            [
                "Nicomette",
                "Vincent",
                "",
                "LAAS"
            ]
        ]
    },
    {
        "id": "0704.0865",
        "submitter": "Mohamed Kaaniche",
        "authors": "Ana-Elena Rugina (LAAS), Karama Kanoun (LAAS), Mohamed Kaaniche (LAAS)",
        "title": "An architecture-based dependability modeling framework using AADL",
        "comments": null,
        "journal-ref": "Proc. 10th IASTED International Conference on Software Engineering\n  and Applications (SEA'2006), Dallas (USA), 13-15 November2006 (13/11/2006)\n  222-227",
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.SE",
        "license": null,
        "abstract": "  For efficiency reasons, the software system designers' will is to use an\nintegrated set of methods and tools to describe specifications and designs, and\nalso to perform analyses such as dependability, schedulability and performance.\nAADL (Architecture Analysis and Design Language) has proved to be efficient for\nsoftware architecture modeling. In addition, AADL was designed to accommodate\nseveral types of analyses. This paper presents an iterative dependency-driven\napproach for dependability modeling using AADL. It is illustrated on a small\nexample. This approach is part of a complete framework that allows the\ngeneration of dependability analysis and evaluation models from AADL models to\nsupport the analysis of software and system architectures, in critical\napplication domains.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Apr 2007 09:33:06 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Rugina",
                "Ana-Elena",
                "",
                "LAAS"
            ],
            [
                "Kanoun",
                "Karama",
                "",
                "LAAS"
            ],
            [
                "Kaaniche",
                "Mohamed",
                "",
                "LAAS"
            ]
        ]
    },
    {
        "id": "0704.0865",
        "submitter": "Mohamed Kaaniche",
        "authors": "Ana-Elena Rugina (LAAS), Karama Kanoun (LAAS), Mohamed Kaaniche (LAAS)",
        "title": "An architecture-based dependability modeling framework using AADL",
        "comments": null,
        "journal-ref": "Proc. 10th IASTED International Conference on Software Engineering\n  and Applications (SEA'2006), Dallas (USA), 13-15 November2006 (13/11/2006)\n  222-227",
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.SE",
        "license": null,
        "abstract": "  For efficiency reasons, the software system designers' will is to use an\nintegrated set of methods and tools to describe specifications and designs, and\nalso to perform analyses such as dependability, schedulability and performance.\nAADL (Architecture Analysis and Design Language) has proved to be efficient for\nsoftware architecture modeling. In addition, AADL was designed to accommodate\nseveral types of analyses. This paper presents an iterative dependency-driven\napproach for dependability modeling using AADL. It is illustrated on a small\nexample. This approach is part of a complete framework that allows the\ngeneration of dependability analysis and evaluation models from AADL models to\nsupport the analysis of software and system architectures, in critical\napplication domains.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Apr 2007 09:33:06 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Rugina",
                "Ana-Elena",
                "",
                "LAAS"
            ],
            [
                "Kanoun",
                "Karama",
                "",
                "LAAS"
            ],
            [
                "Kaaniche",
                "Mohamed",
                "",
                "LAAS"
            ]
        ]
    },
    {
        "id": "0704.0879",
        "submitter": "Mohamed Kaaniche",
        "authors": "Mohamed Kaaniche (LAAS), Luigi Romano (UIUC), Zbigniew Kalbarczyk\n  (UIUC), Ravishankar Iyer (UIUC), Rick Karcich (STORAGETEK)",
        "title": "A Hierarchical Approach for Dependability Analysis of a Commercial\n  Cache-Based RAID Storage Architecture",
        "comments": null,
        "journal-ref": "Proc. 28th IEEE International Symposium on Fault-Tolerant\n  Computing (FTCS-28), Munich (Germany), IEEE Computer Society, June 1998,\n  pp.6-15 (1998) 6-15",
        "doi": null,
        "report-no": null,
        "categories": "cs.PF",
        "license": null,
        "abstract": "  We present a hierarchical simulation approach for the dependability analysis\nand evaluation of a highly available commercial cache-based RAID storage\nsystem. The archi-tecture is complex and includes several layers of\noverlap-ping error detection and recovery mechanisms. Three ab-straction levels\nhave been developed to model the cache architecture, cache operations, and\nerror detection and recovery mechanism. The impact of faults and errors\noc-curring in the cache and in the disks is analyzed at each level of the\nhierarchy. A simulation submodel is associated with each abstraction level. The\nmodels have been devel-oped using DEPEND, a simulation-based environment for\nsystem-level dependability analysis, which provides facili-ties to inject\nfaults into a functional behavior model, to simulate error detection and\nrecovery mechanisms, and to evaluate quantitative measures. Several fault\nmodels are defined for each submodel to simulate cache component failures, disk\nfailures, transmission errors, and data errors in the cache memory and in the\ndisks. Some of the parame-ters characterizing fault injection in a given\nsubmodel cor-respond to probabilities evaluated from the simulation of the\nlower-level submodel. Based on the proposed method-ology, we evaluate and\nanalyze 1) the system behavior un-der a real workload and high error rate\n(focusing on error bursts), 2) the coverage of the error detection mechanisms\nimplemented in the system and the error latency distribu-tions, and 3) the\naccumulation of errors in the cache and in the disks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Apr 2007 11:46:49 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Kaaniche",
                "Mohamed",
                "",
                "LAAS"
            ],
            [
                "Romano",
                "Luigi",
                "",
                "UIUC"
            ],
            [
                "Kalbarczyk",
                "Zbigniew",
                "",
                "UIUC"
            ],
            [
                "Iyer",
                "Ravishankar",
                "",
                "UIUC"
            ],
            [
                "Karcich",
                "Rick",
                "",
                "STORAGETEK"
            ]
        ]
    },
    {
        "id": "0704.0954",
        "submitter": "Jos\\'e M. F. Moura",
        "authors": "Soummya Kar and Jose M. F. Moura",
        "title": "Sensor Networks with Random Links: Topology Design for Distributed\n  Consensus",
        "comments": "Submitted to IEEE Transactions",
        "journal-ref": null,
        "doi": "10.1109/TSP.2008.920143",
        "report-no": null,
        "categories": "cs.IT cs.LG math.IT",
        "license": null,
        "abstract": "  In a sensor network, in practice, the communication among sensors is subject\nto:(1) errors or failures at random times; (3) costs; and(2) constraints since\nsensors and networks operate under scarce resources, such as power, data rate,\nor communication. The signal-to-noise ratio (SNR) is usually a main factor in\ndetermining the probability of error (or of communication failure) in a link.\nThese probabilities are then a proxy for the SNR under which the links operate.\nThe paper studies the problem of designing the topology, i.e., assigning the\nprobabilities of reliable communication among sensors (or of link failures) to\nmaximize the rate of convergence of average consensus, when the link\ncommunication costs are taken into account, and there is an overall\ncommunication budget constraint. To consider this problem, we address a number\nof preliminary issues: (1) model the network as a random topology; (2)\nestablish necessary and sufficient conditions for mean square sense (mss) and\nalmost sure (a.s.) convergence of average consensus when network links fail;\nand, in particular, (3) show that a necessary and sufficient condition for both\nmss and a.s. convergence is for the algebraic connectivity of the mean graph\ndescribing the network topology to be strictly positive. With these results, we\nformulate topology design, subject to random link failures and to a\ncommunication cost constraint, as a constrained convex optimization problem to\nwhich we apply semidefinite programming techniques. We show by an extensive\nnumerical study that the optimal design improves significantly the convergence\nspeed of the consensus algorithm and can achieve the asymptotic performance of\na non-random network at a fraction of the communication cost.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Apr 2007 21:58:52 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Kar",
                "Soummya",
                ""
            ],
            [
                "Moura",
                "Jose M. F.",
                ""
            ]
        ]
    },
    {
        "id": "0704.0954",
        "submitter": "Jos\\'e M. F. Moura",
        "authors": "Soummya Kar and Jose M. F. Moura",
        "title": "Sensor Networks with Random Links: Topology Design for Distributed\n  Consensus",
        "comments": "Submitted to IEEE Transactions",
        "journal-ref": null,
        "doi": "10.1109/TSP.2008.920143",
        "report-no": null,
        "categories": "cs.IT cs.LG math.IT",
        "license": null,
        "abstract": "  In a sensor network, in practice, the communication among sensors is subject\nto:(1) errors or failures at random times; (3) costs; and(2) constraints since\nsensors and networks operate under scarce resources, such as power, data rate,\nor communication. The signal-to-noise ratio (SNR) is usually a main factor in\ndetermining the probability of error (or of communication failure) in a link.\nThese probabilities are then a proxy for the SNR under which the links operate.\nThe paper studies the problem of designing the topology, i.e., assigning the\nprobabilities of reliable communication among sensors (or of link failures) to\nmaximize the rate of convergence of average consensus, when the link\ncommunication costs are taken into account, and there is an overall\ncommunication budget constraint. To consider this problem, we address a number\nof preliminary issues: (1) model the network as a random topology; (2)\nestablish necessary and sufficient conditions for mean square sense (mss) and\nalmost sure (a.s.) convergence of average consensus when network links fail;\nand, in particular, (3) show that a necessary and sufficient condition for both\nmss and a.s. convergence is for the algebraic connectivity of the mean graph\ndescribing the network topology to be strictly positive. With these results, we\nformulate topology design, subject to random link failures and to a\ncommunication cost constraint, as a constrained convex optimization problem to\nwhich we apply semidefinite programming techniques. We show by an extensive\nnumerical study that the optimal design improves significantly the convergence\nspeed of the consensus algorithm and can achieve the asymptotic performance of\na non-random network at a fraction of the communication cost.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Apr 2007 21:58:52 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Kar",
                "Soummya",
                ""
            ],
            [
                "Moura",
                "Jose M. F.",
                ""
            ]
        ]
    },
    {
        "id": "0704.0967",
        "submitter": "Jia Liu",
        "authors": "Jia Liu and Y. Thomas Hou",
        "title": "Cross-Layer Optimization of MIMO-Based Mesh Networks with Gaussian\n  Vector Broadcast Channels",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.AR math.IT",
        "license": null,
        "abstract": "  MIMO technology is one of the most significant advances in the past decade to\nincrease channel capacity and has a great potential to improve network capacity\nfor mesh networks. In a MIMO-based mesh network, the links outgoing from each\nnode sharing the common communication spectrum can be modeled as a Gaussian\nvector broadcast channel. Recently, researchers showed that ``dirty paper\ncoding'' (DPC) is the optimal transmission strategy for Gaussian vector\nbroadcast channels. So far, there has been little study on how this fundamental\nresult will impact the cross-layer design for MIMO-based mesh networks. To fill\nthis gap, we consider the problem of jointly optimizing DPC power allocation in\nthe link layer at each node and multihop/multipath routing in a MIMO-based mesh\nnetworks. It turns out that this optimization problem is a very challenging\nnon-convex problem. To address this difficulty, we transform the original\nproblem to an equivalent problem by exploiting the channel duality. For the\ntransformed problem, we develop an efficient solution procedure that integrates\nLagrangian dual decomposition method, conjugate gradient projection method\nbased on matrix differential calculus, cutting-plane method, and subgradient\nmethod. In our numerical example, it is shown that we can achieve a network\nperformance gain of 34.4% by using DPC.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 7 Apr 2007 03:18:46 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Liu",
                "Jia",
                ""
            ],
            [
                "Hou",
                "Y. Thomas",
                ""
            ]
        ]
    },
    {
        "id": "0704.0985",
        "submitter": "Mohd Abubakr",
        "authors": "Mohd Abubakr, R.M.Vinay",
        "title": "Architecture for Pseudo Acausal Evolvable Embedded Systems",
        "comments": "4 pages, 2 figures. Submitted to SASO 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  Advances in semiconductor technology are contributing to the increasing\ncomplexity in the design of embedded systems. Architectures with novel\ntechniques such as evolvable nature and autonomous behavior have engrossed lot\nof attention. This paper demonstrates conceptually evolvable embedded systems\ncan be characterized basing on acausal nature. It is noted that in acausal\nsystems, future input needs to be known, here we make a mechanism such that the\nsystem predicts the future inputs and exhibits pseudo acausal nature. An\nembedded system that uses theoretical framework of acausality is proposed. Our\nmethod aims at a novel architecture that features the hardware evolability and\nautonomous behavior alongside pseudo acausality. Various aspects of this\narchitecture are discussed in detail along with the limitations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 7 Apr 2007 13:40:49 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Abubakr",
                "Mohd",
                ""
            ],
            [
                "Vinay",
                "R. M.",
                ""
            ]
        ]
    },
    {
        "id": "0704.0985",
        "submitter": "Mohd Abubakr",
        "authors": "Mohd Abubakr, R.M.Vinay",
        "title": "Architecture for Pseudo Acausal Evolvable Embedded Systems",
        "comments": "4 pages, 2 figures. Submitted to SASO 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  Advances in semiconductor technology are contributing to the increasing\ncomplexity in the design of embedded systems. Architectures with novel\ntechniques such as evolvable nature and autonomous behavior have engrossed lot\nof attention. This paper demonstrates conceptually evolvable embedded systems\ncan be characterized basing on acausal nature. It is noted that in acausal\nsystems, future input needs to be known, here we make a mechanism such that the\nsystem predicts the future inputs and exhibits pseudo acausal nature. An\nembedded system that uses theoretical framework of acausality is proposed. Our\nmethod aims at a novel architecture that features the hardware evolability and\nautonomous behavior alongside pseudo acausality. Various aspects of this\narchitecture are discussed in detail along with the limitations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 7 Apr 2007 13:40:49 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Abubakr",
                "Mohd",
                ""
            ],
            [
                "Vinay",
                "R. M.",
                ""
            ]
        ]
    },
    {
        "id": "0704.1020",
        "submitter": "Gyorgy Ottucsak",
        "authors": "Andras Gyorgy, Tamas Linder, Gabor Lugosi, Gyorgy Ottucsak",
        "title": "The on-line shortest path problem under partial monitoring",
        "comments": "35 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.SC",
        "license": null,
        "abstract": "  The on-line shortest path problem is considered under various models of\npartial monitoring. Given a weighted directed acyclic graph whose edge weights\ncan change in an arbitrary (adversarial) way, a decision maker has to choose in\neach round of a game a path between two distinguished vertices such that the\nloss of the chosen path (defined as the sum of the weights of its composing\nedges) be as small as possible. In a setting generalizing the multi-armed\nbandit problem, after choosing a path, the decision maker learns only the\nweights of those edges that belong to the chosen path. For this problem, an\nalgorithm is given whose average cumulative loss in n rounds exceeds that of\nthe best path, matched off-line to the entire sequence of the edge weights, by\na quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on\nthe number of edges of the graph. The algorithm can be implemented with linear\ncomplexity in the number of rounds n and in the number of edges. An extension\nto the so-called label efficient setting is also given, in which the decision\nmaker is informed about the weights of the edges corresponding to the chosen\npath at a total of m << n time instances. Another extension is shown where the\ndecision maker competes against a time-varying path, a generalization of the\nproblem of tracking the best expert. A version of the multi-armed bandit\nsetting for shortest path is also discussed where the decision maker learns\nonly the total weight of the chosen path but not the weights of the individual\nedges on the path. Applications to routing in packet switched networks along\nwith simulation results are also presented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 8 Apr 2007 10:15:54 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Gyorgy",
                "Andras",
                ""
            ],
            [
                "Linder",
                "Tamas",
                ""
            ],
            [
                "Lugosi",
                "Gabor",
                ""
            ],
            [
                "Ottucsak",
                "Gyorgy",
                ""
            ]
        ]
    },
    {
        "id": "0704.1020",
        "submitter": "Gyorgy Ottucsak",
        "authors": "Andras Gyorgy, Tamas Linder, Gabor Lugosi, Gyorgy Ottucsak",
        "title": "The on-line shortest path problem under partial monitoring",
        "comments": "35 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.SC",
        "license": null,
        "abstract": "  The on-line shortest path problem is considered under various models of\npartial monitoring. Given a weighted directed acyclic graph whose edge weights\ncan change in an arbitrary (adversarial) way, a decision maker has to choose in\neach round of a game a path between two distinguished vertices such that the\nloss of the chosen path (defined as the sum of the weights of its composing\nedges) be as small as possible. In a setting generalizing the multi-armed\nbandit problem, after choosing a path, the decision maker learns only the\nweights of those edges that belong to the chosen path. For this problem, an\nalgorithm is given whose average cumulative loss in n rounds exceeds that of\nthe best path, matched off-line to the entire sequence of the edge weights, by\na quantity that is proportional to 1/\\sqrt{n} and depends only polynomially on\nthe number of edges of the graph. The algorithm can be implemented with linear\ncomplexity in the number of rounds n and in the number of edges. An extension\nto the so-called label efficient setting is also given, in which the decision\nmaker is informed about the weights of the edges corresponding to the chosen\npath at a total of m << n time instances. Another extension is shown where the\ndecision maker competes against a time-varying path, a generalization of the\nproblem of tracking the best expert. A version of the multi-armed bandit\nsetting for shortest path is also discussed where the decision maker learns\nonly the total weight of the chosen path but not the weights of the individual\nedges on the path. Applications to routing in packet switched networks along\nwith simulation results are also presented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 8 Apr 2007 10:15:54 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Gyorgy",
                "Andras",
                ""
            ],
            [
                "Linder",
                "Tamas",
                ""
            ],
            [
                "Lugosi",
                "Gabor",
                ""
            ],
            [
                "Ottucsak",
                "Gyorgy",
                ""
            ]
        ]
    },
    {
        "id": "0704.1028",
        "submitter": "Jianlin Cheng",
        "authors": "Jianlin Cheng",
        "title": "A neural network approach to ordinal regression",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI cs.NE",
        "license": null,
        "abstract": "  Ordinal regression is an important type of learning, which has properties of\nboth classification and regression. Here we describe a simple and effective\napproach to adapt a traditional neural network to learn ordinal categories. Our\napproach is a generalization of the perceptron method for ordinal regression.\nOn several benchmark datasets, our method (NNRank) outperforms a neural network\nclassification method. Compared with the ordinal regression methods using\nGaussian processes and support vector machines, NNRank achieves comparable\nperformance. Moreover, NNRank has the advantages of traditional neural\nnetworks: learning in both online and batch modes, handling very large training\ndatasets, and making rapid predictions. These features make NNRank a useful and\ncomplementary tool for large-scale data processing tasks such as information\nretrieval, web page ranking, collaborative filtering, and protein ranking in\nBioinformatics.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 8 Apr 2007 17:36:00 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Cheng",
                "Jianlin",
                ""
            ]
        ]
    },
    {
        "id": "0704.1028",
        "submitter": "Jianlin Cheng",
        "authors": "Jianlin Cheng",
        "title": "A neural network approach to ordinal regression",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI cs.NE",
        "license": null,
        "abstract": "  Ordinal regression is an important type of learning, which has properties of\nboth classification and regression. Here we describe a simple and effective\napproach to adapt a traditional neural network to learn ordinal categories. Our\napproach is a generalization of the perceptron method for ordinal regression.\nOn several benchmark datasets, our method (NNRank) outperforms a neural network\nclassification method. Compared with the ordinal regression methods using\nGaussian processes and support vector machines, NNRank achieves comparable\nperformance. Moreover, NNRank has the advantages of traditional neural\nnetworks: learning in both online and batch modes, handling very large training\ndatasets, and making rapid predictions. These features make NNRank a useful and\ncomplementary tool for large-scale data processing tasks such as information\nretrieval, web page ranking, collaborative filtering, and protein ranking in\nBioinformatics.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 8 Apr 2007 17:36:00 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Cheng",
                "Jianlin",
                ""
            ]
        ]
    },
    {
        "id": "0704.1028",
        "submitter": "Jianlin Cheng",
        "authors": "Jianlin Cheng",
        "title": "A neural network approach to ordinal regression",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI cs.NE",
        "license": null,
        "abstract": "  Ordinal regression is an important type of learning, which has properties of\nboth classification and regression. Here we describe a simple and effective\napproach to adapt a traditional neural network to learn ordinal categories. Our\napproach is a generalization of the perceptron method for ordinal regression.\nOn several benchmark datasets, our method (NNRank) outperforms a neural network\nclassification method. Compared with the ordinal regression methods using\nGaussian processes and support vector machines, NNRank achieves comparable\nperformance. Moreover, NNRank has the advantages of traditional neural\nnetworks: learning in both online and batch modes, handling very large training\ndatasets, and making rapid predictions. These features make NNRank a useful and\ncomplementary tool for large-scale data processing tasks such as information\nretrieval, web page ranking, collaborative filtering, and protein ranking in\nBioinformatics.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 8 Apr 2007 17:36:00 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Cheng",
                "Jianlin",
                ""
            ]
        ]
    },
    {
        "id": "0704.1043",
        "submitter": "Hector Zenil",
        "authors": "Jean-Paul Delahaye and Hector Zenil",
        "title": "On the Kolmogorov-Chaitin Complexity for short sequences",
        "comments": "21 pages. Paper webpage: http://www.mathrix.org/experimentalAIT/",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CC cs.IT math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A drawback of Kolmogorov-Chaitin complexity (K) as a function from s to the\nshortest program producing s is its noncomputability which limits its range of\napplicability. Moreover, when strings are short, the dependence of K on a\nparticular universal Turing machine U can be arbitrary. In practice one can\napproximate it by computable compression methods. However, such compression\nmethods do not always provide meaningful approximations--for strings shorter,\nfor example, than typical compiler lengths. In this paper we suggest an\nempirical approach to overcome this difficulty and to obtain a stable\ndefinition of the Kolmogorov-Chaitin complexity for short sequences.\nAdditionally, a correlation in terms of distribution frequencies was found\nacross the output of two models of abstract machines, namely unidimensional\ncellular automata and deterministic Turing machine.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 8 Apr 2007 20:01:47 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 19 Apr 2007 13:00:55 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 30 May 2007 10:16:36 GMT"
            },
            {
                "version": "v4",
                "created": "Tue, 1 Jun 2010 20:03:20 GMT"
            },
            {
                "version": "v5",
                "created": "Fri, 17 Dec 2010 01:36:26 GMT"
            }
        ],
        "update_date": "2010-12-20",
        "authors_parsed": [
            [
                "Delahaye",
                "Jean-Paul",
                ""
            ],
            [
                "Zenil",
                "Hector",
                ""
            ]
        ]
    },
    {
        "id": "0704.1068",
        "submitter": "Leo Liberti",
        "authors": "Giacomo Nannicini, Philippe Baptiste, Gilles Barbier, Daniel Krob, Leo\n  Liberti",
        "title": "Fast paths in large-scale dynamic road networks",
        "comments": "12 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.DS",
        "license": null,
        "abstract": "  Efficiently computing fast paths in large scale dynamic road networks (where\ndynamic traffic information is known over a part of the network) is a practical\nproblem faced by several traffic information service providers who wish to\noffer a realistic fast path computation to GPS terminal enabled vehicles. The\nheuristic solution method we propose is based on a highway hierarchy-based\nshortest path algorithm for static large-scale networks; we maintain a static\nhighway hierarchy and perform each query on the dynamically evaluated network.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 9 Apr 2007 07:04:19 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 27 Jun 2007 18:17:35 GMT"
            }
        ],
        "update_date": "2007-06-27",
        "authors_parsed": [
            [
                "Nannicini",
                "Giacomo",
                ""
            ],
            [
                "Baptiste",
                "Philippe",
                ""
            ],
            [
                "Barbier",
                "Gilles",
                ""
            ],
            [
                "Krob",
                "Daniel",
                ""
            ],
            [
                "Liberti",
                "Leo",
                ""
            ]
        ]
    },
    {
        "id": "0704.1070",
        "submitter": "Hua Fu",
        "authors": "Hua Fu and Pooi Yuen Kam",
        "title": "Differential Diversity Reception of MDPSK over Independent Rayleigh\n  Channels with Nonidentical Branch Statistics and Asymmetric Fading Spectrum",
        "comments": "5 pages, 3 figures, to present at ISIT2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.PF math.IT",
        "license": null,
        "abstract": "  This paper is concerned with optimum diversity receiver structure and its\nperformance analysis of differential phase shift keying (DPSK) with\ndifferential detection over nonselective, independent, nonidentically\ndistributed, Rayleigh fading channels. The fading process in each branch is\nassumed to have an arbitrary Doppler spectrum with arbitrary Doppler bandwidth,\nbut to have distinct, asymmetric fading power spectral density characteristic.\nUsing 8-DPSK as an example, the average bit error probability (BEP) of the\noptimum diversity receiver is obtained by calculating the BEP for each of the\nthree individual bits. The BEP results derived are given in exact, explicit,\nclosed-form expressions which show clearly the behavior of the performance as a\nfunction of various system parameters.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 9 Apr 2007 07:16:39 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Fu",
                "Hua",
                ""
            ],
            [
                "Kam",
                "Pooi Yuen",
                ""
            ]
        ]
    },
    {
        "id": "0704.1070",
        "submitter": "Hua Fu",
        "authors": "Hua Fu and Pooi Yuen Kam",
        "title": "Differential Diversity Reception of MDPSK over Independent Rayleigh\n  Channels with Nonidentical Branch Statistics and Asymmetric Fading Spectrum",
        "comments": "5 pages, 3 figures, to present at ISIT2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.PF math.IT",
        "license": null,
        "abstract": "  This paper is concerned with optimum diversity receiver structure and its\nperformance analysis of differential phase shift keying (DPSK) with\ndifferential detection over nonselective, independent, nonidentically\ndistributed, Rayleigh fading channels. The fading process in each branch is\nassumed to have an arbitrary Doppler spectrum with arbitrary Doppler bandwidth,\nbut to have distinct, asymmetric fading power spectral density characteristic.\nUsing 8-DPSK as an example, the average bit error probability (BEP) of the\noptimum diversity receiver is obtained by calculating the BEP for each of the\nthree individual bits. The BEP results derived are given in exact, explicit,\nclosed-form expressions which show clearly the behavior of the performance as a\nfunction of various system parameters.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 9 Apr 2007 07:16:39 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Fu",
                "Hua",
                ""
            ],
            [
                "Kam",
                "Pooi Yuen",
                ""
            ]
        ]
    },
    {
        "id": "0704.1196",
        "submitter": "Shengchao Ding",
        "authors": "Qing Yang and Shengchao Ding",
        "title": "Novel algorithm to calculate hypervolume indicator of Pareto\n  approximation set",
        "comments": "9 pages, 2 figures. Comments are welcome",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CG cs.NE",
        "license": null,
        "abstract": "  Hypervolume indicator is a commonly accepted quality measure for comparing\nPareto approximation set generated by multi-objective optimizers. The best\nknown algorithm to calculate it for $n$ points in $d$-dimensional space has a\nrun time of $O(n^{d/2})$ with special data structures. This paper presents a\nrecursive, vertex-splitting algorithm for calculating the hypervolume indicator\nof a set of $n$ non-comparable points in $d>2$ dimensions. It splits out\nmultiple child hyper-cuboids which can not be dominated by a splitting\nreference point. In special, the splitting reference point is carefully chosen\nto minimize the number of points in the child hyper-cuboids. The complexity\nanalysis shows that the proposed algorithm achieves $O((\\frac{d}{2})^n)$ time\nand $O(dn^2)$ space complexity in the worst case.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Apr 2007 07:21:02 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Yang",
                "Qing",
                ""
            ],
            [
                "Ding",
                "Shengchao",
                ""
            ]
        ]
    },
    {
        "id": "0704.1198",
        "submitter": "Minkyu Kim",
        "authors": "Minkyu Kim, Varun Aggarwal, Una-May O'Reilly, Muriel Medard",
        "title": "A Doubly Distributed Genetic Algorithm for Network Coding",
        "comments": "8 pages, 7 figures, accepted to the Genetic and Evolutionary\n  Computation Conference (GECCO 2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.NI",
        "license": null,
        "abstract": "  We present a genetic algorithm which is distributed in two novel ways: along\ngenotype and temporal axes. Our algorithm first distributes, for every member\nof the population, a subset of the genotype to each network node, rather than a\nsubset of the population to each. This genotype distribution is shown to offer\na significant gain in running time. Then, for efficient use of the\ncomputational resources in the network, our algorithm divides the candidate\nsolutions into pipelined sets and thus the distribution is in the temporal\ndomain, rather that in the spatial domain. This temporal distribution may lead\nto temporal inconsistency in selection and replacement, however our experiments\nyield better efficiency in terms of the time to convergence without incurring\nsignificant penalties.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Apr 2007 13:36:44 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Kim",
                "Minkyu",
                ""
            ],
            [
                "Aggarwal",
                "Varun",
                ""
            ],
            [
                "O'Reilly",
                "Una-May",
                ""
            ],
            [
                "Medard",
                "Muriel",
                ""
            ]
        ]
    },
    {
        "id": "0704.1198",
        "submitter": "Minkyu Kim",
        "authors": "Minkyu Kim, Varun Aggarwal, Una-May O'Reilly, Muriel Medard",
        "title": "A Doubly Distributed Genetic Algorithm for Network Coding",
        "comments": "8 pages, 7 figures, accepted to the Genetic and Evolutionary\n  Computation Conference (GECCO 2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.NI",
        "license": null,
        "abstract": "  We present a genetic algorithm which is distributed in two novel ways: along\ngenotype and temporal axes. Our algorithm first distributes, for every member\nof the population, a subset of the genotype to each network node, rather than a\nsubset of the population to each. This genotype distribution is shown to offer\na significant gain in running time. Then, for efficient use of the\ncomputational resources in the network, our algorithm divides the candidate\nsolutions into pipelined sets and thus the distribution is in the temporal\ndomain, rather that in the spatial domain. This temporal distribution may lead\nto temporal inconsistency in selection and replacement, however our experiments\nyield better efficiency in terms of the time to convergence without incurring\nsignificant penalties.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Apr 2007 13:36:44 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Kim",
                "Minkyu",
                ""
            ],
            [
                "Aggarwal",
                "Varun",
                ""
            ],
            [
                "O'Reilly",
                "Una-May",
                ""
            ],
            [
                "Medard",
                "Muriel",
                ""
            ]
        ]
    },
    {
        "id": "0704.1267",
        "submitter": "Laurence Likforman",
        "authors": "Laurence Likforman-Sulem, Abderrazak Zahour, Bruno Taconet",
        "title": "Text Line Segmentation of Historical Documents: a Survey",
        "comments": "25 pages, submitted version, To appear in International Journal on\n  Document Analysis and Recognition, On line version available at\n  http://www.springerlink.com/content/k2813176280456k3/",
        "journal-ref": "Vol. 9, no 2-4, April 2007, pp. 123-138",
        "doi": "10.1007/s10032-006-0023-z",
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  There is a huge amount of historical documents in libraries and in various\nNational Archives that have not been exploited electronically. Although\nautomatic reading of complete pages remains, in most cases, a long-term\nobjective, tasks such as word spotting, text/image alignment, authentication\nand extraction of specific fields are in use today. For all these tasks, a\nmajor step is document segmentation into text lines. Because of the low quality\nand the complexity of these documents (background noise, artifacts due to\naging, interfering lines),automatic text line segmentation remains an open\nresearch field. The objective of this paper is to present a survey of existing\nmethods, developed during the last decade, and dedicated to documents of\nhistorical interest.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Apr 2007 16:26:42 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Likforman-Sulem",
                "Laurence",
                ""
            ],
            [
                "Zahour",
                "Abderrazak",
                ""
            ],
            [
                "Taconet",
                "Bruno",
                ""
            ]
        ]
    },
    {
        "id": "0704.1274",
        "submitter": "Dev Rajnarayan",
        "authors": "David H. Wolpert and Dev G. Rajnarayan",
        "title": "Parametric Learning and Monte Carlo Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  This paper uncovers and explores the close relationship between Monte Carlo\nOptimization of a parametrized integral (MCO), Parametric machine-Learning\n(PL), and `blackbox' or `oracle'-based optimization (BO). We make four\ncontributions. First, we prove that MCO is mathematically identical to a broad\nclass of PL problems. This identity potentially provides a new application\ndomain for all broadly applicable PL techniques: MCO. Second, we introduce\nimmediate sampling, a new version of the Probability Collectives (PC) algorithm\nfor blackbox optimization. Immediate sampling transforms the original BO\nproblem into an MCO problem. Accordingly, by combining these first two\ncontributions, we can apply all PL techniques to BO. In our third contribution\nwe validate this way of improving BO by demonstrating that cross-validation and\nbagging improve immediate sampling. Finally, conventional MC and MCO procedures\nignore the relationship between the sample point locations and the associated\nvalues of the integrand; only the values of the integrand at those locations\nare considered. We demonstrate that one can exploit the sample location\ninformation using PL techniques, for example by forming a fit of the sample\nlocations to the associated values of the integrand. This provides an\nadditional way to apply PL techniques to improve MCO.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Apr 2007 17:01:07 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Wolpert",
                "David H.",
                ""
            ],
            [
                "Rajnarayan",
                "Dev G.",
                ""
            ]
        ]
    },
    {
        "id": "0704.1294",
        "submitter": "Ahmed Sidky Ahmed Sidky",
        "authors": "Ahmed Sidky, James Arthur, Shawn Bohner",
        "title": "A Disciplined Approach to Adopting Agile Practices: The Agile Adoption\n  Framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  Many organizations aspire to adopt agile processes to take advantage of the\nnumerous benefits that it offers to an organization. Those benefits include,\nbut are not limited to, quicker return on investment, better software quality,\nand higher customer satisfaction. To date however, there is no structured\nprocess (at least in the public domain) that guides organizations in adopting\nagile practices. To address this problem we present the Agile Adoption\nFramework. The framework consists of two components: an agile measurement\nindex, and a 4-Stage process, that together guide and assist the agile adoption\nefforts of organizations. More specifically, the agile measurement index is\nused to identify the agile potential of projects and organizations. The 4-Stage\nprocess, on the other hand, helps determine (a) whether or not organizations\nare ready for agile adoption, and (b) guided by their potential, what set of\nagile practices can and should be introduced.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Apr 2007 19:11:51 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Sidky",
                "Ahmed",
                ""
            ],
            [
                "Arthur",
                "James",
                ""
            ],
            [
                "Bohner",
                "Shawn",
                ""
            ]
        ]
    },
    {
        "id": "0704.1308",
        "submitter": "Nihar Jindal",
        "authors": "Nihar Jindal",
        "title": "Antenna Combining for the MIMO Downlink Channel",
        "comments": "Submitted to IEEE Trans. Wireless Communications April 2007. Revised\n  August 2007",
        "journal-ref": null,
        "doi": "10.1109/T-WC.2008.070383",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  A multiple antenna downlink channel where limited channel feedback is\navailable to the transmitter is considered. In a vector downlink channel\n(single antenna at each receiver), the transmit antenna array can be used to\ntransmit separate data streams to multiple receivers only if the transmitter\nhas very accurate channel knowledge, i.e., if there is high-rate channel\nfeedback from each receiver. In this work it is shown that channel feedback\nrequirements can be significantly reduced if each receiver has a small number\nof antennas and appropriately combines its antenna outputs. A combining method\nthat minimizes channel quantization error at each receiver, and thereby\nminimizes multi-user interference, is proposed and analyzed. This technique is\nshown to outperform traditional techniques such as maximum-ratio combining\nbecause minimization of interference power is more critical than maximization\nof signal power in the multiple antenna downlink. Analysis is provided to\nquantify the feedback savings, and the technique is seen to work well with user\nselection and is also robust to receiver estimation error.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Apr 2007 20:56:14 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 19 Aug 2007 17:06:50 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Jindal",
                "Nihar",
                ""
            ]
        ]
    },
    {
        "id": "0704.1317",
        "submitter": "Naftali Sommer",
        "authors": "Naftali Sommer, Meir Feder and Ofir Shalvi",
        "title": "Low Density Lattice Codes",
        "comments": "24 pages, 4 figures. Submitted for publication in IEEE transactions\n  on Information Theory",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  Low density lattice codes (LDLC) are novel lattice codes that can be decoded\nefficiently and approach the capacity of the additive white Gaussian noise\n(AWGN) channel. In LDLC a codeword x is generated directly at the n-dimensional\nEuclidean space as a linear transformation of a corresponding integer message\nvector b, i.e., x = Gb, where H, the inverse of G, is restricted to be sparse.\nThe fact that H is sparse is utilized to develop a linear-time iterative\ndecoding scheme which attains, as demonstrated by simulations, good error\nperformance within ~0.5dB from capacity at block length of n = 100,000 symbols.\nThe paper also discusses convergence results and implementation considerations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Apr 2007 16:07:34 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Sommer",
                "Naftali",
                ""
            ],
            [
                "Feder",
                "Meir",
                ""
            ],
            [
                "Shalvi",
                "Ofir",
                ""
            ]
        ]
    },
    {
        "id": "0704.1353",
        "submitter": "Paul Prekop",
        "authors": "Paul Prekop",
        "title": "Supporting Knowledge and Expertise Finding within Australia's Defence\n  Science and Technology Organisation",
        "comments": "40th Hawaii International Conference on System Sciences (HICSS40)\n  2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OH cs.DB cs.DL cs.HC",
        "license": null,
        "abstract": "  This paper reports on work aimed at supporting knowledge and expertise\nfinding within a large Research and Development (R&D) organisation. The paper\nfirst discusses the nature of knowledge important to R&D organisations and\npresents a prototype information system developed to support knowledge and\nexpertise finding. The paper then discusses a trial of the system within an R&D\norganisation, the implications and limitations of the trial, and discusses\nfuture research questions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Apr 2007 06:49:06 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Prekop",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0704.1353",
        "submitter": "Paul Prekop",
        "authors": "Paul Prekop",
        "title": "Supporting Knowledge and Expertise Finding within Australia's Defence\n  Science and Technology Organisation",
        "comments": "40th Hawaii International Conference on System Sciences (HICSS40)\n  2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OH cs.DB cs.DL cs.HC",
        "license": null,
        "abstract": "  This paper reports on work aimed at supporting knowledge and expertise\nfinding within a large Research and Development (R&D) organisation. The paper\nfirst discusses the nature of knowledge important to R&D organisations and\npresents a prototype information system developed to support knowledge and\nexpertise finding. The paper then discusses a trial of the system within an R&D\norganisation, the implications and limitations of the trial, and discusses\nfuture research questions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Apr 2007 06:49:06 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Prekop",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0704.1358",
        "submitter": "Torleiv Kl{\\o}ve",
        "authors": "Jyh-Shyan Lin, Jen-Chun Chang, Rong-Jaye Chen, Torleiv Kl{\\o}ve",
        "title": "Distance preserving mappings from ternary vectors to permutations",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DM cs.IT math.IT",
        "license": null,
        "abstract": "  Distance-preserving mappings (DPMs) are mappings from the set of all q-ary\nvectors of a fixed length to the set of permutations of the same or longer\nlength such that every two distinct vectors are mapped to permutations with the\nsame or even larger Hamming distance than that of the vectors. In this paper,\nwe propose a construction of DPMs from ternary vectors. The constructed DPMs\nimprove the lower bounds on the maximal size of permutation arrays.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Apr 2007 07:20:34 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Lin",
                "Jyh-Shyan",
                ""
            ],
            [
                "Chang",
                "Jen-Chun",
                ""
            ],
            [
                "Chen",
                "Rong-Jaye",
                ""
            ],
            [
                "Kl\u00f8ve",
                "Torleiv",
                ""
            ]
        ]
    },
    {
        "id": "0704.1373",
        "submitter": "Burgy Laurent",
        "authors": "Burgy Laurent (INRIA Futurs), Laurent R\\'eveill\\`ere (INRIA Futurs),\n  Julia Lawall (DIKU), Gilles Muller (INRIA Rennes)",
        "title": "A Language-Based Approach for Improving the Robustness of Network\n  Application Protocol Implementations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  The secure and robust functioning of a network relies on the defect-free\nimplementation of network applications. As network protocols have become\nincreasingly complex, however, hand-writing network message processing code has\nbecome increasingly error-prone. In this paper, we present a domain-specific\nlanguage, Zebu, for describing protocol message formats and related processing\nconstraints. From a Zebu specification, a compiler automatically generates\nstubs to be used by an application to parse network messages. Zebu is easy to\nuse, as it builds on notations used in RFCs to describe protocol grammars. Zebu\nis also efficient, as the memory usage is tailored to application needs and\nmessage fragments can be specified to be processed on demand. Finally,\nZebu-based applications are robust, as the Zebu compiler automatically checks\nspecification consistency and generates parsing stubs that include validation\nof the message structure. Using a mutation analysis in the context of SIP and\nRTSP, we show that Zebu significantly improves application robustness.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Apr 2007 08:35:32 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Laurent",
                "Burgy",
                "",
                "INRIA Futurs"
            ],
            [
                "R\u00e9veill\u00e8re",
                "Laurent",
                "",
                "INRIA Futurs"
            ],
            [
                "Lawall",
                "Julia",
                "",
                "DIKU"
            ],
            [
                "Muller",
                "Gilles",
                "",
                "INRIA Rennes"
            ]
        ]
    },
    {
        "id": "0704.1394",
        "submitter": "Tarik Had\\v{z}i\\'c",
        "authors": "Tarik Hadzic, Rune Moller Jensen, Henrik Reif Andersen",
        "title": "Calculating Valid Domains for BDD-Based Interactive Configuration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  In these notes we formally describe the functionality of Calculating Valid\nDomains from the BDD representing the solution space of valid configurations.\nThe formalization is largely based on the CLab configuration framework.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Apr 2007 10:59:56 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Hadzic",
                "Tarik",
                ""
            ],
            [
                "Jensen",
                "Rune Moller",
                ""
            ],
            [
                "Andersen",
                "Henrik Reif",
                ""
            ]
        ]
    },
    {
        "id": "0704.1409",
        "submitter": "Yao Hengshuai",
        "authors": "Yao HengShuai",
        "title": "Preconditioned Temporal Difference Learning",
        "comments": "This paper has been withdrawn by the author. Look at the ICML version\n  instead: http://icml2008.cs.helsinki.fi/papers/111.pdf",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI",
        "license": null,
        "abstract": "  This paper has been withdrawn by the author. This draft is withdrawn for its\npoor quality in english, unfortunately produced by the author when he was just\nstarting his science route. Look at the ICML version instead:\nhttp://icml2008.cs.helsinki.fi/papers/111.pdf\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Apr 2007 13:17:01 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 12 Apr 2007 03:33:26 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 8 Jun 2012 14:08:19 GMT"
            }
        ],
        "update_date": "2012-06-11",
        "authors_parsed": [
            [
                "HengShuai",
                "Yao",
                ""
            ]
        ]
    },
    {
        "id": "0704.1409",
        "submitter": "Yao Hengshuai",
        "authors": "Yao HengShuai",
        "title": "Preconditioned Temporal Difference Learning",
        "comments": "This paper has been withdrawn by the author. Look at the ICML version\n  instead: http://icml2008.cs.helsinki.fi/papers/111.pdf",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI",
        "license": null,
        "abstract": "  This paper has been withdrawn by the author. This draft is withdrawn for its\npoor quality in english, unfortunately produced by the author when he was just\nstarting his science route. Look at the ICML version instead:\nhttp://icml2008.cs.helsinki.fi/papers/111.pdf\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Apr 2007 13:17:01 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 12 Apr 2007 03:33:26 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 8 Jun 2012 14:08:19 GMT"
            }
        ],
        "update_date": "2012-06-11",
        "authors_parsed": [
            [
                "HengShuai",
                "Yao",
                ""
            ]
        ]
    },
    {
        "id": "0704.1411",
        "submitter": "Lorenzo Cappellari",
        "authors": "Lorenzo Cappellari",
        "title": "Trellis-Coded Quantization Based on Maximum-Hamming-Distance Binary\n  Codes",
        "comments": "4 pages, 4 graphic files (2 figures using subfigures), LaTeX; minor\n  revisions; submitted to IEEE Trans. Commun",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  Most design approaches for trellis-coded quantization take advantage of the\nduality of trellis-coded quantization with trellis-coded modulation, and use\nthe same empirically-found convolutional codes to label the trellis branches.\nThis letter presents an alternative approach that instead takes advantage of\nmaximum-Hamming-distance convolutional codes. The proposed source codes are\nshown to be competitive with the best in the literature for the same\ncomputational complexity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Apr 2007 13:21:39 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 4 Oct 2007 12:33:03 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 20 Dec 2007 16:31:47 GMT"
            }
        ],
        "update_date": "2007-12-20",
        "authors_parsed": [
            [
                "Cappellari",
                "Lorenzo",
                ""
            ]
        ]
    },
    {
        "id": "0704.1455",
        "submitter": "Aaron Wagner",
        "authors": "Aaron B. Wagner, Pramod Viswanath, and Sanjeev R. Kulkarni",
        "title": "A Better Good-Turing Estimator for Sequence Probabilities",
        "comments": "ISIT 2007, to appear",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  We consider the problem of estimating the probability of an observed string\ndrawn i.i.d. from an unknown distribution. The key feature of our study is that\nthe length of the observed string is assumed to be of the same order as the\nsize of the underlying alphabet. In this setting, many letters are unseen and\nthe empirical distribution tends to overestimate the probability of the\nobserved letters. To overcome this problem, the traditional approach to\nprobability estimation is to use the classical Good-Turing estimator. We\nintroduce a natural scaling model and use it to show that the Good-Turing\nsequence probability estimator is not consistent. We then introduce a novel\nsequence probability estimator that is indeed consistent under the natural\nscaling model.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Apr 2007 17:02:33 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 21 Apr 2007 21:23:56 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Wagner",
                "Aaron B.",
                ""
            ],
            [
                "Viswanath",
                "Pramod",
                ""
            ],
            [
                "Kulkarni",
                "Sanjeev R.",
                ""
            ]
        ]
    },
    {
        "id": "0704.1524",
        "submitter": "Daniel Ryan",
        "authors": "Daniel J. Ryan, Iain B. Collings and I. Vaughan L. Clarkson",
        "title": "GLRT-Optimal Noncoherent Lattice Decoding",
        "comments": "30 pages, 6 figures Accepted to IEEE Transactions on Signal\n  Processing",
        "journal-ref": null,
        "doi": "10.1109/TSP.2007.894237",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  This paper presents new low-complexity lattice-decoding algorithms for\nnoncoherent block detection of QAM and PAM signals over complex-valued fading\nchannels. The algorithms are optimal in terms of the generalized likelihood\nratio test (GLRT). The computational complexity is polynomial in the block\nlength; making GLRT-optimal noncoherent detection feasible for implementation.\nWe also provide even lower complexity suboptimal algorithms. Simulations show\nthat the suboptimal algorithms have performance indistinguishable from the\noptimal algorithms. Finally, we consider block based transmission, and propose\nto use noncoherent detection as an alternative to pilot assisted transmission\n(PAT). The new technique is shown to outperform PAT.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 12 Apr 2007 04:30:35 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Ryan",
                "Daniel J.",
                ""
            ],
            [
                "Collings",
                "Iain B.",
                ""
            ],
            [
                "Clarkson",
                "I. Vaughan L.",
                ""
            ]
        ]
    },
    {
        "id": "0704.1675",
        "submitter": "Kristina Lerman",
        "authors": "Anon Plangprasopchok and Kristina Lerman",
        "title": "Exploiting Social Annotation for Automatic Resource Discovery",
        "comments": "6 pages, submitted to AAAI07 workshop on Information Integration on\n  the Web",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.CY cs.DL",
        "license": null,
        "abstract": "  Information integration applications, such as mediators or mashups, that\nrequire access to information resources currently rely on users manually\ndiscovering and integrating them in the application. Manual resource discovery\nis a slow process, requiring the user to sift through results obtained via\nkeyword-based search. Although search methods have advanced to include evidence\nfrom document contents, its metadata and the contents and link structure of the\nreferring pages, they still do not adequately cover information sources --\noften called ``the hidden Web''-- that dynamically generate documents in\nresponse to a query. The recently popular social bookmarking sites, which allow\nusers to annotate and share metadata about various information sources, provide\nrich evidence for resource discovery. In this paper, we describe a\nprobabilistic model of the user annotation process in a social bookmarking\nsystem del.icio.us. We then use the model to automatically find resources\nrelevant to a particular information domain. Our experimental results on data\nobtained from \\emph{del.icio.us} show this approach as a promising method for\nhelping automate the resource discovery task.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 12 Apr 2007 23:24:19 GMT"
            }
        ],
        "update_date": "2016-09-08",
        "authors_parsed": [
            [
                "Plangprasopchok",
                "Anon",
                ""
            ],
            [
                "Lerman",
                "Kristina",
                ""
            ]
        ]
    },
    {
        "id": "0704.1676",
        "submitter": "Kristina Lerman",
        "authors": "Kristina Lerman, Anon Plangprasopchok and Chio Wong",
        "title": "Personalizing Image Search Results on Flickr",
        "comments": "12 pages, submitted to AAAI07 workshop on Intelligent Information\n  Personalization",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IR cs.AI cs.CY cs.DL cs.HC",
        "license": null,
        "abstract": "  The social media site Flickr allows users to upload their photos, annotate\nthem with tags, submit them to groups, and also to form social networks by\nadding other users as contacts. Flickr offers multiple ways of browsing or\nsearching it. One option is tag search, which returns all images tagged with a\nspecific keyword. If the keyword is ambiguous, e.g., ``beetle'' could mean an\ninsect or a car, tag search results will include many images that are not\nrelevant to the sense the user had in mind when executing the query. We claim\nthat users express their photography interests through the metadata they add in\nthe form of contacts and image annotations. We show how to exploit this\nmetadata to personalize search results for the user, thereby improving search\nperformance. First, we show that we can significantly improve search precision\nby filtering tag search results by user's contacts or a larger social network\nthat includes those contact's contacts. Secondly, we describe a probabilistic\nmodel that takes advantage of tag information to discover latent topics\ncontained in the search results. The users' interests can similarly be\ndescribed by the tags they used for annotating their images. The latent topics\nfound by the model are then used to personalize search results by finding\nimages on topics that are of interest to the user.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 12 Apr 2007 23:31:04 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Lerman",
                "Kristina",
                ""
            ],
            [
                "Plangprasopchok",
                "Anon",
                ""
            ],
            [
                "Wong",
                "Chio",
                ""
            ]
        ]
    },
    {
        "id": "0704.1676",
        "submitter": "Kristina Lerman",
        "authors": "Kristina Lerman, Anon Plangprasopchok and Chio Wong",
        "title": "Personalizing Image Search Results on Flickr",
        "comments": "12 pages, submitted to AAAI07 workshop on Intelligent Information\n  Personalization",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IR cs.AI cs.CY cs.DL cs.HC",
        "license": null,
        "abstract": "  The social media site Flickr allows users to upload their photos, annotate\nthem with tags, submit them to groups, and also to form social networks by\nadding other users as contacts. Flickr offers multiple ways of browsing or\nsearching it. One option is tag search, which returns all images tagged with a\nspecific keyword. If the keyword is ambiguous, e.g., ``beetle'' could mean an\ninsect or a car, tag search results will include many images that are not\nrelevant to the sense the user had in mind when executing the query. We claim\nthat users express their photography interests through the metadata they add in\nthe form of contacts and image annotations. We show how to exploit this\nmetadata to personalize search results for the user, thereby improving search\nperformance. First, we show that we can significantly improve search precision\nby filtering tag search results by user's contacts or a larger social network\nthat includes those contact's contacts. Secondly, we describe a probabilistic\nmodel that takes advantage of tag information to discover latent topics\ncontained in the search results. The users' interests can similarly be\ndescribed by the tags they used for annotating their images. The latent topics\nfound by the model are then used to personalize search results by finding\nimages on topics that are of interest to the user.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 12 Apr 2007 23:31:04 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Lerman",
                "Kristina",
                ""
            ],
            [
                "Plangprasopchok",
                "Anon",
                ""
            ],
            [
                "Wong",
                "Chio",
                ""
            ]
        ]
    },
    {
        "id": "0704.1709",
        "submitter": "Marie Cottrell",
        "authors": "Marie Cottrell (SAMOS, Matisse), Smail Ibbou (SAMOS, Matisse), Patrick\n  Letr\\'emy (SAMOS, Matisse)",
        "title": "Traitement Des Donnees Manquantes Au Moyen De L'Algorithme De Kohonen",
        "comments": null,
        "journal-ref": "Actes de la dixi\\`eme conf\\'erence ACSEG 2003 (Nantes) (2003)\n  201-217",
        "doi": null,
        "report-no": null,
        "categories": "stat.AP cs.NE",
        "license": null,
        "abstract": "  Nous montrons comment il est possible d'utiliser l'algorithme d'auto\norganisation de Kohonen pour traiter des donn\\'ees avec valeurs manquantes et\nestimer ces derni\\`eres. Apr\\`es un rappel m\\'ethodologique, nous illustrons\nnotre propos \\`a partir de trois applications \\`a des donn\\'ees r\\'eelles.\n  -----\n  We show how it is possible to use the Kohonen self-organizing algorithm to\ndeal with data which contain missing values and to estimate them. After a\nmethodological recall, we illustrate our purpose from three real databases\napplications.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 13 Apr 2007 07:33:15 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Cottrell",
                "Marie",
                "",
                "SAMOS, Matisse"
            ],
            [
                "Ibbou",
                "Smail",
                "",
                "SAMOS, Matisse"
            ],
            [
                "Letr\u00e9my",
                "Patrick",
                "",
                "SAMOS, Matisse"
            ]
        ]
    },
    {
        "id": "0704.1751",
        "submitter": "Olivier Rioul",
        "authors": "Olivier Rioul",
        "title": "Information Theoretic Proofs of Entropy Power Inequalities",
        "comments": "submitted for publication in the IEEE Transactions on Information\n  Theory, revised version",
        "journal-ref": null,
        "doi": "10.1109/TIT.2010.2090193",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While most useful information theoretic inequalities can be deduced from the\nbasic properties of entropy or mutual information, up to now Shannon's entropy\npower inequality (EPI) is an exception: Existing information theoretic proofs\nof the EPI hinge on representations of differential entropy using either Fisher\ninformation or minimum mean-square error (MMSE), which are derived from de\nBruijn's identity. In this paper, we first present an unified view of these\nproofs, showing that they share two essential ingredients: 1) a data processing\nargument applied to a covariance-preserving linear transformation; 2) an\nintegration over a path of a continuous Gaussian perturbation. Using these\ningredients, we develop a new and brief proof of the EPI through a mutual\ninformation inequality, which replaces Stam and Blachman's Fisher information\ninequality (FII) and an inequality for MMSE by Guo, Shamai and Verd\\'u used in\nearlier proofs. The result has the advantage of being very simple in that it\nrelies only on the basic properties of mutual information. These ideas are then\ngeneralized to various extended versions of the EPI: Zamir and Feder's\ngeneralized EPI for linear transformations of the random variables, Takano and\nJohnson's EPI for dependent variables, Liu and Viswanath's\ncovariance-constrained EPI, and Costa's concavity inequality for the entropy\npower.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 13 Apr 2007 12:42:07 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 24 Aug 2010 10:41:34 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Rioul",
                "Olivier",
                ""
            ]
        ]
    },
    {
        "id": "0704.1756",
        "submitter": "Jose M. Martin-Garcia",
        "authors": "Jose M. Martin-Garcia, Renato Portugal, Leon R. U. Manssur",
        "title": "The Invar Tensor Package",
        "comments": "Accepted in Computer Physics Communications. Package can be\n  downloaded from http://metric.iem.csic.es/Martin-Garcia/xAct/Invar/\n  (Mathematica version) or http://www.lncc.br/~portugal/Invar.html (Maple\n  version)",
        "journal-ref": "Comp. Phys. Commun. 177 (2007) 640-648",
        "doi": "10.1016/j.cpc.2007.05.015",
        "report-no": null,
        "categories": "cs.SC gr-qc hep-th",
        "license": null,
        "abstract": "  The Invar package is introduced, a fast manipulator of generic scalar\npolynomial expressions formed from the Riemann tensor of a four-dimensional\nmetric-compatible connection. The package can maximally simplify any polynomial\ncontaining tensor products of up to seven Riemann tensors within seconds. It\nhas been implemented both in Mathematica and Maple algebraic systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 13 Apr 2007 13:03:59 GMT"
            }
        ],
        "update_date": "2015-05-13",
        "authors_parsed": [
            [
                "Martin-Garcia",
                "Jose M.",
                ""
            ],
            [
                "Portugal",
                "Renato",
                ""
            ],
            [
                "Manssur",
                "Leon R. U.",
                ""
            ]
        ]
    },
    {
        "id": "0704.1768",
        "submitter": "Enrique ter Horst A",
        "authors": "Henryk Gzyl, German Molina, Enrique ter Horst",
        "title": "Assessment and Propagation of Input Uncertainty in Tree-based Option\n  Pricing Models",
        "comments": "39 pages, 21 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.GT",
        "license": null,
        "abstract": "  This paper aims to provide a practical example on the assessment and\npropagation of input uncertainty for option pricing when using tree-based\nmethods. Input uncertainty is propagated into output uncertainty, reflecting\nthat option prices are as unknown as the inputs they are based on. Option\npricing formulas are tools whose validity is conditional not only on how close\nthe model represents reality, but also on the quality of the inputs they use,\nand those inputs are usually not observable. We provide three alternative\nframeworks to calibrate option pricing tree models, propagating parameter\nuncertainty into the resulting option prices. We finally compare our methods\nwith classical calibration-based results assuming that there is no options\nmarket established. These methods can be applied to pricing of instruments for\nwhich there is not an options market, as well as a methodological tool to\naccount for parameter and model uncertainty in theoretical option pricing.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 13 Apr 2007 14:48:41 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Gzyl",
                "Henryk",
                ""
            ],
            [
                "Molina",
                "German",
                ""
            ],
            [
                "ter Horst",
                "Enrique",
                ""
            ]
        ]
    },
    {
        "id": "0704.1783",
        "submitter": "Francesco Santini",
        "authors": "Stefano Bistarelli, Ugo Montanari, Francesca Rossi, Francesco Santini",
        "title": "Unicast and Multicast Qos Routing with Soft Constraint Logic Programming",
        "comments": "45 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.AI cs.NI",
        "license": null,
        "abstract": "  We present a formal model to represent and solve the unicast/multicast\nrouting problem in networks with Quality of Service (QoS) requirements. To\nattain this, first we translate the network adapting it to a weighted graph\n(unicast) or and-or graph (multicast), where the weight on a connector\ncorresponds to the multidimensional cost of sending a packet on the related\nnetwork link: each component of the weights vector represents a different QoS\nmetric value (e.g. bandwidth, cost, delay, packet loss). The second step\nconsists in writing this graph as a program in Soft Constraint Logic\nProgramming (SCLP): the engine of this framework is then able to find the best\npaths/trees by optimizing their costs and solving the constraints imposed on\nthem (e.g. delay < 40msec), thus finding a solution to QoS routing problems.\nMoreover, c-semiring structures are a convenient tool to model QoS metrics. At\nlast, we provide an implementation of the framework over scale-free networks\nand we suggest how the performance can be improved.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 13 Apr 2007 15:53:44 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 29 Apr 2007 15:40:10 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 21 Apr 2008 17:25:06 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Bistarelli",
                "Stefano",
                ""
            ],
            [
                "Montanari",
                "Ugo",
                ""
            ],
            [
                "Rossi",
                "Francesca",
                ""
            ],
            [
                "Santini",
                "Francesco",
                ""
            ]
        ]
    },
    {
        "id": "0704.1783",
        "submitter": "Francesco Santini",
        "authors": "Stefano Bistarelli, Ugo Montanari, Francesca Rossi, Francesco Santini",
        "title": "Unicast and Multicast Qos Routing with Soft Constraint Logic Programming",
        "comments": "45 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.AI cs.NI",
        "license": null,
        "abstract": "  We present a formal model to represent and solve the unicast/multicast\nrouting problem in networks with Quality of Service (QoS) requirements. To\nattain this, first we translate the network adapting it to a weighted graph\n(unicast) or and-or graph (multicast), where the weight on a connector\ncorresponds to the multidimensional cost of sending a packet on the related\nnetwork link: each component of the weights vector represents a different QoS\nmetric value (e.g. bandwidth, cost, delay, packet loss). The second step\nconsists in writing this graph as a program in Soft Constraint Logic\nProgramming (SCLP): the engine of this framework is then able to find the best\npaths/trees by optimizing their costs and solving the constraints imposed on\nthem (e.g. delay < 40msec), thus finding a solution to QoS routing problems.\nMoreover, c-semiring structures are a convenient tool to model QoS metrics. At\nlast, we provide an implementation of the framework over scale-free networks\nand we suggest how the performance can be improved.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 13 Apr 2007 15:53:44 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 29 Apr 2007 15:40:10 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 21 Apr 2008 17:25:06 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Bistarelli",
                "Stefano",
                ""
            ],
            [
                "Montanari",
                "Ugo",
                ""
            ],
            [
                "Rossi",
                "Francesca",
                ""
            ],
            [
                "Santini",
                "Francesco",
                ""
            ]
        ]
    },
    {
        "id": "0704.1818",
        "submitter": "Martin Wainwright",
        "authors": "Martin J. Wainwright, Emin Martinian",
        "title": "Low-density graph codes that are optimal for source/channel coding and\n  binning",
        "comments": "Appeared as technical report 730, Dept. of Statistics, UC Berkeley.\n  Portions previously published in conference form (DCC 2006; ISIT 2006)",
        "journal-ref": null,
        "doi": null,
        "report-no": "Technical report 730",
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  We describe and analyze the joint source/channel coding properties of a class\nof sparse graphical codes based on compounding a low-density generator matrix\n(LDGM) code with a low-density parity check (LDPC) code. Our first pair of\ntheorems establish that there exist codes from this ensemble, with all degrees\nremaining bounded independently of block length, that are simultaneously\noptimal as both source and channel codes when encoding and decoding are\nperformed optimally. More precisely, in the context of lossy compression, we\nprove that finite degree constructions can achieve any pair $(R, D)$ on the\nrate-distortion curve of the binary symmetric source. In the context of channel\ncoding, we prove that finite degree codes can achieve any pair $(C, p)$ on the\ncapacity-noise curve of the binary symmetric channel. Next, we show that our\ncompound construction has a nested structure that can be exploited to achieve\nthe Wyner-Ziv bound for source coding with side information (SCSI), as well as\nthe Gelfand-Pinsker bound for channel coding with side information (CCSI).\nAlthough the current results are based on optimal encoding and decoding, the\nproposed graphical codes have sparse structure and high girth that renders them\nwell-suited to message-passing and other efficient decoding procedures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 13 Apr 2007 20:33:43 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Wainwright",
                "Martin J.",
                ""
            ],
            [
                "Martinian",
                "Emin",
                ""
            ]
        ]
    },
    {
        "id": "0704.1827",
        "submitter": "Gerald Krafft",
        "authors": "Gerald Krafft",
        "title": "Transaction-Oriented Simulation In Ad Hoc Grids",
        "comments": "101 pages (plus 131 appendix), 23 figures, 11 tables, MSc thesis for\n  degree in Advanced Computer Science at the Westminster University, London",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  This paper analyses the possibilities of performing parallel\ntransaction-oriented simulations with a special focus on the space-parallel\napproach and discrete event simulation synchronisation algorithms that are\nsuitable for transaction-oriented simulation and the target environment of Ad\nHoc Grids. To demonstrate the findings a Java-based parallel\ntransaction-oriented simulator for the simulation language GPSS/H is\nimplemented on the basis of the promising Shock Resistant Time Warp\nsynchronisation algorithm and using the Grid framework ProActive. The\nvalidation of this parallel simulator shows that the Shock Resistant Time Warp\nalgorithm can successfully reduce the number of rolled back Transaction moves\nbut it also reveals circumstances in which the Shock Resistant Time Warp\nalgorithm can be outperformed by the normal Time Warp algorithm. The conclusion\nof this paper suggests possible improvements to the Shock Resistant Time Warp\nalgorithm to avoid such problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Apr 2007 15:59:27 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Krafft",
                "Gerald",
                ""
            ]
        ]
    },
    {
        "id": "0704.1833",
        "submitter": "Inanc Inan",
        "authors": "Inanc Inan, Feyza Keceli, Ender Ayanoglu",
        "title": "Analysis of the 802.11e Enhanced Distributed Channel Access Function",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  The IEEE 802.11e standard revises the Medium Access Control (MAC) layer of\nthe former IEEE 802.11 standard for Quality-of-Service (QoS) provision in the\nWireless Local Area Networks (WLANs). The Enhanced Distributed Channel Access\n(EDCA) function of 802.11e defines multiple Access Categories (AC) with\nAC-specific Contention Window (CW) sizes, Arbitration Interframe Space (AIFS)\nvalues, and Transmit Opportunity (TXOP) limits to support MAC-level QoS and\nprioritization. We propose an analytical model for the EDCA function which\nincorporates an accurate CW, AIFS, and TXOP differentiation at any traffic\nload. The proposed model is also shown to capture the effect of MAC layer\nbuffer size on the performance. Analytical and simulation results are compared\nto demonstrate the accuracy of the proposed approach for varying traffic loads,\nEDCA parameters, and MAC layer buffer space.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 13 Apr 2007 22:40:05 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 26 Oct 2007 05:59:08 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 28 Mar 2008 05:21:34 GMT"
            }
        ],
        "update_date": "2008-03-28",
        "authors_parsed": [
            [
                "Inan",
                "Inanc",
                ""
            ],
            [
                "Keceli",
                "Feyza",
                ""
            ],
            [
                "Ayanoglu",
                "Ender",
                ""
            ]
        ]
    },
    {
        "id": "0704.1873",
        "submitter": "Yi Cao",
        "authors": "Yi Cao and Biao Chen",
        "title": "An Achievable Rate Region for Interference Channels with Conferencing",
        "comments": "5 pages, 2 figures, accepted by ISIT'07",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In this paper, we propose an achievable rate region for discrete memoryless\ninterference channels with conferencing at the transmitter side. We employ\nsuperposition block Markov encoding, combined with simultaneous superposition\ncoding, dirty paper coding, and random binning to obtain the achievable rate\nregion. We show that, under respective conditions, the proposed achievable\nregion reduces to Han and Kobayashi achievable region for interference\nchannels, the capacity region for degraded relay channels, and the capacity\nregion for the Gaussian vector broadcast channel. Numerical examples for the\nGaussian case are given.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 14 Apr 2007 15:42:44 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Cao",
                "Yi",
                ""
            ],
            [
                "Chen",
                "Biao",
                ""
            ]
        ]
    },
    {
        "id": "0704.1925",
        "submitter": "Yuanning Yu",
        "authors": "Yuanning Yu, Athina P. Petropulu and H. Vincent Poor",
        "title": "Blind Identification of Distributed Antenna Systems with Multiple\n  Carrier Frequency Offsets",
        "comments": "To appear in the Proceedings of the 8th IEEE International Workshop\n  on Signal Processing Advances in Wireless Communications (SPAWC), Helsinki,\n  Finland, June 17-20, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In spatially distributed multiuser antenna systems, the received signal\ncontains multiple carrier-frequency offsets (CFOs) arising from mismatch\nbetween the oscillators of transmitters and receivers. This results in a\ntime-varying rotation of the data constellation, which needs to be compensated\nat the receiver before symbol recovery. In this paper, a new approach for blind\nCFO estimation and symbol recovery is proposed. The received base-band signal\nis over-sampled, and its polyphase components are used to formulate a virtual\nMultiple-Input Multiple-Output (MIMO) problem. By applying blind MIMO system\nestimation techniques, the system response can be estimated and decoupled\nversions of the user symbols can be recovered, each one of which contains a\ndistinct CFO. By applying a decision feedback Phase Lock Loop (PLL), the CFO\ncan be mitigated and the transmitted symbols can be recovered. The estimated\nMIMO system response provides information about the CFOs that can be used to\ninitialize the PLL, speed up its convergence, and avoid ambiguities usually\nlinked with PLL.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 15 Apr 2007 23:58:08 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Yu",
                "Yuanning",
                ""
            ],
            [
                "Petropulu",
                "Athina P.",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0704.2010",
        "submitter": "Juliana Bernardes",
        "authors": "Juliana S Bernardes, Alberto Davila, Vitor Santos Costa, Gerson\n  Zaverucha",
        "title": "A study of structural properties on profiles HMMs",
        "comments": "6 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivation: Profile hidden Markov Models (pHMMs) are a popular and very\nuseful tool in the detection of the remote homologue protein families.\nUnfortunately, their performance is not always satisfactory when proteins are\nin the 'twilight zone'. We present HMMER-STRUCT, a model construction algorithm\nand tool that tries to improve pHMM performance by using structural information\nwhile training pHMMs. As a first step, HMMER-STRUCT constructs a set of pHMMs.\nEach pHMM is constructed by weighting each residue in an aligned protein\naccording to a specific structural property of the residue. Properties used\nwere primary, secondary and tertiary structures, accessibility and packing.\nHMMER-STRUCT then prioritizes the results by voting. Results: We used the SCOP\ndatabase to perform our experiments. Throughout, we apply leave-one-family-out\ncross-validation over protein superfamilies. First, we used the MAMMOTH-mult\nstructural aligner to align the training set proteins. Then, we performed two\nsets of experiments. In a first experiment, we compared structure weighted\nmodels against standard pHMMs and against each other. In a second experiment,\nwe compared the voting model against individual pHMMs. We compare method\nperformance through ROC curves and through Precision/Recall curves, and assess\nsignificance through the paired two tailed t-test. Our results show significant\nperformance improvements of all structurally weighted models over default\nHMMER, and a significant improvement in sensitivity of the combined models over\nboth the original model and the structurally weighted models.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 16 Apr 2007 13:10:35 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 11 Dec 2008 18:47:26 GMT"
            }
        ],
        "update_date": "2008-12-11",
        "authors_parsed": [
            [
                "Bernardes",
                "Juliana S",
                ""
            ],
            [
                "Davila",
                "Alberto",
                ""
            ],
            [
                "Costa",
                "Vitor Santos",
                ""
            ],
            [
                "Zaverucha",
                "Gerson",
                ""
            ]
        ]
    },
    {
        "id": "0704.2017",
        "submitter": "Giacomo Bacci",
        "authors": "G. Bacci, M. Luise, H.V. Poor",
        "title": "Large System Analysis of Game-Theoretic Power Control in UWB Wireless\n  Networks with Rake Receivers",
        "comments": "To appear in the Proceedings of the 8th IEEE International Workshop\n  on Signal Processing Advances in Wireless Communications (SPAWC), Helsinki,\n  Finland, June 17-20, 2007",
        "journal-ref": null,
        "doi": "10.1109/SPAWC.2007.4401311",
        "report-no": null,
        "categories": "cs.IT cs.GT math.IT",
        "license": null,
        "abstract": "  This paper studies the performance of partial-Rake (PRake) receivers in\nimpulse-radio ultrawideband wireless networks when an energy-efficient power\ncontrol scheme is adopted. Due to the large bandwidth of the system, the\nmultipath channel is assumed to be frequency-selective. By using noncooperative\ngame-theoretic models and large system analysis, explicit expressions are\nderived in terms of network parameters to measure the effects of self- and\nmultiple-access interference at a receiving access point. Performance of the\nPRake is compared in terms of achieved utilities and loss to that of the\nall-Rake receiver.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 16 Apr 2007 13:44:23 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Bacci",
                "G.",
                ""
            ],
            [
                "Luise",
                "M.",
                ""
            ],
            [
                "Poor",
                "H. V.",
                ""
            ]
        ]
    },
    {
        "id": "0704.2083",
        "submitter": "Hassan Satori",
        "authors": "H. Satori, M. Harti and N. Chenfour",
        "title": "Introduction to Arabic Speech Recognition Using CMUSphinx System",
        "comments": "4 pages, 3 figures and 2 tables, was in Information and Communication\n  Technologies International Symposium proceeding ICTIS07 Fes (2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CL cs.AI",
        "license": null,
        "abstract": "  In this paper Arabic was investigated from the speech recognition problem\npoint of view. We propose a novel approach to build an Arabic Automated Speech\nRecognition System (ASR). This system is based on the open source CMU Sphinx-4,\nfrom the Carnegie Mellon University. CMU Sphinx is a large-vocabulary;\nspeaker-independent, continuous speech recognition system based on discrete\nHidden Markov Models (HMMs). We build a model using utilities from the\nOpenSource CMU Sphinx. We will demonstrate the possible adaptability of this\nsystem to Arabic voice recognition.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 17 Apr 2007 01:04:01 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Satori",
                "H.",
                ""
            ],
            [
                "Harti",
                "M.",
                ""
            ],
            [
                "Chenfour",
                "N.",
                ""
            ]
        ]
    },
    {
        "id": "0704.2092",
        "submitter": "Jinsong Tan",
        "authors": "Jinsong Tan",
        "title": "A Note on the Inapproximability of Correlation Clustering",
        "comments": null,
        "journal-ref": "Information Processing Letters, 108: 331-335, 2008",
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider inapproximability of the correlation clustering problem defined\nas follows: Given a graph $G = (V,E)$ where each edge is labeled either \"+\"\n(similar) or \"-\" (dissimilar), correlation clustering seeks to partition the\nvertices into clusters so that the number of pairs correctly (resp.\nincorrectly) classified with respect to the labels is maximized (resp.\nminimized). The two complementary problems are called MaxAgree and MinDisagree,\nrespectively, and have been studied on complete graphs, where every edge is\nlabeled, and general graphs, where some edge might not have been labeled.\nNatural edge-weighted versions of both problems have been studied as well. Let\nS-MaxAgree denote the weighted problem where all weights are taken from set S,\nwe show that S-MaxAgree with weights bounded by $O(|V|^{1/2-\\delta})$\nessentially belongs to the same hardness class in the following sense: if there\nis a polynomial time algorithm that approximates S-MaxAgree within a factor of\n$\\lambda = O(\\log{|V|})$ with high probability, then for any choice of S',\nS'-MaxAgree can be approximated in polynomial time within a factor of $(\\lambda\n+ \\epsilon)$, where $\\epsilon > 0$ can be arbitrarily small, with high\nprobability. A similar statement also holds for $S-MinDisagree. This result\nimplies it is hard (assuming $NP \\neq RP$) to approximate unweighted MaxAgree\nwithin a factor of $80/79-\\epsilon$, improving upon a previous known factor of\n$116/115-\\epsilon$ by Charikar et. al. \\cite{Chari05}.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 17 Apr 2007 03:52:41 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 23 Mar 2009 03:22:02 GMT"
            }
        ],
        "update_date": "2009-03-23",
        "authors_parsed": [
            [
                "Tan",
                "Jinsong",
                ""
            ]
        ]
    },
    {
        "id": "0704.2201",
        "submitter": "Hassan Satori",
        "authors": "H. Satori, M. Harti and N. Chenfour",
        "title": "Arabic Speech Recognition System using CMU-Sphinx4",
        "comments": "5 pages, 3 figures and 2 tables, in French",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CL cs.AI",
        "license": null,
        "abstract": "  In this paper we present the creation of an Arabic version of Automated\nSpeech Recognition System (ASR). This system is based on the open source\nSphinx-4, from the Carnegie Mellon University. Which is a speech recognition\nsystem based on discrete hidden Markov models (HMMs). We investigate the\nchanges that must be made to the model to adapt Arabic voice recognition.\n  Keywords: Speech recognition, Acoustic model, Arabic language, HMMs,\nCMUSphinx-4, Artificial intelligence.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 17 Apr 2007 17:04:26 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Satori",
                "H.",
                ""
            ],
            [
                "Harti",
                "M.",
                ""
            ],
            [
                "Chenfour",
                "N.",
                ""
            ]
        ]
    },
    {
        "id": "0704.2258",
        "submitter": "Stefan Laendner",
        "authors": "Andrew McGregor and Olgica Milenkovic",
        "title": "On the Hardness of Approximating Stopping and Trapping Sets in LDPC\n  Codes",
        "comments": "16 pages, 6 figure, submitted journal version",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We prove that approximating the size of stopping and trapping sets in Tanner\ngraphs of linear block codes, and more restrictively, the class of low-density\nparity-check (LDPC) codes, is NP-hard. The ramifications of our findings are\nthat methods used for estimating the height of the error-floor of moderate- and\nlong-length LDPC codes based on stopping and trapping set enumeration cannot\nprovide accurate worst-case performance predictions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Apr 2007 01:49:33 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 3 Aug 2008 04:49:39 GMT"
            }
        ],
        "update_date": "2008-08-03",
        "authors_parsed": [
            [
                "McGregor",
                "Andrew",
                ""
            ],
            [
                "Milenkovic",
                "Olgica",
                ""
            ]
        ]
    },
    {
        "id": "0704.2259",
        "submitter": "Lifeng Lai",
        "authors": "Lifeng Lai, Hesham El Gamal and H. Vincent Poor",
        "title": "The Wiretap Channel with Feedback: Encryption over the Channel",
        "comments": "Submitted to IEEE Transactions on Information Theory",
        "journal-ref": null,
        "doi": "10.1109/TIT.2008.929914",
        "report-no": null,
        "categories": "cs.IT cs.CR math.IT",
        "license": null,
        "abstract": "  In this work, the critical role of noisy feedback in enhancing the secrecy\ncapacity of the wiretap channel is established. Unlike previous works, where a\nnoiseless public discussion channel is used for feedback, the feed-forward and\nfeedback signals share the same noisy channel in the present model. Quite\ninterestingly, this noisy feedback model is shown to be more advantageous in\nthe current setting. More specifically, the discrete memoryless modulo-additive\nchannel with a full-duplex destination node is considered first, and it is\nshown that the judicious use of feedback increases the perfect secrecy capacity\nto the capacity of the source-destination channel in the absence of the\nwiretapper. In the achievability scheme, the feedback signal corresponds to a\nprivate key, known only to the destination. In the half-duplex scheme, a novel\nfeedback technique that always achieves a positive perfect secrecy rate (even\nwhen the source-wiretapper channel is less noisy than the source-destination\nchannel) is proposed. These results hinge on the modulo-additive property of\nthe channel, which is exploited by the destination to perform encryption over\nthe channel without revealing its key to the source. Finally, this scheme is\nextended to the continuous real valued modulo-$\\Lambda$ channel where it is\nshown that the perfect secrecy capacity with feedback is also equal to the\ncapacity in the absence of the wiretapper.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Apr 2007 02:43:40 GMT"
            }
        ],
        "update_date": "2016-11-18",
        "authors_parsed": [
            [
                "Lai",
                "Lifeng",
                ""
            ],
            [
                "Gamal",
                "Hesham El",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0704.2259",
        "submitter": "Lifeng Lai",
        "authors": "Lifeng Lai, Hesham El Gamal and H. Vincent Poor",
        "title": "The Wiretap Channel with Feedback: Encryption over the Channel",
        "comments": "Submitted to IEEE Transactions on Information Theory",
        "journal-ref": null,
        "doi": "10.1109/TIT.2008.929914",
        "report-no": null,
        "categories": "cs.IT cs.CR math.IT",
        "license": null,
        "abstract": "  In this work, the critical role of noisy feedback in enhancing the secrecy\ncapacity of the wiretap channel is established. Unlike previous works, where a\nnoiseless public discussion channel is used for feedback, the feed-forward and\nfeedback signals share the same noisy channel in the present model. Quite\ninterestingly, this noisy feedback model is shown to be more advantageous in\nthe current setting. More specifically, the discrete memoryless modulo-additive\nchannel with a full-duplex destination node is considered first, and it is\nshown that the judicious use of feedback increases the perfect secrecy capacity\nto the capacity of the source-destination channel in the absence of the\nwiretapper. In the achievability scheme, the feedback signal corresponds to a\nprivate key, known only to the destination. In the half-duplex scheme, a novel\nfeedback technique that always achieves a positive perfect secrecy rate (even\nwhen the source-wiretapper channel is less noisy than the source-destination\nchannel) is proposed. These results hinge on the modulo-additive property of\nthe channel, which is exploited by the destination to perform encryption over\nthe channel without revealing its key to the source. Finally, this scheme is\nextended to the continuous real valued modulo-$\\Lambda$ channel where it is\nshown that the perfect secrecy capacity with feedback is also equal to the\ncapacity in the absence of the wiretapper.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Apr 2007 02:43:40 GMT"
            }
        ],
        "update_date": "2016-11-18",
        "authors_parsed": [
            [
                "Lai",
                "Lifeng",
                ""
            ],
            [
                "Gamal",
                "Hesham El",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0704.2295",
        "submitter": "Hassan Jameel",
        "authors": "Hassan Jameel, Heejo Lee and Sungyoung Lee",
        "title": "Using Image Attributes for Human Identification Protocols",
        "comments": "24 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  A secure human identification protocol aims at authenticating human users to\na remote server when even the users' inputs are not hidden from an adversary.\nRecently, the authors proposed a human identification protocol in the RSA\nConference 2007, which is loosely based on the ability of humans to efficiently\nprocess an image. The advantage being that an automated adversary is not\neffective in attacking the protocol without human assistance. This paper\nextends that work by trying to solve some of the open problems. First, we\nanalyze the complexity of defeating the proposed protocols by quantifying the\nworkload of a human adversary. Secondly, we propose a new construction based on\ntextual CAPTCHAs (Reverse Turing Tests) in order to make the generation of\nautomated challenges easier. We also present a brief experiment involving real\nhuman users to find out the number of possible attributes in a given image and\ngive some guidelines for the selection of challenge questions based on the\nresults. Finally, we analyze the previously proposed protocol in detail for the\nrelationship between the secrets. Our results show that we can construct human\nidentification protocols based on image evaluation with reasonably\n``quantified'' security guarantees based on our model.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Apr 2007 09:13:43 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Jameel",
                "Hassan",
                ""
            ],
            [
                "Lee",
                "Heejo",
                ""
            ],
            [
                "Lee",
                "Sungyoung",
                ""
            ]
        ]
    },
    {
        "id": "0704.2344",
        "submitter": "Publications Ampere",
        "authors": "Christian Vollaire (CEGELY), Laurent Nicolas (CEGELY), Alain Nicolas\n  (CEGELY)",
        "title": "Parallel computing for the finite element method",
        "comments": null,
        "journal-ref": "EUROPEAN PHYSICAL JOURNAL Applied Physics 1, 3 (03/1998) 305-314",
        "doi": "10.1051/epjap:1998151",
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  A finite element method is presented to compute time harmonic microwave\nfields in three dimensional configurations. Nodal-based finite elements have\nbeen coupled with an absorbing boundary condition to solve open boundary\nproblems. This paper describes how the modeling of large devices has been made\npossible using parallel computation, New algorithms are then proposed to\nimplement this formulation on a cluster of workstations (10 DEC ALPHA 300X) and\non a CRAY C98. Analysis of the computation efficiency is performed using simple\nproblems. The electromagnetic scattering of a plane wave by a perfect electric\nconducting airplane is finally given as example.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Apr 2007 13:20:25 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Vollaire",
                "Christian",
                "",
                "CEGELY"
            ],
            [
                "Nicolas",
                "Laurent",
                "",
                "CEGELY"
            ],
            [
                "Nicolas",
                "Alain",
                "",
                "CEGELY"
            ]
        ]
    },
    {
        "id": "0704.2351",
        "submitter": "Giorgi Pascal",
        "authors": "Jean-Guillaume Dumas (LMC - IMAG), Philippe Elbaz-Vincent (I3M),\n  Pascal Giorgi (LP2A), Anna Urbanska (LMC - IMAG)",
        "title": "Parallel computation of the rank of large sparse matrices from algebraic\n  K-theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.KT cs.DC cs.SC math.NT",
        "license": null,
        "abstract": "  This paper deals with the computation of the rank and of some integer Smith\nforms of a series of sparse matrices arising in algebraic K-theory. The number\nof non zero entries in the considered matrices ranges from 8 to 37 millions.\nThe largest rank computation took more than 35 days on 50 processors. We report\non the actual algorithms we used to build the matrices, their link to the\nmotivic cohomology and the linear algebra and parallelizations required to\nperform such huge computations. In particular, these results are part of the\nfirst computation of the cohomology of the linear group GL_7(Z).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Apr 2007 14:29:28 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 24 Apr 2007 15:28:48 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Dumas",
                "Jean-Guillaume",
                "",
                "LMC - IMAG"
            ],
            [
                "Elbaz-Vincent",
                "Philippe",
                "",
                "I3M"
            ],
            [
                "Giorgi",
                "Pascal",
                "",
                "LP2A"
            ],
            [
                "Urbanska",
                "Anna",
                "",
                "LMC - IMAG"
            ]
        ]
    },
    {
        "id": "0704.2351",
        "submitter": "Giorgi Pascal",
        "authors": "Jean-Guillaume Dumas (LMC - IMAG), Philippe Elbaz-Vincent (I3M),\n  Pascal Giorgi (LP2A), Anna Urbanska (LMC - IMAG)",
        "title": "Parallel computation of the rank of large sparse matrices from algebraic\n  K-theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.KT cs.DC cs.SC math.NT",
        "license": null,
        "abstract": "  This paper deals with the computation of the rank and of some integer Smith\nforms of a series of sparse matrices arising in algebraic K-theory. The number\nof non zero entries in the considered matrices ranges from 8 to 37 millions.\nThe largest rank computation took more than 35 days on 50 processors. We report\non the actual algorithms we used to build the matrices, their link to the\nmotivic cohomology and the linear algebra and parallelizations required to\nperform such huge computations. In particular, these results are part of the\nfirst computation of the cohomology of the linear group GL_7(Z).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Apr 2007 14:29:28 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 24 Apr 2007 15:28:48 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Dumas",
                "Jean-Guillaume",
                "",
                "LMC - IMAG"
            ],
            [
                "Elbaz-Vincent",
                "Philippe",
                "",
                "I3M"
            ],
            [
                "Giorgi",
                "Pascal",
                "",
                "LP2A"
            ],
            [
                "Urbanska",
                "Anna",
                "",
                "LMC - IMAG"
            ]
        ]
    },
    {
        "id": "0704.2353",
        "submitter": "Natasha Devroye",
        "authors": "Mai Vu, Natasha Devroye, Masoud Sharif and Vahid Tarokh",
        "title": "Scaling Laws of Cognitive Networks",
        "comments": "significantly revised and extended, 30 pages, 13 figures, submitted\n  to IEEE Journal of Special Topics in Signal Processing",
        "journal-ref": null,
        "doi": "10.1109/CROWNCOM.2007.4549764",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  We consider a cognitive network consisting of n random pairs of cognitive\ntransmitters and receivers communicating simultaneously in the presence of\nmultiple primary users. Of interest is how the maximum throughput achieved by\nthe cognitive users scales with n. Furthermore, how far these users must be\nfrom a primary user to guarantee a given primary outage. Two scenarios are\nconsidered for the network scaling law: (i) when each cognitive transmitter\nuses constant power to communicate with a cognitive receiver at a bounded\ndistance away, and (ii) when each cognitive transmitter scales its power\naccording to the distance to a considered primary user, allowing the cognitive\ntransmitter-receiver distances to grow. Using single-hop transmission, suitable\nfor cognitive devices of opportunistic nature, we show that, in both scenarios,\nwith path loss larger than 2, the cognitive network throughput scales linearly\nwith the number of cognitive users. We then explore the radius of a primary\nexclusive region void of cognitive transmitters. We obtain bounds on this\nradius for a given primary outage constraint. These bounds can help in the\ndesign of a primary network with exclusive regions, outside of which cognitive\nusers may transmit freely. Our results show that opportunistic secondary\nspectrum access using single-hop transmission is promising.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Apr 2007 15:57:37 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 24 Oct 2007 20:56:38 GMT"
            }
        ],
        "update_date": "2016-11-18",
        "authors_parsed": [
            [
                "Vu",
                "Mai",
                ""
            ],
            [
                "Devroye",
                "Natasha",
                ""
            ],
            [
                "Sharif",
                "Masoud",
                ""
            ],
            [
                "Tarokh",
                "Vahid",
                ""
            ]
        ]
    },
    {
        "id": "0704.2355",
        "submitter": "Luigi Santocanale",
        "authors": "Luigi Santocanale (LIF)",
        "title": "A Nice Labelling for Tree-Like Event Structures of Degree 3",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  We address the problem of &#64257;nding nice labellings for event structures\nof degree 3. We develop a minimum theory by which we prove that the labelling\nnumber of an event structure of degree 3 is bounded by a linear function of the\nheight. The main theorem we present in this paper states that event structures\nof degree 3 whose causality order is a tree have a nice labelling with 3\ncolors. Finally, we exemplify how to use this theorem to construct upper bounds\nfor the labelling number of other event structures of degree 3.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Apr 2007 14:39:09 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Santocanale",
                "Luigi",
                "",
                "LIF"
            ]
        ]
    },
    {
        "id": "0704.2375",
        "submitter": "Stefano Buzzi",
        "authors": "Stefano Buzzi and H. Vincent Poor",
        "title": "Power control algorithms for CDMA networks based on large system\n  analysis",
        "comments": "To appear in the Proceedings of the 2007 IEEE International Symposium\n  on Information Theory, Nice, France, June 24 - 29, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  Power control is a fundamental task accomplished in any wireless cellular\nnetwork; its aim is to set the transmit power of any mobile terminal, so that\neach user is able to achieve its own target SINR. While conventional power\ncontrol algorithms require knowledge of a number of parameters of the signal of\ninterest and of the multiaccess interference, in this paper it is shown that in\na large CDMA system much of this information can be dispensed with, and\neffective distributed power control algorithms may be implemented with very\nlittle information on the user of interest. An uplink CDMA system subject to\nflat fading is considered with a focus on the cases in which a linear MMSE\nreceiver and a non-linear MMSE serial interference cancellation receiver are\nadopted; for the latter case new formulas are also given for the system SINR in\nthe large system asymptote. Experimental results show an excellent agreement\nbetween the performance and the power profile of the proposed distributed\nalgorithms and that of conventional ones that require much greater prior\nknowledge.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Apr 2007 16:12:59 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Buzzi",
                "Stefano",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0704.2383",
        "submitter": "Stefano Buzzi",
        "authors": "Stefano Buzzi, Valeria Massaro, and H. Vincent Poor",
        "title": "Power control and receiver design for energy efficiency in multipath\n  CDMA channels with bandlimited waveforms",
        "comments": "appeared in the Proceedings of the 41st Annual Conference on\n  Information Sciences and Systems, John Hopkins University, March 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  This paper is focused on the cross-layer design problem of joint multiuser\ndetection and power control for energy-efficiency optimization in a wireless\ndata network through a game-theoretic approach. Building on work of Meshkati,\net al., wherein the tools of game-theory are used in order to achieve\nenergy-efficiency in a simple synchronous code division multiple access system,\nsystem asynchronism, the use of bandlimited chip-pulses, and the multipath\ndistortion induced by the wireless channel are explicitly incorporated into the\nanalysis. Several non-cooperative games are proposed wherein users may vary\ntheir transmit power and their uplink receiver in order to maximize their\nutility, which is defined here as the ratio of data throughput to transmit\npower. In particular, the case in which a linear multiuser detector is adopted\nat the receiver is considered first, and then, the more challenging case in\nwhich non-linear decision feedback multiuser detectors are employed is\nconsidered. The proposed games are shown to admit a unique Nash equilibrium\npoint, while simulation results show the effectiveness of the proposed\nsolutions, as well as that the use of a decision-feedback multiuser receiver\nbrings remarkable performance improvements.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Apr 2007 16:35:59 GMT"
            }
        ],
        "update_date": "2015-03-13",
        "authors_parsed": [
            [
                "Buzzi",
                "Stefano",
                ""
            ],
            [
                "Massaro",
                "Valeria",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0704.2386",
        "submitter": "Pilar Albert",
        "authors": "Pilar Albert, Elvira Mayordomo, and Philippe Moser",
        "title": "Bounded Pushdown dimension vs Lempel Ziv information density",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CC cs.IT math.IT",
        "license": null,
        "abstract": "  In this paper we introduce a variant of pushdown dimension called bounded\npushdown (BPD) dimension, that measures the density of information contained in\na sequence, relative to a BPD automata, i.e. a finite state machine equipped\nwith an extra infinite memory stack, with the additional requirement that every\ninput symbol only allows a bounded number of stack movements. BPD automata are\na natural real-time restriction of pushdown automata. We show that BPD\ndimension is a robust notion by giving an equivalent characterization of BPD\ndimension in terms of BPD compressors. We then study the relationships between\nBPD compression, and the standard Lempel-Ziv (LZ) compression algorithm, and\nshow that in contrast to the finite-state compressor case, LZ is not universal\nfor bounded pushdown compressors in a strong sense: we construct a sequence\nthat LZ fails to compress signicantly, but that is compressed by at least a\nfactor 2 by a BPD compressor. As a corollary we obtain a strong separation\nbetween finite-state and BPD dimension.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Apr 2007 16:49:48 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Albert",
                "Pilar",
                ""
            ],
            [
                "Mayordomo",
                "Elvira",
                ""
            ],
            [
                "Moser",
                "Philippe",
                ""
            ]
        ]
    },
    {
        "id": "0704.2448",
        "submitter": "Ugo Dal Lago",
        "authors": "Patrick Baillot, Paolo Coppola and Ugo Dal Lago",
        "title": "Light Logics and Optimal Reduction: Completeness and Complexity",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.PL",
        "license": null,
        "abstract": "  Typing of lambda-terms in Elementary and Light Affine Logic (EAL, LAL, resp.)\nhas been studied for two different reasons: on the one hand the evaluation of\ntyped terms using LAL (EAL, resp.) proof-nets admits a guaranteed polynomial\n(elementary, resp.) bound; on the other hand these terms can also be evaluated\nby optimal reduction using the abstract version of Lamping's algorithm. The\nfirst reduction is global while the second one is local and asynchronous. We\nprove that for LAL (EAL, resp.) typed terms, Lamping's abstract algorithm also\nadmits a polynomial (elementary, resp.) bound. We also show its soundness and\ncompleteness (for EAL and LAL with type fixpoints), by using a simple geometry\nof interaction model (context semantics).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 19 Apr 2007 01:17:29 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Baillot",
                "Patrick",
                ""
            ],
            [
                "Coppola",
                "Paolo",
                ""
            ],
            [
                "Lago",
                "Ugo Dal",
                ""
            ]
        ]
    },
    {
        "id": "0704.2452",
        "submitter": "Raman Yazdani",
        "authors": "Raman Yazdani, Masoud Ardakani",
        "title": "Optimum Linear LLR Calculation for Iterative Decoding on Fading Channels",
        "comments": "This paper will be presented in IEEE International Symposium on\n  Information Theory (ISIT) 2007 in Nice, France",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2007.4557204",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  On a fading channel with no channel state information at the receiver,\ncalculating true log-likelihood ratios (LLR) is complicated. Existing work\nassume that the power of the additive noise is known and use the expected value\nof the fading gain in a linear function of the channel output to find\napproximate LLRs. In this work, we first assume that the power of the additive\nnoise is known and we find the optimum linear approximation of LLRs in the\nsense of maximum achievable transmission rate on the channel. The maximum\nachievable rate under this linear LLR calculation is almost equal to the\nmaximum achievable rate under true LLR calculation. We also observe that this\nmethod appears to be the optimum in the sense of bit error rate performance\ntoo. These results are then extended to the case that the noise power is\nunknown at the receiver and a performance almost identical to the case that the\nnoise power is perfectly known is obtained.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 19 Apr 2007 02:22:52 GMT"
            }
        ],
        "update_date": "2012-04-17",
        "authors_parsed": [
            [
                "Yazdani",
                "Raman",
                ""
            ],
            [
                "Ardakani",
                "Masoud",
                ""
            ]
        ]
    },
    {
        "id": "0704.2475",
        "submitter": "Zhang Shengli",
        "authors": "Zhang Shengli, Soung-Chang Liew, Patrick P.K. Lam",
        "title": "Physical Layer Network Coding",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  A main distinguishing feature of a wireless network compared with a wired\nnetwork is its broadcast nature, in which the signal transmitted by a node may\nreach several other nodes, and a node may receive signals from several other\nnodes simultaneously. Rather than a blessing, this feature is treated more as\nan interference-inducing nuisance in most wireless networks today (e.g., IEEE\n802.11). This paper shows that the concept of network coding can be applied at\nthe physical layer to turn the broadcast property into a capacity-boosting\nadvantage in wireless ad hoc networks. Specifically, we propose a\nphysical-layer network coding (PNC) scheme to coordinate transmissions among\nnodes. In contrast to straightforward network coding which performs coding\narithmetic on digital bit streams after they have been received, PNC makes use\nof the additive nature of simultaneously arriving electromagnetic (EM) waves\nfor equivalent coding operation. And in doing so, PNC can potentially achieve\n100% and 50% throughput increases compared with traditional transmission and\nstraightforward network coding, respectively, in multi-hop networks. More\nspecifically, the information-theoretic capacity of PNC is almost double that\nof traditional transmission in the SNR region of practical interest (higher\nthan 0dB). We believe this is a first paper that ventures into EM-wave-based\nnetwork coding at the physical layer and demonstrates its potential for\nboosting network capacity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 19 Apr 2007 08:15:37 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Shengli",
                "Zhang",
                ""
            ],
            [
                "Liew",
                "Soung-Chang",
                ""
            ],
            [
                "Lam",
                "Patrick P. K.",
                ""
            ]
        ]
    },
    {
        "id": "0704.2505",
        "submitter": "G.Susinder Rajan",
        "authors": "G. Susinder Rajan and B. Sundar Rajan",
        "title": "Algebraic Distributed Space-Time Codes with Low ML Decoding Complexity",
        "comments": "5 pages, no figures. To appear in Proceedings of IEEE ISIT 2007,\n  Nice, France",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2007.4557437",
        "report-no": null,
        "categories": "cs.IT cs.DM math.IT",
        "license": null,
        "abstract": "  \"Extended Clifford algebras\" are introduced as a means to obtain low ML\ndecoding complexity space-time block codes. Using left regular matrix\nrepresentations of two specific classes of extended Clifford algebras, two\nsystematic algebraic constructions of full diversity Distributed Space-Time\nCodes (DSTCs) are provided for any power of two number of relays. The left\nregular matrix representation has been shown to naturally result in space-time\ncodes meeting the additional constraints required for DSTCs. The DSTCs so\nconstructed have the salient feature of reduced Maximum Likelihood (ML)\ndecoding complexity. In particular, the ML decoding of these codes can be\nperformed by applying the lattice decoder algorithm on a lattice of four times\nlesser dimension than what is required in general. Moreover these codes have a\nuniform distribution of power among the relays and in time, thus leading to a\nlow Peak to Average Power Ratio at the relays.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 19 Apr 2007 11:05:31 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Rajan",
                "G. Susinder",
                ""
            ],
            [
                "Rajan",
                "B. Sundar",
                ""
            ]
        ]
    },
    {
        "id": "0704.2507",
        "submitter": "G.Susinder Rajan",
        "authors": "G. Susinder Rajan and B. Sundar Rajan",
        "title": "STBCs from Representation of Extended Clifford Algebras",
        "comments": "5 pages, no figures, To appear in Proceedings of IEEE ISIT 2007,\n  Nice, France",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2007.4557141",
        "report-no": null,
        "categories": "cs.IT cs.DM math.IT",
        "license": null,
        "abstract": "  A set of sufficient conditions to construct $\\lambda$-real symbol Maximum\nLikelihood (ML) decodable STBCs have recently been provided by Karmakar et al.\nSTBCs satisfying these sufficient conditions were named as Clifford Unitary\nWeight (CUW) codes. In this paper, the maximal rate (as measured in complex\nsymbols per channel use) of CUW codes for $\\lambda=2^a,a\\in\\mathbb{N}$ is\nobtained using tools from representation theory. Two algebraic constructions of\ncodes achieving this maximal rate are also provided. One of the constructions\nis obtained using linear representation of finite groups whereas the other\nconstruction is based on the concept of right module algebra over\nnon-commutative rings. To the knowledge of the authors, this is the first paper\nin which matrices over non-commutative rings is used to construct STBCs. An\nalgebraic explanation is provided for the 'ABBA' construction first proposed by\nTirkkonen et al and the tensor product construction proposed by Karmakar et al.\nFurthermore, it is established that the 4 transmit antenna STBC originally\nproposed by Tirkkonen et al based on the ABBA construction is actually a single\ncomplex symbol ML decodable code if the design variables are permuted and\nsignal sets of appropriate dimensions are chosen.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 19 Apr 2007 11:16:40 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Rajan",
                "G. Susinder",
                ""
            ],
            [
                "Rajan",
                "B. Sundar",
                ""
            ]
        ]
    },
    {
        "id": "0704.2509",
        "submitter": "G.Susinder Rajan",
        "authors": "G. Susinder Rajan and B. Sundar Rajan",
        "title": "Signal Set Design for Full-Diversity Low-Decoding-Complexity\n  Differential Scaled-Unitary STBCs",
        "comments": "5 pages, 2 figures. To appear in Proceedings of IEEE ISIT 2007, Nice,\n  France",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2007.4557453",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  The problem of designing high rate, full diversity noncoherent space-time\nblock codes (STBCs) with low encoding and decoding complexity is addressed.\nFirst, the notion of $g$-group encodable and $g$-group decodable linear STBCs\nis introduced. Then for a known class of rate-1 linear designs, an explicit\nconstruction of fully-diverse signal sets that lead to four-group encodable and\nfour-group decodable differential scaled unitary STBCs for any power of two\nnumber of antennas is provided. Previous works on differential STBCs either\nsacrifice decoding complexity for higher rate or sacrifice rate for lower\ndecoding complexity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 19 Apr 2007 11:25:23 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Rajan",
                "G. Susinder",
                ""
            ],
            [
                "Rajan",
                "B. Sundar",
                ""
            ]
        ]
    },
    {
        "id": "0704.2511",
        "submitter": "G.Susinder Rajan",
        "authors": "G. Susinder Rajan and B. Sundar Rajan",
        "title": "Noncoherent Low-Decoding-Complexity Space-Time Codes for Wireless Relay\n  Networks",
        "comments": "5 pages, no figures. To appear in Proceedings of IEEE ISIT 2007,\n  Nice, France",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2007.4557438",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  The differential encoding/decoding setup introduced by Kiran et al, Oggier et\nal and Jing et al for wireless relay networks that use codebooks consisting of\nunitary matrices is extended to allow codebooks consisting of scaled unitary\nmatrices. For such codebooks to be used in the Jing-Hassibi protocol for\ncooperative diversity, the conditions that need to be satisfied by the relay\nmatrices and the codebook are identified. A class of previously known rate one,\nfull diversity, four-group encodable and four-group decodable Differential\nSpace-Time Codes (DSTCs) is proposed for use as Distributed DSTCs (DDSTCs) in\nthe proposed set up. To the best of our knowledge, this is the first known low\ndecoding complexity DDSTC scheme for cooperative wireless networks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 19 Apr 2007 11:28:49 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Rajan",
                "G. Susinder",
                ""
            ],
            [
                "Rajan",
                "B. Sundar",
                ""
            ]
        ]
    },
    {
        "id": "0704.2542",
        "submitter": "Joan Llobera",
        "authors": "Joan Llobera",
        "title": "Narratives within immersive technologies",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  The main goal of this project is to research technical advances in order to\nenhance the possibility to develop narratives within immersive mediated\nenvironments. An important part of the research is concerned with the question\nof how a script can be written, annotated and realized for an immersive\ncontext. A first description of the main theoretical framework and the ongoing\nwork and a first script example is provided. This project is part of the\nprogram for presence research, and it will exploit physiological feedback and\nComputational Intelligence within virtual reality.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 19 Apr 2007 14:27:25 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Llobera",
                "Joan",
                ""
            ]
        ]
    },
    {
        "id": "0704.2544",
        "submitter": "Vishwambhar Rathi",
        "authors": "Vishwambhar Rathi, Ruediger Urbanke",
        "title": "Existence Proofs of Some EXIT Like Functions",
        "comments": "To appear in proc. of ISIT 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  The Extended BP (EBP) Generalized EXIT (GEXIT) function introduced in\n\\cite{MMRU05} plays a fundamental role in the asymptotic analysis of sparse\ngraph codes. For transmission over the binary erasure channel (BEC) the\nanalytic properties of the EBP GEXIT function are relatively simple and well\nunderstood. The general case is much harder and even the existence of the curve\nis not known in general. We introduce some tools from non-linear analysis which\ncan be useful to prove the existence of EXIT like curves in some cases. The\nmain tool is the Krasnoselskii-Rabinowitz (KR) bifurcation theorem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 19 Apr 2007 14:36:43 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Rathi",
                "Vishwambhar",
                ""
            ],
            [
                "Urbanke",
                "Ruediger",
                ""
            ]
        ]
    },
    {
        "id": "0704.2596",
        "submitter": "Markus Grassl",
        "authors": "Markus Grassl",
        "title": "Computing Extensions of Linear Codes",
        "comments": "accepted for publication at ISIT 07",
        "journal-ref": "Proceedings 2007 IEEE International Symposium on Information\n  Theory (ISIT 2007), Nice, France, June 2007, pp. 476-480",
        "doi": "10.1109/ISIT.2007.4557095",
        "report-no": null,
        "categories": "cs.IT cs.DM math.IT",
        "license": null,
        "abstract": "  This paper deals with the problem of increasing the minimum distance of a\nlinear code by adding one or more columns to the generator matrix. Several\nmethods to compute extensions of linear codes are presented. Many codes\nimproving the previously known lower bounds on the minimum distance have been\nfound.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 19 Apr 2007 19:28:33 GMT"
            }
        ],
        "update_date": "2011-03-31",
        "authors_parsed": [
            [
                "Grassl",
                "Markus",
                ""
            ]
        ]
    },
    {
        "id": "0704.2644",
        "submitter": "Maxim Raginsky",
        "authors": "Maxim Raginsky",
        "title": "Joint universal lossy coding and identification of stationary mixing\n  sources",
        "comments": "5 pages, 1 eps figure; to appear in Proc. ISIT 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.LG math.IT",
        "license": null,
        "abstract": "  The problem of joint universal source coding and modeling, treated in the\ncontext of lossless codes by Rissanen, was recently generalized to fixed-rate\nlossy coding of finitely parametrized continuous-alphabet i.i.d. sources. We\nextend these results to variable-rate lossy block coding of stationary ergodic\nsources and show that, for bounded metric distortion measures, any finitely\nparametrized family of stationary sources satisfying suitable mixing,\nsmoothness and Vapnik-Chervonenkis learnability conditions admits universal\nschemes for joint lossy source coding and identification. We also give several\nexplicit examples of parametric sources satisfying the regularity conditions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 20 Apr 2007 01:25:22 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Raginsky",
                "Maxim",
                ""
            ]
        ]
    },
    {
        "id": "0704.2644",
        "submitter": "Maxim Raginsky",
        "authors": "Maxim Raginsky",
        "title": "Joint universal lossy coding and identification of stationary mixing\n  sources",
        "comments": "5 pages, 1 eps figure; to appear in Proc. ISIT 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.LG math.IT",
        "license": null,
        "abstract": "  The problem of joint universal source coding and modeling, treated in the\ncontext of lossless codes by Rissanen, was recently generalized to fixed-rate\nlossy coding of finitely parametrized continuous-alphabet i.i.d. sources. We\nextend these results to variable-rate lossy block coding of stationary ergodic\nsources and show that, for bounded metric distortion measures, any finitely\nparametrized family of stationary sources satisfying suitable mixing,\nsmoothness and Vapnik-Chervonenkis learnability conditions admits universal\nschemes for joint lossy source coding and identification. We also give several\nexplicit examples of parametric sources satisfying the regularity conditions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 20 Apr 2007 01:25:22 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Raginsky",
                "Maxim",
                ""
            ]
        ]
    },
    {
        "id": "0704.2651",
        "submitter": "Lalitha Sankar",
        "authors": "Lalitha Sankar, Yingbin Liang, H. Vincent Poor, Narayan B. Mandayam",
        "title": "Opportunistic Communications in an Orthogonal Multiaccess Relay Channel",
        "comments": "To appear in the Proceedings of the 2007 IEEE International Symposium\n  on Information Theory, Nice, France, June 24 - 29, 2007",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2007.4557396",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  The problem of resource allocation is studied for a two-user fading\northogonal multiaccess relay channel (MARC) where both users (sources)\ncommunicate with a destination in the presence of a relay. A half-duplex relay\nis considered that transmits on a channel orthogonal to that used by the\nsources. The instantaneous fading state between every transmit-receive pair in\nthis network is assumed to be known at both the transmitter and receiver. Under\nan average power constraint at each source and the relay, the sum-rate for the\nachievable strategy of decode-and-forward (DF) is maximized over all power\nallocations (policies) at the sources and relay. It is shown that the sum-rate\nmaximizing policy exploits the multiuser fading diversity to reveal the\noptimality of opportunistic channel use by each user. A geometric\ninterpretation of the optimal power policy is also presented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 20 Apr 2007 04:02:33 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Sankar",
                "Lalitha",
                ""
            ],
            [
                "Liang",
                "Yingbin",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ],
            [
                "Mandayam",
                "Narayan B.",
                ""
            ]
        ]
    },
    {
        "id": "0704.2659",
        "submitter": "Chris Ng",
        "authors": "Chris T. K. Ng, Deniz Gunduz, Andrea J. Goldsmith, Elza Erkip",
        "title": "Minimum Expected Distortion in Gaussian Layered Broadcast Coding with\n  Successive Refinement",
        "comments": "To appear in the proceedings of the 2007 IEEE International Symposium\n  on Information Theory, Nice, France, June 24-29, 2007",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2007.4557165",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  A transmitter without channel state information (CSI) wishes to send a\ndelay-limited Gaussian source over a slowly fading channel. The source is coded\nin superimposed layers, with each layer successively refining the description\nin the previous one. The receiver decodes the layers that are supported by the\nchannel realization and reconstructs the source up to a distortion. In the\nlimit of a continuum of infinite layers, the optimal power distribution that\nminimizes the expected distortion is given by the solution to a set of linear\ndifferential equations in terms of the density of the fading distribution. In\nthe optimal power distribution, as SNR increases, the allocation over the\nhigher layers remains unchanged; rather the extra power is allocated towards\nthe lower layers. On the other hand, as the bandwidth ratio b (channel uses per\nsource symbol) tends to zero, the power distribution that minimizes expected\ndistortion converges to the power distribution that maximizes expected\ncapacity. While expected distortion can be improved by acquiring CSI at the\ntransmitter (CSIT) or by increasing diversity from the realization of\nindependent fading paths, at high SNR the performance benefit from diversity\nexceeds that from CSIT, especially when b is large.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 20 Apr 2007 07:41:35 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 24 Apr 2007 00:23:10 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Ng",
                "Chris T. K.",
                ""
            ],
            [
                "Gunduz",
                "Deniz",
                ""
            ],
            [
                "Goldsmith",
                "Andrea J.",
                ""
            ],
            [
                "Erkip",
                "Elza",
                ""
            ]
        ]
    },
    {
        "id": "0704.2668",
        "submitter": "Alex Smola J",
        "authors": "Le Song, Alex Smola, Arthur Gretton, Karsten Borgwardt, Justin Bedo",
        "title": "Supervised Feature Selection via Dependence Estimation",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  We introduce a framework for filtering features that employs the\nHilbert-Schmidt Independence Criterion (HSIC) as a measure of dependence\nbetween the features and the labels. The key idea is that good features should\nmaximise such dependence. Feature selection for various supervised learning\nproblems (including classification and regression) is unified under this\nframework, and the solutions can be approximated using a backward-elimination\nalgorithm. We demonstrate the usefulness of our method on both artificial and\nreal world datasets.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 20 Apr 2007 08:26:29 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Song",
                "Le",
                ""
            ],
            [
                "Smola",
                "Alex",
                ""
            ],
            [
                "Gretton",
                "Arthur",
                ""
            ],
            [
                "Borgwardt",
                "Karsten",
                ""
            ],
            [
                "Bedo",
                "Justin",
                ""
            ]
        ]
    },
    {
        "id": "0704.2680",
        "submitter": "Tobias Koch",
        "authors": "Tobias Koch, Amos Lapidoth and Paul P. Sotiriadis",
        "title": "A Channel that Heats Up",
        "comments": "to appear in Proceedings of the 2007 IEEE International Symposium on\n  Information Theory (ISIT), Nice, France",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  Motivated by on-chip communication, a channel model is proposed where the\nvariance of the additive noise depends on the weighted sum of the past channel\ninput powers. For this channel, an expression for the capacity per unit cost is\nderived, and it is shown that the expression holds also in the presence of\nfeedback.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 20 Apr 2007 10:26:53 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Koch",
                "Tobias",
                ""
            ],
            [
                "Lapidoth",
                "Amos",
                ""
            ],
            [
                "Sotiriadis",
                "Paul P.",
                ""
            ]
        ]
    },
    {
        "id": "0704.2725",
        "submitter": "Manuel Cebri\\'an",
        "authors": "Manuel Cebrian and Ivan Cantador",
        "title": "Exploiting Heavy Tails in Training Times of Multilayer Perceptrons: A\n  Case Study with the UCI Thyroid Disease Database",
        "comments": "8 pages, 4 figures, submitted for consideration to the \"Statistics\n  and Its Interface\" journal",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  The random initialization of weights of a multilayer perceptron makes it\npossible to model its training process as a Las Vegas algorithm, i.e. a\nrandomized algorithm which stops when some required training error is obtained,\nand whose execution time is a random variable. This modeling is used to perform\na case study on a well-known pattern recognition benchmark: the UCI Thyroid\nDisease Database. Empirical evidence is presented of the training time\nprobability distribution exhibiting a heavy tail behavior, meaning a big\nprobability mass of long executions. This fact is exploited to reduce the\ntraining time cost by applying two simple restart strategies. The first assumes\nfull knowledge of the distribution yielding a 40% cut down in expected time\nwith respect to the training without restarts. The second, assumes null\nknowledge, yielding a reduction ranging from 9% to 23%.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 20 Apr 2007 15:58:04 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 7 Dec 2007 03:06:49 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Cebrian",
                "Manuel",
                ""
            ],
            [
                "Cantador",
                "Ivan",
                ""
            ]
        ]
    },
    {
        "id": "0704.2778",
        "submitter": "Brooke Shrader",
        "authors": "Brooke Shrader and Anthony Ephremides",
        "title": "Random Access Broadcast: Stability and Throughput Analysis",
        "comments": "19 pages, 5 figures. Submitted as correspondence to IEEE Transactions\n  on Information Theory, Sept 2006. Revised April 2007",
        "journal-ref": "IEEE Transactions on Information Theory, vol. 53, no. 8, pp.\n  2915-2921, August 2007.",
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  A wireless network in which packets are broadcast to a group of receivers\nthrough use of a random access protocol is considered in this work. The\nrelation to previous work on networks of interacting queues is discussed and\nsubsequently, the stability and throughput regions of the system are analyzed\nand presented. A simple network of two source nodes and two destination nodes\nis considered first. The broadcast service process is analyzed assuming a\nchannel that allows for packet capture and multipacket reception. In this small\nnetwork, the stability and throughput regions are observed to coincide. The\nsame problem for a network with N sources and M destinations is considered\nnext. The channel model is simplified in that multipacket reception is no\nlonger permitted. Bounds on the stability region are developed using the\nconcept of stability rank and the throughput region of the system is compared\nto the bounds. Our results show that as the number of destination nodes\nincreases, the stability and throughput regions diminish. Additionally, a\nprevious conjecture that the stability and throughput regions coincide for a\nnetwork of arbitrarily many sources is supported for a broadcast scenario by\nthe results presented in this work.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 20 Apr 2007 19:17:43 GMT"
            }
        ],
        "update_date": "2007-11-05",
        "authors_parsed": [
            [
                "Shrader",
                "Brooke",
                ""
            ],
            [
                "Ephremides",
                "Anthony",
                ""
            ]
        ]
    },
    {
        "id": "0704.2786",
        "submitter": "Wenyi Zhang",
        "authors": "Wenyi Zhang, Shivaprasad Kotagiri, and J. Nicholas Laneman",
        "title": "Writing on Dirty Paper with Resizing and its Application to Quasi-Static\n  Fading Broadcast Channels",
        "comments": "To appear in IEEE International Symposium on Information Theory 2007",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2007.4557255",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  This paper studies a variant of the classical problem of ``writing on dirty\npaper'' in which the sum of the input and the interference, or dirt, is\nmultiplied by a random variable that models resizing, known to the decoder but\nnot to the encoder. The achievable rate of Costa's dirty paper coding (DPC)\nscheme is calculated and compared to the case of the decoder's also knowing the\ndirt. In the ergodic case, the corresponding rate loss vanishes asymptotically\nin the limits of both high and low signal-to-noise ratio (SNR), and is small at\nall finite SNR for typical distributions like Rayleigh, Rician, and Nakagami.\nIn the quasi-static case, the DPC scheme is lossless at all SNR in terms of\noutage probability. Quasi-static fading broadcast channels (BC) without\ntransmit channel state information (CSI) are investigated as an application of\nthe robustness properties. It is shown that the DPC scheme leads to an outage\nachievable rate region that strictly dominates that of time division.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 20 Apr 2007 20:29:32 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Zhang",
                "Wenyi",
                ""
            ],
            [
                "Kotagiri",
                "Shivaprasad",
                ""
            ],
            [
                "Laneman",
                "J. Nicholas",
                ""
            ]
        ]
    },
    {
        "id": "0704.2808",
        "submitter": "Aditya Ramamoorthy",
        "authors": "Aditya Ramamoorthy",
        "title": "Minimum cost distributed source coding over a network",
        "comments": "First version apppeared in the Proceedings of the 2007 IEEE\n  International Symposium on Information Theory, Nice, France, June 24 - 29,\n  2007. The second version is an expanded journal submission under\n  consideration at the IEEE Transactions on Information Theory",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.NI math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work considers the problem of transmitting multiple compressible sources\nover a network at minimum cost. The aim is to find the optimal rates at which\nthe sources should be compressed and the network flows using which they should\nbe transmitted so that the cost of the transmission is minimal. We consider\nnetworks with capacity constraints and linear cost functions. The problem is\ncomplicated by the fact that the description of the feasible rate region of\ndistributed source coding problems typically has a number of constraints that\nis exponential in the number of sources. This renders general purpose solvers\ninefficient. We present a framework in which these problems can be solved\nefficiently by exploiting the structure of the feasible rate regions coupled\nwith dual decomposition and optimization techniques such as the subgradient\nmethod and the proximal bundle method.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Apr 2007 17:41:35 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 12 Aug 2009 22:56:01 GMT"
            }
        ],
        "update_date": "2009-08-13",
        "authors_parsed": [
            [
                "Ramamoorthy",
                "Aditya",
                ""
            ]
        ]
    },
    {
        "id": "0704.2808",
        "submitter": "Aditya Ramamoorthy",
        "authors": "Aditya Ramamoorthy",
        "title": "Minimum cost distributed source coding over a network",
        "comments": "First version apppeared in the Proceedings of the 2007 IEEE\n  International Symposium on Information Theory, Nice, France, June 24 - 29,\n  2007. The second version is an expanded journal submission under\n  consideration at the IEEE Transactions on Information Theory",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.NI math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work considers the problem of transmitting multiple compressible sources\nover a network at minimum cost. The aim is to find the optimal rates at which\nthe sources should be compressed and the network flows using which they should\nbe transmitted so that the cost of the transmission is minimal. We consider\nnetworks with capacity constraints and linear cost functions. The problem is\ncomplicated by the fact that the description of the feasible rate region of\ndistributed source coding problems typically has a number of constraints that\nis exponential in the number of sources. This renders general purpose solvers\ninefficient. We present a framework in which these problems can be solved\nefficiently by exploiting the structure of the feasible rate regions coupled\nwith dual decomposition and optimization techniques such as the subgradient\nmethod and the proximal bundle method.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Apr 2007 17:41:35 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 12 Aug 2009 22:56:01 GMT"
            }
        ],
        "update_date": "2009-08-13",
        "authors_parsed": [
            [
                "Ramamoorthy",
                "Aditya",
                ""
            ]
        ]
    },
    {
        "id": "0704.2811",
        "submitter": "Nandakishore Santhi",
        "authors": "Nandakishore Santhi",
        "title": "On Algebraic Decoding of $q$-ary Reed-Muller and Product-Reed-Solomon\n  Codes",
        "comments": "5 pages, 5 figures, to be presented at 2007 IEEE International\n  Symposium on Information Theory, Nice, France (ISIT 2007)",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2007.4557130",
        "report-no": "LA-UR-07-0469",
        "categories": "cs.IT cs.DM math.IT",
        "license": null,
        "abstract": "  We consider a list decoding algorithm recently proposed by Pellikaan-Wu\n\\cite{PW2005} for $q$-ary Reed-Muller codes $\\mathcal{RM}_q(\\ell, m, n)$ of\nlength $n \\leq q^m$ when $\\ell \\leq q$. A simple and easily accessible\ncorrectness proof is given which shows that this algorithm achieves a relative\nerror-correction radius of $\\tau \\leq (1 - \\sqrt{{\\ell q^{m-1}}/{n}})$. This is\nan improvement over the proof using one-point Algebraic-Geometric codes given\nin \\cite{PW2005}. The described algorithm can be adapted to decode\nProduct-Reed-Solomon codes.\n  We then propose a new low complexity recursive algebraic decoding algorithm\nfor Reed-Muller and Product-Reed-Solomon codes. Our algorithm achieves a\nrelative error correction radius of $\\tau \\leq \\prod_{i=1}^m (1 -\n\\sqrt{k_i/q})$. This technique is then proved to outperform the Pellikaan-Wu\nmethod in both complexity and error correction radius over a wide range of code\nrates.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 22 Apr 2007 00:22:46 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Santhi",
                "Nandakishore",
                ""
            ]
        ]
    },
    {
        "id": "0704.2841",
        "submitter": "Lun Dong",
        "authors": "Athina P. Petropulu, Lun Dong, H. Vincent Poor",
        "title": "A High-Throughput Cross-Layer Scheme for Distributed Wireless Ad Hoc\n  Networks",
        "comments": "6 pages, Appeared in the Proceedings of the 41st Annual Conference on\n  Information Sciences and Systems, John Hopkins University, March 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In wireless ad hoc networks, distributed nodes can collaboratively form an\nantenna array for long-distance communications to achieve high energy\nefficiency. In recent work, Ochiai, et al., have shown that such collaborative\nbeamforming can achieve a statistically nice beampattern with a narrow main\nlobe and low sidelobes. However, the process of collaboration introduces\nsignificant delay, since all collaborating nodes need access to the same\ninformation. In this paper, a technique that significantly reduces the\ncollaboration overhead is proposed. It consists of two phases. In the first\nphase, nodes transmit locally in a random access fashion. Collisions, when they\noccur, are viewed as linear mixtures of the collided packets. In the second\nphase, a set of cooperating nodes acts as a distributed antenna system and\nbeamform the received analog waveform to one or more faraway destinations. This\nstep requires multiplication of the received analog waveform by a complex\nnumber, which is independently computed by each cooperating node, and which\nenables separation of the collided packets based on their final destination.\nThe scheme requires that each node has global knowledge of the network\ncoordinates. The proposed scheme can achieve high throughput, which in certain\ncases exceeds one.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 21 Apr 2007 17:44:24 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Petropulu",
                "Athina P.",
                ""
            ],
            [
                "Dong",
                "Lun",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0704.2857",
        "submitter": "Andrea Montanari",
        "authors": "Andrea Montanari and Rudiger Urbanke",
        "title": "Modern Coding Theory: The Statistical Mechanics and Computer Science\n  Point of View",
        "comments": "Lectures at Les Houches Summer School on `Complex Systems', July\n  2006, 44 pages, 25 ps figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cond-mat.stat-mech math.IT",
        "license": null,
        "abstract": "  These are the notes for a set of lectures delivered by the two authors at the\nLes Houches Summer School on `Complex Systems' in July 2006. They provide an\nintroduction to the basic concepts in modern (probabilistic) coding theory,\nhighlighting connections with statistical mechanics. We also stress common\nconcepts with other disciplines dealing with similar problems that can be\ngenerically referred to as `large graphical models'.\n  While most of the lectures are devoted to the classical channel coding\nproblem over simple memoryless channels, we present a discussion of more\ncomplex channel models. We conclude with an overview of the main open\nchallenges in the field.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 22 Apr 2007 01:57:03 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Montanari",
                "Andrea",
                ""
            ],
            [
                "Urbanke",
                "Rudiger",
                ""
            ]
        ]
    },
    {
        "id": "0704.2926",
        "submitter": "Lawrence Ong",
        "authors": "Lawrence Ong, Mehul Motani",
        "title": "Optimal Routing for the Gaussian Multiple-Relay Channel with\n  Decode-and-Forward",
        "comments": "Accepted and to be presented at the 2007 IEEE International Symposium\n  on Information Theory (ISIT 2007), Acropolis Congress and Exhibition Center,\n  Nice, France, June 24-29 2007",
        "journal-ref": "Proceedings of the 2007 IEEE International Symposium on\n  Information Theory (ISIT 2007), Acropolis Congress and Exhibition Center,\n  Nice, France, pp. 1061-1065, Jun. 24-29 2007.",
        "doi": "10.1109/ISIT.2007.4557364",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In this paper, we study a routing problem on the Gaussian multiple relay\nchannel, in which nodes employ a decode-and-forward coding strategy. We are\ninterested in routes for the information flow through the relays that achieve\nthe highest DF rate. We first construct an algorithm that provably finds\noptimal DF routes. As the algorithm runs in factorial time in the worst case,\nwe propose a polynomial time heuristic algorithm that finds an optimal route\nwith high probability. We demonstrate that that the optimal (and near optimal)\nDF routes are good in practice by simulating a distributed DF coding scheme\nusing low density parity check codes with puncturing and incremental\nredundancy.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Apr 2007 06:33:38 GMT"
            }
        ],
        "update_date": "2010-04-15",
        "authors_parsed": [
            [
                "Ong",
                "Lawrence",
                ""
            ],
            [
                "Motani",
                "Mehul",
                ""
            ]
        ]
    },
    {
        "id": "0704.3019",
        "submitter": "Oliver Henkel",
        "authors": "Oliver Henkel",
        "title": "Arbitrary Rate Permutation Modulation for the Gaussian Channel",
        "comments": "to appear in the Proceedings of the International Symposium of\n  Information Theory (ISIT 2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In this paper non-group permutation modulated sequences for the Gaussian\nchannel are considered. Without the restriction to group codes rather than\nsubsets of group codes, arbitrary rates are achievable. The code construction\nutilizes the known optimal group constellations to ensure at least the same\nperformance but exploit the Gray code ordering structure of multiset\npermutations as a selection criterion at the decoder. The decoder achieves near\nmaximum likelihood performance at low computational cost and low additional\nmemory requirements at the receiver.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Apr 2007 14:31:17 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Henkel",
                "Oliver",
                ""
            ]
        ]
    },
    {
        "id": "0704.3035",
        "submitter": "Ender Tekin",
        "authors": "Ender Tekin, Aylin Yener",
        "title": "Achievable Rates for Two-Way Wire-Tap Channels",
        "comments": "International Symposium on Information Theory (ISIT) 2007, June 24-29",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.CR math.IT",
        "license": null,
        "abstract": "  We consider two-way wire-tap channels, where two users are communicating with\neach other in the presence of an eavesdropper, who has access to the\ncommunications through a multiple-access channel. We find achievable rates for\ntwo different scenarios, the Gaussian two-way wire-tap channel, (GTW-WT), and\nthe binary additive two-way wire-tap channel, (BATW-WT). It is shown that the\ntwo-way channels inherently provide a unique advantage for wire-tapped\nscenarios, as the users know their own transmitted signals and in effect help\nencrypt the other user's messages, similar to a one-time pad. We compare the\nachievable rates to that of the Gaussian multiple-access wire-tap channel\n(GMAC-WT) to illustrate this advantage.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Apr 2007 19:26:59 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Tekin",
                "Ender",
                ""
            ],
            [
                "Yener",
                "Aylin",
                ""
            ]
        ]
    },
    {
        "id": "0704.3035",
        "submitter": "Ender Tekin",
        "authors": "Ender Tekin, Aylin Yener",
        "title": "Achievable Rates for Two-Way Wire-Tap Channels",
        "comments": "International Symposium on Information Theory (ISIT) 2007, June 24-29",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.CR math.IT",
        "license": null,
        "abstract": "  We consider two-way wire-tap channels, where two users are communicating with\neach other in the presence of an eavesdropper, who has access to the\ncommunications through a multiple-access channel. We find achievable rates for\ntwo different scenarios, the Gaussian two-way wire-tap channel, (GTW-WT), and\nthe binary additive two-way wire-tap channel, (BATW-WT). It is shown that the\ntwo-way channels inherently provide a unique advantage for wire-tapped\nscenarios, as the users know their own transmitted signals and in effect help\nencrypt the other user's messages, similar to a one-time pad. We compare the\nachievable rates to that of the Gaussian multiple-access wire-tap channel\n(GMAC-WT) to illustrate this advantage.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Apr 2007 19:26:59 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Tekin",
                "Ender",
                ""
            ],
            [
                "Yener",
                "Aylin",
                ""
            ]
        ]
    },
    {
        "id": "0704.3094",
        "submitter": "Olympia Hadjiliadis",
        "authors": "Olympia Hadjiliadis and H.Vincent Poor",
        "title": "Detection of two-sided alternatives in a Brownian motion model",
        "comments": "4 pages, to appear in the Proceedings of the 56th Session of the\n  International Statistical Institute, Lisbon, Portugal, August 22- 29, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  This work examines the problem of sequential detection of a change in the\ndrift of a Brownian motion in the case of two-sided alternatives. Applications\nto real life situations in which two-sided changes can occur are discussed.\nTraditionally, 2-CUSUM stopping rules have been used for this problem due to\ntheir asymptotically optimal character as the mean time between false alarms\ntends to $\\infty$. In particular, attention has focused on 2-CUSUM harmonic\nmean rules due to the simplicity in calculating their first moments. In this\npaper, we derive closed-form expressions for the first moment of a general\n2-CUSUM stopping rule. We use these expressions to obtain explicit upper and\nlower bounds for it. Moreover, we derive an expression for the rate of change\nof this first moment as one of the threshold parameters changes. Based on these\nexpressions we obtain explicit upper and lower bounds to this rate of change.\nUsing these expressions we are able to find the best 2-CUSUM stopping rule with\nrespect to the extended Lorden criterion. In fact, we demonstrate not only the\nexistence but also the uniqueness of the best 2-CUSUM stopping both in the case\nof a symmetric change and in the case of a non-symmetric case. Furthermore, we\ndiscuss the existence of a modification of the 2-CUSUM stopping rule that has a\nstrictly better performance than its classical 2-CUSUM counterpart for small\nvalues of the mean time between false alarms. We conclude with a discussion on\nthe open problem of strict optimality in the case of two-sided alternatives.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Apr 2007 22:28:30 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Hadjiliadis",
                "Olympia",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0704.3120",
        "submitter": "Oliver Henkel",
        "authors": "Oliver Henkel",
        "title": "Space Time Codes from Permutation Codes",
        "comments": null,
        "journal-ref": "Proc. IEEE GlobeCom, San Francisco, California, Nov. 2006",
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  A new class of space time codes with high performance is presented. The code\ndesign utilizes tailor-made permutation codes, which are known to have large\nminimal distances as spherical codes. A geometric connection between spherical\nand space time codes has been used to translate them into the final space time\ncodes. Simulations demonstrate that the performance increases with the block\nlengths, a result that has been conjectured already in previous work. Further,\nthe connection to permutation codes allows for moderate complex en-/decoding\nalgorithms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Apr 2007 05:45:30 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Henkel",
                "Oliver",
                ""
            ]
        ]
    },
    {
        "id": "0704.3141",
        "submitter": "Evgueni Petrov",
        "authors": "Evgueni Petrov",
        "title": "Algorithm for Evaluation of the Interval Power Function of Unconstrained\n  Arguments",
        "comments": "3 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS",
        "license": null,
        "abstract": "  We describe an algorithm for evaluation of the interval extension of the\npower function of variables x and y given by the expression x^y. Our algorithm\nreduces the general case to the case of non-negative bases.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Apr 2007 08:33:52 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Petrov",
                "Evgueni",
                ""
            ]
        ]
    },
    {
        "id": "0704.3157",
        "submitter": "Giorgio Terracina",
        "authors": "Giorgio Terracina, Nicola Leone, Vincenzino Lio, Claudio Panetta",
        "title": "Experimenting with recursive queries in database and logic programming\n  systems",
        "comments": "To appear in Theory and Practice of Logic Programming (TPLP)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.DB",
        "license": null,
        "abstract": "  This paper considers the problem of reasoning on massive amounts of (possibly\ndistributed) data. Presently, existing proposals show some limitations: {\\em\n(i)} the quantity of data that can be handled contemporarily is limited, due to\nthe fact that reasoning is generally carried out in main-memory; {\\em (ii)} the\ninteraction with external (and independent) DBMSs is not trivial and, in\nseveral cases, not allowed at all; {\\em (iii)} the efficiency of present\nimplementations is still not sufficient for their utilization in complex\nreasoning tasks involving massive amounts of data. This paper provides a\ncontribution in this setting; it presents a new system, called DLV$^{DB}$,\nwhich aims to solve these problems. Moreover, the paper reports the results of\na thorough experimental analysis we have carried out for comparing our system\nwith several state-of-the-art systems (both logic and databases) on some\nclassical deductive problems; the other tested systems are: LDL++, XSB, Smodels\nand three top-level commercial DBMSs. DLV$^{DB}$ significantly outperforms even\nthe commercial Database Systems on recursive queries. To appear in Theory and\nPractice of Logic Programming (TPLP)\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Apr 2007 10:58:40 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Terracina",
                "Giorgio",
                ""
            ],
            [
                "Leone",
                "Nicola",
                ""
            ],
            [
                "Lio",
                "Vincenzino",
                ""
            ],
            [
                "Panetta",
                "Claudio",
                ""
            ]
        ]
    },
    {
        "id": "0704.3157",
        "submitter": "Giorgio Terracina",
        "authors": "Giorgio Terracina, Nicola Leone, Vincenzino Lio, Claudio Panetta",
        "title": "Experimenting with recursive queries in database and logic programming\n  systems",
        "comments": "To appear in Theory and Practice of Logic Programming (TPLP)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.DB",
        "license": null,
        "abstract": "  This paper considers the problem of reasoning on massive amounts of (possibly\ndistributed) data. Presently, existing proposals show some limitations: {\\em\n(i)} the quantity of data that can be handled contemporarily is limited, due to\nthe fact that reasoning is generally carried out in main-memory; {\\em (ii)} the\ninteraction with external (and independent) DBMSs is not trivial and, in\nseveral cases, not allowed at all; {\\em (iii)} the efficiency of present\nimplementations is still not sufficient for their utilization in complex\nreasoning tasks involving massive amounts of data. This paper provides a\ncontribution in this setting; it presents a new system, called DLV$^{DB}$,\nwhich aims to solve these problems. Moreover, the paper reports the results of\na thorough experimental analysis we have carried out for comparing our system\nwith several state-of-the-art systems (both logic and databases) on some\nclassical deductive problems; the other tested systems are: LDL++, XSB, Smodels\nand three top-level commercial DBMSs. DLV$^{DB}$ significantly outperforms even\nthe commercial Database Systems on recursive queries. To appear in Theory and\nPractice of Logic Programming (TPLP)\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Apr 2007 10:58:40 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Terracina",
                "Giorgio",
                ""
            ],
            [
                "Leone",
                "Nicola",
                ""
            ],
            [
                "Lio",
                "Vincenzino",
                ""
            ],
            [
                "Panetta",
                "Claudio",
                ""
            ]
        ]
    },
    {
        "id": "0704.3199",
        "submitter": "Enrico Paolini",
        "authors": "E. Paolini, M. Fossorier, M. Chiani",
        "title": "Generalized Stability Condition for Generalized and Doubly-Generalized\n  LDPC Codes",
        "comments": "5 pages, 2 figures, to appear in Proc. of IEEE ISIT 2007",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2007.4557440",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In this paper, the stability condition for low-density parity-check (LDPC)\ncodes on the binary erasure channel (BEC) is extended to generalized LDPC\n(GLDPC) codes and doublygeneralized LDPC (D-GLDPC) codes. It is proved that, in\nboth cases, the stability condition only involves the component codes with\nminimum distance 2. The stability condition for GLDPC codes is always expressed\nas an upper bound to the decoding threshold. This is not possible for D-GLDPC\ncodes, unless all the generalized variable nodes have minimum distance at least\n3. Furthermore, a condition called derivative matching is defined in the paper.\nThis condition is sufficient for a GLDPC or DGLDPC code to achieve the\nstability condition with equality. If this condition is satisfied, the\nthreshold of D-GLDPC codes (whose generalized variable nodes have all minimum\ndistance at least 3) and GLDPC codes can be expressed in closed form.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Apr 2007 14:11:58 GMT"
            }
        ],
        "update_date": "2016-11-15",
        "authors_parsed": [
            [
                "Paolini",
                "E.",
                ""
            ],
            [
                "Fossorier",
                "M.",
                ""
            ],
            [
                "Chiani",
                "M.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3228",
        "submitter": "Thomas Silverston",
        "authors": "Thomas Silverston, Olivier Fourmaux and Kave Salamatian",
        "title": "Characterization of P2P IPTV Traffic: Scaling Analysis",
        "comments": "27p, submitted to a conference",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.MM",
        "license": null,
        "abstract": "  P2P IPTV applications arise on the Internet and will be massively used in the\nfuture. It is expected that P2P IPTV will contribute to increase the overall\nInternet traffic. In this context, it is important to measure the impact of P2P\nIPTV on the networks and to characterize this traffic. Dur- ing the 2006 FIFA\nWorld Cup, we performed an extensive measurement campaign. We measured network\ntraffic generated by broadcasting soc- cer games by the most popular P2P IPTV\napplications, namely PPLive, PPStream, SOPCast and TVAnts. From the collected\ndata, we charac- terized the P2P IPTV traffic structure at different time\nscales by using wavelet based transform method. To the best of our knowledge,\nthis is the first work, which presents a complete multiscale analysis of the\nP2P IPTV traffic. Our results show that the scaling properties of the TCP\ntraffic present periodic behavior whereas the UDP traffic is stationary and\nlead to long- range depedency characteristics. For all the applications, the\ndownload traffic has different characteristics than the upload traffic. The\nsignaling traffic has a significant impact on the download traffic but it has\nnegligible impact on the upload. Both sides of the traffic and its granularity\nhas to be taken into account to design accurate P2P IPTV traffic models.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Apr 2007 16:18:13 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 17 Jul 2007 14:26:05 GMT"
            }
        ],
        "update_date": "2007-07-17",
        "authors_parsed": [
            [
                "Silverston",
                "Thomas",
                ""
            ],
            [
                "Fourmaux",
                "Olivier",
                ""
            ],
            [
                "Salamatian",
                "Kave",
                ""
            ]
        ]
    },
    {
        "id": "0704.3241",
        "submitter": "Marco Lops",
        "authors": "Daniele Angelosante, Ezio Biglieri, Marco Lops",
        "title": "Neighbor Discovery in Wireless Networks:A Multiuser-Detection Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  We examine the problem of determining which nodes are neighbors of a given\none in a wireless network. We consider an unsupervised network operating on a\nfrequency-flat Gaussian channel, where $K+1$ nodes associate their identities\nto nonorthogonal signatures, transmitted at random times, synchronously, and\nindependently. A number of neighbor-discovery algorithms, based on different\noptimization criteria, are introduced and analyzed. Numerical results show how\nreduced-complexity algorithms can achieve a satisfactory performance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Apr 2007 16:46:02 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Angelosante",
                "Daniele",
                ""
            ],
            [
                "Biglieri",
                "Ezio",
                ""
            ],
            [
                "Lops",
                "Marco",
                ""
            ]
        ]
    },
    {
        "id": "0704.3268",
        "submitter": "Koray Karahaliloglu",
        "authors": "Koray Karahaliloglu",
        "title": "2D Path Solutions from a Single Layer Excitable CNN Model",
        "comments": "24 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.RO cs.NE",
        "license": null,
        "abstract": "  An easily implementable path solution algorithm for 2D spatial problems,\nbased on excitable/programmable characteristics of a specific cellular\nnonlinear network (CNN) model is presented and numerically investigated. The\nnetwork is a single layer bioinspired model which was also implemented in CMOS\ntechnology. It exhibits excitable characteristics with regionally bistable\ncells. The related response realizes propagations of trigger autowaves, where\nthe excitable mode can be globally preset and reset. It is shown that, obstacle\ndistributions in 2D space can also be directly mapped onto the coupled cell\narray in the network. Combining these two features, the network model can serve\nas the main block in a 2D path computing processor. The related algorithm and\nconfigurations are numerically experimented with circuit level parameters and\nperformance estimations are also presented. The simplicity of the model also\nallows alternative technology and device level implementation, which may become\ncritical in autonomous processor design of related micro or nanoscale robotic\napplications.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Apr 2007 20:20:46 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Karahaliloglu",
                "Koray",
                ""
            ]
        ]
    },
    {
        "id": "0704.3287",
        "submitter": "N. Raj Rao",
        "authors": "N. Raj Rao, Alan Edelman",
        "title": "Sample size cognizant detection of signals in white noise",
        "comments": "To appear in the Proceedings of the 8th IEEE International Workshop\n  on Signal Processing Advances in Wireless Communications (SPAWC), Helsinki,\n  Finland, June 17-20, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  The detection and estimation of signals in noisy, limited data is a problem\nof interest to many scientific and engineering communities. We present a\ncomputationally simple, sample eigenvalue based procedure for estimating the\nnumber of high-dimensional signals in white noise when there are relatively few\nsamples. We highlight a fundamental asymptotic limit of sample eigenvalue based\ndetection of weak high-dimensional signals from a limited sample size and\ndiscuss its implication for the detection of two closely spaced signals.\n  This motivates our heuristic definition of the 'effective number of\nidentifiable signals.' Numerical simulations are used to demonstrate the\nconsistency of the algorithm with respect to the effective number of signals\nand the superior performance of the algorithm with respect to Wax and Kailath's\n\"asymptotically consistent\" MDL based estimator.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Apr 2007 00:23:20 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Rao",
                "N. Raj",
                ""
            ],
            [
                "Edelman",
                "Alan",
                ""
            ]
        ]
    },
    {
        "id": "0704.3292",
        "submitter": "Zhu Han",
        "authors": "Zhu Han and H. Vincent Poor",
        "title": "Coalition Games with Cooperative Transmission: A Cure for the Curse of\n  Boundary Nodes in Selfish Packet-Forwarding Wireless Networks",
        "comments": null,
        "journal-ref": "in the Proceedings of the 5th International Symposium on Modeling\n  and Optimization in Mobile Ad Hoc and Wireless Networks, WiOpt07, Limassol,\n  Cyprus, April 16-20, 2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In wireless packet-forwarding networks with selfish nodes, applications of a\nrepeated game can induce the nodes to forward each others' packets, so that the\nnetwork performance can be improved. However, the nodes on the boundary of such\nnetworks cannot benefit from this strategy, as the other nodes do not depend on\nthem. This problem is sometimes known as the curse of the boundary nodes. To\novercome this problem, an approach based on coalition games is proposed, in\nwhich the boundary nodes can use cooperative transmission to help the backbone\nnodes in the middle of the network. In return, the backbone nodes are willing\nto forward the boundary nodes' packets. The stability of the coalitions is\nstudied using the concept of a core. Then two types of fairness, namely, the\nmin-max fairness using nucleolus and the average fairness using the Shapley\nfunction are investigated. Finally, a protocol is designed using both repeated\ngames and coalition games. Simulation results show how boundary nodes and\nbackbone nodes form coalitions together according to different fairness\ncriteria. The proposed protocol can improve the network connectivity by about\n50%, compared with pure repeated game schemes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Apr 2007 15:37:22 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 25 Apr 2007 21:15:48 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Han",
                "Zhu",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0704.3359",
        "submitter": "Alex Smola J",
        "authors": "Quoc Le and Alexander Smola",
        "title": "Direct Optimization of Ranking Measures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IR cs.AI",
        "license": null,
        "abstract": "  Web page ranking and collaborative filtering require the optimization of\nsophisticated performance measures. Current Support Vector approaches are\nunable to optimize them directly and focus on pairwise comparisons instead. We\npresent a new approach which allows direct optimization of the relevant loss\nfunctions. This is achieved via structured estimation in Hilbert spaces. It is\nmost related to Max-Margin-Markov networks optimization of multivariate\nperformance measures. Key to our approach is that during training the ranking\nproblem can be viewed as a linear assignment problem, which can be solved by\nthe Hungarian Marriage algorithm. At test time, a sort operation is sufficient,\nas our algorithm assigns a relevance score to every (document, query) pair.\nExperiments show that the our algorithm is fast and that it works very well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Apr 2007 12:36:55 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Le",
                "Quoc",
                ""
            ],
            [
                "Smola",
                "Alexander",
                ""
            ]
        ]
    },
    {
        "id": "0704.3391",
        "submitter": "Zhu Han",
        "authors": "Zhu Han and H. Vincent Poor",
        "title": "Lifetime Improvement in Wireless Sensor Networks via Collaborative\n  Beamforming and Cooperative Transmission",
        "comments": "Invited paper to appear in the IEE Proceedings: Microwaves, Antennas\n  and Propagation, Special Issue on Antenna Systems and Propagation for Future\n  Wireless Communications",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  Collaborative beamforming (CB) and cooperative transmission (CT) have\nrecently emerged as communication techniques that can make effective use of\ncollaborative/cooperative nodes to create a virtual\nmultiple-input/multiple-output (MIMO) system. Extending the lifetime of\nnetworks composed of battery-operated nodes is a key issue in the design and\noperation of wireless sensor networks. This paper considers the effects on\nnetwork lifetime of allowing closely located nodes to use CB/CT to reduce the\nload or even to avoid packet-forwarding requests to nodes that have critical\nbattery life. First, the effectiveness of CB/CT in improving the signal\nstrength at a faraway destination using energy in nearby nodes is studied.\nThen, the performance improvement obtained by this technique is analyzed for a\nspecial 2D disk case. Further, for general networks in which\ninformation-generation rates are fixed, a new routing problem is formulated as\na linear programming problem, while for other general networks, the cost for\nrouting is dynamically adjusted according to the amount of energy remaining and\nthe effectiveness of CB/CT. From the analysis and the simulation results, it is\nseen that the proposed method can reduce the payloads of energy-depleting nodes\nby about 90% in the special case network considered and improve the lifetimes\nof general networks by about 10%, compared with existing techniques.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Apr 2007 15:14:32 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 26 Apr 2007 02:49:33 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Han",
                "Zhu",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0704.3395",
        "submitter": "Marko A. Rodriguez",
        "authors": "Marko A. Rodriguez",
        "title": "General-Purpose Computing on a Semantic Network Substrate",
        "comments": null,
        "journal-ref": "Emergent Web Intelligence: Advanced Semantic Technologies,\n  Advanced Information and Knowledge Processing series, Springer-Verlag, pages\n  57-104, ISBN:978-1-84996-076-2, June 2010",
        "doi": null,
        "report-no": "LA-UR-07-2885",
        "categories": "cs.AI cs.PL",
        "license": "http://creativecommons.org/licenses/publicdomain/",
        "abstract": "  This article presents a model of general-purpose computing on a semantic\nnetwork substrate. The concepts presented are applicable to any semantic\nnetwork representation. However, due to the standards and technological\ninfrastructure devoted to the Semantic Web effort, this article is presented\nfrom this point of view. In the proposed model of computing, the application\nprogramming interface, the run-time program, and the state of the computing\nvirtual machine are all represented in the Resource Description Framework\n(RDF). The implementation of the concepts presented provides a practical\ncomputing paradigm that leverages the highly-distributed and standardized\nrepresentational-layer of the Semantic Web.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Apr 2007 15:37:52 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 4 Oct 2007 20:08:21 GMT"
            },
            {
                "version": "v3",
                "created": "Sun, 7 Oct 2007 21:44:01 GMT"
            },
            {
                "version": "v4",
                "created": "Sun, 6 Jun 2010 05:29:22 GMT"
            }
        ],
        "update_date": "2010-06-08",
        "authors_parsed": [
            [
                "Rodriguez",
                "Marko A.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3395",
        "submitter": "Marko A. Rodriguez",
        "authors": "Marko A. Rodriguez",
        "title": "General-Purpose Computing on a Semantic Network Substrate",
        "comments": null,
        "journal-ref": "Emergent Web Intelligence: Advanced Semantic Technologies,\n  Advanced Information and Knowledge Processing series, Springer-Verlag, pages\n  57-104, ISBN:978-1-84996-076-2, June 2010",
        "doi": null,
        "report-no": "LA-UR-07-2885",
        "categories": "cs.AI cs.PL",
        "license": "http://creativecommons.org/licenses/publicdomain/",
        "abstract": "  This article presents a model of general-purpose computing on a semantic\nnetwork substrate. The concepts presented are applicable to any semantic\nnetwork representation. However, due to the standards and technological\ninfrastructure devoted to the Semantic Web effort, this article is presented\nfrom this point of view. In the proposed model of computing, the application\nprogramming interface, the run-time program, and the state of the computing\nvirtual machine are all represented in the Resource Description Framework\n(RDF). The implementation of the concepts presented provides a practical\ncomputing paradigm that leverages the highly-distributed and standardized\nrepresentational-layer of the Semantic Web.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Apr 2007 15:37:52 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 4 Oct 2007 20:08:21 GMT"
            },
            {
                "version": "v3",
                "created": "Sun, 7 Oct 2007 21:44:01 GMT"
            },
            {
                "version": "v4",
                "created": "Sun, 6 Jun 2010 05:29:22 GMT"
            }
        ],
        "update_date": "2010-06-08",
        "authors_parsed": [
            [
                "Rodriguez",
                "Marko A.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3396",
        "submitter": "Zhu Han",
        "authors": "Zhu Han and H. Vincent Poor",
        "title": "Lifetime Improvement of Wireless Sensor Networks by Collaborative\n  Beamforming and Cooperative Transmission",
        "comments": "to appear, in the proceedings of IEEE International Conference on\n  Communications, Glasgow, Scotland, 24-28 June 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  Extending network lifetime of battery-operated devices is a key design issue\nthat allows uninterrupted information exchange among distributive nodes in\nwireless sensor networks. Collaborative beamforming (CB) and cooperative\ntransmission (CT) have recently emerged as new communication techniques that\nenable and leverage effective resource sharing among collaborative/cooperative\nnodes. In this paper, we seek to maximize the lifetime of sensor networks by\nusing the new idea that closely located nodes can use CB/CT to reduce the load\nor even avoid packet forwarding requests to nodes that have critical battery\nlife. First, we study the effectiveness of CB/CT to improve the signal strength\nat a faraway destination using energy in nearby nodes. Then, a 2D disk case is\nanalyzed to assess the resulting performance improvement. For general networks,\nif information-generation rates are fixed, the new routing problem is\nformulated as a linear programming problem; otherwise, the cost for routing is\ndynamically adjusted according to the amount of energy remaining and the\neffectiveness of CB/CT. From the analysis and simulation results, it is seen\nthat the proposed schemes can improve the lifetime by about 90% in the 2D disk\nnetwork and by about 10% in the general networks, compared to existing schemes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Apr 2007 15:55:27 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Han",
                "Zhu",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0704.3399",
        "submitter": "Zhu Han",
        "authors": "Zhu Han, Xin Zhang, and H. Vincent Poor",
        "title": "Cooperative Transmission Protocols with High Spectral Efficiency and\n  High Diversity Order Using Multiuser Detection and Network Coding",
        "comments": "to appear, in the proceedings of IEEE International Conference on\n  Communications, Glasgow, Scotland, 24-28 June 2007",
        "journal-ref": null,
        "doi": "10.1109/ICC.2007.698",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  Cooperative transmission is an emerging communication technique that takes\nadvantages of the broadcast nature of wireless channels. However, due to low\nspectral efficiency and the requirement of orthogonal channels, its potential\nfor use in future wireless networks is limited. In this paper, by making use of\nmultiuser detection (MUD) and network coding, cooperative transmission\nprotocols with high spectral efficiency, diversity order, and coding gain are\ndeveloped. Compared with the traditional cooperative transmission protocols\nwith single-user detection, in which the diversity gain is only for one source\nuser, the proposed MUD cooperative transmission protocols have the merits that\nthe improvement of one user's link can also benefit the other users. In\naddition, using MUD at the relay provides an environment in which network\ncoding can be employed. The coding gain and high diversity order can be\nobtained by fully utilizing the link between the relay and the destination.\n  From the analysis and simulation results, it is seen that the proposed\nprotocols achieve higher diversity gain, better asymptotic efficiency, and\nlower bit error rate, compared to traditional MUD and to existing cooperative\ntransmission protocols.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Apr 2007 15:58:10 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Han",
                "Zhu",
                ""
            ],
            [
                "Zhang",
                "Xin",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0704.3402",
        "submitter": "Pedro Coronel",
        "authors": "Pedro Coronel and Helmut B\\\"olcskei",
        "title": "Diversity-Multiplexing Tradeoff in Selective-Fading MIMO Channels",
        "comments": "To be presented at IEEE Int. Symp. Inf. Theory 2007, Nice, France",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  We establish the optimal diversity-multiplexing (DM) tradeoff of coherent\ntime, frequency and time-frequency selective-fading MIMO channels and provide a\ncode design criterion for DM-tradeoff optimality. Our results are based on the\nanalysis of the \"Jensen channel\" associated to a given selective-fading MIMO\nchannel. While the original problem seems analytically intractable due to the\nmutual information being a sum of correlated random variables, the Jensen\nchannel is equivalent to the original channel in the sense of the DM-tradeoff\nand lends itself nicely to analytical treatment. Finally, as a consequence of\nour results, we find that the classical rank criterion for space-time code\ndesign (in selective-fading MIMO channels) ensures optimality in the sense of\nthe DM-tradeoff.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Apr 2007 16:09:06 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Coronel",
                "Pedro",
                ""
            ],
            [
                "B\u00f6lcskei",
                "Helmut",
                ""
            ]
        ]
    },
    {
        "id": "0704.3405",
        "submitter": "Shuguang Cui",
        "authors": "Shuguang Cui and Jinjun Xiao and Andrea Goldsmith and Zhi-Quan Luo and\n  H. Vincent Poor",
        "title": "Estimation Diversity and Energy Efficiency in Distributed Sensing",
        "comments": "To appear at IEEE Transactions on Signal Processing",
        "journal-ref": null,
        "doi": "10.1109/TSP.2007.896019",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  Distributed estimation based on measurements from multiple wireless sensors\nis investigated. It is assumed that a group of sensors observe the same\nquantity in independent additive observation noises with possibly different\nvariances. The observations are transmitted using amplify-and-forward (analog)\ntransmissions over non-ideal fading wireless channels from the sensors to a\nfusion center, where they are combined to generate an estimate of the observed\nquantity. Assuming that the Best Linear Unbiased Estimator (BLUE) is used by\nthe fusion center, the equal-power transmission strategy is first discussed,\nwhere the system performance is analyzed by introducing the concept of\nestimation outage and estimation diversity, and it is shown that there is an\nachievable diversity gain on the order of the number of sensors. The optimal\npower allocation strategies are then considered for two cases: minimum\ndistortion under power constraints; and minimum power under distortion\nconstraints. In the first case, it is shown that by turning off bad sensors,\ni.e., sensors with bad channels and bad observation quality, adaptive power\ngain can be achieved without sacrificing diversity gain. Here, the adaptive\npower gain is similar to the array gain achieved in Multiple-Input\nSingle-Output (MISO) multi-antenna systems when channel conditions are known to\nthe transmitter. In the second case, the sum power is minimized under\nzero-outage estimation distortion constraint, and some related energy\nefficiency issues in sensor networks are discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Apr 2007 16:30:31 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Cui",
                "Shuguang",
                ""
            ],
            [
                "Xiao",
                "Jinjun",
                ""
            ],
            [
                "Goldsmith",
                "Andrea",
                ""
            ],
            [
                "Luo",
                "Zhi-Quan",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0704.3408",
        "submitter": "Sinan Gezici Dr",
        "authors": "Sinan Gezici, Andreas F. Molisch, H. Vincent Poor, and Hisashi\n  Kobayashi",
        "title": "The Trade-off between Processing Gains of an Impulse Radio UWB System in\n  the Presence of Timing Jitter",
        "comments": "To appear in the IEEE Transactions on Communications",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In time hopping impulse radio, $N_f$ pulses of duration $T_c$ are transmitted\nfor each information symbol. This gives rise to two types of processing gain:\n(i) pulse combining gain, which is a factor $N_f$, and (ii) pulse spreading\ngain, which is $N_c=T_f/T_c$, where $T_f$ is the mean interval between two\nsubsequent pulses. This paper investigates the trade-off between these two\ntypes of processing gain in the presence of timing jitter. First, an additive\nwhite Gaussian noise (AWGN) channel is considered and approximate closed form\nexpressions for bit error probability are derived for impulse radio systems\nwith and without pulse-based polarity randomization. Both symbol-synchronous\nand chip-synchronous scenarios are considered. The effects of multiple-access\ninterference and timing jitter on the selection of optimal system parameters\nare explained through theoretical analysis. Finally, a multipath scenario is\nconsidered and the trade-off between processing gains of a synchronous impulse\nradio system with pulse-based polarity randomization is analyzed. The effects\nof the timing jitter, multiple-access interference and inter-frame interference\nare investigated. Simulation studies support the theoretical results.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Apr 2007 16:47:35 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Gezici",
                "Sinan",
                ""
            ],
            [
                "Molisch",
                "Andreas F.",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ],
            [
                "Kobayashi",
                "Hisashi",
                ""
            ]
        ]
    },
    {
        "id": "0704.3433",
        "submitter": "Tshilidzi Marwala",
        "authors": "Tshilidzi Marwala and Bodie Crossingham",
        "title": "Bayesian approach to rough set",
        "comments": "20 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  This paper proposes an approach to training rough set models using Bayesian\nframework trained using Markov Chain Monte Carlo (MCMC) method. The prior\nprobabilities are constructed from the prior knowledge that good rough set\nmodels have fewer rules. Markov Chain Monte Carlo sampling is conducted through\nsampling in the rough set granule space and Metropolis algorithm is used as an\nacceptance criteria. The proposed method is tested to estimate the risk of HIV\ngiven demographic data. The results obtained shows that the proposed approach\nis able to achieve an average accuracy of 58% with the accuracy varying up to\n66%. In addition the Bayesian rough set give the probabilities of the estimated\nHIV status as well as the linguistic rules describing how the demographic\nparameters drive the risk of HIV.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Apr 2007 19:50:59 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Marwala",
                "Tshilidzi",
                ""
            ],
            [
                "Crossingham",
                "Bodie",
                ""
            ]
        ]
    },
    {
        "id": "0704.3434",
        "submitter": "Shuchin Aeron",
        "authors": "Shuchin Aeron, Manqi Zhao and Venkatesh Saligrama",
        "title": "On sensing capacity of sensor networks for the class of linear\n  observation, fixed SNR models",
        "comments": "37 pages, single column",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In this paper we address the problem of finding the sensing capacity of\nsensor networks for a class of linear observation models and a fixed SNR\nregime. Sensing capacity is defined as the maximum number of signal dimensions\nreliably identified per sensor observation. In this context sparsity of the\nphenomena is a key feature that determines sensing capacity. Precluding the SNR\nof the environment the effect of sparsity on the number of measurements\nrequired for accurate reconstruction of a sparse phenomena has been widely\ndealt with under compressed sensing. Nevertheless the development there was\nmotivated from an algorithmic perspective. In this paper our aim is to derive\nthese bounds in an information theoretic set-up and thus provide algorithm\nindependent conditions for reliable reconstruction of sparse signals. In this\ndirection we first generalize the Fano's inequality and provide lower bounds to\nthe probability of error in reconstruction subject to an arbitrary distortion\ncriteria. Using these lower bounds to the probability of error, we derive upper\nbounds to sensing capacity and show that for fixed SNR regime sensing capacity\ngoes down to zero as sparsity goes down to zero. This means that\ndisproportionately more sensors are required to monitor very sparse events. Our\nnext main contribution is that we show the effect of sensing diversity on\nsensing capacity, an effect that has not been considered before. Sensing\ndiversity is related to the effective \\emph{coverage} of a sensor with respect\nto the field. In this direction we show the following results (a) Sensing\ncapacity goes down as sensing diversity per sensor goes down; (b) Random\nsampling (coverage) of the field by sensors is better than contiguous location\nsampling (coverage).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Apr 2007 19:52:47 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 14 May 2007 19:08:44 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 4 Jun 2007 18:21:46 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Aeron",
                "Shuchin",
                ""
            ],
            [
                "Zhao",
                "Manqi",
                ""
            ],
            [
                "Saligrama",
                "Venkatesh",
                ""
            ]
        ]
    },
    {
        "id": "0704.3453",
        "submitter": "Tshilidzi Marwala",
        "authors": "S. Mohamed, D. Rubin, and T. Marwala",
        "title": "An Adaptive Strategy for the Classification of G-Protein Coupled\n  Receptors",
        "comments": "9 pages, 5 tables, 3 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI q-bio.QM",
        "license": null,
        "abstract": "  One of the major problems in computational biology is the inability of\nexisting classification models to incorporate expanding and new domain\nknowledge. This problem of static classification models is addressed in this\npaper by the introduction of incremental learning for problems in\nbioinformatics. Many machine learning tools have been applied to this problem\nusing static machine learning structures such as neural networks or support\nvector machines that are unable to accommodate new information into their\nexisting models. We utilize the fuzzy ARTMAP as an alternate machine learning\nsystem that has the ability of incrementally learning new data as it becomes\navailable. The fuzzy ARTMAP is found to be comparable to many of the widespread\nmachine learning systems. The use of an evolutionary strategy in the selection\nand combination of individual classifiers into an ensemble system, coupled with\nthe incremental learning ability of the fuzzy ARTMAP is proven to be suitable\nas a pattern classifier. The algorithm presented is tested using data from the\nG-Coupled Protein Receptors Database and shows good accuracy of 83%. The system\npresented is also generally applicable, and can be used in problems in genomics\nand proteomics.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Apr 2007 21:23:31 GMT"
            }
        ],
        "update_date": "2007-06-25",
        "authors_parsed": [
            [
                "Mohamed",
                "S.",
                ""
            ],
            [
                "Rubin",
                "D.",
                ""
            ],
            [
                "Marwala",
                "T.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3500",
        "submitter": "Jerome Darmont",
        "authors": "Zhen He, J\\'er\\^ome Darmont (ERIC)",
        "title": "Une plate-forme dynamique pour l'\\'evaluation des performances des bases\n  de donn\\'ees \\`a objets",
        "comments": "20 pages",
        "journal-ref": "19\\`emes Journ\\'ees de Bases de Donn\\'ees Avanc\\'ees (BDA 03),\n  Lyon (20/10/2003) 423-442",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  In object-oriented or object-relational databases such as multimedia\ndatabases or most XML databases, access patterns are not static, i.e.,\napplications do not always access the same objects in the same order\nrepeatedly. However, this has been the way these databases and associated\noptimisation techniques such as clustering have been evaluated up to now. This\npaper opens up research regarding this issue by proposing a dynamic object\nevaluation framework (DOEF). DOEF accomplishes access pattern change by\ndefining configurable styles of change. It is a preliminary prototype that has\nbeen designed to be open and fully extensible. Though originally designed for\nthe object-oriented model, it can also be used within the object-relational\nmodel with few adaptations. Furthermore, new access pattern change models can\nbe added too. To illustrate the capabilities of DOEF, we conducted two\ndifferent sets of experiments. In the first set of experiments, we used DOEF to\ncompare the performances of four state of the art dynamic clustering\nalgorithms. The results show that DOEF is effective at determining the\nadaptability of each dynamic clustering algorithm to changes in access pattern.\nThey also led us to conclude that dynamic clustering algorithms can cope with\nmoderate levels of access pattern change, but that performance rapidly degrades\nto be worse than no clustering when vigorous styles of access pattern change\nare applied. In the second set of experiments, we used DOEF to compare the\nperformance of two different object stores: Platypus and SHORE. The use of DOEF\nexposed the poor swapping performance of Platypus.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 26 Apr 2007 09:10:41 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "He",
                "Zhen",
                "",
                "ERIC"
            ],
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                "",
                "ERIC"
            ]
        ]
    },
    {
        "id": "0704.3501",
        "submitter": "Jerome Darmont",
        "authors": "J\\'er\\^ome Darmont (ERIC), Fadila Bentayeb (ERIC), Omar Boussa\\\"id\n  (ERIC)",
        "title": "Conception d'un banc d'essais d\\'ecisionnel",
        "comments": "20 pages",
        "journal-ref": "20\\`emes Journ\\'ees Bases de Donn\\'ees Avanc\\'ees (BDA 04),\n  Montpellier (19/10/2004) 493-511",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  We present in this paper a new benchmark for evaluating the performances of\ndata warehouses. Benchmarking is useful either to system users for comparing\nthe performances of different systems, or to system engineers for testing the\neffect of various design choices. While the TPC (Transaction Processing\nPerformance Council) standard benchmarks address the first point, they are not\ntuneable enough to address the second one. Our Data Warehouse Engineering\nBenchmark (DWEB) allows to generate various ad-hoc synthetic data warehouses\nand workloads. DWEB is fully parameterized. However, two levels of\nparameterization keep it easy to tune. Since DWEB mainly meets engineering\nbenchmarking needs, it is complimentary to the TPC standard benchmarks, and not\na competitor. Finally, DWEB is implemented as a Java free software that can be\ninterfaced with most existing relational database management systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 26 Apr 2007 09:13:04 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                "",
                "ERIC"
            ],
            [
                "Bentayeb",
                "Fadila",
                "",
                "ERIC"
            ],
            [
                "Boussa\u00efd",
                "Omar",
                "",
                "ERIC"
            ]
        ]
    },
    {
        "id": "0704.3504",
        "submitter": "Berry Schoenmakers",
        "authors": "Berry Schoenmakers, Jilles Tjoelker, Pim Tuyls and Evgeny Verbitskiy",
        "title": "Smooth R\\'enyi Entropy of Ergodic Quantum Information Sources",
        "comments": "5 pages, no figures, ISIT 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "quant-ph cs.IT math.IT",
        "license": null,
        "abstract": "  We prove that the average smooth Renyi entropy rate will approach the entropy\nrate of a stationary, ergodic information source, which is equal to the Shannon\nentropy rate for a classical information source and the von Neumann entropy\nrate for a quantum information source.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 26 Apr 2007 09:28:58 GMT"
            }
        ],
        "update_date": "2018-02-13",
        "authors_parsed": [
            [
                "Schoenmakers",
                "Berry",
                ""
            ],
            [
                "Tjoelker",
                "Jilles",
                ""
            ],
            [
                "Tuyls",
                "Pim",
                ""
            ],
            [
                "Verbitskiy",
                "Evgeny",
                ""
            ]
        ]
    },
    {
        "id": "0704.3515",
        "submitter": "Jegor Uglov Mr",
        "authors": "J. Uglov, V. Schetinin, C. Maple",
        "title": "Comparing Robustness of Pairwise and Multiclass Neural-Network Systems\n  for Face Recognition",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1155/2008/468693",
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  Noise, corruptions and variations in face images can seriously hurt the\nperformance of face recognition systems. To make such systems robust,\nmulticlass neuralnetwork classifiers capable of learning from noisy data have\nbeen suggested. However on large face data sets such systems cannot provide the\nrobustness at a high level. In this paper we explore a pairwise neural-network\nsystem as an alternative approach to improving the robustness of face\nrecognition. In our experiments this approach is shown to outperform the\nmulticlass neural-network system in terms of the predictive accuracy on the\nface images corrupted by noise.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 26 Apr 2007 11:29:19 GMT"
            }
        ],
        "update_date": "2016-02-17",
        "authors_parsed": [
            [
                "Uglov",
                "J.",
                ""
            ],
            [
                "Schetinin",
                "V.",
                ""
            ],
            [
                "Maple",
                "C.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3520",
        "submitter": "Jerome Darmont",
        "authors": "Kamel Aouiche (ERIC), J\\'er\\^ome Darmont (ERIC)",
        "title": "Vers l'auto-administration des entrep\\^ots de donn\\'ees",
        "comments": "Version courte de 4 pages",
        "journal-ref": "XXXV\\`emes Journ\\'ees de Statistique, Session sp\\'eciale\n  Entreposage et Fouille de Donn\\'ees, Lyon (02/06/2003) 105-108",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  With the wide development of databases in general and data warehouses in\nparticular, it is important to reduce the tasks that a database administrator\nmust perform manually. The idea of using data mining techniques to extract\nuseful knowledge for administration from the data themselves has existed for\nsome years. However, little research has been achieved. The aim of this study\nis to search for a way of extracting useful knowledge from stored data to\nautomatically apply performance optimization techniques, and more particularly\nindexing techniques. We have designed a tool that extracts frequent itemsets\nfrom a given workload to compute an index configuration that helps optimizing\ndata access time. The experiments we performed showed that the index\nconfigurations generated by our tool allowed performance gains of 15% to 25% on\na test database and a test data warehouse.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 26 Apr 2007 11:47:35 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Aouiche",
                "Kamel",
                "",
                "ERIC"
            ],
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                "",
                "ERIC"
            ]
        ]
    },
    {
        "id": "0704.3536",
        "submitter": "Carlos Galindo",
        "authors": "C. Galindo and F.Monserrat",
        "title": "$\\delta$-sequences and Evaluation Codes defined by Plane Valuations at\n  Infinity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the concept of $\\delta$-sequence. A $\\delta$-sequence $\\Delta$\ngenerates a well-ordered semigroup $S$ in $\\mathbb{Z}^2$ or $\\mathbb{R}$. We\nshow how to construct (and compute parameters) for the dual code of any\nevaluation code associated with a weight function defined by $\\Delta$ from the\npolynomial ring in two indeterminates to a semigroup $S$ as above. We prove\nthat this is a simple procedure which can be understood by considering a\nparticular class of valuations of function fields of surfaces, called plane\nvaluations at infinity. We also give algorithms to construct an unlimited\nnumber of $\\delta$-sequences of the different existing types, and so this paper\nprovides the tools to know and use a new large set of codes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 26 Apr 2007 13:18:53 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 14 Jul 2008 09:17:57 GMT"
            }
        ],
        "update_date": "2008-07-14",
        "authors_parsed": [
            [
                "Galindo",
                "C.",
                ""
            ],
            [
                "Monserrat",
                "F.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3588",
        "submitter": "Cristina Comaniciu",
        "authors": "Cristina Comaniciu and H. Vincent Poor",
        "title": "On Energy Efficient Hierarchical Cross-Layer Design: Joint Power Control\n  and Routing for Ad Hoc Networks",
        "comments": "To appear in the EURASIP Journal on Wireless Communications and\n  Networking, Special Issue on Wireless Mobile Ad Hoc Networks",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In this paper, a hierarchical cross-layer design approach is proposed to\nincrease energy efficiency in ad hoc networks through joint adaptation of\nnodes' transmitting powers and route selection. The design maintains the\nadvantages of the classic OSI model, while accounting for the cross-coupling\nbetween layers, through information sharing. The proposed joint power control\nand routing algorithm is shown to increase significantly the overall energy\nefficiency of the network, at the expense of a moderate increase in complexity.\nPerformance enhancement of the joint design using multiuser detection is also\ninvestigated, and it is shown that the use of multiuser detection can increase\nthe capacity of the ad hoc network significantly for a given level of energy\nconsumption.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 26 Apr 2007 16:33:55 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Comaniciu",
                "Cristina",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0704.3591",
        "submitter": "Peyman Razaghi",
        "authors": "Marko Aleksic, Peyman Razaghi, Wei Yu",
        "title": "Capacity of a Class of Modulo-Sum Relay Channels",
        "comments": "To be presented in IEEE Int. Symp. on Inform. Theory (ISIT) 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  This paper characterizes the capacity of a class of modulo additive noise\nrelay channels, in which the relay observes a corrupted version of the noise\nand has a separate channel to the destination. The capacity is shown to be\nstrictly below the cut-set bound in general and achievable using a\nquantize-and-forward strategy at the relay. This result confirms a conjecture\nby Ahlswede and Han about the capacity of channels with rate limited state\ninformation at the destination for this particular class of channels.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 26 Apr 2007 17:13:17 GMT"
            }
        ],
        "update_date": "2016-09-08",
        "authors_parsed": [
            [
                "Aleksic",
                "Marko",
                ""
            ],
            [
                "Razaghi",
                "Peyman",
                ""
            ],
            [
                "Yu",
                "Wei",
                ""
            ]
        ]
    },
    {
        "id": "0704.3635",
        "submitter": "Fulufhelo Vincent Nelwamondo",
        "authors": "Fulufhelo Vincent Nelwamondo and Tshilidzi Marwala",
        "title": "Rough Sets Computations to Impute Missing Data",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.IR",
        "license": null,
        "abstract": "  Many techniques for handling missing data have been proposed in the\nliterature. Most of these techniques are overly complex. This paper explores an\nimputation technique based on rough set computations. In this paper,\ncharacteristic relations are introduced to describe incompletely specified\ndecision tables.It is shown that the basic rough set idea of lower and upper\napproximations for incompletely specified decision tables may be defined in a\nvariety of different ways. Empirical results obtained using real data are given\nand they provide a valuable and promising insight to the problem of missing\ndata. Missing data were predicted with an accuracy of up to 99%.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 26 Apr 2007 22:22:45 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Nelwamondo",
                "Fulufhelo Vincent",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0704.3643",
        "submitter": "Allison Woodruff",
        "authors": "Allison Woodruff, Sally Augustin, and Brooke Foucault",
        "title": "Sabbath Day Home Automation: \"It's Like Mixing Technology and Religion\"",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  We present a qualitative study of 20 American Orthodox Jewish families' use\nof home automation for religious purposes. These lead users offer insight into\nreal-life, long-term experience with home automation technologies. We discuss\nhow automation was seen by participants to contribute to spiritual experience\nand how participants oriented to the use of automation as a religious custom.\nWe also discuss the relationship of home automation to family life. We draw\ndesign implications for the broader population, including surrender of control\nas a design resource, home technologies that support long-term goals and\nlifestyle choices, and respite from technology.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Apr 2007 00:42:22 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Woodruff",
                "Allison",
                ""
            ],
            [
                "Augustin",
                "Sally",
                ""
            ],
            [
                "Foucault",
                "Brooke",
                ""
            ]
        ]
    },
    {
        "id": "0704.3644",
        "submitter": "Chris Ng",
        "authors": "Chris T. K. Ng, Nihar Jindal, Andrea J. Goldsmith, Urbashi Mitra",
        "title": "Capacity Gain from Two-Transmitter and Two-Receiver Cooperation",
        "comments": "Accepted for publication in IEEE Transactions on Information Theory",
        "journal-ref": null,
        "doi": "10.1109/TIT.2007.904987",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  Capacity improvement from transmitter and receiver cooperation is\ninvestigated in a two-transmitter, two-receiver network with phase fading and\nfull channel state information available at all terminals. The transmitters\ncooperate by first exchanging messages over an orthogonal transmitter\ncooperation channel, then encoding jointly with dirty paper coding. The\nreceivers cooperate by using Wyner-Ziv compress-and-forward over an analogous\northogonal receiver cooperation channel. To account for the cost of\ncooperation, the allocation of network power and bandwidth among the data and\ncooperation channels is studied. It is shown that transmitter cooperation\noutperforms receiver cooperation and improves capacity over non-cooperative\ntransmission under most operating conditions when the cooperation channel is\nstrong. However, a weak cooperation channel limits the transmitter cooperation\nrate; in this case receiver cooperation is more advantageous.\nTransmitter-and-receiver cooperation offers sizable additional capacity gain\nover transmitter-only cooperation at low SNR, whereas at high SNR transmitter\ncooperation alone captures most of the cooperative capacity improvement.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Apr 2007 00:52:11 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Ng",
                "Chris T. K.",
                ""
            ],
            [
                "Jindal",
                "Nihar",
                ""
            ],
            [
                "Goldsmith",
                "Andrea J.",
                ""
            ],
            [
                "Mitra",
                "Urbashi",
                ""
            ]
        ]
    },
    {
        "id": "0704.3646",
        "submitter": "Joseph Y. Halpern",
        "authors": "Ittai Abraham, Danny Dolev, and Joseph Y. Halpern",
        "title": "Lower Bounds on Implementing Robust and Resilient Mediators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.GT cs.CR cs.DC",
        "license": null,
        "abstract": "  We consider games that have (k,t)-robust equilibria when played with a\nmediator, where an equilibrium is (k,t)-robust if it tolerates deviations by\ncoalitions of size up to k and deviations by up to $t$ players with unknown\nutilities. We prove lower bounds that match upper bounds on the ability to\nimplement such mediators using cheap talk (that is, just allowing communication\namong the players). The bounds depend on (a) the relationship between k, t, and\nn, the total number of players in the system; (b) whether players know the\nexact utilities of other players; (c) whether there are broadcast channels or\njust point-to-point channels; (d) whether cryptography is available; and (e)\nwhether the game has a $k+t)-punishment strategy; that is, a strategy that, if\nused by all but at most $k+t$ players, guarantees that every player gets a\nworse outcome than they do with the equilibrium strategy.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Apr 2007 01:32:15 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 6 Dec 2007 22:17:40 GMT"
            }
        ],
        "update_date": "2007-12-07",
        "authors_parsed": [
            [
                "Abraham",
                "Ittai",
                ""
            ],
            [
                "Dolev",
                "Danny",
                ""
            ],
            [
                "Halpern",
                "Joseph Y.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3646",
        "submitter": "Joseph Y. Halpern",
        "authors": "Ittai Abraham, Danny Dolev, and Joseph Y. Halpern",
        "title": "Lower Bounds on Implementing Robust and Resilient Mediators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.GT cs.CR cs.DC",
        "license": null,
        "abstract": "  We consider games that have (k,t)-robust equilibria when played with a\nmediator, where an equilibrium is (k,t)-robust if it tolerates deviations by\ncoalitions of size up to k and deviations by up to $t$ players with unknown\nutilities. We prove lower bounds that match upper bounds on the ability to\nimplement such mediators using cheap talk (that is, just allowing communication\namong the players). The bounds depend on (a) the relationship between k, t, and\nn, the total number of players in the system; (b) whether players know the\nexact utilities of other players; (c) whether there are broadcast channels or\njust point-to-point channels; (d) whether cryptography is available; and (e)\nwhether the game has a $k+t)-punishment strategy; that is, a strategy that, if\nused by all but at most $k+t$ players, guarantees that every player gets a\nworse outcome than they do with the equilibrium strategy.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Apr 2007 01:32:15 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 6 Dec 2007 22:17:40 GMT"
            }
        ],
        "update_date": "2007-12-07",
        "authors_parsed": [
            [
                "Abraham",
                "Ittai",
                ""
            ],
            [
                "Dolev",
                "Danny",
                ""
            ],
            [
                "Halpern",
                "Joseph Y.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3647",
        "submitter": "Catherine Marshall",
        "authors": "Catherine C. Marshall, Frank McCown, and Michael L. Nelson",
        "title": "Evaluating Personal Archiving Strategies for Internet-based Information",
        "comments": "6 pages, 2 tables, to be published in the Proceedings of IS&T\n  Archiving 2007, May 21-24 2007, Arlington, Virginia, USA",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DL cs.CY cs.HC",
        "license": null,
        "abstract": "  Internet-based personal digital belongings present different vulnerabilities\nthan locally stored materials. We use responses to a survey of people who have\nrecovered lost websites, in combination with supplementary interviews, to paint\na fuller picture of current curatorial strategies and practices. We examine the\ntypes of personal, topical, and commercial websites that respondents have lost\nand the reasons they have lost this potentially valuable material. We further\nexplore what they have tried to recover and how the loss influences their\nsubsequent practices. We found that curation of personal digital materials in\nonline stores bears some striking similarities to the curation of similar\nmaterials stored locally in that study participants continue to archive\npersonal assets by relying on a combination of benign neglect, sporadic\nbackups, and unsystematic file replication. However, we have also identified\nissues specific to Internet-based material: how risk is spread by distributing\nthe files among multiple servers and services; the circular reasoning\nparticipants use when they discuss the safety of their digital assets; and the\ntypes of online material that are particularly vulnerable to loss. The study\nreveals ways in which expectations of permanence and notification are violated\nand situations in which benign neglect has far greater consequences for the\nlong-term fate of important digital assets.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Apr 2007 01:38:55 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Marshall",
                "Catherine C.",
                ""
            ],
            [
                "McCown",
                "Frank",
                ""
            ],
            [
                "Nelson",
                "Michael L.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3653",
        "submitter": "Catherine Marshall",
        "authors": "Catherine C. Marshall, Sara Bly, and Francoise Brun-Cottan",
        "title": "The Long Term Fate of Our Digital Belongings: Toward a Service Model for\n  Personal Archives",
        "comments": "6 pages, published in the Proceedings of IS&T Archiving 2006,\n  (Ottawa, Canada, May 23-26, 2006), Society for Imaging Science and\n  Technology, Springfield, VA, 2006, pp. 25-30",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DL cs.CY cs.HC",
        "license": null,
        "abstract": "  We conducted a preliminary field study to understand the current state of\npersonal digital archiving in practice. Our aim is to design a service for the\nlong-term storage, preservation, and access of digital belongings by examining\nhow personal archiving needs intersect with existing and emerging archiving\ntechnologies, best practices, and policies. Our findings not only confirmed\nthat experienced home computer users are creating, receiving, and finding an\nincreasing number of digital belongings, but also that they have already lost\nirreplaceable digital artifacts such as photos, creative efforts, and records.\nAlthough participants reported strategies such as backup and file replication\nfor digital safekeeping, they were seldom able to implement them consistently.\nFour central archiving themes emerged from the data: (1) people find it\ndifficult to evaluate the worth of accumulated materials; (2) personal storage\nis highly distributed both on- and offline; (3) people are experiencing\nmagnified curatorial problems associated with managing files in the aggregate,\ncreating appropriate metadata, and migrating materials to maintainable formats;\nand (4) facilities for long-term access are not supported by the current\ndesktop metaphor. Four environmental factors further complicate archiving in\nconsumer settings: the pervasive influence of malware; consumer reliance on ad\nhoc IT providers; an accretion of minor system and registry inconsistencies;\nand strong consumer beliefs about the incorruptibility of digital forms, the\nreliability of digital technologies, and the social vulnerability of networked\nstorage.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Apr 2007 02:35:57 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Marshall",
                "Catherine C.",
                ""
            ],
            [
                "Bly",
                "Sara",
                ""
            ],
            [
                "Brun-Cottan",
                "Francoise",
                ""
            ]
        ]
    },
    {
        "id": "0704.3662",
        "submitter": "Tian-Jian Jiang",
        "authors": "Mike Tian-Jian Jiang, James Zhan, Jaimie Lin, Jerry Lin, Wen-Lien Hsu",
        "title": "An Automated Evaluation Metric for Chinese Text Entry",
        "comments": "8 pages",
        "journal-ref": "Jiang, Mike Tian-Jian, et al. \"Robustness analysis of adaptive\n  chinese input methods.\" Advances in Text Input Methods (WTIM 2011) (2011): 53",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.CL",
        "license": null,
        "abstract": "  In this paper, we propose an automated evaluation metric for text entry. We\nalso consider possible improvements to existing text entry evaluation metrics,\nsuch as the minimum string distance error rate, keystrokes per character, cost\nper correction, and a unified approach proposed by MacKenzie, so they can\naccommodate the special characteristics of Chinese text. Current methods lack\nan integrated concern about both typing speed and accuracy for Chinese text\nentry evaluation. Our goal is to remove the bias that arises due to human\nfactors. First, we propose a new metric, called the correction penalty (P),\nbased on Fitts' law and Hick's law. Next, we transform it into the approximate\namortized cost (AAC) of information theory. An analysis of the AAC of Chinese\ntext input methods with different context lengths is also presented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Apr 2007 05:34:10 GMT"
            }
        ],
        "update_date": "2013-10-29",
        "authors_parsed": [
            [
                "Jiang",
                "Mike Tian-Jian",
                ""
            ],
            [
                "Zhan",
                "James",
                ""
            ],
            [
                "Lin",
                "Jaimie",
                ""
            ],
            [
                "Lin",
                "Jerry",
                ""
            ],
            [
                "Hsu",
                "Wen-Lien",
                ""
            ]
        ]
    },
    {
        "id": "0704.3665",
        "submitter": "Tian-Jian Jiang",
        "authors": "Mike Tian-Jian Jiang, Deng Liu, Meng-Juei Hsieh, Wen-Lien Hsu",
        "title": "On the Development of Text Input Method - Lessons Learned",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CL cs.HC",
        "license": null,
        "abstract": "  Intelligent Input Methods (IM) are essential for making text entries in many\nEast Asian scripts, but their application to other languages has not been fully\nexplored. This paper discusses how such tools can contribute to the development\nof computer processing of other oriental languages. We propose a design\nphilosophy that regards IM as a text service platform, and treats the study of\nIM as a cross disciplinary subject from the perspectives of software\nengineering, human-computer interaction (HCI), and natural language processing\n(NLP). We discuss these three perspectives and indicate a number of possible\nfuture research directions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Apr 2007 05:58:32 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Jiang",
                "Mike Tian-Jian",
                ""
            ],
            [
                "Liu",
                "Deng",
                ""
            ],
            [
                "Hsieh",
                "Meng-Juei",
                ""
            ],
            [
                "Hsu",
                "Wen-Lien",
                ""
            ]
        ]
    },
    {
        "id": "0704.3746",
        "submitter": "Yufang Xi",
        "authors": "Yufang Xi, Edmund M. Yeh",
        "title": "Distributed Algorithms for Spectrum Allocation, Power Control, Routing,\n  and Congestion Control in Wireless Networks",
        "comments": "14 pages, 5 figures, submitted to IEEE/ACM Transactions on Networking",
        "journal-ref": null,
        "doi": "10.1145/1288107.1288132",
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  We develop distributed algorithms to allocate resources in multi-hop wireless\nnetworks with the aim of minimizing total cost. In order to observe the\nfundamental duplexing constraint that co-located transmitters and receivers\ncannot operate simultaneously on the same frequency band, we first devise a\nspectrum allocation scheme that divides the whole spectrum into multiple\nsub-bands and activates conflict-free links on each sub-band. We show that the\nminimum number of required sub-bands grows asymptotically at a logarithmic rate\nwith the chromatic number of network connectivity graph. A simple distributed\nand asynchronous algorithm is developed to feasibly activate links on the\navailable sub-bands. Given a feasible spectrum allocation, we then design\nnode-based distributed algorithms for optimally controlling the transmission\npowers on active links for each sub-band, jointly with traffic routes and user\ninput rates in response to channel states and traffic demands. We show that\nunder specified conditions, the algorithms asymptotically converge to the\noptimal operating point.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Apr 2007 20:43:30 GMT"
            }
        ],
        "update_date": "2016-11-15",
        "authors_parsed": [
            [
                "Xi",
                "Yufang",
                ""
            ],
            [
                "Yeh",
                "Edmund M.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3780",
        "submitter": "Jean-Philippe Rennard",
        "authors": "Pierre Collet, Jean-Philippe Rennard",
        "title": "Stochastic Optimization Algorithms",
        "comments": "16 pages, 4 figures, 2 tables",
        "journal-ref": "Rennard, J.-P., Handbook of Research on Nature Inspired Computing\n  for Economics and Management, IGR, 2006",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  When looking for a solution, deterministic methods have the enormous\nadvantage that they do find global optima. Unfortunately, they are very\nCPU-intensive, and are useless on untractable NP-hard problems that would\nrequire thousands of years for cutting-edge computers to explore. In order to\nget a result, one needs to revert to stochastic algorithms, that sample the\nsearch space without exploring it thoroughly. Such algorithms can find very\ngood results, without any guarantee that the global optimum has been reached;\nbut there is often no other choice than using them. This chapter is a short\nintroduction to the main methods used in stochastic optimization.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 28 Apr 2007 06:52:19 GMT"
            }
        ],
        "update_date": "2011-12-20",
        "authors_parsed": [
            [
                "Collet",
                "Pierre",
                ""
            ],
            [
                "Rennard",
                "Jean-Philippe",
                ""
            ]
        ]
    },
    {
        "id": "0704.3878",
        "submitter": "Farhad Meshkati",
        "authors": "Farhad Meshkati, Andrea J. Goldsmith, H. Vincent Poor and Stuart C.\n  Schwartz",
        "title": "A Game-Theoretic Approach to Energy-Efficient Modulation in CDMA\n  Networks with Delay Constraints",
        "comments": "Appeared in the Proceedings of the 2007 IEEE Radio and Wireless\n  Symposium, Long Beach, CA, January 9-11, 2007",
        "journal-ref": null,
        "doi": "10.1109/RWS.2007.351784",
        "report-no": null,
        "categories": "cs.IT cs.GT math.IT",
        "license": null,
        "abstract": "  A game-theoretic framework is used to study the effect of constellation size\non the energy efficiency of wireless networks for M-QAM modulation. A\nnon-cooperative game is proposed in which each user seeks to choose its\ntransmit power (and possibly transmit symbol rate) as well as the constellation\nsize in order to maximize its own utility while satisfying its delay\nquality-of-service (QoS) constraint. The utility function used here measures\nthe number of reliable bits transmitted per joule of energy consumed, and is\nparticularly suitable for energy-constrained networks. The best-response\nstrategies and Nash equilibrium solution for the proposed game are derived. It\nis shown that in order to maximize its utility (in bits per joule), a user must\nchoose the lowest constellation size that can accommodate the user's delay\nconstraint. Using this framework, the tradeoffs among energy efficiency, delay,\nthroughput and constellation size are also studied and quantified. The effect\nof trellis-coded modulation on energy efficiency is also discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 30 Apr 2007 06:33:05 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Meshkati",
                "Farhad",
                ""
            ],
            [
                "Goldsmith",
                "Andrea J.",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ],
            [
                "Schwartz",
                "Stuart C.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3880",
        "submitter": "Farhad Meshkati",
        "authors": "Farhad Meshkati, H. Vincent Poor, Stuart C. Schwartz and Radu V. Balan",
        "title": "Energy-Efficient Resource Allocation in Wireless Networks with\n  Quality-of-Service Constraints",
        "comments": "Accpeted for publication in the IEEE Transactions on Communications",
        "journal-ref": null,
        "doi": "10.1109/TCOMM.2009.11.050638",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  A game-theoretic model is proposed to study the cross-layer problem of joint\npower and rate control with quality of service (QoS) constraints in\nmultiple-access networks. In the proposed game, each user seeks to choose its\ntransmit power and rate in a distributed manner in order to maximize its own\nutility while satisfying its QoS requirements. The user's QoS constraints are\nspecified in terms of the average source rate and an upper bound on the average\ndelay where the delay includes both transmission and queuing delays. The\nutility function considered here measures energy efficiency and is particularly\nsuitable for wireless networks with energy constraints. The Nash equilibrium\nsolution for the proposed non-cooperative game is derived and a closed-form\nexpression for the utility achieved at equilibrium is obtained. It is shown\nthat the QoS requirements of a user translate into a \"size\" for the user which\nis an indication of the amount of network resources consumed by the user. Using\nthis competitive multiuser framework, the tradeoffs among throughput, delay,\nnetwork capacity and energy efficiency are studied. In addition, analytical\nexpressions are given for users' delay profiles and the delay performance of\nthe users at Nash equilibrium is quantified.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 30 Apr 2007 06:43:21 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Meshkati",
                "Farhad",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ],
            [
                "Schwartz",
                "Stuart C.",
                ""
            ],
            [
                "Balan",
                "Radu V.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3881",
        "submitter": "Farhad Meshkati",
        "authors": "Farhad Meshkati, Dongning Guo, H. Vincent Poor and Stuart C. Schwartz",
        "title": "A Unified Approach to Energy-Efficient Power Control in Large CDMA\n  Systems",
        "comments": "Accepted for publication in the IEEE Transactions on Wireless\n  Communications",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  A unified approach to energy-efficient power control is proposed for\ncode-division multiple access (CDMA) networks. The approach is applicable to a\nlarge family of multiuser receivers including the matched filter, the\ndecorrelator, the linear minimum mean-square error (MMSE) receiver, and the\n(nonlinear) optimal detectors. It exploits the linear relationship that has\nbeen shown to exist between the transmit power and the output\nsignal-to-interference-plus-noise ratio (SIR) in the large-system limit. It is\nshown that, for this family of receivers, when users seek to selfishly maximize\ntheir own energy efficiency, the Nash equilibrium is SIR-balanced. In addition,\na unified power control (UPC) algorithm for reaching the Nash equilibrium is\nproposed. The algorithm adjusts the user's transmit powers by iteratively\ncomputing the large-system multiuser efficiency, which is independent of\ninstantaneous spreading sequences. The convergence of the algorithm is proved\nfor the matched filter, the decorrelator, and the MMSE receiver, and is\ndemonstrated by means of simulation for an optimal detector. Moreover, the\nperformance of the algorithm in finite-size systems is studied and compared\nwith that of a conventional power control scheme, in which user powers depend\non the instantaneous spreading sequences.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 30 Apr 2007 06:52:23 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Meshkati",
                "Farhad",
                ""
            ],
            [
                "Guo",
                "Dongning",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ],
            [
                "Schwartz",
                "Stuart C.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3886",
        "submitter": "W Saba",
        "authors": "Walid S. Saba",
        "title": "A Note on Ontology and Ordinary Language",
        "comments": "19 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.CL",
        "license": null,
        "abstract": "  We argue for a compositional semantics grounded in a strongly typed ontology\nthat reflects our commonsense view of the world and the way we talk about it.\nAssuming such a structure we show that the semantics of various natural\nlanguage phenomena may become nearly trivial.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 30 Apr 2007 17:55:39 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 1 May 2007 13:43:32 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 2 May 2007 18:13:22 GMT"
            },
            {
                "version": "v4",
                "created": "Thu, 3 May 2007 08:34:47 GMT"
            },
            {
                "version": "v5",
                "created": "Fri, 4 May 2007 17:49:03 GMT"
            },
            {
                "version": "v6",
                "created": "Mon, 7 May 2007 16:04:50 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Saba",
                "Walid S.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3890",
        "submitter": "Valmir Barbosa",
        "authors": "Rodolfo M. Pussente, Valmir C. Barbosa",
        "title": "An algorithm for clock synchronization with the gradient property in\n  sensor networks",
        "comments": null,
        "journal-ref": "Journal of Parallel and Distributed Computing 69 (2009), 261-265",
        "doi": "10.1016/j.jpdc.2008.11.001",
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  We introduce a distributed algorithm for clock synchronization in sensor\nnetworks. Our algorithm assumes that nodes in the network only know their\nimmediate neighborhoods and an upper bound on the network's diameter.\nClock-synchronization messages are only sent as part of the communication,\nassumed reasonably frequent, that already takes place among nodes. The\nalgorithm has the gradient property of [2], achieving an O(1) worst-case skew\nbetween the logical clocks of neighbors. As in the case of [3,8], the\nalgorithm's actions are such that no constant lower bound exists on the rate at\nwhich logical clocks progress in time, and for this reason the lower bound of\n[2,5] that forbids constant skew between neighbors does not apply.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 30 Apr 2007 19:59:14 GMT"
            }
        ],
        "update_date": "2009-02-05",
        "authors_parsed": [
            [
                "Pussente",
                "Rodolfo M.",
                ""
            ],
            [
                "Barbosa",
                "Valmir C.",
                ""
            ]
        ]
    },
    {
        "id": "0704.3905",
        "submitter": "Marc Schoenauer",
        "authors": "Christian Gagn\\'e (INFORMATIQUE WGZ INC.), Mich\\`ele Sebag (INRIA\n  Futurs), Marc Schoenauer (INRIA Futurs), Marco Tomassini (ISI)",
        "title": "Ensemble Learning for Free with Evolutionary Algorithms ?",
        "comments": null,
        "journal-ref": "Dans GECCO (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  Evolutionary Learning proceeds by evolving a population of classifiers, from\nwhich it generally returns (with some notable exceptions) the single\nbest-of-run classifier as final result. In the meanwhile, Ensemble Learning,\none of the most efficient approaches in supervised Machine Learning for the\nlast decade, proceeds by building a population of diverse classifiers. Ensemble\nLearning with Evolutionary Computation thus receives increasing attention. The\nEvolutionary Ensemble Learning (EEL) approach presented in this paper features\ntwo contributions. First, a new fitness function, inspired by co-evolution and\nenforcing the classifier diversity, is presented. Further, a new selection\ncriterion based on the classification margin is proposed. This criterion is\nused to extract the classifier ensemble from the final population only\n(Off-line) or incrementally along evolution (On-line). Experiments on a set of\nbenchmark problems show that Off-line outperforms single-hypothesis\nevolutionary learning and state-of-art Boosting and generates smaller\nclassifier ensembles.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 30 Apr 2007 09:29:22 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Gagn\u00e9",
                "Christian",
                "",
                "INFORMATIQUE WGZ INC."
            ],
            [
                "Sebag",
                "Mich\u00e8le",
                "",
                "INRIA\n  Futurs"
            ],
            [
                "Schoenauer",
                "Marc",
                "",
                "INRIA Futurs"
            ],
            [
                "Tomassini",
                "Marco",
                "",
                "ISI"
            ]
        ]
    },
    {
        "id": "0704.3969",
        "submitter": "Jean-Claude Belfiore",
        "authors": "Sheng Yang and Jean-Claude Belfiore",
        "title": "Diversity of MIMO Multihop Relay Channels - Part I: Amplify-and-Forward",
        "comments": "37 pages, 15 figures. submitted to IEEE Transactions on Information\n  Theory",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In this two-part paper, we consider the multiantenna multihop relay channels\nin which the source signal arrives at the destination through N independent\nrelaying hops in series. The main concern of this work is to design relaying\nstrategies that utilize efficiently the relays in such a way that the diversity\nis maximized. In part I, we focus on the amplify-and-forward (AF) strategy with\nwhich the relays simply scale the received signal and retransmit it. More\nspecifically, we characterize the diversity-multiplexing tradeoff (DMT) of the\nAF scheme in a general multihop channel with arbitrary number of antennas and\narbitrary number of hops. The DMT is in closed-form expression as a function of\nthe number of antennas at each node. First, we provide some basic results on\nthe DMT of the general Rayleigh product channels. It turns out that these\nresults have very simple and intuitive interpretation. Then, the results are\napplied to the AF multihop channels which is shown to be equivalent to the\nRayleigh product channel, in the DMT sense. Finally, the project-and-forward\n(PF) scheme, a variant of the AF scheme, is proposed. We show that the PF\nscheme has the same DMT as the AF scheme, while the PF can have significant\npower gain over the AF scheme in some cases. In part II, we will derive the\nupper bound on the diversity of the multihop channels and show that it can be\nachieved by partitioning the multihop channel into AF subchannels.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 30 Apr 2007 16:28:51 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Yang",
                "Sheng",
                ""
            ],
            [
                "Belfiore",
                "Jean-Claude",
                ""
            ]
        ]
    },
    {
        "id": "0705.0010",
        "submitter": "Sergey Dorogovtsev",
        "authors": "S. N. Dorogovtsev, A. V. Goltsev, J. F. F. Mendes",
        "title": "Critical phenomena in complex networks",
        "comments": "Review article, 79 pages, 43 figures, 1 table, 508 references,\n  extended",
        "journal-ref": "Rev. Mod. Phys. 80, 1275 (2008)",
        "doi": "10.1103/RevModPhys.80.1275",
        "report-no": null,
        "categories": "cond-mat.stat-mech cs.NI math-ph math.MP physics.soc-ph",
        "license": null,
        "abstract": "  The combination of the compactness of networks, featuring small diameters,\nand their complex architectures results in a variety of critical effects\ndramatically different from those in cooperative systems on lattices. In the\nlast few years, researchers have made important steps toward understanding the\nqualitatively new critical phenomena in complex networks. We review the\nresults, concepts, and methods of this rapidly developing field. Here we mostly\nconsider two closely related classes of these critical phenomena, namely\nstructural phase transitions in the network architectures and transitions in\ncooperative models on networks as substrates. We also discuss systems where a\nnetwork and interacting agents on it influence each other. We overview a wide\nrange of critical phenomena in equilibrium and growing networks including the\nbirth of the giant connected component, percolation, k-core percolation,\nphenomena near epidemic thresholds, condensation transitions, critical\nphenomena in spin models placed on networks, synchronization, and\nself-organized criticality effects in interacting systems on networks. We also\ndiscuss strong finite size effects in these systems and highlight open problems\nand perspectives.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 30 Apr 2007 20:21:37 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 1 May 2007 22:31:15 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 30 Oct 2007 18:12:18 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 9 Nov 2007 16:57:31 GMT"
            },
            {
                "version": "v5",
                "created": "Tue, 13 Nov 2007 20:57:23 GMT"
            },
            {
                "version": "v6",
                "created": "Fri, 16 Nov 2007 21:46:13 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Dorogovtsev",
                "S. N.",
                ""
            ],
            [
                "Goltsev",
                "A. V.",
                ""
            ],
            [
                "Mendes",
                "J. F. F.",
                ""
            ]
        ]
    },
    {
        "id": "0705.0025",
        "submitter": "Andreas Martin Lisewski",
        "authors": "Andreas Martin Lisewski",
        "title": "Can the Internet cope with stress?",
        "comments": "4 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.AI",
        "license": null,
        "abstract": "  When will the Internet become aware of itself? In this note the problem is\napproached by asking an alternative question: Can the Internet cope with\nstress? By extrapolating the psychological difference between coping and\ndefense mechanisms a distributed software experiment is outlined which could\nreject the hypothesis that the Internet is not a conscious entity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 May 2007 15:44:17 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Lisewski",
                "Andreas Martin",
                ""
            ]
        ]
    },
    {
        "id": "0705.0025",
        "submitter": "Andreas Martin Lisewski",
        "authors": "Andreas Martin Lisewski",
        "title": "Can the Internet cope with stress?",
        "comments": "4 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.AI",
        "license": null,
        "abstract": "  When will the Internet become aware of itself? In this note the problem is\napproached by asking an alternative question: Can the Internet cope with\nstress? By extrapolating the psychological difference between coping and\ndefense mechanisms a distributed software experiment is outlined which could\nreject the hypothesis that the Internet is not a conscious entity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 May 2007 15:44:17 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Lisewski",
                "Andreas Martin",
                ""
            ]
        ]
    },
    {
        "id": "0705.0043",
        "submitter": "Christian Goulding",
        "authors": "Savas Dayanik, Christian Goulding, H. Vincent Poor",
        "title": "Joint Detection and Identification of an Unobservable Change in the\n  Distribution of a Random Sequence",
        "comments": "Appeared in the Proceedings of the 41st Annual Conference on\n  Information Sciences and Systems, John Hopkins University, March 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  This paper examines the joint problem of detection and identification of a\nsudden and unobservable change in the probability distribution function (pdf)\nof a sequence of independent and identically distributed (i.i.d.) random\nvariables to one of finitely many alternative pdf's. The objective is quick\ndetection of the change and accurate inference of the ensuing pdf. Following a\nBayesian approach, a new sequential decision strategy for this problem is\nrevealed and is proven optimal. Geometrical properties of this strategy are\ndemonstrated via numerical examples.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 30 Apr 2007 23:59:09 GMT"
            }
        ],
        "update_date": "2009-04-16",
        "authors_parsed": [
            [
                "Dayanik",
                "Savas",
                ""
            ],
            [
                "Goulding",
                "Christian",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0705.0044",
        "submitter": "Shashi Kiran Chilappagari",
        "authors": "Shashi Kiran Chilappagari and Bane Vasic",
        "title": "Reliable Memories Built from Unreliable Components Based on Expander\n  Graphs",
        "comments": "Submitted to IEEE Transactions on Information Theory",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In this paper, memories built from components subject to transient faults are\nconsidered. A fault-tolerant memory architecture based on low-density\nparity-check codes is proposed and the existence of reliable memories for the\nadversarial failure model is proved. The proof relies on the expansion property\nof the underlying Tanner graph of the code. An equivalence between the\nTaylor-Kuznetsov (TK) scheme and Gallager B algorithm is established and the\nresults are extended to the independent failure model. It is also shown that\nthe proposed memory architecture has lower redundancy compared to the TK\nscheme. The results are illustrated with specific numerical examples.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 May 2007 00:07:46 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Chilappagari",
                "Shashi Kiran",
                ""
            ],
            [
                "Vasic",
                "Bane",
                ""
            ]
        ]
    },
    {
        "id": "0705.0081",
        "submitter": "Yeow Meng Chee",
        "authors": "Yeow Meng Chee, San Ling",
        "title": "Constructions of q-Ary Constant-Weight Codes",
        "comments": "12 pages",
        "journal-ref": "IEEE Transactions on Information Theory, Vol. 53, No. 1, January\n  2007, pp. 135-146",
        "doi": "10.1109/TIT.2006.887499",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  This paper introduces a new combinatorial construction for q-ary\nconstant-weight codes which yields several families of optimal codes and\nasymptotically optimal codes. The construction reveals intimate connection\nbetween q-ary constant-weight codes and sets of pairwise disjoint combinatorial\ndesigns of various types.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 May 2007 07:16:43 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Chee",
                "Yeow Meng",
                ""
            ],
            [
                "Ling",
                "San",
                ""
            ]
        ]
    },
    {
        "id": "0705.0085",
        "submitter": "{\\O}yvind Ytrehus",
        "authors": "Angela I. Barbero Diez and Oyvind Ytrehus",
        "title": "An efficient centralized binary multicast network coding algorithm for\n  any cyclic network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  We give an algorithm for finding network encoding and decoding equations for\nerror-free multicasting networks with multiple sources and sinks. The algorithm\ngiven is efficient (polynomial complexity) and works on any kind of network\n(acyclic, link cyclic, flow cyclic, or even in the presence of knots). The key\nidea will be the appropriate use of the delay (both natural and additional)\nduring the encoding. The resulting code will always work with finite delay with\nbinary encoding coefficients.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 May 2007 08:03:18 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Diez",
                "Angela I. Barbero",
                ""
            ],
            [
                "Ytrehus",
                "Oyvind",
                ""
            ]
        ]
    },
    {
        "id": "0705.0123",
        "submitter": "Mustafa Cenk Gursoy",
        "authors": "Mustafa Cenk Gursoy",
        "title": "An Energy Efficiency Perspective on Training for Fading Channels",
        "comments": "To appear in the Proc. of the 2007 IEEE International Symposium on\n  Information Theory",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2007.4557387",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In this paper, the bit energy requirements of training-based transmission\nover block Rayleigh fading channels are studied. Pilot signals are employed to\nobtain the minimum mean-square-error (MMSE) estimate of the channel fading\ncoefficients. Energy efficiency is analyzed in the worst case scenario where\nthe channel estimate is assumed to be perfect and the error in the estimate is\nconsidered as another source of additive Gaussian noise. It is shown that bit\nenergy requirement grows without bound as the snr goes to zero, and the minimum\nbit energy is achieved at a nonzero snr value below which one should not\noperate. The effect of the block length on both the minimum bit energy and the\nsnr value at which the minimum is achieved is investigated. Flash training\nschemes are analyzed and shown to improve the energy efficiency in the low-snr\nregime. Energy efficiency analysis is also carried out when peak power\nconstraints are imposed on pilot signals.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 May 2007 15:27:41 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Gursoy",
                "Mustafa Cenk",
                ""
            ]
        ]
    },
    {
        "id": "0705.0124",
        "submitter": "Mustafa Cenk Gursoy",
        "authors": "Mustafa Cenk Gursoy",
        "title": "On the Low-SNR Capacity of Phase-Shift Keying with Hard-Decision\n  Detection",
        "comments": "To appear in the Proc. of the 2007 IEEE International Symposium on\n  Information Theory",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2007.4557221",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  The low-snr capacity of M-ary PSK transmission over both the additive white\nGaussian noise (AWGN) and fading channels is analyzed when hard-decision\ndetection is employed at the receiver. Closed-form expressions for the first\nand second derivatives of the capacity at zero snr are obtained. The\nspectral-efficiency/bit-energy tradeoff in the low-snr regime is analyzed by\nfinding the wideband slope and the bit energy required at zero spectral\nefficiency. Practical design guidelines are drawn from the\ninformation-theoretic analysis. The fading channel analysis is conducted for\nboth coherent and noncoherent cases, and the performance penalty in the\nlow-power regime for not knowing the channel is identified.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 May 2007 15:38:30 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Gursoy",
                "Mustafa Cenk",
                ""
            ]
        ]
    },
    {
        "id": "0705.0128",
        "submitter": "Mustafa Cenk Gursoy",
        "authors": "Sami Akin, Mustafa Cenk Gursoy",
        "title": "Training Optimization for Gauss-Markov Rayleigh Fading Channels",
        "comments": "To appear in the Proc. of the 2007 IEEE International Conference on\n  Communications",
        "journal-ref": null,
        "doi": "10.1109/ICC.2007.994",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In this paper, pilot-assisted transmission over Gauss-Markov Rayleigh fading\nchannels is considered. A simple scenario, where a single pilot signal is\ntransmitted every T symbols and T-1 data symbols are transmitted in between the\npilots, is studied. First, it is assumed that binary phase-shift keying (BPSK)\nmodulation is employed at the transmitter. With this assumption, the training\nperiod, and data and training power allocation are jointly optimized by\nmaximizing an achievable rate expression. Achievable rates and energy-per-bit\nrequirements are computed using the optimal training parameters. Secondly, a\ncapacity lower bound is obtained by considering the error in the estimate as\nanother source of additive Gaussian noise, and the training parameters are\noptimized by maximizing this lower bound.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 May 2007 16:04:39 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Akin",
                "Sami",
                ""
            ],
            [
                "Gursoy",
                "Mustafa Cenk",
                ""
            ]
        ]
    },
    {
        "id": "0705.0130",
        "submitter": "Mustafa Cenk Gursoy",
        "authors": "Qingyun Wang, Mustafa Cenk Gursoy",
        "title": "Performance Analysis for Multichannel Reception of OOFSK Signaling",
        "comments": "Proc. of the 2007 IEEE Wireless Communications and Networking\n  Conference",
        "journal-ref": null,
        "doi": "10.1109/WCNC.2007.444",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In this paper, the error performance of on-off frequency shift keying (OOFSK)\nmodulation over fading channels is analyzed when the receiver is equipped with\nmultiple antennas. The analysis is conducted in two cases: the coherent\nscenario where the fading is perfectly known at the receiver, and the\nnoncoherent scenario where neither the receiver nor the transmitter knows the\nfading coefficients. For both cases, the maximum a posteriori probability (MAP)\ndetection rule is derived and analytical probability of error expressions are\nobtained. The effect of fading correlation among the receiver antennas is also\nstudied. Simulation results indicate that for sufficiently low duty cycle\nvalues, lower probability of error values with respect to FSK signaling are\nachieved. Equivalently, when compared to FSK modulation, OOFSK with low duty\ncycle requires less energy to achieve the same probability of error, which\nrenders this modulation a more energy efficient transmission technique.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 May 2007 16:11:28 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Wang",
                "Qingyun",
                ""
            ],
            [
                "Gursoy",
                "Mustafa Cenk",
                ""
            ]
        ]
    },
    {
        "id": "0705.0132",
        "submitter": "Mustafa Cenk Gursoy",
        "authors": "Mustafa Cenk Gursoy",
        "title": "Error Probability Analysis of Peaky Signaling over Fading Channels",
        "comments": "Proc. of the 40th Annual Asilomar Conference on Signals, Systems, and\n  Computers, 2006",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  In this paper, the performance of signaling strategies with high\npeak-to-average power ratio is analyzed in both coherent and noncoherent fading\nchannels. Two recently proposed modulation schemes, namely on-off binary\nphase-shift keying and on-off quaternary phase-shift keying, are considered.\nFor these modulation formats, the optimal decision rules used at the detector\nare identified and analytical expressions for the error probabilities are\nobtained. Numerical techniques are employed to compute the error probabilities.\nIt is concluded that increasing the peakedness of the signals results in\nreduced error rates for a given power level and hence improve the energy\nefficiency.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 May 2007 16:20:05 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Gursoy",
                "Mustafa Cenk",
                ""
            ]
        ]
    },
    {
        "id": "0705.0150",
        "submitter": "Myung-Sin Song",
        "authors": "Palle E. T. Jorgensen, Myung-Sin Song",
        "title": "Comparison of Discrete and Continuous Wavelet Transforms",
        "comments": "22 pages, Springer Encyclopedia of Complexity and Systems Science,\n  the full version with figures is available at\n  http://www.siue.edu/~msong/Research/ency.pdf",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": null,
        "abstract": "  In this paper we outline several points of view on the interplay between\ndiscrete and continuous wavelet transforms; stressing both pure and applied\naspects of both. We outline some new links between the two transform\ntechnologies based on the theory of representations of generators and\nrelations. By this we mean a finite system of generators which are represented\nby operators in Hilbert space. We further outline how these representations\nyield sub-band filter banks for signal and image processing algorithms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 May 2007 18:24:52 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 24 Aug 2007 17:53:30 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Jorgensen",
                "Palle E. T.",
                ""
            ],
            [
                "Song",
                "Myung-Sin",
                ""
            ]
        ]
    },
    {
        "id": "0705.0178",
        "submitter": "Abhishek Parakh",
        "authors": "Abhishek Parakh",
        "title": "Oblivious Transfer based on Key Exchange",
        "comments": "10 pages",
        "journal-ref": "Cryptologia, Volume 32, Issue 1 January 2008, pages 37 - 44",
        "doi": "10.1080/01611190701593228",
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  Key-exchange protocols have been overlooked as a possible means for\nimplementing oblivious transfer (OT). In this paper we present a protocol for\nmutual exchange of secrets, 1-out-of-2 OT and coin flipping similar to\nDiffie-Hellman protocol using the idea of obliviously exchanging encryption\nkeys. Since, Diffie-Hellman scheme is widely used, our protocol may provide a\nuseful alternative to the conventional methods for implementation of oblivious\ntransfer and a useful primitive in building larger cryptographic schemes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 May 2007 02:56:25 GMT"
            }
        ],
        "update_date": "2008-01-10",
        "authors_parsed": [
            [
                "Parakh",
                "Abhishek",
                ""
            ]
        ]
    },
    {
        "id": "0705.0197",
        "submitter": "Tshilidzi Marwala",
        "authors": "Tshilidzi Marwala, Unathi Mahola and Snehashish Chakraverty",
        "title": "Fault Classification in Cylinders Using Multilayer Perceptrons, Support\n  Vector Machines and Guassian Mixture Models",
        "comments": "10 pages, 2 figures, 4 tables",
        "journal-ref": "Computer Assisted Mechanics and Engineering Sciences, Vol. 14, No.\n  2, 2007.",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  Gaussian mixture models (GMM) and support vector machines (SVM) are\nintroduced to classify faults in a population of cylindrical shells. The\nproposed procedures are tested on a population of 20 cylindrical shells and\ntheir performance is compared to the procedure, which uses multi-layer\nperceptrons (MLP). The modal properties extracted from vibration data are used\nto train the GMM, SVM and MLP. It is observed that the GMM produces 98%, SVM\nproduces 94% classification accuracy while the MLP produces 88% classification\nrates.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 May 2007 03:13:28 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Marwala",
                "Tshilidzi",
                ""
            ],
            [
                "Mahola",
                "Unathi",
                ""
            ],
            [
                "Chakraverty",
                "Snehashish",
                ""
            ]
        ]
    },
    {
        "id": "0705.0199",
        "submitter": "Erik Berglund",
        "authors": "Erik Berglund, Joaquin Sitte",
        "title": "The Parameter-Less Self-Organizing Map algorithm",
        "comments": "29 pages, 27 figures. Based on publication in IEEE Trans. on Neural\n  Networks",
        "journal-ref": "IEEE Transactions on Neural Networks, 2006 v.17, n.2, pp.305-316",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI cs.CV",
        "license": null,
        "abstract": "  The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network\nalgorithm based on the Self-Organizing Map (SOM). It eliminates the need for a\nlearning rate and annealing schemes for learning rate and neighbourhood size.\nWe discuss the relative performance of the PLSOM and the SOM and demonstrate\nsome tasks in which the SOM fails but the PLSOM performs satisfactory. Finally\nwe discuss some example applications of the PLSOM and present a proof of\nordering under certain limited conditions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 May 2007 04:04:51 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 8 May 2007 01:06:10 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Berglund",
                "Erik",
                ""
            ],
            [
                "Sitte",
                "Joaquin",
                ""
            ]
        ]
    },
    {
        "id": "0705.0199",
        "submitter": "Erik Berglund",
        "authors": "Erik Berglund, Joaquin Sitte",
        "title": "The Parameter-Less Self-Organizing Map algorithm",
        "comments": "29 pages, 27 figures. Based on publication in IEEE Trans. on Neural\n  Networks",
        "journal-ref": "IEEE Transactions on Neural Networks, 2006 v.17, n.2, pp.305-316",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI cs.CV",
        "license": null,
        "abstract": "  The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network\nalgorithm based on the Self-Organizing Map (SOM). It eliminates the need for a\nlearning rate and annealing schemes for learning rate and neighbourhood size.\nWe discuss the relative performance of the PLSOM and the SOM and demonstrate\nsome tasks in which the SOM fails but the PLSOM performs satisfactory. Finally\nwe discuss some example applications of the PLSOM and present a proof of\nordering under certain limited conditions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 May 2007 04:04:51 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 8 May 2007 01:06:10 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Berglund",
                "Erik",
                ""
            ],
            [
                "Sitte",
                "Joaquin",
                ""
            ]
        ]
    },
    {
        "id": "0705.0199",
        "submitter": "Erik Berglund",
        "authors": "Erik Berglund, Joaquin Sitte",
        "title": "The Parameter-Less Self-Organizing Map algorithm",
        "comments": "29 pages, 27 figures. Based on publication in IEEE Trans. on Neural\n  Networks",
        "journal-ref": "IEEE Transactions on Neural Networks, 2006 v.17, n.2, pp.305-316",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI cs.CV",
        "license": null,
        "abstract": "  The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network\nalgorithm based on the Self-Organizing Map (SOM). It eliminates the need for a\nlearning rate and annealing schemes for learning rate and neighbourhood size.\nWe discuss the relative performance of the PLSOM and the SOM and demonstrate\nsome tasks in which the SOM fails but the PLSOM performs satisfactory. Finally\nwe discuss some example applications of the PLSOM and present a proof of\nordering under certain limited conditions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 May 2007 04:04:51 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 8 May 2007 01:06:10 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Berglund",
                "Erik",
                ""
            ],
            [
                "Sitte",
                "Joaquin",
                ""
            ]
        ]
    },
    {
        "id": "0705.0214",
        "submitter": "Mourad Zerai",
        "authors": "Mourad Zerai, Maher Moakher",
        "title": "Riemannian level-set methods for tensor-valued data",
        "comments": "11 pages, 03 figures, to be published in the proceedings of SSVM\n  2007, LNCS Springer",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  We present a novel approach for the derivation of PDE modeling\ncurvature-driven flows for matrix-valued data. This approach is based on the\nRiemannian geometry of the manifold of Symmetric Positive Definite Matrices\nPos(n).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 May 2007 07:32:58 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Zerai",
                "Mourad",
                ""
            ],
            [
                "Moakher",
                "Maher",
                ""
            ]
        ]
    },
    {
        "id": "0705.0252",
        "submitter": "Khoa Nguyen",
        "authors": "Khoa D. Nguyen, Albert Guillen i Fabregas and Lars K. Rasmussen",
        "title": "Power Allocation for Discrete-Input Non-Ergodic Block-Fading Channels",
        "comments": "6 pages, 4 figures, submitted to Information Theory Workshop 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  We consider power allocation algorithms for fixed-rate transmission over\nNakagami-m non-ergodic block-fading channels with perfect transmitter and\nreceiver channel state information and discrete input signal constellations\nunder both short- and long-term power constraints. Optimal power allocation\nschemes are shown to be direct applications of previous results in the\nliterature. We show that the SNR exponent of the optimal short-term scheme is\ngiven by the Singleton bound. We also illustrate the significant gains\navailable by employing long-term power constraints. Due to the nature of the\nexpressions involved, the complexity of optimal schemes may be prohibitive for\nsystem implementation. We propose simple sub-optimal power allocation schemes\nwhose outage probability performance is very close to the minimum outage\nprobability obtained by optimal schemes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 May 2007 11:36:53 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 3 May 2007 03:03:55 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 4 Jul 2007 13:10:52 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Nguyen",
                "Khoa D.",
                ""
            ],
            [
                "Fabregas",
                "Albert Guillen i",
                ""
            ],
            [
                "Rasmussen",
                "Lars K.",
                ""
            ]
        ]
    },
    {
        "id": "0705.0253",
        "submitter": "Jian Li",
        "authors": "Mordecai Golin and Li Jian",
        "title": "More Efficient Algorithms and Analyses for Unequal Letter Cost\n  Prefix-Free Coding",
        "comments": "29 pages;9 figures;",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.DS math.IT",
        "license": null,
        "abstract": "  There is a large literature devoted to the problem of finding an optimal\n(min-cost) prefix-free code with an unequal letter-cost encoding alphabet of\nsize. While there is no known polynomial time algorithm for solving it\noptimally there are many good heuristics that all provide additive errors to\noptimal. The additive error in these algorithms usually depends linearly upon\nthe largest encoding letter size.\n  This paper was motivated by the problem of finding optimal codes when the\nencoding alphabet is infinite. Because the largest letter cost is infinite, the\nprevious analyses could give infinite error bounds. We provide a new algorithm\nthat works with infinite encoding alphabets. When restricted to the finite\nalphabet case, our algorithm often provides better error bounds than the best\nprevious ones known.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 May 2007 11:23:52 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 3 May 2007 09:00:23 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Golin",
                "Mordecai",
                ""
            ],
            [
                "Jian",
                "Li",
                ""
            ]
        ]
    },
    {
        "id": "0705.0281",
        "submitter": "Jerome Darmont",
        "authors": "J\\'er\\^ome Darmont (LIMOS), Christophe Fromantin (LIMOS), St\\'ephane\n  R\\'egnier (LIMOS), Le Gruenwald, Michel Schneider (LIMOS)",
        "title": "Dynamic Clustering in Object-Oriented Databases: An Advocacy for\n  Simplicity",
        "comments": null,
        "journal-ref": "LNCS, Vol. 1944 (06/2000) 71-85",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  We present in this paper three dynamic clustering techniques for\nObject-Oriented Databases (OODBs). The first two, Dynamic, Statistical &\nTunable Clustering (DSTC) and StatClust, exploit both comprehensive usage\nstatistics and the inter-object reference graph. They are quite elaborate.\nHowever, they are also complex to implement and induce a high overhead. The\nthird clustering technique, called Detection & Reclustering of Objects (DRO),\nis based on the same principles, but is much simpler to implement. These three\nclustering algorithm have been implemented in the Texas persistent object store\nand compared in terms of clustering efficiency (i.e., overall performance\nincrease) and overhead using the Object Clustering Benchmark (OCB). The results\nobtained showed that DRO induced a lighter overhead while still achieving\nbetter overall performance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 May 2007 12:50:39 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                "",
                "LIMOS"
            ],
            [
                "Fromantin",
                "Christophe",
                "",
                "LIMOS"
            ],
            [
                "R\u00e9gnier",
                "St\u00e9phane",
                "",
                "LIMOS"
            ],
            [
                "Gruenwald",
                "Le",
                "",
                "LIMOS"
            ],
            [
                "Schneider",
                "Michel",
                "",
                "LIMOS"
            ]
        ]
    },
    {
        "id": "0705.0286",
        "submitter": "Hajime Matsui",
        "authors": "Hajime Matsui, Seiichi Mita",
        "title": "Inverse-free Berlekamp-Massey-Sakata Algorithm and Small Decoders for\n  Algebraic-Geometric Codes",
        "comments": "15 pages, submitted to IEEE Transactions on Information Theory",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  This paper proposes a novel algorithm for finding error-locators of\nalgebraic-geometric codes that can eliminate the division-calculations of\nfinite fields from the Berlekamp-Massey-Sakata algorithm. This inverse-free\nalgorithm provides full performance in correcting a certain class of errors,\ngeneric errors, which includes most errors, and can decode codes on algebraic\ncurves without the determination of unknown syndromes. Moreover, we propose\nthree different kinds of architectures that our algorithm can be applied to,\nand we represent the control operation of shift-registers and switches at each\nclock-timing with numerical simulations. We estimate the performance in\ncomparison of the total running time and the numbers of multipliers and\nshift-registers in three architectures with those of the conventional ones for\ncodes on algebraic curves.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 May 2007 13:13:48 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Matsui",
                "Hajime",
                ""
            ],
            [
                "Mita",
                "Seiichi",
                ""
            ]
        ]
    },
    {
        "id": "0705.0315",
        "submitter": "Omid Amini",
        "authors": "Omid Amini, Frederic Havet, Florian Huc, Stephan Thomasse",
        "title": "WDM and Directed Star Arboricity",
        "comments": "18 pages, 2 figures. Final version",
        "journal-ref": "Combinatorics, Probability and Computing, Volume 19, Issue 02,\n  March 2010, pp 161-182",
        "doi": null,
        "report-no": null,
        "categories": "cs.NI math.CO",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A digraph is $m$-labelled if every arc is labelled by an integer in $\\{1,\n\\dots,m\\}$. Motivated by wavelength assignment for multicasts in optical\nnetworks, we introduce and study $n$-fibre colourings of labelled digraphs.\nThese are colourings of the arcs of $D$ such that at each vertex $v$, and for\neach colour $\\alpha$, $in(v,\\alpha)+out(v,\\alpha)\\leq n$ with $in(v,\\alpha)$\nthe number of arcs coloured $\\alpha$ entering $v$ and $out(v,\\alpha)$ the\nnumber of labels $l$ such that there is at least one arc of label $l$ leaving\n$v$ and coloured with $\\alpha$. The problem is to find the minimum number of\ncolours $\\lambda_n(D)$ such that the $m$-labelled digraph $D$ has an $n$-fibre\ncolouring. In the particular case when $D$ is $1$-labelled, $\\lambda_1(D)$ is\ncalled the directed star arboricity of $D$, and is denoted by $dst(D)$. We\nfirst show that $dst(D)\\leq 2\\Delta^-(D)+1$, and conjecture that if\n$\\Delta^-(D)\\geq 2$, then $dst(D)\\leq 2\\Delta^-(D)$. We also prove that for a\nsubcubic digraph $D$, then $dst(D)\\leq 3$, and that if $\\Delta^+(D),\n\\Delta^-(D)\\leq 2$, then $dst(D)\\leq 4$. Finally, we study\n$\\lambda_n(m,k)=\\max\\{\\lambda_n(D) \\tq D \\mbox{is $m$-labelled} \\et\n\\Delta^-(D)\\leq k\\}$. We show that if $m\\geq n$, then $\\ds\n\\left\\lceil\\frac{m}{n}\\left\\lceil \\frac{k}{n}\\right\\rceil + \\frac{k}{n}\n\\right\\rceil\\leq \\lambda_n(m,k) \\leq\\left\\lceil\\frac{m}{n}\\left\\lceil\n\\frac{k}{n}\\right\\rceil + \\frac{k}{n} \\right\\rceil + C \\frac{m^2\\log k}{n}$ for\nsome constant $C$. We conjecture that the lower bound should be the right value\nof $\\lambda_n(m,k)$.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 May 2007 15:54:55 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 3 May 2007 08:38:06 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 15 Jul 2010 01:05:23 GMT"
            }
        ],
        "update_date": "2010-07-16",
        "authors_parsed": [
            [
                "Amini",
                "Omid",
                ""
            ],
            [
                "Havet",
                "Frederic",
                ""
            ],
            [
                "Huc",
                "Florian",
                ""
            ],
            [
                "Thomasse",
                "Stephan",
                ""
            ]
        ]
    },
    {
        "id": "0705.0326",
        "submitter": "Lei Ying",
        "authors": "Lei Ying and R. Srikant",
        "title": "Optimal Delay-Throughput Trade-offs in Mobile Ad-Hoc Networks: Hybrid\n  Random Walk and One-Dimensional Mobility Models",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.IT math.IT",
        "license": null,
        "abstract": "  Optimal delay-throughput trade-offs for two-dimensional i.i.d mobility models\nhave been established in [23], where we showed that the optimal trade-offs can\nbe achieved using rate-less codes when the required delay guarantees are\nsufficient large. In this paper, we extend the results to other mobility models\nincluding two-dimensional hybrid random walk model, one-dimensional i.i.d.\nmobility model and one-dimensional hybrid random walk model. We consider both\nfast mobiles and slow mobiles, and establish the optimal delay-throughput\ntrade-offs under some conditions. Joint coding-scheduling algorithms are also\nproposed to achieve the optimal trade-offs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 May 2007 16:43:30 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Ying",
                "Lei",
                ""
            ],
            [
                "Srikant",
                "R.",
                ""
            ]
        ]
    },
    {
        "id": "0705.0326",
        "submitter": "Lei Ying",
        "authors": "Lei Ying and R. Srikant",
        "title": "Optimal Delay-Throughput Trade-offs in Mobile Ad-Hoc Networks: Hybrid\n  Random Walk and One-Dimensional Mobility Models",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.IT math.IT",
        "license": null,
        "abstract": "  Optimal delay-throughput trade-offs for two-dimensional i.i.d mobility models\nhave been established in [23], where we showed that the optimal trade-offs can\nbe achieved using rate-less codes when the required delay guarantees are\nsufficient large. In this paper, we extend the results to other mobility models\nincluding two-dimensional hybrid random walk model, one-dimensional i.i.d.\nmobility model and one-dimensional hybrid random walk model. We consider both\nfast mobiles and slow mobiles, and establish the optimal delay-throughput\ntrade-offs under some conditions. Joint coding-scheduling algorithms are also\nproposed to achieve the optimal trade-offs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 May 2007 16:43:30 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Ying",
                "Lei",
                ""
            ],
            [
                "Srikant",
                "R.",
                ""
            ]
        ]
    },
    {
        "id": "0705.0422",
        "submitter": "Rapport De Recherche Inria",
        "authors": "Omid Amini (INRIA Sophia Antipolis), Louis Esperet (LaBRI), Jan Van\n  Den Heuvel (LSE)",
        "title": "Frugal Colouring of Graphs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DM cs.NI",
        "license": null,
        "abstract": "  A $k$-frugal colouring of a graph $G$ is a proper colouring of the vertices\nof $G$ such that no colour appears more than $k$ times in the neighbourhood of\na vertex. This type of colouring was introduced by Hind, Molloy and Reed in\n1997. In this paper, we study the frugal chromatic number of planar graphs,\nplanar graphs with large girth, and outerplanar graphs, and relate this\nparameter with several well-studied colourings, such as colouring of the\nsquare, cyclic colouring, and $L(p,q)$-labelling. We also study frugal\nedge-colourings of multigraphs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 May 2007 08:52:00 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Amini",
                "Omid",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Esperet",
                "Louis",
                "",
                "LaBRI"
            ],
            [
                "Heuvel",
                "Jan Van Den",
                "",
                "LSE"
            ]
        ]
    },
    {
        "id": "0705.0423",
        "submitter": "Farbod Kayhan",
        "authors": "A. Braunstein, F. Kayhan, G. Montorsi and R. Zecchina",
        "title": "Encoding for the Blackwell Channel with Reinforced Belief Propagation",
        "comments": "5 pages, 8 figures, submitted to ISIT 2007",
        "journal-ref": "IEEE International Symposium on Information Theory (ISIT07); 2007.\n  p. 1891-5",
        "doi": "10.1109/ISIT.2007.4557497",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  A key idea in coding for the broadcast channel (BC) is binning, in which the\ntransmitter encode information by selecting a codeword from an appropriate bin\n(the messages are thus the bin indexes). This selection is normally done by\nsolving an appropriate (possibly difficult) combinatorial problem. Recently it\nhas been shown that binning for the Blackwell channel --a particular BC-- can\nbe done by iterative schemes based on Survey Propagation (SP). This method uses\ndecimation for SP and suffers a complexity of O(n^2). In this paper we propose\na new variation of the Belief Propagation (BP) algorithm, named Reinforced BP\nalgorithm, that turns BP into a solver. Our simulations show that this new\nalgorithm has complexity O(n log n). Using this new algorithm together with a\nnon-linear coding scheme, we can efficiently achieve rates close to the border\nof the capacity region of the Blackwell channel.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 May 2007 09:49:15 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Braunstein",
                "A.",
                ""
            ],
            [
                "Kayhan",
                "F.",
                ""
            ],
            [
                "Montorsi",
                "G.",
                ""
            ],
            [
                "Zecchina",
                "R.",
                ""
            ]
        ]
    },
    {
        "id": "0705.0425",
        "submitter": "Natalia Osipova",
        "authors": "Natalia Osipova (INRIA Sophia Antipolis)",
        "title": "Batch Processor Sharing with Hyper-Exponential Service Time",
        "comments": "Sophia Antipolis, France, 03 May 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  We study Batch Processor-Sharing (BPS) queuing model with hyper-exponential\nservice time distribution and Poisson batch arrival process. One of the main\ngoals to study BPS is the possibility of its application in size-based\nscheduling, which is used in differentiation between Short and Long flows in\nthe Internet. In the case of hyper-exponential service time distribution we\nfind an analytical expression of the expected conditional response time for the\nBPS queue. We show, that the expected conditional response time is a concave\nfunction of the service time. We apply the received results to the Two Level\nProcessor-Sharing (TLPS) model with hyper-exponential service time distribution\nand find the expression of the expected response time for the TLPS model. TLPS\nscheduling discipline can be applied to size-based differentiation in TCP/IP\nnetworks and Web server request handling.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 May 2007 10:09:06 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 4 May 2007 12:51:53 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 22 Jun 2007 09:53:50 GMT"
            }
        ],
        "update_date": "2016-09-08",
        "authors_parsed": [
            [
                "Osipova",
                "Natalia",
                "",
                "INRIA Sophia Antipolis"
            ]
        ]
    },
    {
        "id": "0705.0449",
        "submitter": "Pierre-Francois Marteau",
        "authors": "Pierre-Fran\\c{c}ois Marteau (VALORIA), Gilbas M\\'enier (VALORIA)",
        "title": "Multiresolution Approximation of Polygonal Curves in Linear Complexity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  We propose a new algorithm to the problem of polygonal curve approximation\nbased on a multiresolution approach. This algorithm is suboptimal but still\nmaintains some optimality between successive levels of resolution using dynamic\nprogramming. We show theoretically and experimentally that this algorithm has a\nlinear complexity in time and space. We experimentally compare the outcomes of\nour algorithm to the optimal \"full search\" dynamic programming solution and\nfinally to classical merge and split approaches. The experimental evaluations\nconfirm the theoretical derivations and show that the proposed approach\nevaluated on 2D coastal maps either show a lower time complexity or provide\npolygonal approximations closer to the input discrete curves.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 May 2007 12:47:31 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Marteau",
                "Pierre-Fran\u00e7ois",
                "",
                "VALORIA"
            ],
            [
                "M\u00e9nier",
                "Gilbas",
                "",
                "VALORIA"
            ]
        ]
    },
    {
        "id": "0705.0450",
        "submitter": "Jerome Darmont",
        "authors": "J\\'er\\^ome Darmont (LIMOS), Michel Schneider (LIMOS)",
        "title": "VOODB: A Generic Discrete-Event Random Simulation Model to Evaluate the\n  Performances of OODBs",
        "comments": null,
        "journal-ref": "25th International Conference on Very Large Databases (VLDB 99)\n  (09/1999) 254-265",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  Performance of object-oriented database systems (OODBs) is still an issue to\nboth designers and users nowadays. The aim of this paper is to propose a\ngeneric discrete-event random simulation model, called VOODB, in order to\nevaluate the performances of OODBs in general, and the performances of\noptimization methods like clustering in particular. Such optimization methods\nundoubtedly improve the performances of OODBs. Yet, they also always induce\nsome kind of overhead for the system. Therefore, it is important to evaluate\ntheir exact impact on the overall performances. VOODB has been designed as a\ngeneric discrete-event random simulation model by putting to use a modelling\napproach, and has been validated by simulating the behavior of the O2 OODB and\nthe Texas persistent object store. Since our final objective is to compare\nobject clustering algorithms, some experiments have also been conducted on the\nDSTC clustering technique, which is implemented in Texas. To validate VOODB,\nperformance results obtained by simulation for a given experiment have been\ncompared to the results obtained by benchmarking the real systems in the same\nconditions. Benchmarking and simulation performance evaluations have been\nobserved to be consistent, so it appears that simulation can be a reliable\napproach to evaluate the performances of OODBs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 May 2007 12:50:04 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                "",
                "LIMOS"
            ],
            [
                "Schneider",
                "Michel",
                "",
                "LIMOS"
            ]
        ]
    },
    {
        "id": "0705.0453",
        "submitter": "Jerome Darmont",
        "authors": "J\\'er\\^ome Darmont (LIMOS), Bertrand Petit (LIMOS), Michel Schneider\n  (LIMOS)",
        "title": "OCB: A Generic Benchmark to Evaluate the Performances of Object-Oriented\n  Database Systems",
        "comments": null,
        "journal-ref": "LNCS, Vol. 1377 (03/1998) 326-340",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  We present in this paper a generic object-oriented benchmark (the Object\nClustering Benchmark) that has been designed to evaluate the performances of\nclustering policies in object-oriented databases. OCB is generic because its\nsample database may be customized to fit the databases introduced by the main\nexisting benchmarks (e.g., OO1). OCB's current form is clustering-oriented\nbecause of its clustering-oriented workload, but it can be easily adapted to\nother purposes. Lastly, OCB's code is compact and easily portable. OCB has been\nimplemented in a real system (Texas, running on a Sun workstation), in order to\ntest a specific clustering policy called DSTC. A few results concerning this\ntest are presented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 May 2007 12:54:30 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                "",
                "LIMOS"
            ],
            [
                "Petit",
                "Bertrand",
                "",
                "LIMOS"
            ],
            [
                "Schneider",
                "Michel",
                "",
                "LIMOS"
            ]
        ]
    },
    {
        "id": "0705.0454",
        "submitter": "Jerome Darmont",
        "authors": "J\\'er\\^ome Darmont (LIMOS), Amar Attoui (LIMOS), Michel Gourgand\n  (LIMOS)",
        "title": "Performance Evaluation for Clustering Algorithms in Object-Oriented\n  Database Systems",
        "comments": null,
        "journal-ref": "LNCS, Vol. 978 (09/1995) 187-196",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  It is widely acknowledged that good object clustering is critical to the\nperformance of object-oriented databases. However, object clustering always\ninvolves some kind of overhead for the system. The aim of this paper is to\npropose a modelling methodology in order to evaluate the performances of\ndifferent clustering policies. This methodology has been used to compare the\nperformances of three clustering algorithms found in the literature (Cactis, CK\nand ORION) that we considered representative of the current research in the\nfield of object clustering. The actual performance evaluation was performed\nusing simulation. Simulation experiments we performed showed that the Cactis\nalgorithm is better than the ORION algorithm and that the CK algorithm totally\noutperforms both other algorithms in terms of response time and clustering\noverhead.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 May 2007 13:02:06 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                "",
                "LIMOS"
            ],
            [
                "Attoui",
                "Amar",
                "",
                "LIMOS"
            ],
            [
                "Gourgand",
                "Michel",
                "",
                "LIMOS"
            ]
        ]
    },
    {
        "id": "0705.0543",
        "submitter": "Aditya Ramamoorthy",
        "authors": "Jaehong Kim, Aditya Ramamoorthy and Steven W. McLaughlin",
        "title": "The Design of Efficiently-Encodable Rate-Compatible LDPC Codes",
        "comments": "Accepted subject to minor revision to IEEE Trans. on Comm",
        "journal-ref": null,
        "doi": "10.1109/ICC.2006.254899",
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  We present a new class of irregular low-density parity-check (LDPC) codes for\nmoderate block lengths (up to a few thousand bits) that are well-suited for\nrate-compatible puncturing. The proposed codes show good performance under\npuncturing over a wide range of rates and are suitable for usage in incremental\nredundancy hybrid-automatic repeat request (ARQ) systems. In addition, these\ncodes are linear-time encodable with simple shift-register circuits. For a\nblock length of 1200 bits the codes outperform optimized irregular LDPC codes\nand extended irregular repeat-accumulate (eIRA) codes for all puncturing rates\n0.6~0.9 (base code performance is almost the same) and are particularly good at\nhigh puncturing rates where good puncturing performance has been previously\ndifficult to achieve.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 May 2007 22:17:51 GMT"
            }
        ],
        "update_date": "2016-11-18",
        "authors_parsed": [
            [
                "Kim",
                "Jaehong",
                ""
            ],
            [
                "Ramamoorthy",
                "Aditya",
                ""
            ],
            [
                "McLaughlin",
                "Steven W.",
                ""
            ]
        ]
    },
    {
        "id": "0705.0552",
        "submitter": "Rajeev Raman",
        "authors": "Rajeev Raman, Venkatesh Raman, Srinivasa Rao Satti",
        "title": "Succinct Indexable Dictionaries with Applications to Encoding $k$-ary\n  Trees, Prefix Sums and Multisets",
        "comments": "Final version of SODA 2002 paper; supersedes Leicester Tech report\n  2002/16",
        "journal-ref": "ACM Transactions on Algorithms vol 3 (2007), Article 43, 25pp",
        "doi": "10.1145/1290672.1290680",
        "report-no": null,
        "categories": "cs.DS cs.DM cs.IT math.IT",
        "license": null,
        "abstract": "  We consider the {\\it indexable dictionary} problem, which consists of storing\na set $S \\subseteq \\{0,...,m-1\\}$ for some integer $m$, while supporting the\noperations of $\\Rank(x)$, which returns the number of elements in $S$ that are\nless than $x$ if $x \\in S$, and -1 otherwise; and $\\Select(i)$ which returns\nthe $i$-th smallest element in $S$. We give a data structure that supports both\noperations in O(1) time on the RAM model and requires ${\\cal B}(n,m) + o(n) +\nO(\\lg \\lg m)$ bits to store a set of size $n$, where ${\\cal B}(n,m) = \\ceil{\\lg\n{m \\choose n}}$ is the minimum number of bits required to store any $n$-element\nsubset from a universe of size $m$. Previous dictionaries taking this space\nonly supported (yes/no) membership queries in O(1) time. In the cell probe\nmodel we can remove the $O(\\lg \\lg m)$ additive term in the space bound,\nanswering a question raised by Fich and Miltersen, and Pagh.\n  We present extensions and applications of our indexable dictionary data\nstructure, including:\n  An information-theoretically optimal representation of a $k$-ary cardinal\ntree that supports standard operations in constant time,\n  A representation of a multiset of size $n$ from $\\{0,...,m-1\\}$ in ${\\cal\nB}(n,m+n) + o(n)$ bits that supports (appropriate generalizations of) $\\Rank$\nand $\\Select$ operations in constant time, and\n  A representation of a sequence of $n$ non-negative integers summing up to $m$\nin ${\\cal B}(n,m+n) + o(n)$ bits that supports prefix sum queries in constant\ntime.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 4 May 2007 07:47:05 GMT"
            }
        ],
        "update_date": "2011-08-10",
        "authors_parsed": [
            [
                "Raman",
                "Rajeev",
                ""
            ],
            [
                "Raman",
                "Venkatesh",
                ""
            ],
            [
                "Satti",
                "Srinivasa Rao",
                ""
            ]
        ]
    },
    {
        "id": "0705.0564",
        "submitter": "Caleb Lo",
        "authors": "Caleb K. Lo, Sriram Vishwanath and Robert W. Heath Jr",
        "title": "Rate Bounds for MIMO Relay Channels",
        "comments": "25 pages, 6 figures, submitted to Journal on Communications and\n  Networks in December 2007, revised in April 2008",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  This paper considers the multi-input multi-output (MIMO) relay channel where\nmultiple antennas are employed by each terminal. Compared to single-input\nsingle-output (SISO) relay channels, MIMO relay channels introduce additional\ndegrees of freedom, making the design and analysis of optimal cooperative\nstrategies more complex. In this paper, a partial cooperation strategy that\ncombines transmit-side message splitting and block-Markov encoding is\npresented. Lower bounds on capacity that improve on a previously proposed\nnon-cooperative lower bound are derived for Gaussian MIMO relay channels.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 4 May 2007 06:43:42 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 18 Dec 2007 17:39:33 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 10 Apr 2008 15:52:35 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 2 May 2008 21:33:42 GMT"
            }
        ],
        "update_date": "2008-05-03",
        "authors_parsed": [
            [
                "Lo",
                "Caleb K.",
                ""
            ],
            [
                "Vishwanath",
                "Sriram",
                ""
            ],
            [
                "Heath",
                "Robert W.",
                "Jr"
            ]
        ]
    },
    {
        "id": "0705.0588",
        "submitter": "Edgar Graaf de",
        "authors": "Edgar H. de Graaf, Joost N. Kok, Walter A. Kosters",
        "title": "Clustering Co-occurrence of Maximal Frequent Patterns in Streams",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.DS",
        "license": null,
        "abstract": "  One way of getting a better view of data is using frequent patterns. In this\npaper frequent patterns are subsets that occur a minimal number of times in a\nstream of itemsets. However, the discovery of frequent patterns in streams has\nalways been problematic. Because streams are potentially endless it is in\nprinciple impossible to say if a pattern is often occurring or not. Furthermore\nthe number of patterns can be huge and a good overview of the structure of the\nstream is lost quickly. The proposed approach will use clustering to facilitate\nthe analysis of the structure of the stream.\n  A clustering on the co-occurrence of patterns will give the user an improved\nview on the structure of the stream. Some patterns might occur so much together\nthat they should form a combined pattern. In this way the patterns in the\nclustering will be the largest frequent patterns: maximal frequent patterns.\n  Our approach to decide if patterns occur often together will be based on a\nmethod of clustering when only the distance between pairs is known. The number\nof maximal frequent patterns is much smaller and combined with clustering\nmethods these patterns provide a good view on the structure of the stream.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 4 May 2007 10:36:53 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "de Graaf",
                "Edgar H.",
                ""
            ],
            [
                "Kok",
                "Joost N.",
                ""
            ],
            [
                "Kosters",
                "Walter A.",
                ""
            ]
        ]
    },
    {
        "id": "0705.0593",
        "submitter": "Edgar Graaf de",
        "authors": "Edgar H. de Graaf, Joost N. Kok, Walter A. Kosters",
        "title": "Clustering with Lattices in the Analysis of Graph Patterns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.DS",
        "license": null,
        "abstract": "  Mining frequent subgraphs is an area of research where we have a given set of\ngraphs (each graph can be seen as a transaction), and we search for (connected)\nsubgraphs contained in many of these graphs. In this work we will discuss\ntechniques used in our framework Lattice2SAR for mining and analysing frequent\nsubgraph data and their corresponding lattice information. Lattice information\nis provided by the graph mining algorithm gSpan; it contains all\nsupergraph-subgraph relations of the frequent subgraph patterns -- and their\nsupports.\n  Lattice2SAR is in particular used in the analysis of frequent graph patterns\nwhere the graphs are molecules and the frequent subgraphs are fragments. In the\nanalysis of fragments one is interested in the molecules where patterns occur.\nThis data can be very extensive and in this paper we focus on a technique of\nmaking it better available by using the lattice information in our clustering.\nNow we can reduce the number of times the highly compressed occurrence data\nneeds to be accessed by the user. The user does not have to browse all the\noccurrence data in search of patterns occurring in the same molecules. Instead\none can directly see which frequent subgraphs are of interest.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 4 May 2007 10:52:28 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "de Graaf",
                "Edgar H.",
                ""
            ],
            [
                "Kok",
                "Joost N.",
                ""
            ],
            [
                "Kosters",
                "Walter A.",
                ""
            ]
        ]
    },
    {
        "id": "0705.0599",
        "submitter": "Nathalie Henry",
        "authors": "Nathalie Henry, Jean-Daniel Fekete, Michael Mcguffin",
        "title": "NodeTrix: Hybrid Representation for Analyzing Social Networks",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1109/TVCG.2007.70582",
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  The need to visualize large social networks is growing as hardware\ncapabilities make analyzing large networks feasible and many new data sets\nbecome available. Unfortunately, the visualizations in existing systems do not\nsatisfactorily answer the basic dilemma of being readable both for the global\nstructure of the network and also for detailed analysis of local communities.\nTo address this problem, we present NodeTrix, a hybrid representation for\nnetworks that combines the advantages of two traditional representations:\nnode-link diagrams are used to show the global structure of a network, while\narbitrary portions of the network can be shown as adjacency matrices to better\nsupport the analysis of communities. A key contribution is a set of interaction\ntechniques. These allow analysts to create a NodeTrix visualization by dragging\nselections from either a node-link or a matrix, flexibly manipulate the\nNodeTrix representation to explore the dataset, and create meaningful summary\nvisualizations of their findings. Finally, we present a case study applying\nNodeTrix to the analysis of the InfoVis 2004 coauthorship dataset to illustrate\nthe capabilities of NodeTrix as both an exploration tool and an effective means\nof communicating results.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 4 May 2007 11:50:07 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 7 May 2007 08:53:45 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 21 Jun 2007 13:32:58 GMT"
            }
        ],
        "update_date": "2020-08-04",
        "authors_parsed": [
            [
                "Henry",
                "Nathalie",
                ""
            ],
            [
                "Fekete",
                "Jean-Daniel",
                ""
            ],
            [
                "Mcguffin",
                "Michael",
                ""
            ]
        ]
    },
    {
        "id": "0705.0602",
        "submitter": "Alejandro Chinea Manrique De Lara",
        "authors": "Alejandro Chinea Manrique De Lara (INRIA Rocquencourt), Michel Parent\n  (INRIA Rocquencourt)",
        "title": "Risk Assessment Algorithms Based On Recursive Neural Networks",
        "comments": null,
        "journal-ref": "Dans International Joint Conference On Neural Networks - IJCNN\n  2007 (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  The assessment of highly-risky situations at road intersections have been\nrecently revealed as an important research topic within the context of the\nautomotive industry. In this paper we shall introduce a novel approach to\ncompute risk functions by using a combination of a highly non-linear processing\nmodel in conjunction with a powerful information encoding procedure.\nSpecifically, the elements of information either static or dynamic that appear\nin a road intersection scene are encoded by using directed positional acyclic\nlabeled graphs. The risk assessment problem is then reformulated in terms of an\ninductive learning task carried out by a recursive neural network. Recursive\nneural networks are connectionist models capable of solving supervised and\nnon-supervised learning problems represented by directed ordered acyclic\ngraphs. The potential of this novel approach is demonstrated through well\npredefined scenarios. The major difference of our approach compared to others\nis expressed by the fact of learning the structure of the risk. Furthermore,\nthe combination of a rich information encoding procedure with a generalized\nmodel of dynamical recurrent networks permit us, as we shall demonstrate, a\nsophisticated processing of information that we believe as being a first step\nfor building future advanced intersection safety systems\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 4 May 2007 11:53:35 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "De Lara",
                "Alejandro Chinea Manrique",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Parent",
                "Michel",
                "",
                "INRIA Rocquencourt"
            ]
        ]
    },
    {
        "id": "0705.0693",
        "submitter": "Tshilidzi Marwala",
        "authors": "Evan Hurwitz and Tshilidzi Marwala",
        "title": "Learning to Bluff",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  The act of bluffing confounds game designers to this day. The very nature of\nbluffing is even open for debate, adding further complication to the process of\ncreating intelligent virtual players that can bluff, and hence play,\nrealistically. Through the use of intelligent, learning agents, and carefully\ndesigned agent outlooks, an agent can in fact learn to predict its opponents\nreactions based not only on its own cards, but on the actions of those around\nit. With this wider scope of understanding, an agent can in learn to bluff its\nopponents, with the action representing not an illogical action, as bluffing is\noften viewed, but rather as an act of maximising returns through an effective\nstatistical optimisation. By using a tee dee lambda learning algorithm to\ncontinuously adapt neural network agent intelligence, agents have been shown to\nbe able to learn to bluff without outside prompting, and even to learn to call\neach others bluffs in free, competitive play.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 May 2007 19:15:24 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Hurwitz",
                "Evan",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.0734",
        "submitter": "Sanjiang Li",
        "authors": "Sanjiang Li and Mingsheng Ying",
        "title": "Soft constraint abstraction based on semiring homomorphism",
        "comments": "18 pages, 1 figure",
        "journal-ref": "Theoretical Computer Science 403(2-3) 192-201, 2008",
        "doi": "10.1016/j.tcs.2008.03.029",
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  The semiring-based constraint satisfaction problems (semiring CSPs), proposed\nby Bistarelli, Montanari and Rossi \\cite{BMR97}, is a very general framework of\nsoft constraints. In this paper we propose an abstraction scheme for soft\nconstraints that uses semiring homomorphism. To find optimal solutions of the\nconcrete problem, the idea is, first working in the abstract problem and\nfinding its optimal solutions, then using them to solve the concrete problem.\n  In particular, we show that a mapping preserves optimal solutions if and only\nif it is an order-reflecting semiring homomorphism. Moreover, for a semiring\nhomomorphism $\\alpha$ and a problem $P$ over $S$, if $t$ is optimal in\n$\\alpha(P)$, then there is an optimal solution $\\bar{t}$ of $P$ such that\n$\\bar{t}$ has the same value as $t$ in $\\alpha(P)$.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 5 May 2007 08:47:31 GMT"
            }
        ],
        "update_date": "2010-07-01",
        "authors_parsed": [
            [
                "Li",
                "Sanjiang",
                ""
            ],
            [
                "Ying",
                "Mingsheng",
                ""
            ]
        ]
    },
    {
        "id": "0705.0760",
        "submitter": "Sujay Sanghavi",
        "authors": "Sujay Sanghavi",
        "title": "Equivalence of LP Relaxation and Max-Product for Weighted Matching in\n  General Graphs",
        "comments": "6 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.AI cs.LG cs.NI math.IT",
        "license": null,
        "abstract": "  Max-product belief propagation is a local, iterative algorithm to find the\nmode/MAP estimate of a probability distribution. While it has been successfully\nemployed in a wide variety of applications, there are relatively few\ntheoretical guarantees of convergence and correctness for general loopy graphs\nthat may have many short cycles. Of these, even fewer provide exact ``necessary\nand sufficient'' characterizations.\n  In this paper we investigate the problem of using max-product to find the\nmaximum weight matching in an arbitrary graph with edge weights. This is done\nby first constructing a probability distribution whose mode corresponds to the\noptimal matching, and then running max-product. Weighted matching can also be\nposed as an integer program, for which there is an LP relaxation. This\nrelaxation is not always tight. In this paper we show that \\begin{enumerate}\n\\item If the LP relaxation is tight, then max-product always converges, and\nthat too to the correct answer. \\item If the LP relaxation is loose, then\nmax-product does not converge. \\end{enumerate} This provides an exact,\ndata-dependent characterization of max-product performance, and a precise\nconnection to LP relaxation, which is a well-studied optimization technique.\nAlso, since LP relaxation is known to be tight for bipartite graphs, our\nresults generalize other recent results on using max-product to find weighted\nmatchings in bipartite graphs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 5 May 2007 18:57:47 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Sanghavi",
                "Sujay",
                ""
            ]
        ]
    },
    {
        "id": "0705.0760",
        "submitter": "Sujay Sanghavi",
        "authors": "Sujay Sanghavi",
        "title": "Equivalence of LP Relaxation and Max-Product for Weighted Matching in\n  General Graphs",
        "comments": "6 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.AI cs.LG cs.NI math.IT",
        "license": null,
        "abstract": "  Max-product belief propagation is a local, iterative algorithm to find the\nmode/MAP estimate of a probability distribution. While it has been successfully\nemployed in a wide variety of applications, there are relatively few\ntheoretical guarantees of convergence and correctness for general loopy graphs\nthat may have many short cycles. Of these, even fewer provide exact ``necessary\nand sufficient'' characterizations.\n  In this paper we investigate the problem of using max-product to find the\nmaximum weight matching in an arbitrary graph with edge weights. This is done\nby first constructing a probability distribution whose mode corresponds to the\noptimal matching, and then running max-product. Weighted matching can also be\nposed as an integer program, for which there is an LP relaxation. This\nrelaxation is not always tight. In this paper we show that \\begin{enumerate}\n\\item If the LP relaxation is tight, then max-product always converges, and\nthat too to the correct answer. \\item If the LP relaxation is loose, then\nmax-product does not converge. \\end{enumerate} This provides an exact,\ndata-dependent characterization of max-product performance, and a precise\nconnection to LP relaxation, which is a well-studied optimization technique.\nAlso, since LP relaxation is known to be tight for bipartite graphs, our\nresults generalize other recent results on using max-product to find weighted\nmatchings in bipartite graphs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 5 May 2007 18:57:47 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Sanghavi",
                "Sujay",
                ""
            ]
        ]
    },
    {
        "id": "0705.0760",
        "submitter": "Sujay Sanghavi",
        "authors": "Sujay Sanghavi",
        "title": "Equivalence of LP Relaxation and Max-Product for Weighted Matching in\n  General Graphs",
        "comments": "6 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.AI cs.LG cs.NI math.IT",
        "license": null,
        "abstract": "  Max-product belief propagation is a local, iterative algorithm to find the\nmode/MAP estimate of a probability distribution. While it has been successfully\nemployed in a wide variety of applications, there are relatively few\ntheoretical guarantees of convergence and correctness for general loopy graphs\nthat may have many short cycles. Of these, even fewer provide exact ``necessary\nand sufficient'' characterizations.\n  In this paper we investigate the problem of using max-product to find the\nmaximum weight matching in an arbitrary graph with edge weights. This is done\nby first constructing a probability distribution whose mode corresponds to the\noptimal matching, and then running max-product. Weighted matching can also be\nposed as an integer program, for which there is an LP relaxation. This\nrelaxation is not always tight. In this paper we show that \\begin{enumerate}\n\\item If the LP relaxation is tight, then max-product always converges, and\nthat too to the correct answer. \\item If the LP relaxation is loose, then\nmax-product does not converge. \\end{enumerate} This provides an exact,\ndata-dependent characterization of max-product performance, and a precise\nconnection to LP relaxation, which is a well-studied optimization technique.\nAlso, since LP relaxation is known to be tight for bipartite graphs, our\nresults generalize other recent results on using max-product to find weighted\nmatchings in bipartite graphs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 5 May 2007 18:57:47 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Sanghavi",
                "Sujay",
                ""
            ]
        ]
    },
    {
        "id": "0705.0760",
        "submitter": "Sujay Sanghavi",
        "authors": "Sujay Sanghavi",
        "title": "Equivalence of LP Relaxation and Max-Product for Weighted Matching in\n  General Graphs",
        "comments": "6 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.AI cs.LG cs.NI math.IT",
        "license": null,
        "abstract": "  Max-product belief propagation is a local, iterative algorithm to find the\nmode/MAP estimate of a probability distribution. While it has been successfully\nemployed in a wide variety of applications, there are relatively few\ntheoretical guarantees of convergence and correctness for general loopy graphs\nthat may have many short cycles. Of these, even fewer provide exact ``necessary\nand sufficient'' characterizations.\n  In this paper we investigate the problem of using max-product to find the\nmaximum weight matching in an arbitrary graph with edge weights. This is done\nby first constructing a probability distribution whose mode corresponds to the\noptimal matching, and then running max-product. Weighted matching can also be\nposed as an integer program, for which there is an LP relaxation. This\nrelaxation is not always tight. In this paper we show that \\begin{enumerate}\n\\item If the LP relaxation is tight, then max-product always converges, and\nthat too to the correct answer. \\item If the LP relaxation is loose, then\nmax-product does not converge. \\end{enumerate} This provides an exact,\ndata-dependent characterization of max-product performance, and a precise\nconnection to LP relaxation, which is a well-studied optimization technique.\nAlso, since LP relaxation is known to be tight for bipartite graphs, our\nresults generalize other recent results on using max-product to find weighted\nmatchings in bipartite graphs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 5 May 2007 18:57:47 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Sanghavi",
                "Sujay",
                ""
            ]
        ]
    },
    {
        "id": "0705.0761",
        "submitter": "Tshilidzi Marwala",
        "authors": "Tshilidzi Marwala and Bodie Crossingham",
        "title": "Bayesian Approach to Neuro-Rough Models",
        "comments": "24 pages, 5 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  This paper proposes a neuro-rough model based on multi-layered perceptron and\nrough set. The neuro-rough model is then tested on modelling the risk of HIV\nfrom demographic data. The model is formulated using Bayesian framework and\ntrained using Monte Carlo method and Metropolis criterion. When the model was\ntested to estimate the risk of HIV infection given the demographic data it was\nfound to give the accuracy of 62%. The proposed model is able to combine the\naccuracy of the Bayesian MLP model and the transparency of Bayesian rough set\nmodel.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 6 May 2007 22:55:58 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 9 May 2007 04:13:04 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 28 Aug 2007 09:24:46 GMT"
            }
        ],
        "update_date": "2007-08-28",
        "authors_parsed": [
            [
                "Marwala",
                "Tshilidzi",
                ""
            ],
            [
                "Crossingham",
                "Bodie",
                ""
            ]
        ]
    },
    {
        "id": "0705.0781",
        "submitter": "Tshilidzi Marwala",
        "authors": "Jonathan M.Spiller and T. Marwala",
        "title": "Medical Image Segmentation and Localization using Deformable Templates",
        "comments": "4 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  This paper presents deformable templates as a tool for segmentation and\nlocalization of biological structures in medical images. Structures are\nrepresented by a prototype template, combined with a parametric warp mapping\nused to deform the original shape. The localization procedure is achieved using\na multi-stage, multi-resolution algorithm de-signed to reduce computational\ncomplexity and time. The algorithm initially identifies regions in the image\nmost likely to contain the desired objects and then examines these regions at\nprogressively increasing resolutions. The final stage of the algorithm involves\nwarping the prototype template to match the localized objects. The algorithm is\npresented along with the results of four example applications using MRI, x-ray\nand ultrasound images.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 6 May 2007 06:02:46 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Spiller",
                "Jonathan M.",
                ""
            ],
            [
                "Marwala",
                "T.",
                ""
            ]
        ]
    },
    {
        "id": "0705.0783",
        "submitter": "Stefano Buzzi",
        "authors": "Stefano Buzzi, and H. Vincent Poor",
        "title": "Non-cooperative games for spreading code optimization, power control and\n  receiver design in wireless data networks",
        "comments": "appeared in the Proceedings of the 13th European Wireless Conference,\n  Paris (France), April 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.GT math.IT",
        "license": null,
        "abstract": "  This paper focuses on the issue of energy efficiency in wireless data\nnetworks through a game theoretic approach. The case considered is that in\nwhich each user is allowed to vary its transmit power, spreading code, and\nuplink receiver in order to maximize its own utility, which is here defined as\nthe ratio of data throughput to transmit power. In particular, the case in\nwhich linear multiuser detectors are employed at the receiver is treated first,\nand, then, the more challenging case in which non-linear decision feedback\nmultiuser receivers are adopted is addressed. It is shown that, for both\nreceivers, the problem at hand of utility maximization can be regarded as a\nnon-cooperative game, and it is proved that a unique Nash equilibrium point\nexists. Simulation results show that significant performance gains can be\nobtained through both non-linear processing and spreading code optimization; in\nparticular, for systems with a number of users not larger than the processing\ngain, remarkable gains come from spreading code optimization, while, for\noverloaded systems, the largest gainscome from the use of non-linear\nprocessing. In every case, however, the non-cooperative games proposed here are\nshown to outperform competing alternatives.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 6 May 2007 06:49:07 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Buzzi",
                "Stefano",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0705.0815",
        "submitter": "Andrea Lo Pumo",
        "authors": "Andrea Lo Pumo",
        "title": "Overview of the Netsukuku network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  Netsukuku is a P2P network system designed to handle a large number of nodes\nwith minimal CPU and memory resources. It can be easily used to build a\nworldwide distributed, anonymous and not controlled network, separated from the\nInternet, without the support of any servers, ISPs or authority controls. In\nthis document, we give a generic and non technical description of the Netsukuku\nnetwork, emphasizing its main ideas and features.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 6 May 2007 19:36:47 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Pumo",
                "Andrea Lo",
                ""
            ]
        ]
    },
    {
        "id": "0705.0817",
        "submitter": "Andrea Lo Pumo",
        "authors": "Andrea Lo Pumo",
        "title": "Quantum Shortest Path Netsukuku",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  This document describes the QSPN, the routing discovery algorithm used by\nNetsukuku. Through a deductive analysis the main proprieties of the QSPN are\nshown. Moreover, a second version of the algorithm, is presented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 6 May 2007 20:05:44 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Pumo",
                "Andrea Lo",
                ""
            ]
        ]
    },
    {
        "id": "0705.0819",
        "submitter": "Andrea Lo Pumo",
        "authors": "Andrea Lo Pumo",
        "title": "The Netsukuku network topology",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  In this document, we describe the fractal structure of the Netsukuku\ntopology. Moreover, we show how it is possible to use the QSPN v2 on the high\nlevels of the fractal.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 6 May 2007 20:12:16 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Pumo",
                "Andrea Lo",
                ""
            ]
        ]
    },
    {
        "id": "0705.0820",
        "submitter": "Andrea Lo Pumo",
        "authors": "Andrea Lo Pumo",
        "title": "ANDNA: the distributed hostname management system of Netsukuku",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  We present the Abnormal Netsukuku Domain Name Anarchy system. ANDNA is the\ndistributed, non hierarchical and decentralised system of hostname management\nused in the Netsukuku network.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 6 May 2007 20:19:51 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Pumo",
                "Andrea Lo",
                ""
            ]
        ]
    },
    {
        "id": "0705.0828",
        "submitter": "Tshilidzi Marwala",
        "authors": "D.L. Falk, D. M. Rubin and T. Marwala",
        "title": "Enhancement of Noisy Planar Nuclear Medicine Images using Mean Field\n  Annealing",
        "comments": "4 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  Nuclear medicine (NM) images inherently suffer from large amounts of noise\nand blur. The purpose of this research is to reduce the noise and blur while\nmaintaining image integrity for improved diagnosis. The proposed solution is to\nincrease image quality after the standard pre- and post-processing undertaken\nby a gamma camera system. Mean Field Annealing (MFA) is the image processing\ntechnique used in this research. It is a computational iterative technique that\nmakes use of the Point Spread Function (PSF) and the noise associated with the\nNM image. MFA is applied to NM images with the objective of reducing noise\nwhile not compromising edge integrity. Using a sharpening filter as a\npost-processing technique (after MFA) yields image enhancement of planar NM\nimages.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 6 May 2007 23:08:04 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Falk",
                "D. L.",
                ""
            ],
            [
                "Rubin",
                "D. M.",
                ""
            ],
            [
                "Marwala",
                "T.",
                ""
            ]
        ]
    },
    {
        "id": "0705.0909",
        "submitter": "Giacomo Bacci",
        "authors": "Giacomo Bacci, Marco Luise, H. Vincent Poor",
        "title": "Game-Theoretic Power Control in Impulse Radio UWB Wireless Networks",
        "comments": "Appeared in the Proceedings of the 13th European Wireless Conference,\n  Paris, France, April 1-4, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.GT cs.IT math.IT",
        "license": null,
        "abstract": "  In this paper, a game-theoretic model for studying power control for wireless\ndata networks in frequency-selective multipath environments is analyzed. The\nuplink of an impulse-radio ultrawideband system is considered. The effects of\nself-interference and multiple-access interference on the performance of Rake\nreceivers are investigated for synchronous systems. Focusing on energy\nefficiency, a noncooperative game is proposed in which users in the network are\nallowed to choose their transmit powers to maximize their own utilities, and\nthe Nash equilibrium for the proposed game is derived. It is shown that, due to\nthe frequency selective multipath, the noncooperative solution is achieved at\ndifferent signal-to-interference-plus-noise ratios, respectively of the channel\nrealization. A large-system analysis is performed to derive explicit\nexpressions for the achieved utilities. The Pareto-optimal (cooperative)\nsolution is also discussed and compared with the noncooperative approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 May 2007 13:58:01 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Bacci",
                "Giacomo",
                ""
            ],
            [
                "Luise",
                "Marco",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0705.0932",
        "submitter": "Oliver Kosut",
        "authors": "Oliver Kosut and Lang Tong",
        "title": "Variable-Rate Distributed Source Coding in the Presence of Byzantine\n  Sensors",
        "comments": "5 pages, submitted to ISIT 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT math.IT",
        "license": null,
        "abstract": "  The distributed source coding problem is considered when the sensors, or\nencoders, are under Byzantine attack; that is, an unknown number of sensors\nhave been reprogrammed by a malicious intruder to undermine the reconstruction\nat the fusion center. Three different forms of the problem are considered. The\nfirst is a variable-rate setup, in which the decoder adaptively chooses the\nrates at which the sensors transmit. An explicit characterization of the\nvariable-rate minimum achievable sum rate is stated, given by the maximum\nentropy over the set of distributions indistinguishable from the true source\ndistribution by the decoder. In addition, two forms of the fixed-rate problem\nare considered, one with deterministic coding and one with randomized coding.\nThe achievable rate regions are given for both these problems, with a larger\nregion achievable using randomized coding, though both are suboptimal compared\nto variable-rate coding.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 May 2007 15:55:06 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Kosut",
                "Oliver",
                ""
            ],
            [
                "Tong",
                "Lang",
                ""
            ]
        ]
    },
    {
        "id": "0705.0952",
        "submitter": "Tshilidzi Marwala",
        "authors": "Dhiresh R. Surajpal and Tshilidzi Marwala",
        "title": "An Independent Evaluation of Subspace Face Recognition Algorithms",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  This paper explores a comparative study of both the linear and kernel\nimplementations of three of the most popular Appearance-based Face Recognition\nprojection classes, these being the methodologies of Principal Component\nAnalysis, Linear Discriminant Analysis and Independent Component Analysis. The\nexperimental procedure provides a platform of equal working conditions and\nexamines the ten algorithms in the categories of expression, illumination,\nocclusion and temporal delay. The results are then evaluated based on a\nsequential combination of assessment tools that facilitate both intuitive and\nstatistical decisiveness among the intra and interclass comparisons. The best\ncategorical algorithms are then incorporated into a hybrid methodology, where\nthe advantageous effects of fusion strategies are considered.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 May 2007 19:19:55 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Surajpal",
                "Dhiresh R.",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.0965",
        "submitter": "Guillaume Hanrot",
        "authors": "Guillaume Hanrot (INRIA Lorraine - LORIA), Damien Stehl\\'e (INRIA\n  Rh\\^one-Alpes)",
        "title": "Improved Analysis of Kannan's Shortest Lattice Vector Algorithm",
        "comments": null,
        "journal-ref": "Dans Advances in Cryptology - Crypto'07 4622 (2007) 170-186",
        "doi": "10.1007/978-3-540-74143-5_10",
        "report-no": null,
        "categories": "cs.CR cs.CC",
        "license": null,
        "abstract": "  The security of lattice-based cryptosystems such as NTRU, GGH and Ajtai-Dwork\nessentially relies upon the intractability of computing a shortest non-zero\nlattice vector and a closest lattice vector to a given target vector in high\ndimensions. The best algorithms for these tasks are due to Kannan, and, though\nremarkably simple, their complexity estimates have not been improved since more\nthan twenty years. Kannan's algorithm for solving the shortest vector problem\nis in particular crucial in Schnorr's celebrated block reduction algorithm, on\nwhich are based the best known attacks against the lattice-based encryption\nschemes mentioned above. Understanding precisely Kannan's algorithm is of prime\nimportance for providing meaningful key-sizes. In this paper we improve the\ncomplexity analyses of Kannan's algorithms and discuss the possibility of\nimproving the underlying enumeration strategy.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 May 2007 18:44:05 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 9 May 2007 15:32:57 GMT"
            }
        ],
        "update_date": "2009-04-16",
        "authors_parsed": [
            [
                "Hanrot",
                "Guillaume",
                "",
                "INRIA Lorraine - LORIA"
            ],
            [
                "Stehl\u00e9",
                "Damien",
                "",
                "INRIA\n  Rh\u00f4ne-Alpes"
            ]
        ]
    },
    {
        "id": "0705.0969",
        "submitter": "Tshilidzi Marwala",
        "authors": "Ishmael S. Msiza, Fulufhelo V. Nelwamondo and Tshilidzi Marwala",
        "title": "Artificial Neural Networks and Support Vector Machines for Water Demand\n  Time Series Forecasting",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  Water plays a pivotal role in many physical processes, and most importantly\nin sustaining human life, animal life and plant life. Water supply entities\ntherefore have the responsibility to supply clean and safe water at the rate\nrequired by the consumer. It is therefore necessary to implement mechanisms and\nsystems that can be employed to predict both short-term and long-term water\ndemands. The increasingly growing field of computational intelligence\ntechniques has been proposed as an efficient tool in the modelling of dynamic\nphenomena. The primary objective of this paper is to compare the efficiency of\ntwo computational intelligence techniques in water demand forecasting. The\ntechniques under comparison are the Artificial Neural Networks (ANNs) and the\nSupport Vector Machines (SVMs). In this study it was observed that the ANNs\nperform better than the SVMs. This performance is measured against the\ngeneralisation ability of the two.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 May 2007 19:00:28 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Msiza",
                "Ishmael S.",
                ""
            ],
            [
                "Nelwamondo",
                "Fulufhelo V.",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.1031",
        "submitter": "Tshilidzi Marwala",
        "authors": "F.V. Nelwamondo and T. Marwala",
        "title": "Fuzzy Artmap and Neural Network Approach to Online Processing of Inputs\n  with Missing Values",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  An ensemble based approach for dealing with missing data, without predicting\nor imputing the missing values is proposed. This technique is suitable for\nonline operations of neural networks and as a result, is used for online\ncondition monitoring. The proposed technique is tested in both classification\nand regression problems. An ensemble of Fuzzy-ARTMAPs is used for\nclassification whereas an ensemble of multi-layer perceptrons is used for the\nregression problem. Results obtained using this ensemble-based technique are\ncompared to those obtained using a combination of auto-associative neural\nnetworks and genetic algorithms and findings show that this method can perform\nup to 9% better in regression problems. Another advantage of the proposed\ntechnique is that it eliminates the need for finding the best estimate of the\ndata, and hence, saves time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 8 May 2007 05:12:01 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Nelwamondo",
                "F. V.",
                ""
            ],
            [
                "Marwala",
                "T.",
                ""
            ]
        ]
    },
    {
        "id": "0705.1033",
        "submitter": "Kebin Wang",
        "authors": "Michael A. Bender, Bradley C. Kuszmaul, Shang-Hua Teng, Kebin Wang",
        "title": "Optimal Cache-Oblivious Mesh Layouts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.CE cs.MS cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A mesh is a graph that divides physical space into regularly-shaped regions.\nMeshes computations form the basis of many applications, e.g. finite-element\nmethods, image rendering, and collision detection. In one important mesh\nprimitive, called a mesh update, each mesh vertex stores a value and repeatedly\nupdates this value based on the values stored in all neighboring vertices. The\nperformance of a mesh update depends on the layout of the mesh in memory.\n  This paper shows how to find a memory layout that guarantees that the mesh\nupdate has asymptotically optimal memory performance for any set of memory\nparameters. Such a memory layout is called cache-oblivious. Formally, for a\n$d$-dimensional mesh $G$, block size $B$, and cache size $M$ (where\n$M=\\Omega(B^d)$), the mesh update of $G$ uses $O(1+|G|/B)$ memory transfers.\nThe paper also shows how the mesh-update performance degrades for smaller\ncaches, where $M=o(B^d)$.\n  The paper then gives two algorithms for finding cache-oblivious mesh layouts.\nThe first layout algorithm runs in time $O(|G|\\log^2|G|)$ both in expectation\nand with high probability on a RAM. It uses $O(1+|G|\\log^2(|G|/M)/B)$ memory\ntransfers in expectation and $O(1+(|G|/B)(\\log^2(|G|/M) + \\log|G|))$ memory\ntransfers with high probability in the cache-oblivious and disk-access machine\n(DAM) models. The layout is obtained by finding a fully balanced decomposition\ntree of $G$ and then performing an in-order traversal of the leaves of the\ntree. The second algorithm runs faster by almost a $\\log|G|/\\log\\log|G|$ factor\nin all three memory models, both in expectation and with high probability. The\nlayout obtained by finding a relax-balanced decomposition tree of $G$ and then\nperforming an in-order traversal of the leaves of the tree.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 8 May 2007 05:59:55 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 5 Oct 2009 18:45:25 GMT"
            }
        ],
        "update_date": "2009-10-05",
        "authors_parsed": [
            [
                "Bender",
                "Michael A.",
                ""
            ],
            [
                "Kuszmaul",
                "Bradley C.",
                ""
            ],
            [
                "Teng",
                "Shang-Hua",
                ""
            ],
            [
                "Wang",
                "Kebin",
                ""
            ]
        ]
    },
    {
        "id": "0705.1033",
        "submitter": "Kebin Wang",
        "authors": "Michael A. Bender, Bradley C. Kuszmaul, Shang-Hua Teng, Kebin Wang",
        "title": "Optimal Cache-Oblivious Mesh Layouts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.CE cs.MS cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A mesh is a graph that divides physical space into regularly-shaped regions.\nMeshes computations form the basis of many applications, e.g. finite-element\nmethods, image rendering, and collision detection. In one important mesh\nprimitive, called a mesh update, each mesh vertex stores a value and repeatedly\nupdates this value based on the values stored in all neighboring vertices. The\nperformance of a mesh update depends on the layout of the mesh in memory.\n  This paper shows how to find a memory layout that guarantees that the mesh\nupdate has asymptotically optimal memory performance for any set of memory\nparameters. Such a memory layout is called cache-oblivious. Formally, for a\n$d$-dimensional mesh $G$, block size $B$, and cache size $M$ (where\n$M=\\Omega(B^d)$), the mesh update of $G$ uses $O(1+|G|/B)$ memory transfers.\nThe paper also shows how the mesh-update performance degrades for smaller\ncaches, where $M=o(B^d)$.\n  The paper then gives two algorithms for finding cache-oblivious mesh layouts.\nThe first layout algorithm runs in time $O(|G|\\log^2|G|)$ both in expectation\nand with high probability on a RAM. It uses $O(1+|G|\\log^2(|G|/M)/B)$ memory\ntransfers in expectation and $O(1+(|G|/B)(\\log^2(|G|/M) + \\log|G|))$ memory\ntransfers with high probability in the cache-oblivious and disk-access machine\n(DAM) models. The layout is obtained by finding a fully balanced decomposition\ntree of $G$ and then performing an in-order traversal of the leaves of the\ntree. The second algorithm runs faster by almost a $\\log|G|/\\log\\log|G|$ factor\nin all three memory models, both in expectation and with high probability. The\nlayout obtained by finding a relax-balanced decomposition tree of $G$ and then\nperforming an in-order traversal of the leaves of the tree.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 8 May 2007 05:59:55 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 5 Oct 2009 18:45:25 GMT"
            }
        ],
        "update_date": "2009-10-05",
        "authors_parsed": [
            [
                "Bender",
                "Michael A.",
                ""
            ],
            [
                "Kuszmaul",
                "Bradley C.",
                ""
            ],
            [
                "Teng",
                "Shang-Hua",
                ""
            ],
            [
                "Wang",
                "Kebin",
                ""
            ]
        ]
    },
    {
        "id": "0705.1110",
        "submitter": "Edgar Graaf de",
        "authors": "Edgar de Graaf Joost Kok Walter Kosters",
        "title": "Mining Patterns with a Balanced Interval",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.DB",
        "license": null,
        "abstract": "  In many applications it will be useful to know those patterns that occur with\na balanced interval, e.g., a certain combination of phone numbers are called\nalmost every Friday or a group of products are sold a lot on Tuesday and\nThursday.\n  In previous work we proposed a new measure of support (the number of\noccurrences of a pattern in a dataset), where we count the number of times a\npattern occurs (nearly) in the middle between two other occurrences. If the\nnumber of non-occurrences between two occurrences of a pattern stays almost the\nsame then we call the pattern balanced.\n  It was noticed that some very frequent patterns obviously also occur with a\nbalanced interval, meaning in every transaction. However more interesting\npatterns might occur, e.g., every three transactions. Here we discuss a\nsolution using standard deviation and average. Furthermore we propose a simpler\napproach for pruning patterns with a balanced interval, making estimating the\npruning threshold more intuitive.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 8 May 2007 15:22:38 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Kosters",
                "Edgar de Graaf Joost Kok Walter",
                ""
            ]
        ]
    },
    {
        "id": "0705.1110",
        "submitter": "Edgar Graaf de",
        "authors": "Edgar de Graaf Joost Kok Walter Kosters",
        "title": "Mining Patterns with a Balanced Interval",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.DB",
        "license": null,
        "abstract": "  In many applications it will be useful to know those patterns that occur with\na balanced interval, e.g., a certain combination of phone numbers are called\nalmost every Friday or a group of products are sold a lot on Tuesday and\nThursday.\n  In previous work we proposed a new measure of support (the number of\noccurrences of a pattern in a dataset), where we count the number of times a\npattern occurs (nearly) in the middle between two other occurrences. If the\nnumber of non-occurrences between two occurrences of a pattern stays almost the\nsame then we call the pattern balanced.\n  It was noticed that some very frequent patterns obviously also occur with a\nbalanced interval, meaning in every transaction. However more interesting\npatterns might occur, e.g., every three transactions. Here we discuss a\nsolution using standard deviation and average. Furthermore we propose a simpler\napproach for pruning patterns with a balanced interval, making estimating the\npruning threshold more intuitive.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 8 May 2007 15:22:38 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Kosters",
                "Edgar de Graaf Joost Kok Walter",
                ""
            ]
        ]
    },
    {
        "id": "0705.1209",
        "submitter": "Tshilidzi Marwala",
        "authors": "E. Habtemariam, T. Marwala and M. Lagazio",
        "title": "Artificial Intelligence for Conflict Management",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  Militarised conflict is one of the risks that have a significant impact on\nsociety. Militarised Interstate Dispute (MID) is defined as an outcome of\ninterstate interactions, which result on either peace or conflict. Effective\nprediction of the possibility of conflict between states is an important\ndecision support tool for policy makers. In a previous research, neural\nnetworks (NNs) have been implemented to predict the MID. Support Vector\nMachines (SVMs) have proven to be very good prediction techniques and are\nintroduced for the prediction of MIDs in this study and compared to neural\nnetworks. The results show that SVMs predict MID better than NNs while NNs give\nmore consistent and easy to interpret sensitivity analysis than SVMs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 May 2007 05:53:30 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Habtemariam",
                "E.",
                ""
            ],
            [
                "Marwala",
                "T.",
                ""
            ],
            [
                "Lagazio",
                "M.",
                ""
            ]
        ]
    },
    {
        "id": "0705.1214",
        "submitter": "Tshilidzi Marwala",
        "authors": "Tshilidzi Marwala",
        "title": "Control of Complex Systems Using Bayesian Networks and Genetic Algorithm",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.NE",
        "license": null,
        "abstract": "  A method based on Bayesian neural networks and genetic algorithm is proposed\nto control the fermentation process. The relationship between input and output\nvariables is modelled using Bayesian neural network that is trained using\nhybrid Monte Carlo method. A feedback loop based on genetic algorithm is used\nto change input variables so that the output variables are as close to the\ndesired target as possible without the loss of confidence level on the\nprediction that the neural network gives. The proposed procedure is found to\nreduce the distance between the desired target and measured outputs\nsignificantly.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 May 2007 07:08:58 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.1214",
        "submitter": "Tshilidzi Marwala",
        "authors": "Tshilidzi Marwala",
        "title": "Control of Complex Systems Using Bayesian Networks and Genetic Algorithm",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.NE",
        "license": null,
        "abstract": "  A method based on Bayesian neural networks and genetic algorithm is proposed\nto control the fermentation process. The relationship between input and output\nvariables is modelled using Bayesian neural network that is trained using\nhybrid Monte Carlo method. A feedback loop based on genetic algorithm is used\nto change input variables so that the output variables are as close to the\ndesired target as possible without the loss of confidence level on the\nprediction that the neural network gives. The proposed procedure is found to\nreduce the distance between the desired target and measured outputs\nsignificantly.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 May 2007 07:08:58 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.1227",
        "submitter": "Petar Popovski",
        "authors": "Petar Popovski, Hiroyuki Yomo, Kentaro Nishimori, and Rocco Di Taranto",
        "title": "Rate Adaptation for Cognitive Radio under Interference from Primary\n  Spectrum User",
        "comments": "submitted to IEEE Journal on Selected Areas in Communications\n  \"Cognitive Radio: Theory and Applications\", March 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.NI math.IT",
        "license": null,
        "abstract": "  A cognitive radio can operate as a secondary system in a given spectrum. This\noperation should use limited power in order not to disturb the communication by\nprimary spectrum user. Under such conditions, in this paper we investigate how\nto maximize the spectral efficiency in the secondary system. A secondary\nreceiver observes a multiple access channel of two users, the secondary and the\nprimary transmitter, respectively. We show that, for spectrally-efficient\noperation, the secondary system should apply Opportunistic Interference\nCancellation (OIC). With OIC, the secondary system decodes the primary signal\nwhen such an opportunity is created by the primary rate and the power received\nfrom the primary system. For such an operation, we derive the achievable data\nrate in the secondary system. When the primary signal is decodable, we devise a\nmethod, based on superposition coding, by which the secondary system can\nachieve the maximal possible rate. Finally, we investigate the power allocation\nin the secondary system when multiple channels are used. We show that the\noptimal power allocation with OIC can be achieved through intercepted\nwater-filling instead of the conventional water-filling. The results show a\nsignificant gain for the rate achieved through an opportunistic interference\ncancellation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 May 2007 08:50:53 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Popovski",
                "Petar",
                ""
            ],
            [
                "Yomo",
                "Hiroyuki",
                ""
            ],
            [
                "Nishimori",
                "Kentaro",
                ""
            ],
            [
                "Di Taranto",
                "Rocco",
                ""
            ]
        ]
    },
    {
        "id": "0705.1244",
        "submitter": "Marc Schoenauer",
        "authors": "Nicolas Godzik (INRIA Futurs, INRIA Rocquencourt), Marc Schoenauer\n  (INRIA Futurs, INRIA Rocquencourt), Mich\\`ele Sebag (INRIA Futurs, LRI)",
        "title": "Evolving Symbolic Controllers",
        "comments": null,
        "journal-ref": "Dans 4th European Workshop on Evolutionary Robotics, 2611 (2003)\n  638-650",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  The idea of symbolic controllers tries to bridge the gap between the top-down\nmanual design of the controller architecture, as advocated in Brooks'\nsubsumption architecture, and the bottom-up designer-free approach that is now\nstandard within the Evolutionary Robotics community. The designer provides a\nset of elementary behavior, and evolution is given the goal of assembling them\nto solve complex tasks. Two experiments are presented, demonstrating the\nefficiency and showing the recursiveness of this approach. In particular, the\nsensitivity with respect to the proposed elementary behaviors, and the\nrobustness w.r.t. generalization of the resulting controllers are studied in\ndetail.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 May 2007 09:53:31 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Godzik",
                "Nicolas",
                "",
                "INRIA Futurs, INRIA Rocquencourt"
            ],
            [
                "Schoenauer",
                "Marc",
                "",
                "INRIA Futurs, INRIA Rocquencourt"
            ],
            [
                "Sebag",
                "Mich\u00e8le",
                "",
                "INRIA Futurs, LRI"
            ]
        ]
    },
    {
        "id": "0705.1288",
        "submitter": "Tshilidzi Marwala",
        "authors": "E. Marais, T. Marwala",
        "title": "Predicting the Presence of Internet Worms using Novelty Detection",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  Internet worms cause billions of dollars in damage yearly, affecting millions\nof users worldwide. For countermeasures to be deployed timeously, it is\nnecessary to use an automated system to detect the spread of a worm. This paper\ndiscusses a method of determining the presence of a worm, based on routing\ninformation currently available from Internet routers. An autoencoder, which is\na specialized type of neural network, was used to detect anomalies in normal\nrouting behavior. The autoencoder was trained using information from a single\nrouter, and was able to detect both global instability caused by worms as well\nas localized routing instability.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 May 2007 13:33:30 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Marais",
                "E.",
                ""
            ],
            [
                "Marwala",
                "T.",
                ""
            ]
        ]
    },
    {
        "id": "0705.1309",
        "submitter": "Marc Schoenauer",
        "authors": "Alexandre Devert (INRIA Futurs), Nicolas Bred\\`eche (INRIA Futurs),\n  Marc Schoenauer (INRIA Futurs)",
        "title": "Robust Multi-Cellular Developmental Design",
        "comments": null,
        "journal-ref": "Dans Genetic and Evolutionary Computation COnference (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  This paper introduces a continuous model for Multi-cellular Developmental\nDesign. The cells are fixed on a 2D grid and exchange \"chemicals\" with their\nneighbors during the growth process. The quantity of chemicals that a cell\nproduces, as well as the differentiation value of the cell in the phenotype,\nare controlled by a Neural Network (the genotype) that takes as inputs the\nchemicals produced by the neighboring cells at the previous time step. In the\nproposed model, the number of iterations of the growth process is not\npre-determined, but emerges during evolution: only organisms for which the\ngrowth process stabilizes give a phenotype (the stable state), others are\ndeclared nonviable. The optimization of the controller is done using the NEAT\nalgorithm, that optimizes both the topology and the weights of the Neural\nNetworks. Though each cell only receives local information from its neighbors,\nthe experimental results of the proposed approach on the 'flags' problems (the\nphenotype must match a given 2D pattern) are almost as good as those of a\ndirect regression approach using the same model with global information.\nMoreover, the resulting multi-cellular organisms exhibit almost perfect\nself-healing characteristics.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 May 2007 15:33:34 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Devert",
                "Alexandre",
                "",
                "INRIA Futurs"
            ],
            [
                "Bred\u00e8che",
                "Nicolas",
                "",
                "INRIA Futurs"
            ],
            [
                "Schoenauer",
                "Marc",
                "",
                "INRIA Futurs"
            ]
        ]
    },
    {
        "id": "0705.1390",
        "submitter": "Tshilidzi Marwala",
        "authors": "M.A. Herzog, T. Marwala and P.S. Heyns",
        "title": "Machine and Component Residual Life Estimation through the Application\n  of Neural Networks",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": null,
        "abstract": "  This paper concerns the use of neural networks for predicting the residual\nlife of machines and components. In addition, the advantage of using\ncondition-monitoring data to enhance the predictive capability of these neural\nnetworks was also investigated. A number of neural network variations were\ntrained and tested with the data of two different reliability-related datasets.\nThe first dataset represents the renewal case where the failed unit is repaired\nand restored to a good-as-new condition. Data was collected in the laboratory\nby subjecting a series of similar test pieces to fatigue loading with a\nhydraulic actuator. The average prediction error of the various neural networks\nbeing compared varied from 431 to 841 seconds on this dataset, where test\npieces had a characteristic life of 8,971 seconds. The second dataset was\ncollected from a group of pumps used to circulate a water and magnetite\nsolution within a plant. The data therefore originated from a repaired system\naffected by reliability degradation. When optimized, the multi-layer perceptron\nneural networks trained with the Levenberg-Marquardt algorithm and the general\nregression neural network produced a sum-of-squares error within 11.1% of each\nother. The potential for using neural networks for residual life prediction and\nthe advantage of incorporating condition-based data into the model were proven\nfor both examples.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 May 2007 05:52:22 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Herzog",
                "M. A.",
                ""
            ],
            [
                "Marwala",
                "T.",
                ""
            ],
            [
                "Heyns",
                "P. S.",
                ""
            ]
        ]
    },
    {
        "id": "0705.1395",
        "submitter": "Damien Chablat",
        "authors": "Jean-Fran\\c{c}ois Petiot (IRCCyN), Damien Chablat (IRCCyN)",
        "title": "Subjective Evaluation of Forms in an Immersive Environment",
        "comments": null,
        "journal-ref": "Virtual Concept (2003) 1-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.RO",
        "license": null,
        "abstract": "  User's perception of product, by essence subjective, is a major topic in\nmarketing and industrial design. Many methods, based on users' tests, are used\nso as to characterise this perception. We are interested in three main methods:\nmultidimensional scaling, semantic differential method, and preference mapping.\nThese methods are used to built a perceptual space, in order to position the\nnew product, to specify requirements by the study of user's preferences, to\nevaluate some product attributes, related in particular to style (aesthetic).\nThese early stages of the design are primordial for a good orientation of the\nproject. In parallel, virtual reality tools and interfaces are more and more\nefficient for suggesting to the user complex feelings, and creating in this way\nvarious levels of perceptions. In this article, we present on an example the\nuse of multidimensional scaling, semantic differential method and preference\nmapping for the subjective assessment of virtual products. These products,\nwhich geometrical form is variable, are defined with a CAD model and are\nproposed to the user with a spacemouse and stereoscopic glasses. Advantages and\nlimitations of such evaluation is next discussed..\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 May 2007 06:54:11 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Petiot",
                "Jean-Fran\u00e7ois",
                "",
                "IRCCyN"
            ],
            [
                "Chablat",
                "Damien",
                "",
                "IRCCyN"
            ]
        ]
    },
    {
        "id": "0705.1452",
        "submitter": "Gregoire Henry",
        "authors": "Gr\\'egoire Henry (PPS), Michel Mauny (INRIA Rocquencourt, ENSTA-UMA),\n  Emmanuel Chailloux (PPS)",
        "title": "Typer la d\\'e-s\\'erialisation sans s\\'erialiser les types",
        "comments": null,
        "journal-ref": "Journ\\'ee francophone des langages applicatifs (JFLA) 2006\n  (01/2006)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  In this paper, we propose a way of assigning static type information to\nunmarshalling functions and we describe a verification technique for\nunmarshalled data that preserves the execution safety provided by static type\nchecking. This technique, whose correctness is proven, relies on singleton\ntypes whose values are transmitted to unmarshalling routines at runtime, and on\nan efficient checking algorithm able to deal with sharing and cycles.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 May 2007 12:19:51 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Henry",
                "Gr\u00e9goire",
                "",
                "PPS"
            ],
            [
                "Mauny",
                "Michel",
                "",
                "INRIA Rocquencourt, ENSTA-UMA"
            ],
            [
                "Chailloux",
                "Emmanuel",
                "",
                "PPS"
            ]
        ]
    },
    {
        "id": "0705.1453",
        "submitter": "Jerome Darmont",
        "authors": "J\\'er\\^ome Darmont (ERIC), Fadila Bentayeb (ERIC), Omar Boussa\\\"id\n  (ERIC)",
        "title": "DWEB: A Data Warehouse Engineering Benchmark",
        "comments": null,
        "journal-ref": "LNCS, Vol. 3589 (08/2005) 85-94",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  Data warehouse architectural choices and optimization techniques are critical\nto decision support query performance. To facilitate these choices, the\nperformance of the designed data warehouse must be assessed. This is usually\ndone with the help of benchmarks, which can either help system users comparing\nthe performances of different systems, or help system engineers testing the\neffect of various design choices. While the TPC standard decision support\nbenchmarks address the first point, they are not tuneable enough to address the\nsecond one and fail to model different data warehouse schemas. By contrast, our\nData Warehouse Engineering Benchmark (DWEB) allows to generate various ad-hoc\nsynthetic data warehouses and workloads. DWEB is fully parameterized to fulfill\ndata warehouse design needs. However, two levels of parameterization keep it\nrelatively easy to tune. Finally, DWEB is implemented as a Java free software\nthat can be interfaced with most existing relational database management\nsystems. A sample usage of DWEB is also provided in this paper.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 May 2007 12:23:35 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                "",
                "ERIC"
            ],
            [
                "Bentayeb",
                "Fadila",
                "",
                "ERIC"
            ],
            [
                "Boussa\u00efd",
                "Omar",
                "",
                "ERIC"
            ]
        ]
    },
    {
        "id": "0705.1454",
        "submitter": "Jerome Darmont",
        "authors": "Zhen He, J\\'er\\^ome Darmont (ERIC)",
        "title": "DOEF: A Dynamic Object Evaluation Framework",
        "comments": null,
        "journal-ref": "LNCS, Vol. 2736 (09/2003) 662-671",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  In object-oriented or object-relational databases such as multimedia\ndatabases or most XML databases, access patterns are not static, i.e.,\napplications do not always access the same objects in the same order\nrepeatedly. However, this has been the way these databases and associated\noptimisation techniques like clustering have been evaluated up to now. This\npaper opens up research regarding this issue by proposing a dynamic object\nevaluation framework (DOEF) that accomplishes access pattern change by defining\nconfigurable styles of change. This preliminary prototype has been designed to\nbe open and fully extensible. To illustrate the capabilities of DOEF, we used\nit to compare the performances of four state of the art dynamic clustering\nalgorithms. The results show that DOEF is indeed effective at determining the\nadaptability of each dynamic clustering algorithm to changes in access pattern.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 May 2007 12:24:27 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "He",
                "Zhen",
                "",
                "ERIC"
            ],
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                "",
                "ERIC"
            ]
        ]
    },
    {
        "id": "0705.1455",
        "submitter": "Jerome Darmont",
        "authors": "Fadila Bentayeb (ERIC), J\\'er\\^ome Darmont (ERIC)",
        "title": "Decision tree modeling with relational views",
        "comments": null,
        "journal-ref": "LNAI, Vol. 2366 (06/2002) 423-431",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  Data mining is a useful decision support technique that can be used to\ndiscover production rules in warehouses or corporate data. Data mining research\nhas made much effort to apply various mining algorithms efficiently on large\ndatabases. However, a serious problem in their practical application is the\nlong processing time of such algorithms. Nowadays, one of the key challenges is\nto integrate data mining methods within the framework of traditional database\nsystems. Indeed, such implementations can take advantage of the efficiency\nprovided by SQL engines. In this paper, we propose an integrating approach for\ndecision trees within a classical database system. In other words, we try to\ndiscover knowledge from relational databases, in the form of production rules,\nvia a procedure embedding SQL queries. The obtained decision tree is defined by\nsuccessive, related relational views. Each view corresponds to a given\npopulation in the underlying decision tree. We selected the classical Induction\nDecision Tree (ID3) algorithm to build the decision tree. To prove that our\nimplementation of ID3 works properly, we successfully compared the output of\nour procedure with the output of an existing and validated data mining\nsoftware, SIPINA. Furthermore, since our approach is tuneable, it can be\ngeneralized to any other similar decision tree-based method.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 May 2007 12:25:57 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Bentayeb",
                "Fadila",
                "",
                "ERIC"
            ],
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                "",
                "ERIC"
            ]
        ]
    },
    {
        "id": "0705.1456",
        "submitter": "Jerome Darmont",
        "authors": "J\\'er\\^ome Darmont (ERIC), Omar Boussa\\\"id (ERIC), Fadila Bentayeb\n  (ERIC)",
        "title": "Warehousing Web Data",
        "comments": null,
        "journal-ref": "4th International Conference on Information Integration and\n  Web-based Applications and Services (iiWAS 02) (09/2002) 148-152",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  In a data warehousing process, mastering the data preparation phase allows\nsubstantial gains in terms of time and performance when performing\nmultidimensional analysis or using data mining algorithms. Furthermore, a data\nwarehouse can require external data. The web is a prevalent data source in this\ncontext. In this paper, we propose a modeling process for integrating diverse\nand heterogeneous (so-called multiform) data into a unified format.\nFurthermore, the very schema definition provides first-rate metadata in our\ndata warehousing context. At the conceptual level, a complex object is\nrepresented in UML. Our logical model is an XML schema that can be described\nwith a DTD or the XML-Schema language. Eventually, we have designed a Java\nprototype that transforms our multiform input data into XML documents\nrepresenting our physical model. Then, the XML documents we obtain are mapped\ninto a relational database we view as an ODS (Operational Data Storage), whose\ncontent will have to be re-modeled in a multidimensional way to allow its\nstorage in a star schema-based warehouse and, later, its analysis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 May 2007 12:28:52 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                "",
                "ERIC"
            ],
            [
                "Boussa\u00efd",
                "Omar",
                "",
                "ERIC"
            ],
            [
                "Bentayeb",
                "Fadila",
                "",
                "ERIC"
            ]
        ]
    },
    {
        "id": "0705.1457",
        "submitter": "Jerome Darmont",
        "authors": "Sami Miniaoui (ERIC), J\\'er\\^ome Darmont (ERIC), Omar Boussa\\\"id\n  (ERIC)",
        "title": "Web data modeling for integration in data warehouses",
        "comments": null,
        "journal-ref": "First International Workshop on Multimedia Data and Document\n  Engineering (MDDE 01) (07/2001) 88-97",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  In a data warehousing process, the data preparation phase is crucial.\nMastering this phase allows substantial gains in terms of time and performance\nwhen performing a multidimensional analysis or using data mining algorithms.\nFurthermore, a data warehouse can require external data. The web is a prevalent\ndata source in this context, but the data broadcasted on this medium are very\nheterogeneous. We propose in this paper a UML conceptual model for a complex\nobject representing a superclass of any useful data source (databases, plain\ntexts, HTML and XML documents, images, sounds, video clips...). The translation\ninto a logical model is achieved with XML, which helps integrating all these\ndiverse, heterogeneous data into a unified format, and whose schema definition\nprovides first-rate metadata in our data warehousing context. Moreover, we\nbenefit from XML's flexibility, extensibility and from the richness of the\nsemi-structured data model, but we are still able to later map XML documents\ninto a database if more structuring is needed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 May 2007 12:30:19 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Miniaoui",
                "Sami",
                "",
                "ERIC"
            ],
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                "",
                "ERIC"
            ],
            [
                "Boussa\u00efd",
                "Omar",
                "",
                "ERIC"
            ]
        ]
    },
    {
        "id": "0705.1458",
        "submitter": "Gregoire Henry",
        "authors": "Emmanuel Chailloux (PPS), Gr\\'egoire Henry (PPS), Rapha\\\"el\n  Montelatici (PPS)",
        "title": "Mixing the Objective Caml and C# Programming Models in the .Net\n  Framework",
        "comments": null,
        "journal-ref": "Workshop on MULTIPARADIGM PROGRAMMING WITH OO LANGUAGES (MPOOL),\n  Norv\\`ege (06/2004)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  We present a new code generator, called O'Jacare.net, to inter-operate\nbetween C# and Objective Caml through their object models. O'Jacare.net defines\na basic IDL (Interface Definition Language) that describes classes and\ninterfaces in order to communicate between Objective Caml and C#. O'Jacare.net\ngenerates all needed wrapper classes and takes advantage of static type\nchecking in both worlds. Although the IDL intersects these two object models,\nO'Jacare.net allows to combine features from both.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 May 2007 12:31:17 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Chailloux",
                "Emmanuel",
                "",
                "PPS"
            ],
            [
                "Henry",
                "Gr\u00e9goire",
                "",
                "PPS"
            ],
            [
                "Montelatici",
                "Rapha\u00ebl",
                "",
                "PPS"
            ]
        ]
    },
    {
        "id": "0705.1481",
        "submitter": "Raihan Kibria",
        "authors": "Raihan H. Kibria",
        "title": "Actin - Technical Report",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  The Boolean satisfiability problem (SAT) can be solved efficiently with\nvariants of the DPLL algorithm. For industrial SAT problems, DPLL with conflict\nanalysis dependent dynamic decision heuristics has proved to be particularly\nefficient, e.g. in Chaff. In this work, algorithms that initialize the variable\nactivity values in the solver MiniSAT v1.14 by analyzing the CNF are evolved\nusing genetic programming (GP), with the goal to reduce the total number of\nconflicts of the search and the solving time. The effect of using initial\nactivities other than zero is examined by initializing with random numbers. The\npossibility of countering the detrimental effects of reordering the CNF with\nimproved initialization is investigated. The best result found (with validation\ntesting on further problems) was used in the solver Actin, which was submitted\nto SAT-Race 2006.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 May 2007 14:10:08 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Kibria",
                "Raihan H.",
                ""
            ]
        ]
    },
    {
        "id": "0705.1583",
        "submitter": "Sourav Dhar",
        "authors": "Sourav Dhar and Rabindranath Bera",
        "title": "Wireless Networking to Support Data and Voice Communication Using Spread\n  Spectrum Technology in The Physical Layer",
        "comments": "international conference on information technology, March 2007. 7\n  pages,4 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  Wireless networking is rapidly growing and becomes an inexpensive technology\nwhich allows multiple users to simultaneously access the network and the\ninternet while roaming about the campus. In the present work, the software\ndevelopment of a wireless LAN(WLAN) is highlighted. This WLAN utilizes direct\nsequence spread spectrum (DSSS) technology at 902MHz RF carrier frequency in\nits physical layer. Cost effective installation and antijaming property of\nspread spectrum technology are the major advantages of this work.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 May 2007 04:41:35 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Dhar",
                "Sourav",
                ""
            ],
            [
                "Bera",
                "Rabindranath",
                ""
            ]
        ]
    },
    {
        "id": "0705.1585",
        "submitter": "Tshilidzi Marwala",
        "authors": "Unathi Mahola, Fulufhelo V. Nelwamondo, Tshilidzi Marwala",
        "title": "HMM Speaker Identification Using Linear and Non-linear Merging\n  Techniques",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  Speaker identification is a powerful, non-invasive and in-expensive biometric\ntechnique. The recognition accuracy, however, deteriorates when noise levels\naffect a specific band of frequency. In this paper, we present a sub-band based\nspeaker identification that intends to improve the live testing performance.\nEach frequency sub-band is processed and classified independently. We also\ncompare the linear and non-linear merging techniques for the sub-bands\nrecognizer. Support vector machines and Gaussian Mixture models are the\nnon-linear merging techniques that are investigated. Results showed that the\nsub-band based method used with linear merging techniques enormously improved\nthe performance of the speaker identification over the performance of wide-band\nrecognizers when tested live. A live testing improvement of 9.78% was achieved\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 May 2007 04:54:54 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Mahola",
                "Unathi",
                ""
            ],
            [
                "Nelwamondo",
                "Fulufhelo V.",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.1617",
        "submitter": "Daegene Song",
        "authors": "Daegene Song",
        "title": "Non-Computability of Consciousness",
        "comments": "10 pages, 2 figures, 1 table",
        "journal-ref": "NeuroQuantology 5, 382 (2007).",
        "doi": null,
        "report-no": null,
        "categories": "quant-ph astro-ph cs.AI",
        "license": null,
        "abstract": "  With the great success in simulating many intelligent behaviors using\ncomputing devices, there has been an ongoing debate whether all conscious\nactivities are computational processes. In this paper, the answer to this\nquestion is shown to be no. A certain phenomenon of consciousness is\ndemonstrated to be fully represented as a computational process using a quantum\ncomputer. Based on the computability criterion discussed with Turing machines,\nthe model constructed is shown to necessarily involve a non-computable element.\nThe concept that this is solely a quantum effect and does not work for a\nclassical case is also discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 May 2007 10:16:48 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Song",
                "Daegene",
                ""
            ]
        ]
    },
    {
        "id": "0705.1672",
        "submitter": "Tshilidzi Marwala",
        "authors": "L. Mdlazi, T. Marwala, C.J. Stander, C. Scheffer and P.S. Heyns",
        "title": "Principal Component Analysis and Automatic Relevance Determination in\n  Damage Identification",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": null,
        "abstract": "  This paper compares two neural network input selection schemes, the Principal\nComponent Analysis (PCA) and the Automatic Relevance Determination (ARD) based\non Mac-Kay's evidence framework. The PCA takes all the input data and projects\nit onto a lower dimension space, thereby reduc-ing the dimension of the input\nspace. This input reduction method often results with parameters that have\nsignificant influence on the dynamics of the data being diluted by those that\ndo not influence the dynamics of the data. The ARD selects the most relevant\ninput parameters and discards those that do not contribute significantly to the\ndynamics of the data being modelled. The ARD sometimes results with important\ninput parameters being discarded thereby compromising the dynamics of the data.\nThe PCA and ARD methods are implemented together with a Multi-Layer-Perceptron\n(MLP) network for fault identification in structures and the performance of the\ntwo methods is as-sessed. It is observed that ARD and PCA give similar\naccu-racy levels when used as input-selection schemes. There-fore, the choice\nof input-selection scheme is dependent on the nature of the data being\nprocessed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 May 2007 15:35:22 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Mdlazi",
                "L.",
                ""
            ],
            [
                "Marwala",
                "T.",
                ""
            ],
            [
                "Stander",
                "C. J.",
                ""
            ],
            [
                "Scheffer",
                "C.",
                ""
            ],
            [
                "Heyns",
                "P. S.",
                ""
            ]
        ]
    },
    {
        "id": "0705.1673",
        "submitter": "Tshilidzi Marwala",
        "authors": "L. Mdlazi, C.J. Stander, P.S. Heyns and T. Marwala",
        "title": "Using artificial intelligence for data reduction in mechanical\n  engineering",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.AI cs.NE",
        "license": null,
        "abstract": "  In this paper artificial neural networks and support vector machines are used\nto reduce the amount of vibration data that is required to estimate the Time\nDomain Average of a gear vibration signal. Two models for estimating the time\ndomain average of a gear vibration signal are proposed. The models are tested\non data from an accelerated gear life test rig. Experimental results indicate\nthat the required data for calculating the Time Domain Average of a gear\nvibration signal can be reduced by up to 75% when the proposed models are\nimplemented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 May 2007 15:49:40 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Mdlazi",
                "L.",
                ""
            ],
            [
                "Stander",
                "C. J.",
                ""
            ],
            [
                "Heyns",
                "P. S.",
                ""
            ],
            [
                "Marwala",
                "T.",
                ""
            ]
        ]
    },
    {
        "id": "0705.1673",
        "submitter": "Tshilidzi Marwala",
        "authors": "L. Mdlazi, C.J. Stander, P.S. Heyns and T. Marwala",
        "title": "Using artificial intelligence for data reduction in mechanical\n  engineering",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.AI cs.NE",
        "license": null,
        "abstract": "  In this paper artificial neural networks and support vector machines are used\nto reduce the amount of vibration data that is required to estimate the Time\nDomain Average of a gear vibration signal. Two models for estimating the time\ndomain average of a gear vibration signal are proposed. The models are tested\non data from an accelerated gear life test rig. Experimental results indicate\nthat the required data for calculating the Time Domain Average of a gear\nvibration signal can be reduced by up to 75% when the proposed models are\nimplemented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 May 2007 15:49:40 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Mdlazi",
                "L.",
                ""
            ],
            [
                "Stander",
                "C. J.",
                ""
            ],
            [
                "Heyns",
                "P. S.",
                ""
            ],
            [
                "Marwala",
                "T.",
                ""
            ]
        ]
    },
    {
        "id": "0705.1673",
        "submitter": "Tshilidzi Marwala",
        "authors": "L. Mdlazi, C.J. Stander, P.S. Heyns and T. Marwala",
        "title": "Using artificial intelligence for data reduction in mechanical\n  engineering",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.AI cs.NE",
        "license": null,
        "abstract": "  In this paper artificial neural networks and support vector machines are used\nto reduce the amount of vibration data that is required to estimate the Time\nDomain Average of a gear vibration signal. Two models for estimating the time\ndomain average of a gear vibration signal are proposed. The models are tested\non data from an accelerated gear life test rig. Experimental results indicate\nthat the required data for calculating the Time Domain Average of a gear\nvibration signal can be reduced by up to 75% when the proposed models are\nimplemented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 May 2007 15:49:40 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Mdlazi",
                "L.",
                ""
            ],
            [
                "Stander",
                "C. J.",
                ""
            ],
            [
                "Heyns",
                "P. S.",
                ""
            ],
            [
                "Marwala",
                "T.",
                ""
            ]
        ]
    },
    {
        "id": "0705.1674",
        "submitter": "Tshilidzi Marwala",
        "authors": "Lukasz A Machowski, Tshilidzi Marwala",
        "title": "Evolutionary Optimisation Methods for Template Based Image Registration",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.CV",
        "license": null,
        "abstract": "  This paper investigates the use of evolutionary optimisation techniques to\nregister a template with a scene image. An error function is created to measure\nthe correspondence of the template to the image. The problem presented here is\nto optimise the horizontal, vertical and scaling parameters that register the\ntemplate with the scene. The Genetic Algorithm, Simulated Annealing and\nParticle Swarm Optimisations are compared to a Nelder-Mead Simplex optimisation\nwith starting points chosen in a pre-processing stage. The paper investigates\nthe precision and accuracy of each method and shows that all four methods\nperform favourably for image registration. SA is the most precise, GA is the\nmost accurate. PSO is a good mix of both and the Simplex method returns local\nminima the most. A pre-processing stage should be investigated for the\nevolutionary methods in order to improve performance. Discrete versions of the\noptimisation methods should be investigated to further improve computational\nperformance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 May 2007 15:51:36 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Machowski",
                "Lukasz A",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.1674",
        "submitter": "Tshilidzi Marwala",
        "authors": "Lukasz A Machowski, Tshilidzi Marwala",
        "title": "Evolutionary Optimisation Methods for Template Based Image Registration",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.CV",
        "license": null,
        "abstract": "  This paper investigates the use of evolutionary optimisation techniques to\nregister a template with a scene image. An error function is created to measure\nthe correspondence of the template to the image. The problem presented here is\nto optimise the horizontal, vertical and scaling parameters that register the\ntemplate with the scene. The Genetic Algorithm, Simulated Annealing and\nParticle Swarm Optimisations are compared to a Nelder-Mead Simplex optimisation\nwith starting points chosen in a pre-processing stage. The paper investigates\nthe precision and accuracy of each method and shows that all four methods\nperform favourably for image registration. SA is the most precise, GA is the\nmost accurate. PSO is a good mix of both and the Simplex method returns local\nminima the most. A pre-processing stage should be investigated for the\nevolutionary methods in order to improve performance. Discrete versions of the\noptimisation methods should be investigated to further improve computational\nperformance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 May 2007 15:51:36 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Machowski",
                "Lukasz A",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.1680",
        "submitter": "Tshilidzi Marwala",
        "authors": "Michael Maio Pires, Tshilidzi Marwala",
        "title": "Option Pricing Using Bayesian Neural Networks",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.NE",
        "license": null,
        "abstract": "  Options have provided a field of much study because of the complexity\ninvolved in pricing them. The Black-Scholes equations were developed to price\noptions but they are only valid for European styled options. There is added\ncomplexity when trying to price American styled options and this is why the use\nof neural networks has been proposed. Neural Networks are able to predict\noutcomes based on past data. The inputs to the networks here are stock\nvolatility, strike price and time to maturity with the output of the network\nbeing the call option price. There are two techniques for Bayesian neural\nnetworks used. One is Automatic Relevance Determination (for Gaussian\nApproximation) and one is a Hybrid Monte Carlo method, both used with\nMulti-Layer Perceptrons.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 May 2007 15:55:31 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Pires",
                "Michael Maio",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.1680",
        "submitter": "Tshilidzi Marwala",
        "authors": "Michael Maio Pires, Tshilidzi Marwala",
        "title": "Option Pricing Using Bayesian Neural Networks",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.NE",
        "license": null,
        "abstract": "  Options have provided a field of much study because of the complexity\ninvolved in pricing them. The Black-Scholes equations were developed to price\noptions but they are only valid for European styled options. There is added\ncomplexity when trying to price American styled options and this is why the use\nof neural networks has been proposed. Neural Networks are able to predict\noutcomes based on past data. The inputs to the networks here are stock\nvolatility, strike price and time to maturity with the output of the network\nbeing the call option price. There are two techniques for Bayesian neural\nnetworks used. One is Automatic Relevance Determination (for Gaussian\nApproximation) and one is a Hybrid Monte Carlo method, both used with\nMulti-Layer Perceptrons.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 May 2007 15:55:31 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Pires",
                "Michael Maio",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.1759",
        "submitter": "Tshilidzi Marwala",
        "authors": "Tshilidzi Marwala",
        "title": "Finite Element Model Updating Using Response Surface Method",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": null,
        "abstract": "  This paper proposes the response surface method for finite element model\nupdating. The response surface method is implemented by approximating the\nfinite element model surface response equation by a multi-layer perceptron. The\nupdated parameters of the finite element model were calculated using genetic\nalgorithm by optimizing the surface response equation. The proposed method was\ncompared to the existing methods that use simulated annealing or genetic\nalgorithm together with a full finite element model for finite element model\nupdating. The proposed method was tested on an unsymmetri-cal H-shaped\nstructure. It was observed that the proposed method gave the updated natural\nfrequen-cies and mode shapes that were of the same order of accuracy as those\ngiven by simulated annealing and genetic algorithm. Furthermore, it was\nobserved that the response surface method achieved these results at a\ncomputational speed that was more than 2.5 times as fast as the genetic\nalgorithm and a full finite element model and 24 times faster than the\nsimulated annealing.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 12 May 2007 10:25:22 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.1760",
        "submitter": "Tshilidzi Marwala",
        "authors": "Tshilidzi Marwala",
        "title": "Dynamic Model Updating Using Particle Swarm Optimization Method",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.NE",
        "license": null,
        "abstract": "  This paper proposes the use of particle swarm optimization method (PSO) for\nfinite element (FE) model updating. The PSO method is compared to the existing\nmethods that use simulated annealing (SA) or genetic algorithms (GA) for FE\nmodel for model updating. The proposed method is tested on an unsymmetrical\nH-shaped structure. It is observed that the proposed method gives updated\nnatural frequencies the most accurate and followed by those given by an updated\nmodel that was obtained using the GA and a full FE model. It is also observed\nthat the proposed method gives updated mode shapes that are best correlated to\nthe measured ones, followed by those given by an updated model that was\nobtained using the SA and a full FE model. Furthermore, it is observed that the\nPSO achieves this accuracy at a computational speed that is faster than that by\nthe GA and a full FE model which is faster than the SA and a full FE model.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 12 May 2007 10:27:07 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.1760",
        "submitter": "Tshilidzi Marwala",
        "authors": "Tshilidzi Marwala",
        "title": "Dynamic Model Updating Using Particle Swarm Optimization Method",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.NE",
        "license": null,
        "abstract": "  This paper proposes the use of particle swarm optimization method (PSO) for\nfinite element (FE) model updating. The PSO method is compared to the existing\nmethods that use simulated annealing (SA) or genetic algorithms (GA) for FE\nmodel for model updating. The proposed method is tested on an unsymmetrical\nH-shaped structure. It is observed that the proposed method gives updated\nnatural frequencies the most accurate and followed by those given by an updated\nmodel that was obtained using the GA and a full FE model. It is also observed\nthat the proposed method gives updated mode shapes that are best correlated to\nthe measured ones, followed by those given by an updated model that was\nobtained using the SA and a full FE model. Furthermore, it is observed that the\nPSO achieves this accuracy at a computational speed that is faster than that by\nthe GA and a full FE model which is faster than the SA and a full FE model.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 12 May 2007 10:27:07 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.1789",
        "submitter": "Lu\\'isa Lima",
        "authors": "Lu\\'isa Lima and Muriel M\\'edard and Jo\\~ao Barros",
        "title": "Random Linear Network Coding: A free cipher?",
        "comments": "5 pages, 2 figures, Accepted for the IEEE International Symposium on\n  Information Theory, Nice, France, June, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.CR math.IT",
        "license": null,
        "abstract": "  We consider the level of information security provided by random linear\nnetwork coding in network scenarios in which all nodes comply with the\ncommunication protocols yet are assumed to be potential eavesdroppers (i.e.\n\"nice but curious\"). For this setup, which differs from wiretapping scenarios\nconsidered previously, we develop a natural algebraic security criterion, and\nprove several of its key properties. A preliminary analysis of the impact of\nnetwork topology on the overall network coding security, in particular for\ncomplete directed acyclic graphs, is also included.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 12 May 2007 18:11:48 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Lima",
                "Lu\u00edsa",
                ""
            ],
            [
                "M\u00e9dard",
                "Muriel",
                ""
            ],
            [
                "Barros",
                "Jo\u00e3o",
                ""
            ]
        ]
    },
    {
        "id": "0705.1915",
        "submitter": "John Kouvakis",
        "authors": "John Kouvakis, Fotis Georgatos",
        "title": "A Technical Report On Grid Benchmarking using ATLAS V.O",
        "comments": "29 pages, 35 figures, including charts and results of benchmarking\n  over the grid, ATLAS V.O",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF",
        "license": null,
        "abstract": "  Grids include heterogeneous resources, which are based on different hardware\nand software architectures or components. In correspondence with this diversity\nof the infrastructure, the execution time of any single job, as well as the\ntotal grid performance can both be affected substantially, which can be\ndemonstrated by measurements. Running a simple benchmarking suite can show this\nheterogeneity and give us results about the differences over the grid sites.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 May 2007 11:39:52 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 25 May 2007 07:56:22 GMT"
            },
            {
                "version": "v3",
                "created": "Sun, 3 Jun 2007 06:57:27 GMT"
            }
        ],
        "update_date": "2007-06-13",
        "authors_parsed": [
            [
                "Kouvakis",
                "John",
                ""
            ],
            [
                "Georgatos",
                "Fotis",
                ""
            ]
        ]
    },
    {
        "id": "0705.1919",
        "submitter": "Erez Sabbag",
        "authors": "Neri Merhav and Erez Sabbag",
        "title": "Optimal Watermark Embedding and Detection Strategies Under Limited\n  Detection Resources",
        "comments": "36 pages, 5 figures. Revised version. Submitted to IEEE Transactions\n  on Information Theory",
        "journal-ref": null,
        "doi": "10.1109/ISIT.2006.261759",
        "report-no": null,
        "categories": "cs.IT cs.CR math.IT",
        "license": null,
        "abstract": "  An information-theoretic approach is proposed to watermark embedding and\ndetection under limited detector resources. First, we consider the attack-free\nscenario under which asymptotically optimal decision regions in the\nNeyman-Pearson sense are proposed, along with the optimal embedding rule.\nLater, we explore the case of zero-mean i.i.d. Gaussian covertext distribution\nwith unknown variance under the attack-free scenario. For this case, we propose\na lower bound on the exponential decay rate of the false-negative probability\nand prove that the optimal embedding and detecting strategy is superior to the\ncustomary linear, additive embedding strategy in the exponential sense.\nFinally, these results are extended to the case of memoryless attacks and\ngeneral worst case attacks. Optimal decision regions and embedding rules are\noffered, and the worst attack channel is identified.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 May 2007 12:00:29 GMT"
            }
        ],
        "update_date": "2016-11-15",
        "authors_parsed": [
            [
                "Merhav",
                "Neri",
                ""
            ],
            [
                "Sabbag",
                "Erez",
                ""
            ]
        ]
    },
    {
        "id": "0705.1925",
        "submitter": "Jidong Zhong",
        "authors": "Jidong Zhong and Shangteng Huang",
        "title": "Double Sided Watermark Embedding and Detection with Perceptual Analysis",
        "comments": "This paper is a supplement to a paper to be published in IEEE\n  Transactions on Information Forensics and Security",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MM cs.CR",
        "license": null,
        "abstract": "  In our previous work, we introduced a double-sided technique that utilizes\nbut not reject the host interference. Due to its nice property of utilizing but\nnot rejecting the host interference, it has a big advantage over the host\ninterference schemes in that the perceptual analysis can be easily implemented\nfor our scheme to achieve the locally bounded maximum embedding strength. Thus,\nin this work, we detail how to implement the perceptual analysis in our\ndouble-sided schemes since the perceptual analysis is very important for\nimproving the fidelity of watermarked contents. Through the extensive\nperformance comparisons, we can further validate the performance advantage of\nour double-sided schemes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 May 2007 12:23:43 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Zhong",
                "Jidong",
                ""
            ],
            [
                "Huang",
                "Shangteng",
                ""
            ]
        ]
    },
    {
        "id": "0705.1939",
        "submitter": "Hamed Haddadi MSc MIEE",
        "authors": "Richard G. Clegg, Hamed Haddadi, Raul Landa, Miguel Rio",
        "title": "Towards Informative Statistical Flow Inversion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": null,
        "abstract": "  A problem which has recently attracted research attention is that of\nestimating the distribution of flow sizes in internet traffic. On high traffic\nlinks it is sometimes impossible to record every packet. Researchers have\napproached the problem of estimating flow lengths from sampled packet data in\ntwo separate ways. Firstly, different sampling methodologies can be tried to\nmore accurately measure the desired system parameters. One such method is the\nsample-and-hold method where, if a packet is sampled, all subsequent packets in\nthat flow are sampled. Secondly, statistical methods can be used to ``invert''\nthe sampled data and produce an estimate of flow lengths from a sample.\n  In this paper we propose, implement and test two variants on the\nsample-and-hold method. In addition we show how the sample-and-hold method can\nbe inverted to get an estimation of the genuine distribution of flow sizes.\nExperiments are carried out on real network traces to compare standard packet\nsampling with three variants of sample-and-hold. The methods are compared for\ntheir ability to reconstruct the genuine distribution of flow sizes in the\ntraffic.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 May 2007 13:14:33 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Clegg",
                "Richard G.",
                ""
            ],
            [
                "Haddadi",
                "Hamed",
                ""
            ],
            [
                "Landa",
                "Raul",
                ""
            ],
            [
                "Rio",
                "Miguel",
                ""
            ]
        ]
    },
    {
        "id": "0705.1939",
        "submitter": "Hamed Haddadi MSc MIEE",
        "authors": "Richard G. Clegg, Hamed Haddadi, Raul Landa, Miguel Rio",
        "title": "Towards Informative Statistical Flow Inversion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": null,
        "abstract": "  A problem which has recently attracted research attention is that of\nestimating the distribution of flow sizes in internet traffic. On high traffic\nlinks it is sometimes impossible to record every packet. Researchers have\napproached the problem of estimating flow lengths from sampled packet data in\ntwo separate ways. Firstly, different sampling methodologies can be tried to\nmore accurately measure the desired system parameters. One such method is the\nsample-and-hold method where, if a packet is sampled, all subsequent packets in\nthat flow are sampled. Secondly, statistical methods can be used to ``invert''\nthe sampled data and produce an estimate of flow lengths from a sample.\n  In this paper we propose, implement and test two variants on the\nsample-and-hold method. In addition we show how the sample-and-hold method can\nbe inverted to get an estimation of the genuine distribution of flow sizes.\nExperiments are carried out on real network traces to compare standard packet\nsampling with three variants of sample-and-hold. The methods are compared for\ntheir ability to reconstruct the genuine distribution of flow sizes in the\ntraffic.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 May 2007 13:14:33 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Clegg",
                "Richard G.",
                ""
            ],
            [
                "Haddadi",
                "Hamed",
                ""
            ],
            [
                "Landa",
                "Raul",
                ""
            ],
            [
                "Rio",
                "Miguel",
                ""
            ]
        ]
    },
    {
        "id": "0705.1999",
        "submitter": "Camilla Schwind",
        "authors": "Camilla Schwind (LIF)",
        "title": "A first-order Temporal Logic for Actions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LO",
        "license": null,
        "abstract": "  We present a multi-modal action logic with first-order modalities, which\ncontain terms which can be unified with the terms inside the subsequent\nformulas and which can be quantified. This makes it possible to handle\nsimultaneously time and states. We discuss applications of this language to\naction theory where it is possible to express many temporal aspects of actions,\nas for example, beginning, end, time points, delayed preconditions and results,\nduration and many others. We present tableaux rules for a decidable fragment of\nthis logic.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 May 2007 18:36:25 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Schwind",
                "Camilla",
                "",
                "LIF"
            ]
        ]
    },
    {
        "id": "0705.2011",
        "submitter": "Alex Graves",
        "authors": "Alex Graves, Santiago Fernandez, Juergen Schmidhuber",
        "title": "Multi-Dimensional Recurrent Neural Networks",
        "comments": "10 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": "04-07",
        "categories": "cs.AI cs.CV",
        "license": null,
        "abstract": "  Recurrent neural networks (RNNs) have proved effective at one dimensional\nsequence learning tasks, such as speech and online handwriting recognition.\nSome of the properties that make RNNs suitable for such tasks, for example\nrobustness to input warping, and the ability to access contextual information,\nare also desirable in multidimensional domains. However, there has so far been\nno direct way of applying RNNs to data with more than one spatio-temporal\ndimension. This paper introduces multi-dimensional recurrent neural networks\n(MDRNNs), thereby extending the potential applicability of RNNs to vision,\nvideo processing, medical imaging and many other areas, while avoiding the\nscaling problems that have plagued other multi-dimensional models. Experimental\nresults are provided for two image segmentation tasks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 May 2007 19:49:56 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Graves",
                "Alex",
                ""
            ],
            [
                "Fernandez",
                "Santiago",
                ""
            ],
            [
                "Schmidhuber",
                "Juergen",
                ""
            ]
        ]
    },
    {
        "id": "0705.2011",
        "submitter": "Alex Graves",
        "authors": "Alex Graves, Santiago Fernandez, Juergen Schmidhuber",
        "title": "Multi-Dimensional Recurrent Neural Networks",
        "comments": "10 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": "04-07",
        "categories": "cs.AI cs.CV",
        "license": null,
        "abstract": "  Recurrent neural networks (RNNs) have proved effective at one dimensional\nsequence learning tasks, such as speech and online handwriting recognition.\nSome of the properties that make RNNs suitable for such tasks, for example\nrobustness to input warping, and the ability to access contextual information,\nare also desirable in multidimensional domains. However, there has so far been\nno direct way of applying RNNs to data with more than one spatio-temporal\ndimension. This paper introduces multi-dimensional recurrent neural networks\n(MDRNNs), thereby extending the potential applicability of RNNs to vision,\nvideo processing, medical imaging and many other areas, while avoiding the\nscaling problems that have plagued other multi-dimensional models. Experimental\nresults are provided for two image segmentation tasks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 May 2007 19:49:56 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Graves",
                "Alex",
                ""
            ],
            [
                "Fernandez",
                "Santiago",
                ""
            ],
            [
                "Schmidhuber",
                "Juergen",
                ""
            ]
        ]
    },
    {
        "id": "0705.2065",
        "submitter": "Aaron Harwood",
        "authors": "Aaron Harwood, Olga Ohrimenko",
        "title": "Mean Field Models of Message Throughput in Dynamic Peer-to-Peer Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.PF",
        "license": null,
        "abstract": "  The churn rate of a peer-to-peer system places direct limitations on the rate\nat which messages can be effectively communicated to a group of peers. These\nlimitations are independent of the topology and message transmission latency.\nIn this paper we consider a peer-to-peer network, based on the Engset model,\nwhere peers arrive and depart independently at random. We show how the arrival\nand departure rates directly limit the capacity for message streams to be\nbroadcast to all other peers, by deriving mean field models that accurately\ndescribe the system behavior. Our models cover the unit and more general k\nbuffer cases, i.e. where a peer can buffer at most k messages at any one time,\nand we give results for both single and multi-source message streams. We define\ncoverage rate as peer-messages per unit time, i.e. the rate at which a number\nof peers receive messages, and show that the coverage rate is limited by the\nchurn rate and buffer size. Our theory introduces an Instantaneous Message\nExchange (IME) model and provides a template for further analysis of more\ncomplicated systems. Using the IME model, and assuming random processes, we\nhave obtained very accurate equations of the system dynamics in a variety of\ninteresting cases, that allow us to tune a peer-to-peer system. It remains to\nbe seen if we can maintain this accuracy for general processes and when\napplying a non-instantaneous model.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 May 2007 01:42:44 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Harwood",
                "Aaron",
                ""
            ],
            [
                "Ohrimenko",
                "Olga",
                ""
            ]
        ]
    },
    {
        "id": "0705.2065",
        "submitter": "Aaron Harwood",
        "authors": "Aaron Harwood, Olga Ohrimenko",
        "title": "Mean Field Models of Message Throughput in Dynamic Peer-to-Peer Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.PF",
        "license": null,
        "abstract": "  The churn rate of a peer-to-peer system places direct limitations on the rate\nat which messages can be effectively communicated to a group of peers. These\nlimitations are independent of the topology and message transmission latency.\nIn this paper we consider a peer-to-peer network, based on the Engset model,\nwhere peers arrive and depart independently at random. We show how the arrival\nand departure rates directly limit the capacity for message streams to be\nbroadcast to all other peers, by deriving mean field models that accurately\ndescribe the system behavior. Our models cover the unit and more general k\nbuffer cases, i.e. where a peer can buffer at most k messages at any one time,\nand we give results for both single and multi-source message streams. We define\ncoverage rate as peer-messages per unit time, i.e. the rate at which a number\nof peers receive messages, and show that the coverage rate is limited by the\nchurn rate and buffer size. Our theory introduces an Instantaneous Message\nExchange (IME) model and provides a template for further analysis of more\ncomplicated systems. Using the IME model, and assuming random processes, we\nhave obtained very accurate equations of the system dynamics in a variety of\ninteresting cases, that allow us to tune a peer-to-peer system. It remains to\nbe seen if we can maintain this accuracy for general processes and when\napplying a non-instantaneous model.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 May 2007 01:42:44 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Harwood",
                "Aaron",
                ""
            ],
            [
                "Ohrimenko",
                "Olga",
                ""
            ]
        ]
    },
    {
        "id": "0705.2084",
        "submitter": "Sourav Dhar",
        "authors": "Rabindranath Bera, Jitendranath Bera, Sanjib Sil, Dipak Mondal, Sourav\n  Dhar and Debdatta Kandar",
        "title": "CDMA Technology for Intelligent Transportation Systems",
        "comments": "6pages, 8 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  Scientists and Technologists involved in the development of radar and remote\nsensing systems all over the world are now trying to involve themselves in\nsaving of manpower in the form of developing a new application of their ideas\nin Intelligent Transport system(ITS). The world statistics shows that by\nincorporating such wireless radar system in the car would decrease the world\nroad accident by 8-10% yearly. The wireless technology has to be chosen\nproperly which is capable of tackling the severe interferences present in the\nopen road. A combined digital technology like Spread spectrum along with\ndiversity reception will help a lot in this regard. Accordingly, the choice is\nfor FHSS based space diversity system which will utilize carrier frequency\naround 5.8 GHz ISM band with available bandwidth of 80 MHz and no license. For\nefficient design, the radio channel is characterized on which the design is\nbased. Out of two available modes e.g. Communication and Radar modes, the radar\nmode is providing the conditional measurement of the range of the nearest car\nafter authentication of the received code, thus ensuring the reliability and\naccuracy of measurement. To make the system operational in simultaneous mode,\nwe have started the Software Defined Radio approach for best speed and\nflexibility.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 May 2007 06:14:10 GMT"
            }
        ],
        "update_date": "2019-08-19",
        "authors_parsed": [
            [
                "Bera",
                "Rabindranath",
                ""
            ],
            [
                "Bera",
                "Jitendranath",
                ""
            ],
            [
                "Sil",
                "Sanjib",
                ""
            ],
            [
                "Mondal",
                "Dipak",
                ""
            ],
            [
                "Dhar",
                "Sourav",
                ""
            ],
            [
                "Kandar",
                "Debdatta",
                ""
            ]
        ]
    },
    {
        "id": "0705.2085",
        "submitter": "Sourav Dhar",
        "authors": "Rabindranath Bera, Jitendranath Bera, Sanjib Sil, Sourav Dhar,\n  Debdatta Kandar, Dipak Mondal",
        "title": "RADAR Imaging in the Open field At 300 MHz-3000 MHz Radio Band",
        "comments": "published in IRSI 2005,5pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  With the technological growth of broadband wireless technology like CDMA and\nUWB, a lots of development efforts towards wireless communication system and\nImaging radar system are well justified. Efforts are also being imparted\ntowards a Convergence Technology.. the convergence between a communication and\nradar technology which will result in ITS (Intelligent Transport System) and\nother applications. This encourages present authors for this development. They\nare trying to utilize or converge the communication technologies towards radar\nand to achieve the Interference free and clutter free quality remote images of\ntargets using DS-UWB wireless technology.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 May 2007 06:24:22 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Bera",
                "Rabindranath",
                ""
            ],
            [
                "Bera",
                "Jitendranath",
                ""
            ],
            [
                "Sil",
                "Sanjib",
                ""
            ],
            [
                "Dhar",
                "Sourav",
                ""
            ],
            [
                "Kandar",
                "Debdatta",
                ""
            ],
            [
                "Mondal",
                "Dipak",
                ""
            ]
        ]
    },
    {
        "id": "0705.2126",
        "submitter": "Francois De Ferriere",
        "authors": "Francois De Ferriere",
        "title": "Improvements to the Psi-SSA representation",
        "comments": null,
        "journal-ref": "Published in proceedings for the workshop \"Software and Compilers\n  for Embedded Systems (SCOPES) 2007\" (20/04/2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  Modern compiler implementations use the Static Single Assignment\nrepresentation as a way to efficiently implement optimizing algorithms. However\nthis representation is not well adapted to architectures with a predicated\ninstruction set. The Psi-SSA representation extends the SSA representation such\nthat standard SSA algorithms can be easily adapted to an architecture with a\nfully predicated instruction set. A new pseudo operation, the Psi operation, is\nintroduced to merge several conditional definitions into a unique definition.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 May 2007 12:06:32 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "De Ferriere",
                "Francois",
                ""
            ]
        ]
    },
    {
        "id": "0705.2145",
        "submitter": "Paul Feautrier",
        "authors": "Paul Feautrier (LIP, INRIA Rh\\^one-Alpes)",
        "title": "Elementary transformation analysis for Array-OL",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  Array-OL is a high-level specification language dedicated to the definition\nof intensive signal processing applications. Several tools exist for\nimplementing an Array-OL specification as a data parallel program. While\nArray-OL can be used directly, it is often convenient to be able to deduce part\nof the specification from a sequential version of the application. This paper\nproposes such an analysis and examines its feasibility and its limits.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 May 2007 13:44:35 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 22 May 2007 09:52:57 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Feautrier",
                "Paul",
                "",
                "LIP, INRIA Rh\u00f4ne-Alpes"
            ]
        ]
    },
    {
        "id": "0705.2235",
        "submitter": "Tshilidzi Marwala",
        "authors": "S. Chakraverty, T. Marwala, Pallavi Gupta and Thando Tettey",
        "title": "Response Prediction of Structural System Subject to Earthquake Motions\n  using Artificial Neural Network",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  This paper uses Artificial Neural Network (ANN) models to compute response of\nstructural system subject to Indian earthquakes at Chamoli and Uttarkashi\nground motion data. The system is first trained for a single real earthquake\ndata. The trained ANN architecture is then used to simulate earthquakes with\nvarious intensities and it was found that the predicted responses given by ANN\nmodel are accurate for practical purposes. When the ANN is trained by a part of\nthe ground motion data, it can also identify the responses of the structural\nsystem well. In this way the safeness of the structural systems may be\npredicted in case of future earthquakes without waiting for the earthquake to\noccur for the lessons. Time period and the corresponding maximum response of\nthe building for an earthquake has been evaluated, which is again trained to\npredict the maximum response of the building at different time periods. The\ntrained time period versus maximum response ANN model is also tested for real\nearthquake data of other place, which was not used in the training and was\nfound to be in good agreement.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 May 2007 20:29:06 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Chakraverty",
                "S.",
                ""
            ],
            [
                "Marwala",
                "T.",
                ""
            ],
            [
                "Gupta",
                "Pallavi",
                ""
            ],
            [
                "Tettey",
                "Thando",
                ""
            ]
        ]
    },
    {
        "id": "0705.2236",
        "submitter": "Tshilidzi Marwala",
        "authors": "Tshilidzi Marwala, Thando Tettey and Snehashish Chakraverty",
        "title": "Fault Classification using Pseudomodal Energies and Neuro-fuzzy\n  modelling",
        "comments": "8 pages, In Proceedings of the Asia-Pacific Workshop on Structural\n  Health Monitoring, Yokohama, Japan, 2006",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  This paper presents a fault classification method which makes use of a\nTakagi-Sugeno neuro-fuzzy model and Pseudomodal energies calculated from the\nvibration signals of cylindrical shells. The calculation of Pseudomodal\nEnergies, for the purposes of condition monitoring, has previously been found\nto be an accurate method of extracting features from vibration signals. This\ncalculation is therefore used to extract features from vibration signals\nobtained from a diverse population of cylindrical shells. Some of the cylinders\nin the population have faults in different substructures. The pseudomodal\nenergies calculated from the vibration signals are then used as inputs to a\nneuro-fuzzy model. A leave-one-out cross-validation process is used to test the\nperformance of the model. It is found that the neuro-fuzzy model is able to\nclassify faults with an accuracy of 91.62%, which is higher than the previously\nused multilayer perceptron.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 May 2007 20:34:05 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Marwala",
                "Tshilidzi",
                ""
            ],
            [
                "Tettey",
                "Thando",
                ""
            ],
            [
                "Chakraverty",
                "Snehashish",
                ""
            ]
        ]
    },
    {
        "id": "0705.2305",
        "submitter": "Tshilidzi Marwala",
        "authors": "Sizwe M. Dhlamini, Tshilidzi Marwala, and Thokozani Majozi",
        "title": "Fuzzy and Multilayer Perceptron for Evaluation of HV Bushings",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.NE",
        "license": null,
        "abstract": "  The work proposes the application of fuzzy set theory (FST) to diagnose the\ncondition of high voltage bushings. The diagnosis uses dissolved gas analysis\n(DGA) data from bushings based on IEC60599 and IEEE C57-104 criteria for oil\nimpregnated paper (OIP) bushings. FST and neural networks are compared in terms\nof accuracy and computational efficiency. Both FST and NN simulations were able\nto diagnose the bushings condition with 10% error. By using fuzzy theory, the\nmaintenance department can classify bushings and know the extent of degradation\nin the component.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 16 May 2007 09:06:19 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Dhlamini",
                "Sizwe M.",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ],
            [
                "Majozi",
                "Thokozani",
                ""
            ]
        ]
    },
    {
        "id": "0705.2305",
        "submitter": "Tshilidzi Marwala",
        "authors": "Sizwe M. Dhlamini, Tshilidzi Marwala, and Thokozani Majozi",
        "title": "Fuzzy and Multilayer Perceptron for Evaluation of HV Bushings",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.NE",
        "license": null,
        "abstract": "  The work proposes the application of fuzzy set theory (FST) to diagnose the\ncondition of high voltage bushings. The diagnosis uses dissolved gas analysis\n(DGA) data from bushings based on IEC60599 and IEEE C57-104 criteria for oil\nimpregnated paper (OIP) bushings. FST and neural networks are compared in terms\nof accuracy and computational efficiency. Both FST and NN simulations were able\nto diagnose the bushings condition with 10% error. By using fuzzy theory, the\nmaintenance department can classify bushings and know the extent of degradation\nin the component.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 16 May 2007 09:06:19 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Dhlamini",
                "Sizwe M.",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ],
            [
                "Majozi",
                "Thokozani",
                ""
            ]
        ]
    },
    {
        "id": "0705.2307",
        "submitter": "Tshilidzi Marwala",
        "authors": "Bradley van Aardt, Tshilidzi Marwala",
        "title": "A Study in a Hybrid Centralised-Swarm Agent Community",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  This paper describes a systems architecture for a hybrid Centralised/Swarm\nbased multi-agent system. The issue of local goal assignment for agents is\ninvestigated through the use of a global agent which teaches the agents\nresponses to given situations. We implement a test problem in the form of a\nPursuit game, where the Multi-Agent system is a set of captor agents. The\nagents learn solutions to certain board positions from the global agent if they\nare unable to find a solution. The captor agents learn through the use of\nmulti-layer perceptron neural networks. The global agent is able to solve board\npositions through the use of a Genetic Algorithm. The cooperation between\nagents and the results of the simulation are discussed here. .\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 16 May 2007 09:12:09 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "van Aardt",
                "Bradley",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.2307",
        "submitter": "Tshilidzi Marwala",
        "authors": "Bradley van Aardt, Tshilidzi Marwala",
        "title": "A Study in a Hybrid Centralised-Swarm Agent Community",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  This paper describes a systems architecture for a hybrid Centralised/Swarm\nbased multi-agent system. The issue of local goal assignment for agents is\ninvestigated through the use of a global agent which teaches the agents\nresponses to given situations. We implement a test problem in the form of a\nPursuit game, where the Multi-Agent system is a set of captor agents. The\nagents learn solutions to certain board positions from the global agent if they\nare unable to find a solution. The captor agents learn through the use of\nmulti-layer perceptron neural networks. The global agent is able to solve board\npositions through the use of a Genetic Algorithm. The cooperation between\nagents and the results of the simulation are discussed here. .\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 16 May 2007 09:12:09 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "van Aardt",
                "Bradley",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.2310",
        "submitter": "Tshilidzi Marwala",
        "authors": "C.B. Vilakazi, T. Marwala, P. Mautla and E. Moloto",
        "title": "On-Line Condition Monitoring using Computational Intelligence",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  This paper presents bushing condition monitoring frameworks that use\nmulti-layer perceptrons (MLP), radial basis functions (RBF) and support vector\nmachines (SVM) classifiers. The first level of the framework determines if the\nbushing is faulty or not while the second level determines the type of fault.\nThe diagnostic gases in the bushings are analyzed using the dissolve gas\nanalysis. MLP gives superior performance in terms of accuracy and training time\nthan SVM and RBF. In addition, an on-line bushing condition monitoring\napproach, which is able to adapt to newly acquired data are introduced. This\napproach is able to accommodate new classes that are introduced by incoming\ndata and is implemented using an incremental learning algorithm that uses MLP.\nThe testing results improved from 67.5% to 95.8% as new data were introduced\nand the testing results improved from 60% to 95.3% as new conditions were\nintroduced. On average the confidence value of the framework on its decision\nwas 0.92.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 16 May 2007 09:19:00 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Vilakazi",
                "C. B.",
                ""
            ],
            [
                "Marwala",
                "T.",
                ""
            ],
            [
                "Mautla",
                "P.",
                ""
            ],
            [
                "Moloto",
                "E.",
                ""
            ]
        ]
    },
    {
        "id": "0705.2313",
        "submitter": "Olivier Powell",
        "authors": "Olivier Powell, Luminita Moraru, Jean-Marc Seigneur",
        "title": "TrustMIX: Trustworthy MIX for Energy Saving in Sensor Networks",
        "comments": "19 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.CR cs.NI",
        "license": null,
        "abstract": "  MIX has recently been proposed as a new sensor scheme with better energy\nmanagement for data-gathering in Wireless Sensor Networks. However, it is not\nknown how it performs when some of the sensors carry out sinkhole attacks. In\nthis paper, we propose a variant of MIX with adjunct computational trust\nmanagement to limit the impact of such sinkhole attacks. We evaluate how MIX\nresists sinkhole attacks with and without computational trust management. The\nmain result of this paper is to find that MIX is very vulnerable to sinkhole\nattacks but that the adjunct trust management efficiently reduces the impact of\nsuch attacks while preserving the main feature of MIX: increased lifetime of\nthe network.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 16 May 2007 09:22:21 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Powell",
                "Olivier",
                ""
            ],
            [
                "Moraru",
                "Luminita",
                ""
            ],
            [
                "Seigneur",
                "Jean-Marc",
                ""
            ]
        ]
    },
    {
        "id": "0705.2313",
        "submitter": "Olivier Powell",
        "authors": "Olivier Powell, Luminita Moraru, Jean-Marc Seigneur",
        "title": "TrustMIX: Trustworthy MIX for Energy Saving in Sensor Networks",
        "comments": "19 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.CR cs.NI",
        "license": null,
        "abstract": "  MIX has recently been proposed as a new sensor scheme with better energy\nmanagement for data-gathering in Wireless Sensor Networks. However, it is not\nknown how it performs when some of the sensors carry out sinkhole attacks. In\nthis paper, we propose a variant of MIX with adjunct computational trust\nmanagement to limit the impact of such sinkhole attacks. We evaluate how MIX\nresists sinkhole attacks with and without computational trust management. The\nmain result of this paper is to find that MIX is very vulnerable to sinkhole\nattacks but that the adjunct trust management efficiently reduces the impact of\nsuch attacks while preserving the main feature of MIX: increased lifetime of\nthe network.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 16 May 2007 09:22:21 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Powell",
                "Olivier",
                ""
            ],
            [
                "Moraru",
                "Luminita",
                ""
            ],
            [
                "Seigneur",
                "Jean-Marc",
                ""
            ]
        ]
    },
    {
        "id": "0705.2313",
        "submitter": "Olivier Powell",
        "authors": "Olivier Powell, Luminita Moraru, Jean-Marc Seigneur",
        "title": "TrustMIX: Trustworthy MIX for Energy Saving in Sensor Networks",
        "comments": "19 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.CR cs.NI",
        "license": null,
        "abstract": "  MIX has recently been proposed as a new sensor scheme with better energy\nmanagement for data-gathering in Wireless Sensor Networks. However, it is not\nknown how it performs when some of the sensors carry out sinkhole attacks. In\nthis paper, we propose a variant of MIX with adjunct computational trust\nmanagement to limit the impact of such sinkhole attacks. We evaluate how MIX\nresists sinkhole attacks with and without computational trust management. The\nmain result of this paper is to find that MIX is very vulnerable to sinkhole\nattacks but that the adjunct trust management efficiently reduces the impact of\nsuch attacks while preserving the main feature of MIX: increased lifetime of\nthe network.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 16 May 2007 09:22:21 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Powell",
                "Olivier",
                ""
            ],
            [
                "Moraru",
                "Luminita",
                ""
            ],
            [
                "Seigneur",
                "Jean-Marc",
                ""
            ]
        ]
    },
    {
        "id": "0705.2318",
        "submitter": "Seiji Miyoshi",
        "authors": "Hideto Utsumi, Seiji Miyoshi, Masato Okada",
        "title": "Statistical Mechanics of Nonlinear On-line Learning for Ensemble\n  Teachers",
        "comments": "13 pages, 9 figures",
        "journal-ref": null,
        "doi": "10.1143/JPSJ.76.114001",
        "report-no": null,
        "categories": "cs.LG cond-mat.dis-nn",
        "license": null,
        "abstract": "  We analyze the generalization performance of a student in a model composed of\nnonlinear perceptrons: a true teacher, ensemble teachers, and the student. We\ncalculate the generalization error of the student analytically or numerically\nusing statistical mechanics in the framework of on-line learning. We treat two\nwell-known learning rules: Hebbian learning and perceptron learning. As a\nresult, it is proven that the nonlinear model shows qualitatively different\nbehaviors from the linear model. Moreover, it is clarified that Hebbian\nlearning and perceptron learning show qualitatively different behaviors from\neach other. In Hebbian learning, we can analytically obtain the solutions. In\nthis case, the generalization error monotonically decreases. The steady value\nof the generalization error is independent of the learning rate. The larger the\nnumber of teachers is and the more variety the ensemble teachers have, the\nsmaller the generalization error is. In perceptron learning, we have to\nnumerically obtain the solutions. In this case, the dynamical behaviors of the\ngeneralization error are non-monotonic. The smaller the learning rate is, the\nlarger the number of teachers is; and the more variety the ensemble teachers\nhave, the smaller the minimum value of the generalization error is.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 16 May 2007 09:58:39 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Utsumi",
                "Hideto",
                ""
            ],
            [
                "Miyoshi",
                "Seiji",
                ""
            ],
            [
                "Okada",
                "Masato",
                ""
            ]
        ]
    },
    {
        "id": "0705.2485",
        "submitter": "Bodie Crossingham",
        "authors": "Bodie Crossingham and Tshilidzi Marwala",
        "title": "Using Genetic Algorithms to Optimise Rough Set Partition Sizes for HIV\n  Data Analysis",
        "comments": "10 pages, 1 figure, Update Bibliography",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI q-bio.QM",
        "license": null,
        "abstract": "  In this paper, we present a method to optimise rough set partition sizes, to\nwhich rule extraction is performed on HIV data. The genetic algorithm\noptimisation technique is used to determine the partition sizes of a rough set\nin order to maximise the rough sets prediction accuracy. The proposed method is\ntested on a set of demographic properties of individuals obtained from the\nSouth African antenatal survey. Six demographic variables were used in the\nanalysis, these variables are; race, age of mother, education, gravidity,\nparity, and age of father, with the outcome or decision being either HIV\npositive or negative. Rough set theory is chosen based on the fact that it is\neasy to interpret the extracted rules. The prediction accuracy of equal width\nbin partitioning is 57.7% while the accuracy achieved after optimising the\npartitions is 72.8%. Several other methods have been used to analyse the HIV\ndata and their results are stated and compared to that of rough set theory\n(RST).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 17 May 2007 07:02:23 GMT"
            }
        ],
        "update_date": "2007-06-25",
        "authors_parsed": [
            [
                "Crossingham",
                "Bodie",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.2485",
        "submitter": "Bodie Crossingham",
        "authors": "Bodie Crossingham and Tshilidzi Marwala",
        "title": "Using Genetic Algorithms to Optimise Rough Set Partition Sizes for HIV\n  Data Analysis",
        "comments": "10 pages, 1 figure, Update Bibliography",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI q-bio.QM",
        "license": null,
        "abstract": "  In this paper, we present a method to optimise rough set partition sizes, to\nwhich rule extraction is performed on HIV data. The genetic algorithm\noptimisation technique is used to determine the partition sizes of a rough set\nin order to maximise the rough sets prediction accuracy. The proposed method is\ntested on a set of demographic properties of individuals obtained from the\nSouth African antenatal survey. Six demographic variables were used in the\nanalysis, these variables are; race, age of mother, education, gravidity,\nparity, and age of father, with the outcome or decision being either HIV\npositive or negative. Rough set theory is chosen based on the fact that it is\neasy to interpret the extracted rules. The prediction accuracy of equal width\nbin partitioning is 57.7% while the accuracy achieved after optimising the\npartitions is 72.8%. Several other methods have been used to analyse the HIV\ndata and their results are stated and compared to that of rough set theory\n(RST).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 17 May 2007 07:02:23 GMT"
            }
        ],
        "update_date": "2007-06-25",
        "authors_parsed": [
            [
                "Crossingham",
                "Bodie",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.2516",
        "submitter": "Tshilidzi Marwala",
        "authors": "Sizwe M. Dhlamini*, Fulufhelo V. Nelwamondo**, Tshilidzi Marwala**",
        "title": "Condition Monitoring of HV Bushings in the Presence of Missing Data\n  Using Evolutionary Computing",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  The work proposes the application of neural networks with particle swarm\noptimisation (PSO) and genetic algorithms (GA) to compensate for missing data\nin classifying high voltage bushings. The classification is done using DGA data\nfrom 60966 bushings based on IEEEc57.104, IEC599 and IEEE production rates\nmethods for oil impregnated paper (OIP) bushings. PSO and GA were compared in\nterms of accuracy and computational efficiency. Both GA and PSO simulations\nwere able to estimate missing data values to an average 95% accuracy when only\none variable was missing. However PSO rapidly deteriorated to 66% accuracy with\ntwo variables missing simultaneously, compared to 84% for GA. The data\nestimated using GA was found to classify the conditions of bushings than the\nPSO.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 17 May 2007 11:33:34 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Dhlamini*",
                "Sizwe M.",
                ""
            ],
            [
                "Nelwamondo**",
                "Fulufhelo V.",
                ""
            ],
            [
                "Marwala**",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.2516",
        "submitter": "Tshilidzi Marwala",
        "authors": "Sizwe M. Dhlamini*, Fulufhelo V. Nelwamondo**, Tshilidzi Marwala**",
        "title": "Condition Monitoring of HV Bushings in the Presence of Missing Data\n  Using Evolutionary Computing",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  The work proposes the application of neural networks with particle swarm\noptimisation (PSO) and genetic algorithms (GA) to compensate for missing data\nin classifying high voltage bushings. The classification is done using DGA data\nfrom 60966 bushings based on IEEEc57.104, IEC599 and IEEE production rates\nmethods for oil impregnated paper (OIP) bushings. PSO and GA were compared in\nterms of accuracy and computational efficiency. Both GA and PSO simulations\nwere able to estimate missing data values to an average 95% accuracy when only\none variable was missing. However PSO rapidly deteriorated to 66% accuracy with\ntwo variables missing simultaneously, compared to 84% for GA. The data\nestimated using GA was found to classify the conditions of bushings than the\nPSO.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 17 May 2007 11:33:34 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Dhlamini*",
                "Sizwe M.",
                ""
            ],
            [
                "Nelwamondo**",
                "Fulufhelo V.",
                ""
            ],
            [
                "Marwala**",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0705.2604",
        "submitter": "Tshilidzi Marwala",
        "authors": "Tshilidzi Marwala and Christina Busisiwe Vilakazi",
        "title": "Computational Intelligence for Condition Monitoring",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": null,
        "abstract": "  Condition monitoring techniques are described in this chapter. Two aspects of\ncondition monitoring process are considered: (1) feature extraction; and (2)\ncondition classification. Feature extraction methods described and implemented\nare fractals, Kurtosis and Mel-frequency Cepstral Coefficients. Classification\nmethods described and implemented are support vector machines (SVM), hidden\nMarkov models (HMM), Gaussian mixture models (GMM) and extension neural\nnetworks (ENN). The effectiveness of these features were tested using SVM, HMM,\nGMM and ENN on condition monitoring of bearings and are found to give good\nresults.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 17 May 2007 21:20:58 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Marwala",
                "Tshilidzi",
                ""
            ],
            [
                "Vilakazi",
                "Christina Busisiwe",
                ""
            ]
        ]
    },
    {
        "id": "0705.2626",
        "submitter": "Andrew Knyazev",
        "authors": "A. V. Knyazev, M. E. Argentati, I. Lashuk, and E. E. Ovtchinnikov",
        "title": "Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) in\n  hypre and PETSc",
        "comments": "Submitted to SIAM Journal on Scientific Computing",
        "journal-ref": "SIAM Journal on Scientific Computing (SISC). 25(5): 2224-2239,\n  2007",
        "doi": "10.1137/060661624",
        "report-no": "UCDHSC-CCM-251",
        "categories": "cs.MS cs.NA",
        "license": null,
        "abstract": "  We describe our software package Block Locally Optimal Preconditioned\nEigenvalue Xolvers (BLOPEX) publicly released recently. BLOPEX is available as\na stand-alone serial library, as an external package to PETSc (``Portable,\nExtensible Toolkit for Scientific Computation'', a general purpose suite of\ntools for the scalable solution of partial differential equations and related\nproblems developed by Argonne National Laboratory), and is also built into {\\it\nhypre} (``High Performance Preconditioners'', scalable linear solvers package\ndeveloped by Lawrence Livermore National Laboratory). The present BLOPEX\nrelease includes only one solver--the Locally Optimal Block Preconditioned\nConjugate Gradient (LOBPCG) method for symmetric eigenvalue problems. {\\it\nhypre} provides users with advanced high-quality parallel preconditioners for\nlinear systems, in particular, with domain decomposition and multigrid\npreconditioners. With BLOPEX, the same preconditioners can now be efficiently\nused for symmetric eigenvalue problems. PETSc facilitates the integration of\nindependently developed application modules with strict attention to component\ninteroperability, and makes BLOPEX extremely easy to compile and use with\npreconditioners that are available via PETSc. We present the LOBPCG algorithm\nin BLOPEX for {\\it hypre} and PETSc. We demonstrate numerically the scalability\nof BLOPEX by testing it on a number of distributed and shared memory parallel\nsystems, including a Beowulf system, SUN Fire 880, an AMD dual-core Opteron\nworkstation, and IBM BlueGene/L supercomputer, using PETSc domain decomposition\nand {\\it hypre} multigrid preconditioning. We test BLOPEX on a model problem,\nthe standard 7-point finite-difference approximation of the 3-D Laplacian, with\nthe problem size in the range $10^5-10^8$.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 18 May 2007 02:25:16 GMT"
            }
        ],
        "update_date": "2010-06-02",
        "authors_parsed": [
            [
                "Knyazev",
                "A. V.",
                ""
            ],
            [
                "Argentati",
                "M. E.",
                ""
            ],
            [
                "Lashuk",
                "I.",
                ""
            ],
            [
                "Ovtchinnikov",
                "E. E.",
                ""
            ]
        ]
    },
    {
        "id": "0705.2765",
        "submitter": "Rustem Takhanov",
        "authors": "Rustem Takhanov",
        "title": "On the monotonization of the training set",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI",
        "license": null,
        "abstract": "  We consider the problem of minimal correction of the training set to make it\nconsistent with monotonic constraints. This problem arises during analysis of\ndata sets via techniques that require monotone data. We show that this problem\nis NP-hard in general and is equivalent to finding a maximal independent set in\nspecial orgraphs. Practically important cases of that problem considered in\ndetail. These are the cases when a partial order given on the replies set is a\ntotal order or has a dimension 2. We show that the second case can be reduced\nto maximization of a quadratic convex function on a convex set. For this case\nwe construct an approximate polynomial algorithm based on convex optimization.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 18 May 2007 19:44:19 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Takhanov",
                "Rustem",
                ""
            ]
        ]
    },
    {
        "id": "0705.2765",
        "submitter": "Rustem Takhanov",
        "authors": "Rustem Takhanov",
        "title": "On the monotonization of the training set",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI",
        "license": null,
        "abstract": "  We consider the problem of minimal correction of the training set to make it\nconsistent with monotonic constraints. This problem arises during analysis of\ndata sets via techniques that require monotone data. We show that this problem\nis NP-hard in general and is equivalent to finding a maximal independent set in\nspecial orgraphs. Practically important cases of that problem considered in\ndetail. These are the cases when a partial order given on the replies set is a\ntotal order or has a dimension 2. We show that the second case can be reduced\nto maximization of a quadratic convex function on a convex set. For this case\nwe construct an approximate polynomial algorithm based on convex optimization.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 18 May 2007 19:44:19 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Takhanov",
                "Rustem",
                ""
            ]
        ]
    },
    {
        "id": "0705.2786",
        "submitter": "Joachim Wlodarz",
        "authors": "Joachim J. Wlodarz",
        "title": "Virtualization: A double-edged sword",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.CR",
        "license": null,
        "abstract": "  Virtualization became recently a hot topic once again, after being dormant\nfor more than twenty years. In the meantime, it has been almost forgotten, that\nvirtual machines are not so perfect isolating environments as it seems, when\nlooking at the principles. These lessons were already learnt earlier when the\nfirst virtualized systems have been exposed to real life usage.\n  Contemporary virtualization software enables instant creation and destruction\nof virtual machines on a host, live migration from one host to another,\nexecution history manipulation, etc. These features are very useful in\npractice, but also causing headaches among security specialists, especially in\ncurrent hostile network environments.\n  In the present contribution we discuss the principles, potential benefits and\nrisks of virtualization in a deja vu perspective, related to previous\nexperiences with virtualization in the mainframe era.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 19 May 2007 00:02:24 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Wlodarz",
                "Joachim J.",
                ""
            ]
        ]
    },
    {
        "id": "0705.2786",
        "submitter": "Joachim Wlodarz",
        "authors": "Joachim J. Wlodarz",
        "title": "Virtualization: A double-edged sword",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.CR",
        "license": null,
        "abstract": "  Virtualization became recently a hot topic once again, after being dormant\nfor more than twenty years. In the meantime, it has been almost forgotten, that\nvirtual machines are not so perfect isolating environments as it seems, when\nlooking at the principles. These lessons were already learnt earlier when the\nfirst virtualized systems have been exposed to real life usage.\n  Contemporary virtualization software enables instant creation and destruction\nof virtual machines on a host, live migration from one host to another,\nexecution history manipulation, etc. These features are very useful in\npractice, but also causing headaches among security specialists, especially in\ncurrent hostile network environments.\n  In the present contribution we discuss the principles, potential benefits and\nrisks of virtualization in a deja vu perspective, related to previous\nexperiences with virtualization in the mainframe era.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 19 May 2007 00:02:24 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Wlodarz",
                "Joachim J.",
                ""
            ]
        ]
    },
    {
        "id": "0705.2787",
        "submitter": "David Martin",
        "authors": "David J. Martin, Daniel Kifer, Ashwin Machanavajjhala, Johannes\n  Gehrke, Joseph Y. Halpern",
        "title": "Worst-Case Background Knowledge for Privacy-Preserving Data Publishing",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  Recent work has shown the necessity of considering an attacker's background\nknowledge when reasoning about privacy in data publishing. However, in\npractice, the data publisher does not know what background knowledge the\nattacker possesses. Thus, it is important to consider the worst-case. In this\npaper, we initiate a formal study of worst-case background knowledge. We\npropose a language that can express any background knowledge about the data. We\nprovide a polynomial time algorithm to measure the amount of disclosure of\nsensitive information in the worst case, given that the attacker has at most a\nspecified number of pieces of information in this language. We also provide a\nmethod to efficiently sanitize the data so that the amount of disclosure in the\nworst case is less than a specified threshold.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 19 May 2007 00:12:24 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Martin",
                "David J.",
                ""
            ],
            [
                "Kifer",
                "Daniel",
                ""
            ],
            [
                "Machanavajjhala",
                "Ashwin",
                ""
            ],
            [
                "Gehrke",
                "Johannes",
                ""
            ],
            [
                "Halpern",
                "Joseph Y.",
                ""
            ]
        ]
    },
    {
        "id": "0705.2819",
        "submitter": "Preetam Patil",
        "authors": "Preetam Patil, Varsha Apte (Department of CSE, IIT-Bombay, India)",
        "title": "An Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF",
        "comments": "Submitted to QShine'07",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": null,
        "abstract": "  Admission control as a mechanism for providing QoS requires an accurate\ndescription of the requested flow as well as already admitted flows. Since\n802.11 WLAN capacity is shared between flows belonging to all stations,\nadmission control requires knowledge of all flows in the WLAN. Further,\nestimation of the load-dependent WLAN capacity through analytical model\nrequires inputs about channel data rate, payload size and the number of\nstations. These factors combined point to a centralized admission control\nwhereas for 802.11 DCF it is ideally performed in a distributed manner. The use\nof measurements from the channel avoids explicit inputs about the state of the\nchannel described above. BUFFET, a model based measurement-assisted distributed\nadmission control scheme for DCF proposed in this paper relies on measurements\nto derive model inputs and predict WLAN saturation, thereby maintaining average\ndelay within acceptable limits. Being measurement based, it adapts to a\ncombination of data rates and payload sizes, making it completely autonomous\nand distributed. Performance analysis using OPNET simulations suggests that\nBUFFET is able to ensure average delay under 7ms at a near-optimal throughput.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 19 May 2007 13:54:25 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Patil",
                "Preetam",
                "",
                "Department of CSE, IIT-Bombay, India"
            ],
            [
                "Apte",
                "Varsha",
                "",
                "Department of CSE, IIT-Bombay, India"
            ]
        ]
    },
    {
        "id": "0705.2819",
        "submitter": "Preetam Patil",
        "authors": "Preetam Patil, Varsha Apte (Department of CSE, IIT-Bombay, India)",
        "title": "An Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF",
        "comments": "Submitted to QShine'07",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": null,
        "abstract": "  Admission control as a mechanism for providing QoS requires an accurate\ndescription of the requested flow as well as already admitted flows. Since\n802.11 WLAN capacity is shared between flows belonging to all stations,\nadmission control requires knowledge of all flows in the WLAN. Further,\nestimation of the load-dependent WLAN capacity through analytical model\nrequires inputs about channel data rate, payload size and the number of\nstations. These factors combined point to a centralized admission control\nwhereas for 802.11 DCF it is ideally performed in a distributed manner. The use\nof measurements from the channel avoids explicit inputs about the state of the\nchannel described above. BUFFET, a model based measurement-assisted distributed\nadmission control scheme for DCF proposed in this paper relies on measurements\nto derive model inputs and predict WLAN saturation, thereby maintaining average\ndelay within acceptable limits. Being measurement based, it adapts to a\ncombination of data rates and payload sizes, making it completely autonomous\nand distributed. Performance analysis using OPNET simulations suggests that\nBUFFET is able to ensure average delay under 7ms at a near-optimal throughput.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 19 May 2007 13:54:25 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Patil",
                "Preetam",
                "",
                "Department of CSE, IIT-Bombay, India"
            ],
            [
                "Apte",
                "Varsha",
                "",
                "Department of CSE, IIT-Bombay, India"
            ]
        ]
    },
    {
        "id": "0705.2854",
        "submitter": "Asaf Cohen",
        "authors": "Asaf Cohen, Tsachy Weissman and Neri Merhav",
        "title": "Scanning and Sequential Decision Making for Multi-Dimensional Data -\n  Part II: the Noisy Case",
        "comments": "The second part of a two-part paper. 49 pages, 6 figures. Submitted\n  to IEEE Trans. Inform. Theory",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.CV math.IT",
        "license": null,
        "abstract": "  We consider the problem of sequential decision making on random fields\ncorrupted by noise. In this scenario, the decision maker observes a noisy\nversion of the data, yet judged with respect to the clean data. In particular,\nwe first consider the problem of sequentially scanning and filtering noisy\nrandom fields. In this case, the sequential filter is given the freedom to\nchoose the path over which it traverses the random field (e.g., noisy image or\nvideo sequence), thus it is natural to ask what is the best achievable\nperformance and how sensitive this performance is to the choice of the scan. We\nformally define the problem of scanning and filtering, derive a bound on the\nbest achievable performance and quantify the excess loss occurring when\nnon-optimal scanners are used, compared to optimal scanning and filtering.\n  We then discuss the problem of sequential scanning and prediction of noisy\nrandom fields. This setting is a natural model for applications such as\nrestoration and coding of noisy images. We formally define the problem of\nscanning and prediction of a noisy multidimensional array and relate the\noptimal performance to the clean scandictability defined by Merhav and\nWeissman. Moreover, bounds on the excess loss due to sub-optimal scans are\nderived, and a universal prediction algorithm is suggested.\n  This paper is the second part of a two-part paper. The first paper dealt with\nsequential decision making on noiseless data arrays, namely, when the decision\nmaker is judged with respect to the same data array it observes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 20 May 2007 09:14:06 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Cohen",
                "Asaf",
                ""
            ],
            [
                "Weissman",
                "Tsachy",
                ""
            ],
            [
                "Merhav",
                "Neri",
                ""
            ]
        ]
    },
    {
        "id": "0705.2862",
        "submitter": "Boaz Tsaban",
        "authors": "Dima Ruinskiy, Adi Shamir, and Boaz Tsaban",
        "title": "Cryptanalysis of group-based key agreement protocols using subgroup\n  distance functions",
        "comments": null,
        "journal-ref": "Proceedings of the 10th International Conference on Practice and\n  Theory in Public-Key Cryptography PKC07, Lecture Notes In Computer Science\n  4450 (2007), 61--75",
        "doi": "10.1007/978-3-540-71677-8_5",
        "report-no": null,
        "categories": "cs.CR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new approach for cryptanalysis of key agreement protocols\nbased on noncommutative groups. This approach uses functions that estimate the\ndistance of a group element to a given subgroup. We test it against the\nShpilrain-Ushakov protocol, which is based on Thompson's group F.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 20 May 2007 12:20:56 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 4 Nov 2010 19:31:34 GMT"
            }
        ],
        "update_date": "2010-11-05",
        "authors_parsed": [
            [
                "Ruinskiy",
                "Dima",
                ""
            ],
            [
                "Shamir",
                "Adi",
                ""
            ],
            [
                "Tsaban",
                "Boaz",
                ""
            ]
        ]
    },
    {
        "id": "0705.2876",
        "submitter": "Phillip Bradford",
        "authors": "Phillip G. Bradford and Daniel A. Ray",
        "title": "An online algorithm for generating fractal hash chains applied to\n  digital chains of custody",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.DS",
        "license": null,
        "abstract": "  This paper gives an online algorithm for generating Jakobsson's fractal hash\nchains. Our new algorithm compliments Jakobsson's fractal hash chain algorithm\nfor preimage traversal since his algorithm assumes the entire hash chain is\nprecomputed and a particular list of Ceiling(log n) hash elements or pebbles\nare saved. Our online algorithm for hash chain traversal incrementally\ngenerates a hash chain of n hash elements without knowledge of n before it\nstarts. For any n, our algorithm stores only the Ceiling(log n) pebbles which\nare precisely the inputs for Jakobsson's amortized hash chain preimage\ntraversal algorithm. This compact representation is useful to generate,\ntraverse, and store a number of large digital hash chains on a small and\nconstrained device. We also give an application using both Jakobsson's and our\nnew algorithm applied to digital chains of custody for validating dynamically\nchanging forensics data.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 20 May 2007 17:14:38 GMT"
            }
        ],
        "update_date": "2007-05-23",
        "authors_parsed": [
            [
                "Bradford",
                "Phillip G.",
                ""
            ],
            [
                "Ray",
                "Daniel A.",
                ""
            ]
        ]
    },
    {
        "id": "0705.3015",
        "submitter": "Erik Schnetter",
        "authors": "Dylan Stark, Gabrielle Allen, Tom Goodale, Thomas Radke, Erik\n  Schnetter",
        "title": "An Extensible Timing Infrastructure for Adaptive Large-scale\n  Applications",
        "comments": null,
        "journal-ref": "In Roman Wyrzykowski et al., editors, Parallel Processing and\n  Applied Mathematics (PPAM), 2007, Gdansk, Poland, volume 4967 of Lecture\n  Notes in Computer Science (LNCS), pages 1170-1179. Springer, 2007.",
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.DC",
        "license": null,
        "abstract": "  Real-time access to accurate and reliable timing information is necessary to\nprofile scientific applications, and crucial as simulations become increasingly\ncomplex, adaptive, and large-scale. The Cactus Framework provides flexible and\nextensible capabilities for timing information through a well designed\ninfrastructure and timing API. Applications built with Cactus automatically\ngain access to built-in timers, such as gettimeofday and getrusage,\nsystem-specific hardware clocks, and high-level interfaces such as PAPI. We\ndescribe the Cactus timer interface, its motivation, and its implementation. We\nthen demonstrate how this timing information can be used by an example\nscientific application to profile itself, and to dynamically adapt itself to a\nchanging environment at run time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 21 May 2007 19:00:25 GMT"
            }
        ],
        "update_date": "2009-04-16",
        "authors_parsed": [
            [
                "Stark",
                "Dylan",
                ""
            ],
            [
                "Allen",
                "Gabrielle",
                ""
            ],
            [
                "Goodale",
                "Tom",
                ""
            ],
            [
                "Radke",
                "Thomas",
                ""
            ],
            [
                "Schnetter",
                "Erik",
                ""
            ]
        ]
    },
    {
        "id": "0705.3015",
        "submitter": "Erik Schnetter",
        "authors": "Dylan Stark, Gabrielle Allen, Tom Goodale, Thomas Radke, Erik\n  Schnetter",
        "title": "An Extensible Timing Infrastructure for Adaptive Large-scale\n  Applications",
        "comments": null,
        "journal-ref": "In Roman Wyrzykowski et al., editors, Parallel Processing and\n  Applied Mathematics (PPAM), 2007, Gdansk, Poland, volume 4967 of Lecture\n  Notes in Computer Science (LNCS), pages 1170-1179. Springer, 2007.",
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.DC",
        "license": null,
        "abstract": "  Real-time access to accurate and reliable timing information is necessary to\nprofile scientific applications, and crucial as simulations become increasingly\ncomplex, adaptive, and large-scale. The Cactus Framework provides flexible and\nextensible capabilities for timing information through a well designed\ninfrastructure and timing API. Applications built with Cactus automatically\ngain access to built-in timers, such as gettimeofday and getrusage,\nsystem-specific hardware clocks, and high-level interfaces such as PAPI. We\ndescribe the Cactus timer interface, its motivation, and its implementation. We\nthen demonstrate how this timing information can be used by an example\nscientific application to profile itself, and to dynamically adapt itself to a\nchanging environment at run time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 21 May 2007 19:00:25 GMT"
            }
        ],
        "update_date": "2009-04-16",
        "authors_parsed": [
            [
                "Stark",
                "Dylan",
                ""
            ],
            [
                "Allen",
                "Gabrielle",
                ""
            ],
            [
                "Goodale",
                "Tom",
                ""
            ],
            [
                "Radke",
                "Thomas",
                ""
            ],
            [
                "Schnetter",
                "Erik",
                ""
            ]
        ]
    },
    {
        "id": "0705.3025",
        "submitter": "Majed Haddad",
        "authors": "Majed Haddad, Aawatif Menouni Hayar and Merouane Debbah",
        "title": "Spectral Efficiency of Spectrum Pooling Systems",
        "comments": "24 pages, 8 figures",
        "journal-ref": "IET Special Issue on Cognitive Spectrum Access, Vol. 2, No. 6, pp.\n  733-741, July 2008",
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.NI math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this contribution, we investigate the idea of using cognitive radio to\nreuse locally unused spectrum to increase the total system capacity. We\nconsider a multiband/wideband system in which the primary and cognitive users\nwish to communicate to different receivers, subject to mutual interference and\nassume that each user knows only his channel and the unused spectrum through\nadequate sensing. The basic idea under the proposed scheme is based on the\nnotion of spectrum pooling. The idea is quite simple: a cognitive radio will\nlisten to the channel and, if sensed idle, will transmit during the voids. It\nturns out that, although its simplicity, the proposed scheme showed very\ninteresting features with respect to the spectral efficiency and the maximum\nnumber of possible pairwise cognitive communications. We impose the constraint\nthat users successively transmit over available bands through selfish water\nfilling. For the first time, our study has quantified the asymptotic (with\nrespect to the band) achievable gain of using spectrum pooling in terms of\nspectral efficiency compared to classical radio systems. We then derive the\ntotal spectral efficiency as well as the maximum number of possible pairwise\ncommunications of such a spectrum pooling system.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 21 May 2007 17:26:08 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 9 Oct 2008 15:45:18 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 2 Apr 2010 11:00:17 GMT"
            }
        ],
        "update_date": "2015-03-13",
        "authors_parsed": [
            [
                "Haddad",
                "Majed",
                ""
            ],
            [
                "Hayar",
                "Aawatif Menouni",
                ""
            ],
            [
                "Debbah",
                "Merouane",
                ""
            ]
        ]
    },
    {
        "id": "0705.3360",
        "submitter": "Kyriakos Sgarbas",
        "authors": "Kyriakos N. Sgarbas",
        "title": "The Road to Quantum Artificial Intelligence",
        "comments": "9 pages. Presented at PCI-2007: 11th Panhellenic Conference in\n  Informatics, 18-20 May 2007, Patras, Greece",
        "journal-ref": "In: T.S.Papatheodorou, D.N.Christodoulakis and N.N.Karanikolas\n  (eds), \"Current Trends in Informatics\", Vol.A, pp.469-477, New Technologies\n  Publications, Athens, 2007 (SET 978-960-89784-0-9)",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  This paper overviews the basic principles and recent advances in the emerging\nfield of Quantum Computation (QC), highlighting its potential application to\nArtificial Intelligence (AI). The paper provides a very brief introduction to\nbasic QC issues like quantum registers, quantum gates and quantum algorithms\nand then it presents references, ideas and research guidelines on how QC can be\nused to deal with some basic AI problems, such as search and pattern matching,\nas soon as quantum computers become widely available.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 23 May 2007 12:31:47 GMT"
            }
        ],
        "update_date": "2007-05-24",
        "authors_parsed": [
            [
                "Sgarbas",
                "Kyriakos N.",
                ""
            ]
        ]
    },
    {
        "id": "0705.3468",
        "submitter": "Neng-Fa Zhou",
        "authors": "Neng-Fa Zhou, Taisuke Sato, and Yi-Dong Shen",
        "title": "Linear Tabling Strategies and Optimizations",
        "comments": "29 pages, 1 figure, TPLP",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  Recently, the iterative approach named linear tabling has received\nconsiderable attention because of its simplicity, ease of implementation, and\ngood space efficiency. Linear tabling is a framework from which different\nmethods can be derived based on the strategies used in handling looping\nsubgoals. One decision concerns when answers are consumed and returned. This\npaper describes two strategies, namely, {\\it lazy} and {\\it eager} strategies,\nand compares them both qualitatively and quantitatively. The results indicate\nthat, while the lazy strategy has good locality and is well suited for finding\nall solutions, the eager strategy is comparable in speed with the lazy strategy\nand is well suited for programs with cuts. Linear tabling relies on depth-first\niterative deepening rather than suspension to compute fixpoints. Each cluster\nof inter-dependent subgoals as represented by a top-most looping subgoal is\niteratively evaluated until no subgoal in it can produce any new answers. Naive\nre-evaluation of all looping subgoals, albeit simple, may be computationally\nunacceptable. In this paper, we also introduce semi-naive optimization, an\neffective technique employed in bottom-up evaluation of logic programs to avoid\nredundant joins of answers, into linear tabling. We give the conditions for the\ntechnique to be safe (i.e. sound and complete) and propose an optimization\ntechnique called {\\it early answer promotion} to enhance its effectiveness.\nBenchmarking in B-Prolog demonstrates that with this optimization linear\ntabling compares favorably well in speed with the state-of-the-art\nimplementation of SLG.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 23 May 2007 20:52:42 GMT"
            }
        ],
        "update_date": "2007-05-25",
        "authors_parsed": [
            [
                "Zhou",
                "Neng-Fa",
                ""
            ],
            [
                "Sato",
                "Taisuke",
                ""
            ],
            [
                "Shen",
                "Yi-Dong",
                ""
            ]
        ]
    },
    {
        "id": "0705.3503",
        "submitter": "Catuscia Palamidessi",
        "authors": "Konstantinos Chatzikokolakis and Catuscia Palamidessi",
        "title": "Making Random Choices Invisible to the Scheduler",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.LO",
        "license": null,
        "abstract": "  When dealing with process calculi and automata which express both\nnondeterministic and probabilistic behavior, it is customary to introduce the\nnotion of scheduler to solve the nondeterminism. It has been observed that for\ncertain applications, notably those in security, the scheduler needs to be\nrestricted so not to reveal the outcome of the protocol's random choices, or\notherwise the model of adversary would be too strong even for ``obviously\ncorrect'' protocols. We propose a process-algebraic framework in which the\ncontrol on the scheduler can be specified in syntactic terms, and we show how\nto apply it to solve the problem mentioned above. We also consider the\ndefinition of (probabilistic) may and must preorders, and we show that they are\nprecongruences with respect to the restricted schedulers. Furthermore, we show\nthat all the operators of the language, except replication, distribute over\nprobabilistic summation, which is a useful property for verification.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 May 2007 04:28:47 GMT"
            }
        ],
        "update_date": "2007-06-13",
        "authors_parsed": [
            [
                "Chatzikokolakis",
                "Konstantinos",
                ""
            ],
            [
                "Palamidessi",
                "Catuscia",
                ""
            ]
        ]
    },
    {
        "id": "0705.3561",
        "submitter": "Lucas Bordeaux",
        "authors": "Lucas Bordeaux, Marco Cadoli, Toni Mancini",
        "title": "Generalizing Consistency and other Constraint Properties to Quantified\n  Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.AI",
        "license": null,
        "abstract": "  Quantified constraints and Quantified Boolean Formulae are typically much\nmore difficult to reason with than classical constraints, because quantifier\nalternation makes the usual notion of solution inappropriate. As a consequence,\nbasic properties of Constraint Satisfaction Problems (CSP), such as consistency\nor substitutability, are not completely understood in the quantified case.\nThese properties are important because they are the basis of most of the\nreasoning methods used to solve classical (existentially quantified)\nconstraints, and one would like to benefit from similar reasoning methods in\nthe resolution of quantified constraints. In this paper, we show that most of\nthe properties that are used by solvers for CSP can be generalized to\nquantified CSP. This requires a re-thinking of a number of basic concepts; in\nparticular, we propose a notion of outcome that generalizes the classical\nnotion of solution and on which all definitions are based. We propose a\nsystematic study of the relations which hold between these properties, as well\nas complexity results regarding the decision of these properties. Finally, and\nsince these problems are typically intractable, we generalize the approach used\nin CSP and propose weaker, easier to check notions based on locality, which\nallow to detect these properties incompletely but in polynomial time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 May 2007 11:27:55 GMT"
            }
        ],
        "update_date": "2007-05-25",
        "authors_parsed": [
            [
                "Bordeaux",
                "Lucas",
                ""
            ],
            [
                "Cadoli",
                "Marco",
                ""
            ],
            [
                "Mancini",
                "Toni",
                ""
            ]
        ]
    },
    {
        "id": "0705.3593",
        "submitter": "Wolfgang Jacquet",
        "authors": "W. Jacquet, P. de Groen",
        "title": "MI image registration using prior knowledge",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  Subtraction of aligned images is a means to assess changes in a wide variety\nof clinical applications. In this paper we explore the information theoretical\norigin of Mutual Information (MI), which is based on Shannon's entropy.However,\nthe interpretation of standard MI registration as a communication channel\nsuggests that MI is too restrictive a criterion. In this paper the concept of\nMutual Information (MI) is extended to (Normalized) Focussed Mutual Information\n(FMI) to incorporate prior knowledge to overcome some shortcomings of MI. We\nuse this to develop new methodologies to successfully address specific\nregistration problems, the follow-up of dental restorations, cephalometry, and\nthe monitoring of implants.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 May 2007 14:41:11 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 19 Jun 2007 12:14:51 GMT"
            }
        ],
        "update_date": "2007-06-19",
        "authors_parsed": [
            [
                "Jacquet",
                "W.",
                ""
            ],
            [
                "de Groen",
                "P.",
                ""
            ]
        ]
    },
    {
        "id": "0705.3616",
        "submitter": "Andy Zaidman",
        "authors": "Andy Zaidman, Bart Van Rompaey, Serge Demeyer, Arie van Deursen",
        "title": "On How Developers Test Open Source Software Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "TUD-SERG-2007-012",
        "categories": "cs.SE",
        "license": null,
        "abstract": "  Engineering software systems is a multidisciplinary activity, whereby a\nnumber of artifacts must be created - and maintained - synchronously. In this\npaper we investigate whether production code and the accompanying tests\nco-evolve by exploring a project's versioning system, code coverage reports and\nsize-metrics. Three open source case studies teach us that testing activities\nusually start later on during the lifetime and are more \"phased\", although we\ndid not observe increasing testing activity before releases. Furthermore, we\nnote large differences in the levels of test coverage given the proportion of\ntest code.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 May 2007 16:21:35 GMT"
            }
        ],
        "update_date": "2007-05-25",
        "authors_parsed": [
            [
                "Zaidman",
                "Andy",
                ""
            ],
            [
                "Van Rompaey",
                "Bart",
                ""
            ],
            [
                "Demeyer",
                "Serge",
                ""
            ],
            [
                "van Deursen",
                "Arie",
                ""
            ]
        ]
    },
    {
        "id": "0705.3644",
        "submitter": "Chenguang Lu",
        "authors": "Chenguang Lu",
        "title": "Subjective Information Measure and Rate Fidelity Theory",
        "comments": "5 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.HC math.IT",
        "license": null,
        "abstract": "  Using fish-covering model, this paper intuitively explains how to extend\nHartley's information formula to the generalized information formula step by\nstep for measuring subjective information: metrical information (such as\nconveyed by thermometers), sensory information (such as conveyed by color\nvision), and semantic information (such as conveyed by weather forecasts). The\npivotal step is to differentiate condition probability and logical condition\nprobability of a message. The paper illustrates the rationality of the formula,\ndiscusses the coherence of the generalized information formula and Popper's\nknowledge evolution theory. For optimizing data compression, the paper\ndiscusses rate-of-limiting-errors and its similarity to complexity-distortion\nbased on Kolmogorov's complexity theory, and improves the rate-distortion\ntheory into the rate-fidelity theory by replacing Shannon's distortion with\nsubjective mutual information. It is proved that both the rate-distortion\nfunction and the rate-fidelity function are equivalent to a\nrate-of-limiting-errors function with a group of fuzzy sets as limiting\ncondition, and can be expressed by a formula of generalized mutual information\nfor lossy coding, or by a formula of generalized entropy for lossless coding.\nBy analyzing the rate-fidelity function related to visual discrimination and\ndigitized bits of pixels of images, the paper concludes that subjective\ninformation is less than or equal to objective (Shannon's) information; there\nis an optimal matching point at which two kinds of information are equal; the\nmatching information increases with visual discrimination (defined by confusing\nprobability) rising; for given visual discrimination, too high resolution of\nimages or too much objective information is wasteful.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 May 2007 19:33:43 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Lu",
                "Chenguang",
                ""
            ]
        ]
    },
    {
        "id": "0705.3669",
        "submitter": "Donald Sofge",
        "authors": "Donald A. Sofge",
        "title": "Structural Health Monitoring Using Neural Network Based Vibrational\n  System Identification",
        "comments": "4 pages",
        "journal-ref": "D. Sofge, \"Structural Health Monitoring Using Neural Network Based\n  Vibrational System Identification,\" In Proceedings of the Australia-New\n  Zealand Conference on Intelligent Information Systems, pp. 91-94, IEEE, 1994",
        "doi": "10.1109/ANZIIS.1994.396943",
        "report-no": null,
        "categories": "cs.NE cs.CV cs.SD",
        "license": null,
        "abstract": "  Composite fabrication technologies now provide the means for producing\nhigh-strength, low-weight panels, plates, spars and other structural components\nwhich use embedded fiber optic sensors and piezoelectric transducers. These\nmaterials, often referred to as smart structures, make it possible to sense\ninternal characteristics, such as delaminations or structural degradation. In\nthis effort we use neural network based techniques for modeling and analyzing\ndynamic structural information for recognizing structural defects. This yields\nan adaptable system which gives a measure of structural integrity for composite\nstructures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 May 2007 21:48:18 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Sofge",
                "Donald A.",
                ""
            ]
        ]
    },
    {
        "id": "0705.3669",
        "submitter": "Donald Sofge",
        "authors": "Donald A. Sofge",
        "title": "Structural Health Monitoring Using Neural Network Based Vibrational\n  System Identification",
        "comments": "4 pages",
        "journal-ref": "D. Sofge, \"Structural Health Monitoring Using Neural Network Based\n  Vibrational System Identification,\" In Proceedings of the Australia-New\n  Zealand Conference on Intelligent Information Systems, pp. 91-94, IEEE, 1994",
        "doi": "10.1109/ANZIIS.1994.396943",
        "report-no": null,
        "categories": "cs.NE cs.CV cs.SD",
        "license": null,
        "abstract": "  Composite fabrication technologies now provide the means for producing\nhigh-strength, low-weight panels, plates, spars and other structural components\nwhich use embedded fiber optic sensors and piezoelectric transducers. These\nmaterials, often referred to as smart structures, make it possible to sense\ninternal characteristics, such as delaminations or structural degradation. In\nthis effort we use neural network based techniques for modeling and analyzing\ndynamic structural information for recognizing structural defects. This yields\nan adaptable system which gives a measure of structural integrity for composite\nstructures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 May 2007 21:48:18 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Sofge",
                "Donald A.",
                ""
            ]
        ]
    },
    {
        "id": "0705.3683",
        "submitter": "Hung-Ta Pai",
        "authors": "H.-T. Pai and Y. S. Han",
        "title": "Power-Efficient Direct-Voting Assurance for Data Fusion in Wireless\n  Sensor Networks",
        "comments": "33 pages, 7 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.DC cs.NI",
        "license": null,
        "abstract": "  Wireless sensor networks place sensors into an area to collect data and send\nthem back to a base station. Data fusion, which fuses the collected data before\nthey are sent to the base station, is usually implemented over the network.\nSince the sensor is typically placed in locations accessible to malicious\nattackers, information assurance of the data fusion process is very important.\nA witness-based approach has been proposed to validate the fusion data. In this\napproach, the base station receives the fusion data and \"votes\" on the data\nfrom a randomly chosen sensor node. The vote comes from other sensor nodes,\ncalled \"witnesses,\" to verify the correctness of the fusion data. Because the\nbase station obtains the vote through the chosen node, the chosen node could\nforge the vote if it is compromised. Thus, the witness node must encrypt the\nvote to prevent this forgery. Compared with the vote, the encryption requires\nmore bits, increasing transmission burden from the chosen node to the base\nstation. The chosen node consumes more power. This work improves the\nwitness-based approach using direct voting mechanism such that the proposed\nscheme has better performance in terms of assurance, overhead, and delay. The\nwitness node transmits the vote directly to the base station. Forgery is not a\nproblem in this scheme. Moreover, fewer bits are necessary to represent the\nvote, significantly reducing the power consumption. Performance analysis and\nsimulation results indicate that the proposed approach can achieve a 40 times\nbetter overhead than the witness-based approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 May 2007 02:56:47 GMT"
            }
        ],
        "update_date": "2007-05-28",
        "authors_parsed": [
            [
                "Pai",
                "H. -T.",
                ""
            ],
            [
                "Han",
                "Y. S.",
                ""
            ]
        ]
    },
    {
        "id": "0705.3683",
        "submitter": "Hung-Ta Pai",
        "authors": "H.-T. Pai and Y. S. Han",
        "title": "Power-Efficient Direct-Voting Assurance for Data Fusion in Wireless\n  Sensor Networks",
        "comments": "33 pages, 7 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.DC cs.NI",
        "license": null,
        "abstract": "  Wireless sensor networks place sensors into an area to collect data and send\nthem back to a base station. Data fusion, which fuses the collected data before\nthey are sent to the base station, is usually implemented over the network.\nSince the sensor is typically placed in locations accessible to malicious\nattackers, information assurance of the data fusion process is very important.\nA witness-based approach has been proposed to validate the fusion data. In this\napproach, the base station receives the fusion data and \"votes\" on the data\nfrom a randomly chosen sensor node. The vote comes from other sensor nodes,\ncalled \"witnesses,\" to verify the correctness of the fusion data. Because the\nbase station obtains the vote through the chosen node, the chosen node could\nforge the vote if it is compromised. Thus, the witness node must encrypt the\nvote to prevent this forgery. Compared with the vote, the encryption requires\nmore bits, increasing transmission burden from the chosen node to the base\nstation. The chosen node consumes more power. This work improves the\nwitness-based approach using direct voting mechanism such that the proposed\nscheme has better performance in terms of assurance, overhead, and delay. The\nwitness node transmits the vote directly to the base station. Forgery is not a\nproblem in this scheme. Moreover, fewer bits are necessary to represent the\nvote, significantly reducing the power consumption. Performance analysis and\nsimulation results indicate that the proposed approach can achieve a 40 times\nbetter overhead than the witness-based approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 May 2007 02:56:47 GMT"
            }
        ],
        "update_date": "2007-05-28",
        "authors_parsed": [
            [
                "Pai",
                "H. -T.",
                ""
            ],
            [
                "Han",
                "Y. S.",
                ""
            ]
        ]
    },
    {
        "id": "0705.3683",
        "submitter": "Hung-Ta Pai",
        "authors": "H.-T. Pai and Y. S. Han",
        "title": "Power-Efficient Direct-Voting Assurance for Data Fusion in Wireless\n  Sensor Networks",
        "comments": "33 pages, 7 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.DC cs.NI",
        "license": null,
        "abstract": "  Wireless sensor networks place sensors into an area to collect data and send\nthem back to a base station. Data fusion, which fuses the collected data before\nthey are sent to the base station, is usually implemented over the network.\nSince the sensor is typically placed in locations accessible to malicious\nattackers, information assurance of the data fusion process is very important.\nA witness-based approach has been proposed to validate the fusion data. In this\napproach, the base station receives the fusion data and \"votes\" on the data\nfrom a randomly chosen sensor node. The vote comes from other sensor nodes,\ncalled \"witnesses,\" to verify the correctness of the fusion data. Because the\nbase station obtains the vote through the chosen node, the chosen node could\nforge the vote if it is compromised. Thus, the witness node must encrypt the\nvote to prevent this forgery. Compared with the vote, the encryption requires\nmore bits, increasing transmission burden from the chosen node to the base\nstation. The chosen node consumes more power. This work improves the\nwitness-based approach using direct voting mechanism such that the proposed\nscheme has better performance in terms of assurance, overhead, and delay. The\nwitness node transmits the vote directly to the base station. Forgery is not a\nproblem in this scheme. Moreover, fewer bits are necessary to represent the\nvote, significantly reducing the power consumption. Performance analysis and\nsimulation results indicate that the proposed approach can achieve a 40 times\nbetter overhead than the witness-based approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 May 2007 02:56:47 GMT"
            }
        ],
        "update_date": "2007-05-28",
        "authors_parsed": [
            [
                "Pai",
                "H. -T.",
                ""
            ],
            [
                "Han",
                "Y. S.",
                ""
            ]
        ]
    },
    {
        "id": "0705.3693",
        "submitter": "Jan Mandel",
        "authors": "Jonathan D. Beezley, Jan Mandel",
        "title": "Morphing Ensemble Kalman Filters",
        "comments": "17 pages, 7 figures. Added DDDAS references to the introduction",
        "journal-ref": null,
        "doi": "10.1111/j.1600-0870.2007.00275.x",
        "report-no": "UCDHSC CCM Report 240",
        "categories": "math.DS cs.CV math.ST physics.ao-ph stat.ME stat.TH",
        "license": null,
        "abstract": "  A new type of ensemble filter is proposed, which combines an ensemble Kalman\nfilter (EnKF) with the ideas of morphing and registration from image\nprocessing. This results in filters suitable for nonlinear problems whose\nsolutions exhibit moving coherent features, such as thin interfaces in wildfire\nmodeling. The ensemble members are represented as the composition of one common\nstate with a spatial transformation, called registration mapping, plus a\nresidual. A fully automatic registration method is used that requires only\ngridded data, so the features in the model state do not need to be identified\nby the user. The morphing EnKF operates on a transformed state consisting of\nthe registration mapping and the residual. Essentially, the morphing EnKF uses\nintermediate states obtained by morphing instead of linear combinations of the\nstates.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 May 2007 05:46:33 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 27 May 2007 19:38:42 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 8 Aug 2007 17:17:36 GMT"
            },
            {
                "version": "v4",
                "created": "Thu, 23 Aug 2007 07:45:33 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Beezley",
                "Jonathan D.",
                ""
            ],
            [
                "Mandel",
                "Jan",
                ""
            ]
        ]
    },
    {
        "id": "0705.3740",
        "submitter": "Gilles Z\\'emor",
        "authors": "J. Bringer, H. Chabanne, G. Cohen, B. Kindarji, G. Z\\'emor",
        "title": "Optimal Iris Fuzzy Sketches",
        "comments": "9 pages. Submitted to the IEEE Conference on Biometrics: Theory,\n  Applications and Systems, 2007 Washington DC",
        "journal-ref": "Biometrics: Theory, Applications, and Systems, 2007. BTAS 2007.\n  First IEEE International Conference on",
        "doi": "10.1109/BTAS.2007.4401904",
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  Fuzzy sketches, introduced as a link between biometry and cryptography, are a\nway of handling biometric data matching as an error correction issue. We focus\nhere on iris biometrics and look for the best error-correcting code in that\nrespect. We show that two-dimensional iterative min-sum decoding leads to\nresults near the theoretical limits. In particular, we experiment our\ntechniques on the Iris Challenge Evaluation (ICE) database and validate our\nfindings.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 May 2007 10:38:14 GMT"
            }
        ],
        "update_date": "2008-04-02",
        "authors_parsed": [
            [
                "Bringer",
                "J.",
                ""
            ],
            [
                "Chabanne",
                "H.",
                ""
            ],
            [
                "Cohen",
                "G.",
                ""
            ],
            [
                "Kindarji",
                "B.",
                ""
            ],
            [
                "Z\u00e9mor",
                "G.",
                ""
            ]
        ]
    },
    {
        "id": "0705.3766",
        "submitter": "Anton Eremeev",
        "authors": "Anton Eremeev",
        "title": "On complexity of optimized crossover for binary representations",
        "comments": "Dagstuhl Seminar 06061 \"Theory of Evolutionary Algorithms\", 2006",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  We consider the computational complexity of producing the best possible\noffspring in a crossover, given two solutions of the parents. The crossover\noperators are studied on the class of Boolean linear programming problems,\nwhere the Boolean vector of variables is used as the solution representation.\nBy means of efficient reductions of the optimized gene transmitting crossover\nproblems (OGTC) we show the polynomial solvability of the OGTC for the maximum\nweight set packing problem, the minimum weight set partition problem and for\none of the versions of the simple plant location problem. We study a connection\nbetween the OGTC for linear Boolean programming problem and the maximum weight\nindependent set problem on 2-colorable hypergraph and prove the NP-hardness of\nseveral special cases of the OGTC problem in Boolean linear programming.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 May 2007 13:07:18 GMT"
            }
        ],
        "update_date": "2007-05-28",
        "authors_parsed": [
            [
                "Eremeev",
                "Anton",
                ""
            ]
        ]
    },
    {
        "id": "0705.3766",
        "submitter": "Anton Eremeev",
        "authors": "Anton Eremeev",
        "title": "On complexity of optimized crossover for binary representations",
        "comments": "Dagstuhl Seminar 06061 \"Theory of Evolutionary Algorithms\", 2006",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  We consider the computational complexity of producing the best possible\noffspring in a crossover, given two solutions of the parents. The crossover\noperators are studied on the class of Boolean linear programming problems,\nwhere the Boolean vector of variables is used as the solution representation.\nBy means of efficient reductions of the optimized gene transmitting crossover\nproblems (OGTC) we show the polynomial solvability of the OGTC for the maximum\nweight set packing problem, the minimum weight set partition problem and for\none of the versions of the simple plant location problem. We study a connection\nbetween the OGTC for linear Boolean programming problem and the maximum weight\nindependent set problem on 2-colorable hypergraph and prove the NP-hardness of\nseveral special cases of the OGTC problem in Boolean linear programming.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 May 2007 13:07:18 GMT"
            }
        ],
        "update_date": "2007-05-28",
        "authors_parsed": [
            [
                "Eremeev",
                "Anton",
                ""
            ]
        ]
    },
    {
        "id": "0705.3949",
        "submitter": "Yeb Havinga",
        "authors": "Yeb Havinga",
        "title": "Translating a first-order modal language to relational algebra",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.DB",
        "license": null,
        "abstract": "  This paper is about Kripke structures that are inside a relational database\nand queried with a modal language. At first the modal language that is used is\nintroduced, followed by a definition of the database and relational algebra.\nBased on these definitions two things are presented: a mapping from components\nof the modal structure to a relational database schema and instance, and a\ntranslation from queries in the modal language to relational algebra queries.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 27 May 2007 12:36:58 GMT"
            }
        ],
        "update_date": "2007-05-29",
        "authors_parsed": [
            [
                "Havinga",
                "Yeb",
                ""
            ]
        ]
    },
    {
        "id": "0705.4134",
        "submitter": "Michael Vielhaber",
        "authors": "Michael Vielhaber and Monica del Pilar Canales",
        "title": "The Battery-Discharge-Model: A Class of Stochastic Finite Automata to\n  Simulate Multidimensional Continued Fraction Expansion",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.CC cs.CR math.IT",
        "license": null,
        "abstract": "  We define an infinite stochastic state machine, the Battery-Discharge-Model\n(BDM), which simulates the behaviour of linear and jump complexity of the\ncontinued fraction expansion of multidimensional formal power series, a\nrelevant security measure in the cryptanalysis of stream ciphers.\n  We also obtain finite approximations to the infinite BDM, where polynomially\nmany states suffice to approximate with an exponentially small error the\nprobabilities and averages for linear and jump complexity of M-multisequences\nof length n over the finite field F_q, for any M, n, q.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 29 May 2007 02:50:42 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Vielhaber",
                "Michael",
                ""
            ],
            [
                "Canales",
                "Monica del Pilar",
                ""
            ]
        ]
    },
    {
        "id": "0705.4138",
        "submitter": "Michael Vielhaber",
        "authors": "Michael Vielhaber and Monica del Pilar Canales",
        "title": "The Asymptotic Normalized Linear Complexity of Multisequences",
        "comments": "19 pages, 2 figures, submitted to J. Complexity",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.CC cs.CR math.IT",
        "license": null,
        "abstract": "  We show that the asymptotic linear complexity of a multisequence a in\nF_q^\\infty that is I := liminf L_a(n)/n and S := limsup L_a(n)/n satisfy the\ninequalities M/(M+1) <= S <= 1 and M(1-S) <= I <= 1-S/M, if all M sequences\nhave nonzero discrepancy infinitely often, and all pairs (I,S) satisfying these\nconditions are met by 2^{\\aleph_0} multisequences a.\n  This answers an Open Problem by Dai, Imamura, and Yang.\n  Keywords: Linear complexity, multisequence, Battery Discharge Model,\nisometry.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 29 May 2007 03:41:21 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Vielhaber",
                "Michael",
                ""
            ],
            [
                "Canales",
                "Monica del Pilar",
                ""
            ]
        ]
    },
    {
        "id": "0705.4185",
        "submitter": "Tony Thomas",
        "authors": "Tony Thomas",
        "title": "Secure Two-party Protocols for Point Inclusion Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  It is well known that, in theory, the general secure multi-party computation\nproblem is solvable using circuit evaluation protocols. However, the\ncommunication complexity of the resulting protocols depend on the size of the\ncircuit that expresses the functionality to be computed and hence can be\nimpractical. Hence special solutions are needed for specific problems for\nefficiency reasons. The point inclusion problem in computational geometry is a\nspecial multiparty computation and has got many applications. Previous\nprotocols for the secure point inclusion problem are not adequate. In this\npaper we modify some known solutions to the point inclusion problem in\ncomputational geometry to the frame work of secure two-party computation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 29 May 2007 10:21:25 GMT"
            }
        ],
        "update_date": "2007-05-30",
        "authors_parsed": [
            [
                "Thomas",
                "Tony",
                ""
            ]
        ]
    },
    {
        "id": "0705.4302",
        "submitter": "Jens Oehlschl\\\"agel",
        "authors": "Jens Oehlschl\\\"agel",
        "title": "Truecluster matching",
        "comments": "15 pages, 2 figures. Details the matching needed for \"Truecluster:\n  robust scalable clustering with model selection\" but can also be used in\n  different contexts",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  Cluster matching by permuting cluster labels is important in many clustering\ncontexts such as cluster validation and cluster ensemble techniques. The\nclassic approach is to minimize the euclidean distance between two cluster\nsolutions which induces inappropriate stability in certain settings. Therefore,\nwe present the truematch algorithm that introduces two improvements best\nexplained in the crisp case. First, instead of maximizing the trace of the\ncluster crosstable, we propose to maximize a chi-square transformation of this\ncrosstable. Thus, the trace will not be dominated by the cells with the largest\ncounts but by the cells with the most non-random observations, taking into\naccount the marginals. Second, we suggest a probabilistic component in order to\nbreak ties and to make the matching algorithm truly random on random data. The\ntruematch algorithm is designed as a building block of the truecluster\nframework and scales in polynomial time. First simulation results confirm that\nthe truematch algorithm gives more consistent truecluster results for unequal\ncluster sizes. Free R software is available.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 29 May 2007 21:52:17 GMT"
            }
        ],
        "update_date": "2007-05-31",
        "authors_parsed": [
            [
                "Oehlschl\u00e4gel",
                "Jens",
                ""
            ]
        ]
    },
    {
        "id": "0705.4369",
        "submitter": "Jean-Michel Muller",
        "authors": "Peter Kornerup (IMADA), Vincent Lef\\`evre (LIP), Jean-Michel Muller\n  (LIP)",
        "title": "Computing Integer Powers in Floating-Point Arithmetic",
        "comments": "Laboratoire LIP : CNRS/ENS Lyon/INRIA/Universit\\'e Lyon 1",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NA cs.MS",
        "license": null,
        "abstract": "  We introduce two algorithms for accurately evaluating powers to a positive\ninteger in floating-point arithmetic, assuming a fused multiply-add (fma)\ninstruction is available. We show that our log-time algorithm always produce\nfaithfully-rounded results, discuss the possibility of getting correctly\nrounded results, and show that results correctly rounded in double precision\ncan be obtained if extended-precision is available with the possibility to\nround into double precision (with a single rounding).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 30 May 2007 11:34:39 GMT"
            }
        ],
        "update_date": "2007-06-13",
        "authors_parsed": [
            [
                "Kornerup",
                "Peter",
                "",
                "IMADA"
            ],
            [
                "Lef\u00e8vre",
                "Vincent",
                "",
                "LIP"
            ],
            [
                "Muller",
                "Jean-Michel",
                "",
                "LIP"
            ]
        ]
    },
    {
        "id": "0705.4415",
        "submitter": "Cnrs : Umr 6057 Laboratoire Parole Et Langage",
        "authors": "Carine Andr\\'e (LPL), Alain Ghio (LPL), Christian Cav\\'e (LPL),\n  Bernard Teston (LPL)",
        "title": "PERCEVAL: a Computer-Driven System for Experimentation on Auditory and\n  Visual Perception",
        "comments": null,
        "journal-ref": "Proceedings of International Congress of Phonetic Sciences (ICPhS)\n  (2003) 1421-1424",
        "doi": null,
        "report-no": "1557",
        "categories": "cs.SE",
        "license": null,
        "abstract": "  Since perception tests are highly time-consuming, there is a need to automate\nas many operations as possible, such as stimulus generation, procedure control,\nperception testing, and data analysis. The computer-driven system we are\npresenting here meets these objectives. To achieve large flexibility, the tests\nare controlled by scripts. The system's core software resembles that of a\nlexical-syntactic analyzer, which reads and interprets script files sent to it.\nThe execution sequence (trial) is modified in accordance with the commands and\ndata received. This type of operation provides a great deal of flexibility and\nsupports a wide variety of tests such as auditory-lexical decision making,\nphoneme monitoring, gating, phonetic categorization, word identification, voice\nquality, etc. To achieve good performance, we were careful about timing\naccuracy, which is the greatest problem in computerized perception tests.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 30 May 2007 15:31:07 GMT"
            }
        ],
        "update_date": "2007-05-31",
        "authors_parsed": [
            [
                "Andr\u00e9",
                "Carine",
                "",
                "LPL"
            ],
            [
                "Ghio",
                "Alain",
                "",
                "LPL"
            ],
            [
                "Cav\u00e9",
                "Christian",
                "",
                "LPL"
            ],
            [
                "Teston",
                "Bernard",
                "",
                "LPL"
            ]
        ]
    },
    {
        "id": "0705.4442",
        "submitter": "Dan Olteanu",
        "authors": "Dan Olteanu and Christoph Koch and Lyublena Antova",
        "title": "World-set Decompositions: Expressiveness and Efficient Algorithms",
        "comments": "34 pages, 13 figures, extended version of ICDT'07 paper",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  Uncertain information is commonplace in real-world data management scenarios.\nThe ability to represent large sets of possible instances (worlds) while\nsupporting efficient storage and processing is an important challenge in this\ncontext. The recent formalism of world-set decompositions (WSDs) provides a\nspace-efficient representation for uncertain data that also supports scalable\nprocessing. WSDs are complete for finite world-sets in that they can represent\nany finite set of possible worlds. For possibly infinite world-sets, we show\nthat a natural generalization of WSDs precisely captures the expressive power\nof c-tables. We then show that several important decision problems are\nefficiently solvable on WSDs while they are NP-hard on c-tables. Finally, we\ngive a polynomial-time algorithm for factorizing WSDs, i.e. an efficient\nalgorithm for minimizing such representations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 30 May 2007 17:56:06 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 9 Jan 2008 10:58:06 GMT"
            }
        ],
        "update_date": "2008-01-09",
        "authors_parsed": [
            [
                "Olteanu",
                "Dan",
                ""
            ],
            [
                "Koch",
                "Christoph",
                ""
            ],
            [
                "Antova",
                "Lyublena",
                ""
            ]
        ]
    },
    {
        "id": "0705.4485",
        "submitter": "Edoardo Airoldi",
        "authors": "Edoardo M Airoldi, David M Blei, Stephen E Fienberg, Eric P Xing",
        "title": "Mixed membership stochastic blockmodels",
        "comments": "46 pages, 14 figures, 3 tables",
        "journal-ref": "Journal of Machine Learning Research, 9, 1981-2014.",
        "doi": null,
        "report-no": null,
        "categories": "stat.ME cs.LG math.ST physics.soc-ph stat.ML stat.TH",
        "license": null,
        "abstract": "  Observations consisting of measurements on relationships for pairs of objects\narise in many settings, such as protein interaction and gene regulatory\nnetworks, collections of author-recipient email, and social networks. Analyzing\nsuch data with probabilisic models can be delicate because the simple\nexchangeability assumptions underlying many boilerplate models no longer hold.\nIn this paper, we describe a latent variable model of such data called the\nmixed membership stochastic blockmodel. This model extends blockmodels for\nrelational data to ones which capture mixed membership latent relational\nstructure, thus providing an object-specific low-dimensional representation. We\ndevelop a general variational inference algorithm for fast approximate\nposterior inference. We explore applications to social and protein interaction\nnetworks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 30 May 2007 23:22:59 GMT"
            }
        ],
        "update_date": "2010-02-22",
        "authors_parsed": [
            [
                "Airoldi",
                "Edoardo M",
                ""
            ],
            [
                "Blei",
                "David M",
                ""
            ],
            [
                "Fienberg",
                "Stephen E",
                ""
            ],
            [
                "Xing",
                "Eric P",
                ""
            ]
        ]
    },
    {
        "id": "0705.4566",
        "submitter": "Bastian Wemmenhove",
        "authors": "Bastian Wemmenhove and Bert Kappen",
        "title": "Loop corrections for message passing algorithms in continuous variable\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LG",
        "license": null,
        "abstract": "  In this paper we derive the equations for Loop Corrected Belief Propagation\non a continuous variable Gaussian model. Using the exactness of the averages\nfor belief propagation for Gaussian models, a different way of obtaining the\ncovariances is found, based on Belief Propagation on cavity graphs. We discuss\nthe relation of this loop correction algorithm to Expectation Propagation\nalgorithms for the case in which the model is no longer Gaussian, but slightly\nperturbed by nonlinear terms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 31 May 2007 10:35:07 GMT"
            }
        ],
        "update_date": "2007-06-01",
        "authors_parsed": [
            [
                "Wemmenhove",
                "Bastian",
                ""
            ],
            [
                "Kappen",
                "Bert",
                ""
            ]
        ]
    },
    {
        "id": "0705.4566",
        "submitter": "Bastian Wemmenhove",
        "authors": "Bastian Wemmenhove and Bert Kappen",
        "title": "Loop corrections for message passing algorithms in continuous variable\n  models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LG",
        "license": null,
        "abstract": "  In this paper we derive the equations for Loop Corrected Belief Propagation\non a continuous variable Gaussian model. Using the exactness of the averages\nfor belief propagation for Gaussian models, a different way of obtaining the\ncovariances is found, based on Belief Propagation on cavity graphs. We discuss\nthe relation of this loop correction algorithm to Expectation Propagation\nalgorithms for the case in which the model is no longer Gaussian, but slightly\nperturbed by nonlinear terms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 31 May 2007 10:35:07 GMT"
            }
        ],
        "update_date": "2007-06-01",
        "authors_parsed": [
            [
                "Wemmenhove",
                "Bastian",
                ""
            ],
            [
                "Kappen",
                "Bert",
                ""
            ]
        ]
    },
    {
        "id": "0705.4584",
        "submitter": "Stefan Johansson",
        "authors": "Magnus Boman and Stefan J. Johansson",
        "title": "Modeling Epidemic Spread in Synthetic Populations - Virtual Plagues in\n  Massively Multiplayer Online Games",
        "comments": "Accepted for presentation at Digital Games Research Association\n  (DiGRA) conference in Tokyo in September 2007. All comments to the authors\n  (mail addresses are in the paper) are welcome",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CY cs.AI cs.MA",
        "license": null,
        "abstract": "  A virtual plague is a process in which a behavior-affecting property spreads\namong characters in a Massively Multiplayer Online Game (MMOG). The MMOG\nindividuals constitute a synthetic population, and the game can be seen as a\nform of interactive executable model for studying disease spread, albeit of a\nvery special kind. To a game developer maintaining an MMOG, recognizing,\nmonitoring, and ultimately controlling a virtual plague is important,\nregardless of how it was initiated. The prospect of using tools, methods and\ntheory from the field of epidemiology to do this seems natural and appealing.\nWe will address the feasibility of such a prospect, first by considering some\nbasic measures used in epidemiology, then by pointing out the differences\nbetween real world epidemics and virtual plagues. We also suggest directions\nfor MMOG developer control through epidemiological modeling. Our aim is\nunderstanding the properties of virtual plagues, rather than trying to\neliminate them or mitigate their effects, as would be in the case of real\ninfectious disease.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 31 May 2007 12:15:05 GMT"
            }
        ],
        "update_date": "2007-06-01",
        "authors_parsed": [
            [
                "Boman",
                "Magnus",
                ""
            ],
            [
                "Johansson",
                "Stefan J.",
                ""
            ]
        ]
    },
    {
        "id": "0705.4654",
        "submitter": "Donald Sofge",
        "authors": "Peter F. Lichtenwalner and Donald A. Sofge",
        "title": "Local Area Damage Detection in Composite Structures Using Piezoelectric\n  Transducers",
        "comments": "7 pages",
        "journal-ref": "P.F. Lichtenwalner and D. Sofge, \"Local Area Damage Detection in\n  Composite Structures Using Piezoelectric Transducers,\" In Proc. SPIE Sym. on\n  Smart Structures and Materials, Vol. 3326, SPIE, pp. 509-515, 1998",
        "doi": "10.1117/12.310667",
        "report-no": null,
        "categories": "cs.SD cs.CV",
        "license": null,
        "abstract": "  An integrated and automated smart structures approach for structural health\nmonitoring is presented, utilizing an array of piezoelectric transducers\nattached to or embedded within the structure for both actuation and sensing.\nThe system actively interrogates the structure via broadband excitation of\nmultiple actuators across a desired frequency range. The structure's vibration\nsignature is then characterized by computing the transfer functions between\neach actuator/sensor pair, and compared to the baseline signature. Experimental\nresults applying the system to local area damage detection in a MD Explorer\nrotorcraft composite flexbeam are presented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 31 May 2007 17:19:17 GMT"
            }
        ],
        "update_date": "2015-05-13",
        "authors_parsed": [
            [
                "Lichtenwalner",
                "Peter F.",
                ""
            ],
            [
                "Sofge",
                "Donald A.",
                ""
            ]
        ]
    },
    {
        "id": "0705.4676",
        "submitter": "Daniel Lemire",
        "authors": "Daniel Lemire and Owen Kaser",
        "title": "Recursive n-gram hashing is pairwise independent, at best",
        "comments": "See software at https://github.com/lemire/rollinghashcpp",
        "journal-ref": "Computer Speech & Language 24(4): 698-710 (2010)",
        "doi": "10.1016/j.csl.2009.12.001",
        "report-no": null,
        "categories": "cs.DB cs.CL",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Many applications use sequences of n consecutive symbols (n-grams). Hashing\nthese n-grams can be a performance bottleneck. For more speed, recursive hash\nfamilies compute hash values by updating previous values. We prove that\nrecursive hash families cannot be more than pairwise independent. While hashing\nby irreducible polynomials is pairwise independent, our implementations either\nrun in time O(n) or use an exponential amount of memory. As a more scalable\nalternative, we make hashing by cyclic polynomials pairwise independent by\nignoring n-1 bits. Experimentally, we show that hashing by cyclic polynomials\nis is twice as fast as hashing by irreducible polynomials. We also show that\nrandomized Karp-Rabin hash families are not pairwise independent.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 31 May 2007 18:41:28 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 8 Dec 2008 17:39:33 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 6 Feb 2009 21:37:14 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 23 Feb 2009 16:23:41 GMT"
            },
            {
                "version": "v5",
                "created": "Wed, 5 Aug 2009 03:01:20 GMT"
            },
            {
                "version": "v6",
                "created": "Wed, 19 Aug 2009 14:39:54 GMT"
            },
            {
                "version": "v7",
                "created": "Wed, 4 Jan 2012 20:37:05 GMT"
            },
            {
                "version": "v8",
                "created": "Mon, 6 Jun 2016 15:18:03 GMT"
            }
        ],
        "update_date": "2016-06-07",
        "authors_parsed": [
            [
                "Lemire",
                "Daniel",
                ""
            ],
            [
                "Kaser",
                "Owen",
                ""
            ]
        ]
    },
    {
        "id": "0706.0014",
        "submitter": "Anna Urbanska",
        "authors": "Anna Urbanska (LJK)",
        "title": "Towards an exact adaptive algorithm for the determinant of a rational\n  matrix",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC",
        "license": null,
        "abstract": "  In this paper we propose several strategies for the exact computation of the\ndeterminant of a rational matrix. First, we use the Chinese Remaindering\nTheorem and the rational reconstruction to recover the rational determinant\nfrom its modular images. Then we show a preconditioning for the determinant\nwhich allows us to skip the rational reconstruction process and reconstruct an\ninteger result. We compare those approaches with matrix preconditioning which\nallow us to treat integer instead of rational matrices. This allows us to\nintroduce integer determinant algorithms to the rational determinant problem.\nIn particular, we discuss the applicability of the adaptive determinant\nalgorithm of [9] and compare it with the integer Chinese Remaindering scheme.\nWe present an analysis of the complexity of the strategies and evaluate their\nexperimental performance on numerous examples. This experience allows us to\ndevelop an adaptive strategy which would choose the best solution at the run\ntime, depending on matrix properties. All strategies have been implemented in\nLinBox linear algebra library.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 31 May 2007 20:23:08 GMT"
            }
        ],
        "update_date": "2009-04-16",
        "authors_parsed": [
            [
                "Urbanska",
                "Anna",
                "",
                "LJK"
            ]
        ]
    },
    {
        "id": "0706.0022",
        "submitter": "Marko Antonio Rodriguez",
        "authors": "Marko A. Rodriguez and Johan Bollen",
        "title": "Modeling Computations in a Semantic Network",
        "comments": "project website: http://neno.lanl.gov",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  Semantic network research has seen a resurgence from its early history in the\ncognitive sciences with the inception of the Semantic Web initiative. The\nSemantic Web effort has brought forth an array of technologies that support the\nencoding, storage, and querying of the semantic network data structure at the\nworld stage. Currently, the popular conception of the Semantic Web is that of a\ndata modeling medium where real and conceptual entities are related in\nsemantically meaningful ways. However, new models have emerged that explicitly\nencode procedural information within the semantic network substrate. With these\nnew technologies, the Semantic Web has evolved from a data modeling medium to a\ncomputational medium. This article provides a classification of existing\ncomputational modeling efforts and the requirements of supporting technologies\nthat will aid in the further growth of this burgeoning domain.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 31 May 2007 21:56:25 GMT"
            }
        ],
        "update_date": "2021-08-23",
        "authors_parsed": [
            [
                "Rodriguez",
                "Marko A.",
                ""
            ],
            [
                "Bollen",
                "Johan",
                ""
            ]
        ]
    },
    {
        "id": "0706.0252",
        "submitter": "David Monniaux",
        "authors": "David Monniaux (LIENS)",
        "title": "Applying the Z-transform for the static analysis of floating-point\n  numerical filters",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.NA",
        "license": null,
        "abstract": "  Digital linear filters are used in a variety of applications (sound\ntreatment, control/command, etc.), implemented in software, in hardware, or a\ncombination thereof. For safety-critical applications, it is necessary to bound\nall variables and outputs of all filters. We give a compositional, effective\nabstraction for digital linear filters expressed as block diagrams, yielding\nsound, precise bounds for fixed-point or floating-point implementations of the\nfilters.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 2 Jun 2007 06:18:48 GMT"
            }
        ],
        "update_date": "2007-06-05",
        "authors_parsed": [
            [
                "Monniaux",
                "David",
                "",
                "LIENS"
            ]
        ]
    },
    {
        "id": "0706.0300",
        "submitter": "Tshilidzi Marwala",
        "authors": "Simon Scurrell, Tshilidzi Marwala and David Rubin",
        "title": "Automatic Detection of Pulmonary Embolism using Computational\n  Intelligence",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  This article describes the implementation of a system designed to\nautomatically detect the presence of pulmonary embolism in lung scans. These\nimages are firstly segmented, before alignment and feature extraction using\nPCA. The neural network was trained using the Hybrid Monte Carlo method,\nresulting in a committee of 250 neural networks and good results are obtained.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 3 Jun 2007 05:17:38 GMT"
            }
        ],
        "update_date": "2007-06-05",
        "authors_parsed": [
            [
                "Scurrell",
                "Simon",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ],
            [
                "Rubin",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0706.0427",
        "submitter": "Jidong Zhong",
        "authors": "Jidong Zhong",
        "title": "Watermark Embedding and Detection",
        "comments": "PhD Dissertation (157 pages), Shanghai Jiaotong Universtiy, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MM cs.CR",
        "license": null,
        "abstract": "  The embedder and the detector (or decoder) are the two most important\ncomponents of the digital watermarking systems. Thus in this work, we discuss\nhow to design a better embedder and detector (or decoder). I first give a\nsummary of the prospective applications of watermarking technology and major\nwatermarking schemes in the literature. My review on the literature closely\ncenters upon how the side information is exploited at both embedders and\ndetectors. In Chapter 3, I explore the optimum detector or decoder according to\na particular probability distribution of the host signals. We found that the\nperformance of both multiplicative and additive spread spectrum schemes depends\non the shape parameter of the host signals. For spread spectrum schemes, the\nperformance of the detector or the decoder is reduced by the host interference.\nThus I present a new host-interference rejection technique for the\nmultiplicative spread spectrum schemes. Its embedding rule is tailored to the\noptimum detection or decoding rule. Though the host interference rejection\nschemes enjoy a big performance gain over the traditional spread spectrum\nschemes, their drawbacks that it is difficult for them to be implemented with\nthe perceptual analysis to achieve the maximum allowable embedding level\ndiscourage their use in real scenarios. Thus, in the last chapters of this\nwork, I introduce a double-sided technique to tackle this drawback. It differs\nfrom the host interference rejection schemes in that it utilizes but does not\nreject the host interference at its embedder. The perceptual analysis can be\neasily implemented in our scheme to achieve the maximum allowable level of\nembedding strength.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 2 Jun 2007 05:37:34 GMT"
            }
        ],
        "update_date": "2007-06-13",
        "authors_parsed": [
            [
                "Zhong",
                "Jidong",
                ""
            ]
        ]
    },
    {
        "id": "0706.0430",
        "submitter": "Shishir Nagaraja",
        "authors": "Shishir Nagaraja",
        "title": "Anonymity in the Wild: Mixes on unstructured networks",
        "comments": "published in Privacy Enhancing Technologies 2007, Ottawa",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.NI",
        "license": null,
        "abstract": "  As decentralized computing scenarios get ever more popular, unstructured\ntopologies are natural candidates to consider running mix networks upon. We\nconsider mix network topologies where mixes are placed on the nodes of an\nunstructured network, such as social networks and scale-free random networks.\nWe explore the efficiency and traffic analysis resistance properties of mix\nnetworks based on unstructured topologies as opposed to theoretically optimal\nstructured topologies, under high latency conditions. We consider a mix of\ndirected and undirected network models, as well as one real world case study --\nthe LiveJournal friendship network topology. Our analysis indicates that\nmix-networks based on scale-free and small-world topologies have, firstly,\nmix-route lengths that are roughly comparable to those in expander graphs;\nsecond, that compromise of the most central nodes has little effect on\nanonymization properties, and third, batch sizes required for warding off\nintersection attacks need to be an order of magnitude higher in unstructured\nnetworks in comparison with expander graph topologies.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 4 Jun 2007 13:11:07 GMT"
            }
        ],
        "update_date": "2007-06-05",
        "authors_parsed": [
            [
                "Nagaraja",
                "Shishir",
                ""
            ]
        ]
    },
    {
        "id": "0706.0430",
        "submitter": "Shishir Nagaraja",
        "authors": "Shishir Nagaraja",
        "title": "Anonymity in the Wild: Mixes on unstructured networks",
        "comments": "published in Privacy Enhancing Technologies 2007, Ottawa",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.NI",
        "license": null,
        "abstract": "  As decentralized computing scenarios get ever more popular, unstructured\ntopologies are natural candidates to consider running mix networks upon. We\nconsider mix network topologies where mixes are placed on the nodes of an\nunstructured network, such as social networks and scale-free random networks.\nWe explore the efficiency and traffic analysis resistance properties of mix\nnetworks based on unstructured topologies as opposed to theoretically optimal\nstructured topologies, under high latency conditions. We consider a mix of\ndirected and undirected network models, as well as one real world case study --\nthe LiveJournal friendship network topology. Our analysis indicates that\nmix-networks based on scale-free and small-world topologies have, firstly,\nmix-route lengths that are roughly comparable to those in expander graphs;\nsecond, that compromise of the most central nodes has little effect on\nanonymization properties, and third, batch sizes required for warding off\nintersection attacks need to be an order of magnitude higher in unstructured\nnetworks in comparison with expander graph topologies.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 4 Jun 2007 13:11:07 GMT"
            }
        ],
        "update_date": "2007-06-05",
        "authors_parsed": [
            [
                "Nagaraja",
                "Shishir",
                ""
            ]
        ]
    },
    {
        "id": "0706.0447",
        "submitter": "Francois Rodier",
        "authors": "Fran\\c{c}ois Rodier (IML), Eric F\\'erard (GAATI)",
        "title": "Non lin\\'earit\\'e des fonctions bool\\'eennes donn\\'ees par des traces de\n  polyn\\^omes de degr\\'e binaire 3",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.NT cs.CR cs.DM math.AG",
        "license": null,
        "abstract": "  Nous \\'etudions la non lin\\'earit\\'e des fonctions d\\'efinies sur F_{2^m}\no\\`u $m$ est un entier impair, associ\\'ees aux polyn\\^omes de degr\\'e 7 ou \\`a\ndes polyn\\^omes plus g\\'en\\'eraux.\n  -----\n  We study the nonlinearity of the functions defined on F_{2^m} where $m$ is an\nodd integer, associated to the polynomials of degree 7 or more general\npolynomials.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 4 Jun 2007 14:37:29 GMT"
            }
        ],
        "update_date": "2007-06-05",
        "authors_parsed": [
            [
                "Rodier",
                "Fran\u00e7ois",
                "",
                "IML"
            ],
            [
                "F\u00e9rard",
                "Eric",
                "",
                "GAATI"
            ]
        ]
    },
    {
        "id": "0706.0457",
        "submitter": "Donald Sofge",
        "authors": "D. A. Sofge, M. A. Potter, M. D. Bugajska, A. C. Schultz",
        "title": "Challenges and Opportunities of Evolutionary Robotics",
        "comments": "6 pages",
        "journal-ref": "D.A. Sofge, M.A. Potter, M.D. Bugajska, and A.C. Schultz,\n  \"Challenges and Opportunities of Evolutionary Robotics.\" In Proc. 2nd Int'l\n  Conf. on Computational Intelligence, Robotics, and Autonomous Systems, 2003",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.RO",
        "license": null,
        "abstract": "  Robotic hardware designs are becoming more complex as the variety and number\nof on-board sensors increase and as greater computational power is provided in\never-smaller packages on-board robots. These advances in hardware, however, do\nnot automatically translate into better software for controlling complex\nrobots. Evolutionary techniques hold the potential to solve many difficult\nproblems in robotics which defy simple conventional approaches, but present\nmany challenges as well. Numerous disciplines including artificial life,\ncognitive science and neural networks, rule-based systems, behavior-based\ncontrol, genetic algorithms and other forms of evolutionary computation have\ncontributed to shaping the current state of evolutionary robotics. This paper\nprovides an overview of developments in the emerging field of evolutionary\nrobotics, and discusses some of the opportunities and challenges which\ncurrently face practitioners in the field.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 4 Jun 2007 16:08:22 GMT"
            }
        ],
        "update_date": "2007-06-05",
        "authors_parsed": [
            [
                "Sofge",
                "D. A.",
                ""
            ],
            [
                "Potter",
                "M. A.",
                ""
            ],
            [
                "Bugajska",
                "M. D.",
                ""
            ],
            [
                "Schultz",
                "A. C.",
                ""
            ]
        ]
    },
    {
        "id": "0706.0465",
        "submitter": "Donald Sofge",
        "authors": "D. A. Sofge",
        "title": "Virtual Sensor Based Fault Detection and Classification on a Plasma Etch\n  Reactor",
        "comments": "7 pages",
        "journal-ref": "D. Sofge, \"Virtual Sensor Based Fault Detection and Classification\n  on a Plasma Etch Reactor,\" The 2nd Joint Mexico-US Int'l. Workshop on Neural\n  Networks and Neurocontrol (poster), 1997",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.CV",
        "license": null,
        "abstract": "  The SEMATECH sponsored J-88-E project teaming Texas Instruments with\nNeuroDyne (et al.) focused on Fault Detection and Classification (FDC) on a Lam\n9600 aluminum plasma etch reactor, used in the process of semiconductor\nfabrication. Fault classification was accomplished by implementing a series of\nvirtual sensor models which used data from real sensors (Lam Station sensors,\nOptical Emission Spectroscopy, and RF Monitoring) to predict recipe setpoints\nand wafer state characteristics. Fault detection and classification were\nperformed by comparing predicted recipe and wafer state values with expected\nvalues. Models utilized include linear PLS, Polynomial PLS, and Neural Network\nPLS. Prediction of recipe setpoints based upon sensor data provides a\ncapability for cross-checking that the machine is maintaining the desired\nsetpoints. Wafer state characteristics such as Line Width Reduction and\nRemaining Oxide were estimated on-line using these same process sensors (Lam,\nOES, RFM). Wafer-to-wafer measurement of these characteristics in a production\nsetting (where typically this information may be only sparsely available, if at\nall, after batch processing runs with numerous wafers have been completed)\nwould provide important information to the operator that the process is or is\nnot producing wafers within acceptable bounds of product quality. Production\nyield is increased, and correspondingly per unit cost is reduced, by providing\nthe operator with the opportunity to adjust the process or machine before\netching more wafers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 4 Jun 2007 15:55:27 GMT"
            }
        ],
        "update_date": "2007-06-05",
        "authors_parsed": [
            [
                "Sofge",
                "D. A.",
                ""
            ]
        ]
    },
    {
        "id": "0706.0465",
        "submitter": "Donald Sofge",
        "authors": "D. A. Sofge",
        "title": "Virtual Sensor Based Fault Detection and Classification on a Plasma Etch\n  Reactor",
        "comments": "7 pages",
        "journal-ref": "D. Sofge, \"Virtual Sensor Based Fault Detection and Classification\n  on a Plasma Etch Reactor,\" The 2nd Joint Mexico-US Int'l. Workshop on Neural\n  Networks and Neurocontrol (poster), 1997",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.CV",
        "license": null,
        "abstract": "  The SEMATECH sponsored J-88-E project teaming Texas Instruments with\nNeuroDyne (et al.) focused on Fault Detection and Classification (FDC) on a Lam\n9600 aluminum plasma etch reactor, used in the process of semiconductor\nfabrication. Fault classification was accomplished by implementing a series of\nvirtual sensor models which used data from real sensors (Lam Station sensors,\nOptical Emission Spectroscopy, and RF Monitoring) to predict recipe setpoints\nand wafer state characteristics. Fault detection and classification were\nperformed by comparing predicted recipe and wafer state values with expected\nvalues. Models utilized include linear PLS, Polynomial PLS, and Neural Network\nPLS. Prediction of recipe setpoints based upon sensor data provides a\ncapability for cross-checking that the machine is maintaining the desired\nsetpoints. Wafer state characteristics such as Line Width Reduction and\nRemaining Oxide were estimated on-line using these same process sensors (Lam,\nOES, RFM). Wafer-to-wafer measurement of these characteristics in a production\nsetting (where typically this information may be only sparsely available, if at\nall, after batch processing runs with numerous wafers have been completed)\nwould provide important information to the operator that the process is or is\nnot producing wafers within acceptable bounds of product quality. Production\nyield is increased, and correspondingly per unit cost is reduced, by providing\nthe operator with the opportunity to adjust the process or machine before\netching more wafers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 4 Jun 2007 15:55:27 GMT"
            }
        ],
        "update_date": "2007-06-05",
        "authors_parsed": [
            [
                "Sofge",
                "D. A.",
                ""
            ]
        ]
    },
    {
        "id": "0706.0502",
        "submitter": "Eugen Zalinescu",
        "authors": "Veronique Cortier, Michael Rusinovitch, Eugen Zalinescu",
        "title": "Relating two standard notions of secrecy",
        "comments": "29 pages, published in LMCS",
        "journal-ref": "Logical Methods in Computer Science, Volume 3, Issue 3 (July 6,\n  2007) lmcs:1093",
        "doi": "10.2168/LMCS-3(3:2)2007",
        "report-no": null,
        "categories": "cs.CR cs.LO",
        "license": null,
        "abstract": "  Two styles of definitions are usually considered to express that a security\nprotocol preserves the confidentiality of a data s. Reachability-based secrecy\nmeans that s should never be disclosed while equivalence-based secrecy states\nthat two executions of a protocol with distinct instances for s should be\nindistinguishable to an attacker. Although the second formulation ensures a\nhigher level of security and is closer to cryptographic notions of secrecy,\ndecidability results and automatic tools have mainly focused on the first\ndefinition so far.\n  This paper initiates a systematic investigation of the situations where\nsyntactic secrecy entails strong secrecy. We show that in the passive case,\nreachability-based secrecy actually implies equivalence-based secrecy for\ndigital signatures, symmetric and asymmetric encryption provided that the\nprimitives are probabilistic. For active adversaries, we provide sufficient\n(and rather tight) conditions on the protocol for this implication to hold.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 4 Jun 2007 19:30:33 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 6 Jul 2007 09:06:10 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Cortier",
                "Veronique",
                ""
            ],
            [
                "Rusinovitch",
                "Michael",
                ""
            ],
            [
                "Zalinescu",
                "Eugen",
                ""
            ]
        ]
    },
    {
        "id": "0706.0507",
        "submitter": "Hichem Geryville",
        "authors": "Hichem Geryville (LIESP), Yacine Ouzrout (LIESP), Abdelaziz Bouras\n  (LIESP), Nikolaos Sapidis",
        "title": "A collaborative framework to exchange and share product information\n  within a supply chain context",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  The new requirement for \"collaboration\" between multidisciplinary\ncollaborators induces to exchange and share adequate information on the\nproduct, processes throughout the products' lifecycle. Thus, effective capture\nof information, and also its extraction, recording, exchange, sharing, and\nreuse become increasingly critical. These lead companies to adopt new improved\nmethodologies in managing the exchange and sharing of information. The aim of\nthis paper is to describe a collaborative framework system to exchange and\nshare information, which is based on: (i) The Product Process Collaboration\nOrganization model (PPCO) which defines product and process information, and\nthe various collaboration methods for the organizations involved in the supply\nchain. (ii) Viewpoint model describes relationships between each actor and the\ncomprehensive Product/Process model, defining each actor's \"domain of interest\"\nwithin the evolving product definition. (iii) A layer which defines the\ncomprehensive organization and collaboration relationships between the actors\nwithin the supply chain. (iv) Based on the above relationships, the last layer\nproposes a typology of exchanged messages. A communication method, based on\nXML, is developed that supports optimal exchange/sharing of information. To\nillustrate the proposed framework system, an example is presented related to\ncollaborative design of a new piston for an automotive engine. The focus is on\nuser-viewpoint integration to ensure that the adequate information is retrieved\nfrom the PPCO.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 4 Jun 2007 19:48:07 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Geryville",
                "Hichem",
                "",
                "LIESP"
            ],
            [
                "Ouzrout",
                "Yacine",
                "",
                "LIESP"
            ],
            [
                "Bouras",
                "Abdelaziz",
                "",
                "LIESP"
            ],
            [
                "Sapidis",
                "Nikolaos",
                ""
            ]
        ]
    },
    {
        "id": "0706.0523",
        "submitter": "Ranjit Jhala",
        "authors": "Ranjit Jhala, Kenneth L. McMillan",
        "title": "Interpolant-Based Transition Relation Approximation",
        "comments": "Conference Version at CAV 2005. 17 Pages, 9 Figures",
        "journal-ref": "Logical Methods in Computer Science, Volume 3, Issue 4 (November\n  1, 2007) lmcs:1152",
        "doi": "10.2168/LMCS-3(4:1)2007",
        "report-no": null,
        "categories": "cs.LO cs.PL cs.SE",
        "license": null,
        "abstract": "  In predicate abstraction, exact image computation is problematic, requiring\nin the worst case an exponential number of calls to a decision procedure. For\nthis reason, software model checkers typically use a weak approximation of the\nimage. This can result in a failure to prove a property, even given an adequate\nset of predicates. We present an interpolant-based method for strengthening the\nabstract transition relation in case of such failures. This approach guarantees\nconvergence given an adequate set of predicates, without requiring an exact\nimage computation. We show empirically that the method converges more rapidly\nthan an earlier method based on counterexample analysis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 4 Jun 2007 20:07:54 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 1 Nov 2007 17:00:23 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Jhala",
                "Ranjit",
                ""
            ],
            [
                "McMillan",
                "Kenneth L.",
                ""
            ]
        ]
    },
    {
        "id": "0706.0523",
        "submitter": "Ranjit Jhala",
        "authors": "Ranjit Jhala, Kenneth L. McMillan",
        "title": "Interpolant-Based Transition Relation Approximation",
        "comments": "Conference Version at CAV 2005. 17 Pages, 9 Figures",
        "journal-ref": "Logical Methods in Computer Science, Volume 3, Issue 4 (November\n  1, 2007) lmcs:1152",
        "doi": "10.2168/LMCS-3(4:1)2007",
        "report-no": null,
        "categories": "cs.LO cs.PL cs.SE",
        "license": null,
        "abstract": "  In predicate abstraction, exact image computation is problematic, requiring\nin the worst case an exponential number of calls to a decision procedure. For\nthis reason, software model checkers typically use a weak approximation of the\nimage. This can result in a failure to prove a property, even given an adequate\nset of predicates. We present an interpolant-based method for strengthening the\nabstract transition relation in case of such failures. This approach guarantees\nconvergence given an adequate set of predicates, without requiring an exact\nimage computation. We show empirically that the method converges more rapidly\nthan an earlier method based on counterexample analysis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 4 Jun 2007 20:07:54 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 1 Nov 2007 17:00:23 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Jhala",
                "Ranjit",
                ""
            ],
            [
                "McMillan",
                "Kenneth L.",
                ""
            ]
        ]
    },
    {
        "id": "0706.0564",
        "submitter": "Josephine Yu",
        "authors": "Bernd Sturmfels and Josephine Yu",
        "title": "Tropical Implicitization and Mixed Fiber Polytopes",
        "comments": "21 pages, 2 figures; Typo fixed in Theorem 5.2",
        "journal-ref": "Software for algebraic geometry, 111--131, IMA Vol. Math. Appl.,\n  148, Springer, New York, 2008",
        "doi": null,
        "report-no": null,
        "categories": "cs.SC math.AG math.CO",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The software TrIm offers implementations of tropical implicitization and\ntropical elimination, as developed by Tevelev and the authors. Given a\npolynomial map with generic coefficients, TrIm computes the tropical variety of\nthe image. When the image is a hypersurface, the output is the Newton polytope\nof the defining polynomial. TrIm can thus be used to compute mixed fiber\npolytopes, including secondary polytopes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 5 Jun 2007 00:53:37 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 20 Jun 2010 22:57:11 GMT"
            }
        ],
        "update_date": "2010-06-22",
        "authors_parsed": [
            [
                "Sturmfels",
                "Bernd",
                ""
            ],
            [
                "Yu",
                "Josephine",
                ""
            ]
        ]
    },
    {
        "id": "0706.0580",
        "submitter": "Yoo Chung",
        "authors": "Yoo Chung",
        "title": "Efficient Batch Update of Unique Identifiers in a Distributed Hash Table\n  for Resources in a Mobile Host",
        "comments": "To be presented at the 2010 International Workshop on Cloud\n  Computing, Applications and Technologies",
        "journal-ref": null,
        "doi": "10.1109/ISPA.2010.73",
        "report-no": null,
        "categories": "cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Resources in a distributed system can be identified using identifiers based\non random numbers. When using a distributed hash table to resolve such\nidentifiers to network locations, the straightforward approach is to store the\nnetwork location directly in the hash table entry associated with an\nidentifier. When a mobile host contains a large number of resources, this\nrequires that all of the associated hash table entries must be updated when its\nnetwork address changes.\n  We propose an alternative approach where we store a host identifier in the\nentry associated with a resource identifier and the actual network address of\nthe host in a separate host entry. This can drastically reduce the time\nrequired for updating the distributed hash table when a mobile host changes its\nnetwork address. We also investigate under which circumstances our approach\nshould or should not be used. We evaluate and confirm the usefulness of our\napproach with experiments run on top of OpenDHT.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 5 Jun 2007 05:28:54 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 21 Jun 2010 02:51:05 GMT"
            }
        ],
        "update_date": "2011-04-07",
        "authors_parsed": [
            [
                "Chung",
                "Yoo",
                ""
            ]
        ]
    },
    {
        "id": "0706.0585",
        "submitter": "Zhendong Zhao",
        "authors": "Zhendong Zhao, Lei Yuan, Yuxuan Wang, Forrest Sheng Bao, Shunyi Zhang\n  Yanfei Sun",
        "title": "A Novel Model of Working Set Selection for SMO Decomposition Methods",
        "comments": "8 pages, 12 figures, it was submitted to IEEE International\n  conference of Tools on Artificial Intelligence",
        "journal-ref": null,
        "doi": "10.1109/ICTAI.2007.99",
        "report-no": null,
        "categories": "cs.LG cs.AI",
        "license": null,
        "abstract": "  In the process of training Support Vector Machines (SVMs) by decomposition\nmethods, working set selection is an important technique, and some exciting\nschemes were employed into this field. To improve working set selection, we\npropose a new model for working set selection in sequential minimal\noptimization (SMO) decomposition methods. In this model, it selects B as\nworking set without reselection. Some properties are given by simple proof, and\nexperiments demonstrate that the proposed method is in general faster than\nexisting methods.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 5 Jun 2007 05:55:07 GMT"
            }
        ],
        "update_date": "2016-11-15",
        "authors_parsed": [
            [
                "Zhao",
                "Zhendong",
                ""
            ],
            [
                "Yuan",
                "Lei",
                ""
            ],
            [
                "Wang",
                "Yuxuan",
                ""
            ],
            [
                "Bao",
                "Forrest Sheng",
                ""
            ],
            [
                "Sun",
                "Shunyi Zhang Yanfei",
                ""
            ]
        ]
    },
    {
        "id": "0706.0585",
        "submitter": "Zhendong Zhao",
        "authors": "Zhendong Zhao, Lei Yuan, Yuxuan Wang, Forrest Sheng Bao, Shunyi Zhang\n  Yanfei Sun",
        "title": "A Novel Model of Working Set Selection for SMO Decomposition Methods",
        "comments": "8 pages, 12 figures, it was submitted to IEEE International\n  conference of Tools on Artificial Intelligence",
        "journal-ref": null,
        "doi": "10.1109/ICTAI.2007.99",
        "report-no": null,
        "categories": "cs.LG cs.AI",
        "license": null,
        "abstract": "  In the process of training Support Vector Machines (SVMs) by decomposition\nmethods, working set selection is an important technique, and some exciting\nschemes were employed into this field. To improve working set selection, we\npropose a new model for working set selection in sequential minimal\noptimization (SMO) decomposition methods. In this model, it selects B as\nworking set without reselection. Some properties are given by simple proof, and\nexperiments demonstrate that the proposed method is in general faster than\nexisting methods.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 5 Jun 2007 05:55:07 GMT"
            }
        ],
        "update_date": "2016-11-15",
        "authors_parsed": [
            [
                "Zhao",
                "Zhendong",
                ""
            ],
            [
                "Yuan",
                "Lei",
                ""
            ],
            [
                "Wang",
                "Yuxuan",
                ""
            ],
            [
                "Bao",
                "Forrest Sheng",
                ""
            ],
            [
                "Sun",
                "Shunyi Zhang Yanfei",
                ""
            ]
        ]
    },
    {
        "id": "0706.0870",
        "submitter": "Nachi Gupta",
        "authors": "Nachi Gupta, Raphael Hauser, and Neil F. Johnson",
        "title": "Inferring the Composition of a Trader Population in a Financial Market",
        "comments": "15 pages, 2 figures, to appear as a chapter in \"Econophysics and\n  Sociophysics of Markets and Networks\", Springer-Verlag",
        "journal-ref": null,
        "doi": "10.1007/978-88-470-0665-2_7",
        "report-no": null,
        "categories": "cs.CE nlin.AO",
        "license": null,
        "abstract": "  We discuss a method for predicting financial movements and finding pockets of\npredictability in the price-series, which is built around inferring the\nheterogeneity of trading strategies in a multi-agent trader population. This\nwork explores extensions to our previous framework (arXiv:physics/0506134).\nHere we allow for more intelligent agents possessing a richer strategy set, and\nwe no longer constrain the estimate for the heterogeneity of the agents to a\nprobability space. We also introduce a scheme which allows the incorporation of\nmodels with a wide variety of agent types, and discuss a mechanism for the\nremoval of bias from relevant parameters.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 6 Jun 2007 17:29:42 GMT"
            }
        ],
        "update_date": "2015-05-13",
        "authors_parsed": [
            [
                "Gupta",
                "Nachi",
                ""
            ],
            [
                "Hauser",
                "Raphael",
                ""
            ],
            [
                "Johnson",
                "Neil F.",
                ""
            ]
        ]
    },
    {
        "id": "0706.1001",
        "submitter": "Krzysztof R. Apt",
        "authors": "Krzysztof R. Apt",
        "title": "Epistemic Analysis of Strategic Games with Arbitrary Strategy Sets",
        "comments": "8 pages Proc. of the 11th Conference on Theoretical Aspects of\n  Rationality and Knowledge (TARK XI), 2007. To appear",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.GT cs.AI",
        "license": null,
        "abstract": "  We provide here an epistemic analysis of arbitrary strategic games based on\nthe possibility correspondences. Such an analysis calls for the use of\ntransfinite iterations of the corresponding operators. Our approach is based on\nTarski's Fixpoint Theorem and applies both to the notions of rationalizability\nand the iterated elimination of strictly dominated strategies.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Jun 2007 12:57:21 GMT"
            }
        ],
        "update_date": "2007-06-08",
        "authors_parsed": [
            [
                "Apt",
                "Krzysztof R.",
                ""
            ]
        ]
    },
    {
        "id": "0706.1019",
        "submitter": "Flavio Garcia",
        "authors": "Flavio D. Garcia, Peter van Rossum, and Ana Sokolova",
        "title": "Probabilistic Anonymity and Admissible Schedulers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  When studying safety properties of (formal) protocol models, it is customary\nto view the scheduler as an adversary: an entity trying to falsify the safety\nproperty. We show that in the context of security protocols, and in particular\nof anonymizing protocols, this gives the adversary too much power; for\ninstance, the contents of encrypted messages and internal computations by the\nparties should be considered invisible to the adversary.\n  We restrict the class of schedulers to a class of admissible schedulers which\nbetter model adversarial behaviour. These admissible schedulers base their\ndecision solely on the past behaviour of the system that is visible to the\nadversary.\n  Using this, we propose a definition of anonymity: for all admissible\nschedulers the identity of the users and the observations of the adversary are\nindependent stochastic variables. We also develop a proof technique for typical\ncases that can be used to proof anonymity: a system is anonymous if it is\npossible to `exchange' the behaviour of two users without the adversary\n`noticing'.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Jun 2007 15:17:52 GMT"
            }
        ],
        "update_date": "2007-06-08",
        "authors_parsed": [
            [
                "Garcia",
                "Flavio D.",
                ""
            ],
            [
                "van Rossum",
                "Peter",
                ""
            ],
            [
                "Sokolova",
                "Ana",
                ""
            ]
        ]
    },
    {
        "id": "0706.1051",
        "submitter": "Donald Sofge",
        "authors": "Donald A. Sofge and David L. Elliott",
        "title": "Improved Neural Modeling of Real-World Systems Using Genetic Algorithm\n  Based Variable Selection",
        "comments": "4 pages",
        "journal-ref": "D. Sofge and D. Elliott, \"Improved Neural Modeling of Real-World\n  Systems Using Genetic Algorithm Based Variable Selection,\" In Int'l Conf. on\n  Neural Networks and Brain (ICNN&B'98-Beijing), 1998",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  Neural network models of real-world systems, such as industrial processes,\nmade from sensor data must often rely on incomplete data. System states may not\nall be known, sensor data may be biased or noisy, and it is not often known\nwhich sensor data may be useful for predictive modelling. Genetic algorithms\nmay be used to help to address this problem by determining the near optimal\nsubset of sensor variables most appropriate to produce good models. This paper\ndescribes the use of genetic search to optimize variable selection to determine\ninputs into the neural network model. We discuss genetic algorithm\nimplementation issues including data representation types and genetic operators\nsuch as crossover and mutation. We present the use of this technique for neural\nnetwork modelling of a typical industrial application, a liquid fed ceramic\nmelter, and detail the results of the genetic search to optimize the neural\nnetwork model for this application.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Jun 2007 18:13:59 GMT"
            }
        ],
        "update_date": "2007-06-08",
        "authors_parsed": [
            [
                "Sofge",
                "Donald A.",
                ""
            ],
            [
                "Elliott",
                "David L.",
                ""
            ]
        ]
    },
    {
        "id": "0706.1061",
        "submitter": "Donald Sofge",
        "authors": "Donald Sofge and Gerald Chiang",
        "title": "Design, Implementation, and Cooperative Coevolution of an Autonomous/\n  Teleoperated Control System for a Serpentine Robotic Manipulator",
        "comments": null,
        "journal-ref": "D. Sofge and G. Chiang, \"Design, ... a Serpentine Automated Waste\n  Retrieval Manipulator,\" Amer. Nucl. Soc. 9th Top. Meeting on Robotics and\n  Remote Systems, 2001",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.RO",
        "license": null,
        "abstract": "  Design, implementation, and machine learning issues associated with\ndeveloping a control system for a serpentine robotic manipulator are explored.\nThe controller developed provides autonomous control of the serpentine robotic\nmanipulatorduring operation of the manipulator within an enclosed environment\nsuch as an underground storage tank. The controller algorithms make use of both\nlow-level joint angle control employing force/position feedback constraints,\nand high-level coordinated control of end-effector positioning. This approach\nhas resulted in both high-level full robotic control and low-level telerobotic\ncontrol modes, and provides a high level of dexterity for the operator.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Jun 2007 19:27:12 GMT"
            }
        ],
        "update_date": "2019-08-19",
        "authors_parsed": [
            [
                "Sofge",
                "Donald",
                ""
            ],
            [
                "Chiang",
                "Gerald",
                ""
            ]
        ]
    },
    {
        "id": "0706.1063",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Steffen Rothkugel",
        "title": "Small Worlds: Strong Clustering in Wireless Networks",
        "comments": "To appear in: 1st International Workshop on Localized Algorithms and\n  Protocols for Wireless Sensor Networks (LOCALGOS 2007), 2007, IEEE Compuster\n  Society Press",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.DC cs.DS",
        "license": null,
        "abstract": "  Small-worlds represent efficient communication networks that obey two\ndistinguishing characteristics: a high clustering coefficient together with a\nsmall characteristic path length. This paper focuses on an interesting paradox,\nthat removing links in a network can increase the overall clustering\ncoefficient. Reckful Roaming, as introduced in this paper, is a 2-localized\nalgorithm that takes advantage of this paradox in order to selectively remove\nsuperfluous links, this way optimizing the clustering coefficient while still\nretaining a sufficiently small characteristic path length.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Jun 2007 19:42:51 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 11 Jun 2007 05:36:04 GMT"
            }
        ],
        "update_date": "2007-06-11",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0706.1063",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Steffen Rothkugel",
        "title": "Small Worlds: Strong Clustering in Wireless Networks",
        "comments": "To appear in: 1st International Workshop on Localized Algorithms and\n  Protocols for Wireless Sensor Networks (LOCALGOS 2007), 2007, IEEE Compuster\n  Society Press",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.DC cs.DS",
        "license": null,
        "abstract": "  Small-worlds represent efficient communication networks that obey two\ndistinguishing characteristics: a high clustering coefficient together with a\nsmall characteristic path length. This paper focuses on an interesting paradox,\nthat removing links in a network can increase the overall clustering\ncoefficient. Reckful Roaming, as introduced in this paper, is a 2-localized\nalgorithm that takes advantage of this paradox in order to selectively remove\nsuperfluous links, this way optimizing the clustering coefficient while still\nretaining a sufficiently small characteristic path length.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Jun 2007 19:42:51 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 11 Jun 2007 05:36:04 GMT"
            }
        ],
        "update_date": "2007-06-11",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0706.1080",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Adrian Andronache, Steffen Rothkugel",
        "title": "WACA: A Hierarchical Weighted Clustering Algorithm optimized for Mobile\n  Hybrid Networks",
        "comments": "The Third International Conference on Wireless and Mobile\n  Communications 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.NI",
        "license": null,
        "abstract": "  Clustering techniques create hierarchal network structures, called clusters,\non an otherwise flat network. In a dynamic environment-in terms of node\nmobility as well as in terms of steadily changing device parameters-the\nclusterhead election process has to be re-invoked according to a suitable\nupdate policy. Cluster re-organization causes additional message exchanges and\ncomputational complexity and it execution has to be optimized. Our\ninvestigations focus on the problem of minimizing clusterhead re-elections by\nconsidering stability criteria. These criteria are based on topological\ncharacteristics as well as on device parameters. This paper presents a weighted\nclustering algorithm optimized to avoid needless clusterhead re-elections for\nstable clusters in mobile ad-hoc networks. The proposed localized algorithm\ndeals with mobility, but does not require geographical, speed or distances\ninformation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Jun 2007 20:47:19 GMT"
            }
        ],
        "update_date": "2007-06-11",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Andronache",
                "Adrian",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0706.1080",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Adrian Andronache, Steffen Rothkugel",
        "title": "WACA: A Hierarchical Weighted Clustering Algorithm optimized for Mobile\n  Hybrid Networks",
        "comments": "The Third International Conference on Wireless and Mobile\n  Communications 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.NI",
        "license": null,
        "abstract": "  Clustering techniques create hierarchal network structures, called clusters,\non an otherwise flat network. In a dynamic environment-in terms of node\nmobility as well as in terms of steadily changing device parameters-the\nclusterhead election process has to be re-invoked according to a suitable\nupdate policy. Cluster re-organization causes additional message exchanges and\ncomputational complexity and it execution has to be optimized. Our\ninvestigations focus on the problem of minimizing clusterhead re-elections by\nconsidering stability criteria. These criteria are based on topological\ncharacteristics as well as on device parameters. This paper presents a weighted\nclustering algorithm optimized to avoid needless clusterhead re-elections for\nstable clusters in mobile ad-hoc networks. The proposed localized algorithm\ndeals with mobility, but does not require geographical, speed or distances\ninformation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Jun 2007 20:47:19 GMT"
            }
        ],
        "update_date": "2007-06-11",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Andronache",
                "Adrian",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0706.1087",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Steffen Rothkugel",
        "title": "On Anomalies in Annotation Systems",
        "comments": "The Third International Workshop on E-learning and Mobile Learning on\n  Telecommunications (ELETE 2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.CY",
        "license": null,
        "abstract": "  Today's computer-based annotation systems implement a wide range of\nfunctionalities that often go beyond those available in traditional\npaper-and-pencil annotations. Conceptually, annotation systems are based on\nthoroughly investigated psycho-sociological and pedagogical learning theories.\nThey offer a huge diversity of annotation types that can be placed in textual\nas well as in multimedia format. Additionally, annotations can be published or\nshared with a group of interested parties via well-organized repositories.\nAlthough highly sophisticated annotation systems exist both conceptually as\nwell as technologically, we still observe that their acceptance is somewhat\nlimited. In this paper, we argue that nowadays annotation systems suffer from\nseveral fundamental problems that are inherent in the traditional\npaper-and-pencil annotation paradigm. As a solution, we propose to shift the\nannotation paradigm for the implementation of annotation system.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Jun 2007 21:23:40 GMT"
            }
        ],
        "update_date": "2007-06-11",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0706.1096",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Steffen Rothkugel, Carlos H.C. Ribeiro",
        "title": "Inquiring the Potential of Evoking Small-World Properties for\n  Self-Organizing Communication Networks",
        "comments": null,
        "journal-ref": "Published in: Proceedings of 5th International Conference on\n  Networking (ICN 06), 2006, IEEE Computer Society Press",
        "doi": "10.1109/ICNICONSMCL.2006.124",
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  Mobile multi-hop ad hoc networks allow establishing local groups of\ncommunicating devices in a self-organizing way. However, in a global setting\nsuch networks fail to work properly due to network partitioning. Providing that\ndevices are capable of communicating both locally-e.g. using Wi-Fi or\nBluetooth-and additionally also with arbitrary remote devices-e.g. using\nGSM/UMTS links-the objective is to find efficient ways of inter-linking\nmultiple network partitions. Tackling this problem of topology control, we\nfocus on the class of small-world networks that obey two distinguishing\ncharacteristics: they have a strong local clustering while still retaining a\nsmall average distance between two nodes. This paper reports on results gained\ninvestigating the question if small-world properties are indicative for an\nefficient link management in multiple multi-hop ad hoc network partitions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Jun 2007 22:40:40 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 11 Jun 2007 05:34:49 GMT"
            }
        ],
        "update_date": "2007-06-11",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ],
            [
                "Ribeiro",
                "Carlos H. C.",
                ""
            ]
        ]
    },
    {
        "id": "0706.1119",
        "submitter": "Stefan Stefanov Z",
        "authors": "Stefan Z. Stefanov",
        "title": "Cointegration of the Daily Electric Power System Load and the Weather",
        "comments": "8 pages, extended version",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper makes a thermal predictive analysis of the electric power system\nsecurity for a day ahead. This predictive analysis is set as a thermal\ncomputation of the expected security. This computation is obtained by\ncointegrating the daily electric power systen load and the weather, by finding\nthe daily electric power system thermodynamics and by introducing tests for\nthis thermodynamics. The predictive analysis made shows the electricity\nconsumers' wisdom.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Jun 2007 07:04:24 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 24 Jan 2011 11:53:45 GMT"
            }
        ],
        "update_date": "2011-01-25",
        "authors_parsed": [
            [
                "Stefanov",
                "Stefan Z.",
                ""
            ]
        ]
    },
    {
        "id": "0706.1127",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Christian M. Adriano, Ivan M.L. Ricarte",
        "title": "Redesigning Computer-based Learning Environments: Evaluation as\n  Communication",
        "comments": "International Conference on Education and Information Systems (EISTA\n  04), ISBN 980-6560-11-6",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CY cs.HC",
        "license": null,
        "abstract": "  In the field of evaluation research, computer scientists live constantly upon\ndilemmas and conflicting theories. As evaluation is differently perceived and\nmodeled among educational areas, it is not difficult to become trapped in\ndilemmas, which reflects an epistemological weakness. Additionally, designing\nand developing a computer-based learning scenario is not an easy task.\nAdvancing further, with end-users probing the system in realistic settings, is\neven harder. Computer science research in evaluation faces an immense\nchallenge, having to cope with contributions from several conflicting and\ncontroversial research fields. We believe that deep changes must be made in our\nfield if we are to advance beyond the CBT (computer-based training) learning\nmodel and to build an adequate epistemology for this challenge. The first task\nis to relocate our field by building upon recent results from philosophy,\npsychology, social sciences, and engineering. In this article we locate\nevaluation in respect to communication studies. Evaluation presupposes a\ndefinition of goals to be reached, and we suggest that it is, by many means, a\nsilent communication between teacher and student, peers, and institutional\nentities. If we accept that evaluation can be viewed as set of invisible rules\nknown by nobody, but somehow understood by everybody, we should add\nanthropological inquiries to our research toolkit. The paper is organized\naround some elements of the social communication and how they convey new\ninsights to evaluation research for computer and related scientists. We found\nsome technical limitations and offer discussions on how we relate to technology\nat same time we establish expectancies and perceive others work.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Jun 2007 08:12:21 GMT"
            }
        ],
        "update_date": "2007-06-11",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Adriano",
                "Christian M.",
                ""
            ],
            [
                "Ricarte",
                "Ivan M. L.",
                ""
            ]
        ]
    },
    {
        "id": "0706.1130",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Steffen Rothkugel",
        "title": "A Communication Model for Adaptive Service Provisioning in Hybrid\n  Wireless Networks",
        "comments": "WSEAS Transactions on Circuits and Systems 2004",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.AR cs.CY cs.HC",
        "license": null,
        "abstract": "  Mobile entities with wireless links are able to form a mobile ad-hoc network.\nSuch an infrastructureless network does not have to be administrated. However,\nself-organizing principles have to be applied to deal with upcoming problems,\ne.g. information dissemination. These kinds of problems are not easy to tackle,\nrequiring complex algorithms. Moreover, the usefulness of pure ad-hoc networks\nis arguably limited. Hence, enthusiasm for mobile ad-hoc networks, which could\neliminate the need for any fixed infrastructure, has been damped. The goal is\nto overcome the limitations of pure ad-hoc networks by augmenting them with\ninstant Internet access, e.g. via integration of UMTS respectively GSM links.\nHowever, this raises multiple questions at the technical as well as the\norganizational level. Motivated by characteristics of small-world networks that\ndescribe an efficient network even without central or organized design, this\npaper proposes to combine mobile ad-hoc networks and infrastructured networks\nto form hybrid wireless networks. One main objective is to investigate how this\napproach can reduce the costs of a permanent backbone link and providing in the\nsame way the benefits of useful information from Internet connectivity or\nservice providers. For the purpose of bridging between the different types of\nnetworks, an adequate middleware service is the focus of our investigation.\nThis paper shows our first steps forward to this middleware by introducing the\nInjection Communication paradigm as principal concept.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Jun 2007 08:23:14 GMT"
            }
        ],
        "update_date": "2007-06-11",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0706.1130",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Steffen Rothkugel",
        "title": "A Communication Model for Adaptive Service Provisioning in Hybrid\n  Wireless Networks",
        "comments": "WSEAS Transactions on Circuits and Systems 2004",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.AR cs.CY cs.HC",
        "license": null,
        "abstract": "  Mobile entities with wireless links are able to form a mobile ad-hoc network.\nSuch an infrastructureless network does not have to be administrated. However,\nself-organizing principles have to be applied to deal with upcoming problems,\ne.g. information dissemination. These kinds of problems are not easy to tackle,\nrequiring complex algorithms. Moreover, the usefulness of pure ad-hoc networks\nis arguably limited. Hence, enthusiasm for mobile ad-hoc networks, which could\neliminate the need for any fixed infrastructure, has been damped. The goal is\nto overcome the limitations of pure ad-hoc networks by augmenting them with\ninstant Internet access, e.g. via integration of UMTS respectively GSM links.\nHowever, this raises multiple questions at the technical as well as the\norganizational level. Motivated by characteristics of small-world networks that\ndescribe an efficient network even without central or organized design, this\npaper proposes to combine mobile ad-hoc networks and infrastructured networks\nto form hybrid wireless networks. One main objective is to investigate how this\napproach can reduce the costs of a permanent backbone link and providing in the\nsame way the benefits of useful information from Internet connectivity or\nservice providers. For the purpose of bridging between the different types of\nnetworks, an adequate middleware service is the focus of our investigation.\nThis paper shows our first steps forward to this middleware by introducing the\nInjection Communication paradigm as principal concept.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Jun 2007 08:23:14 GMT"
            }
        ],
        "update_date": "2007-06-11",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0706.1137",
        "submitter": "Thierry Poibeau",
        "authors": "Amanda Bouffier (LIPN), Thierry Poibeau (LIPN)",
        "title": "Automatically Restructuring Practice Guidelines using the GEM DTD",
        "comments": null,
        "journal-ref": "Proceedings of Biomedical Natural Language Processing (BioNLP)\n  (2007) -",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  This paper describes a system capable of semi-automatically filling an XML\ntemplate from free texts in the clinical domain (practice guidelines). The XML\ntemplate includes semantic information not explicitly encoded in the text\n(pairs of conditions and actions/recommendations). Therefore, there is a need\nto compute the exact scope of conditions over text sequences expressing the\nrequired actions. We present a system developed for this task. We show that it\nyields good performance when applied to the analysis of French practice\nguidelines.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Jun 2007 15:39:49 GMT"
            }
        ],
        "update_date": "2007-06-11",
        "authors_parsed": [
            [
                "Bouffier",
                "Amanda",
                "",
                "LIPN"
            ],
            [
                "Poibeau",
                "Thierry",
                "",
                "LIPN"
            ]
        ]
    },
    {
        "id": "0706.1141",
        "submitter": "Matthias Brust R.",
        "authors": "Adrian Andronache, Matthias R. Brust, Steffen Rothkugel",
        "title": "Multimedia Content Distribution in Hybrid Wireless Networks using\n  Weighted Clustering",
        "comments": "2nd ACM Workshop on Wireless Multimedia Networking and Performance\n  Modeling 2006 (ISBN 1-59593-485)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MM cs.NI",
        "license": null,
        "abstract": "  Fixed infrastructured networks naturally support centralized approaches for\ngroup management and information provisioning. Contrary to infrastructured\nnetworks, in multi-hop ad-hoc networks each node acts as a router as well as\nsender and receiver. Some applications, however, requires hierarchical\narrangements that-for practical reasons-has to be done locally and\nself-organized. An additional challenge is to deal with mobility that causes\npermanent network partitioning and re-organizations. Technically, these\nproblems can be tackled by providing additional uplinks to a backbone network,\nwhich can be used to access resources in the Internet as well as to inter-link\nmultiple ad-hoc network partitions, creating a hybrid wireless network. In this\npaper, we present a prototypically implemented hybrid wireless network system\noptimized for multimedia content distribution. To efficiently manage the ad-hoc\ncommunicating devices a weighted clustering algorithm is introduced. The\nproposed localized algorithm deals with mobility, but does not require\ngeographical information or distances.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Jun 2007 09:31:46 GMT"
            }
        ],
        "update_date": "2012-01-16",
        "authors_parsed": [
            [
                "Andronache",
                "Adrian",
                ""
            ],
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0706.1142",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Steffen Rothkugel",
        "title": "Localized Support for Injection Point Election in Hybrid Networks",
        "comments": "The Sixth International Conference on Networking (ICN 2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.NI",
        "license": null,
        "abstract": "  Ad-hoc networks, a promising trend in wireless technology, fail to work\nproperly in a global setting. In most cases, self-organization and cost-free\nlocal communication cannot compensate the need for being connected, gathering\nurgent information just-in-time. Equipping mobile devices additionally with GSM\nor UMTS adapters in order to communicate with arbitrary remote devices or even\na fixed network infrastructure provides an opportunity. Devices that operate as\nintermediate nodes between the ad-hoc network and a reliable backbone network\nare potential injection points. They allow disseminating received information\nwithin the local neighborhood. The effectiveness of different devices to serve\nas injection point differs substantially. For practical reasons the\ndetermination of injection points should be done locally, within the ad-hoc\nnetwork partitions. We analyze different localized algorithms using at most\n2-hop neighboring information. Results show that devices selected this way\nspread information more efficiently through the ad-hoc network. Our results can\nalso be applied in order to support the election process for clusterheads in\nthe field of clustering mechanisms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Jun 2007 09:36:41 GMT"
            }
        ],
        "update_date": "2007-06-12",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0706.1142",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Steffen Rothkugel",
        "title": "Localized Support for Injection Point Election in Hybrid Networks",
        "comments": "The Sixth International Conference on Networking (ICN 2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.NI",
        "license": null,
        "abstract": "  Ad-hoc networks, a promising trend in wireless technology, fail to work\nproperly in a global setting. In most cases, self-organization and cost-free\nlocal communication cannot compensate the need for being connected, gathering\nurgent information just-in-time. Equipping mobile devices additionally with GSM\nor UMTS adapters in order to communicate with arbitrary remote devices or even\na fixed network infrastructure provides an opportunity. Devices that operate as\nintermediate nodes between the ad-hoc network and a reliable backbone network\nare potential injection points. They allow disseminating received information\nwithin the local neighborhood. The effectiveness of different devices to serve\nas injection point differs substantially. For practical reasons the\ndetermination of injection points should be done locally, within the ad-hoc\nnetwork partitions. We analyze different localized algorithms using at most\n2-hop neighboring information. Results show that devices selected this way\nspread information more efficiently through the ad-hoc network. Our results can\nalso be applied in order to support the election process for clusterheads in\nthe field of clustering mechanisms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Jun 2007 09:36:41 GMT"
            }
        ],
        "update_date": "2007-06-12",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0706.1151",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Steffen Rothkugel",
        "title": "A taxonomic Approach to Topology Control in Ad-hoc and Wireless Networks",
        "comments": "The Sixth International Conference on Networking, ICN 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.DC",
        "license": null,
        "abstract": "  Topology Control (TC) aims at tuning the topology of highly dynamic networks\nto provide better control over network resources and to increase the efficiency\nof communication. Recently, many TC protocols have been proposed. The protocols\nare designed for preserving connectivity, minimizing energy consumption,\nmaximizing the overall network coverage or network capacity. Each TC protocol\nmakes different assumptions about the network topology, environment detection\nresources, and control capacities. This circumstance makes it extremely\ndifficult to comprehend the role and purpose of each protocol. To tackle this\nsituation, a taxonomy for TC protocols is presented throughout this paper.\nAdditionally, some TC protocols are classified based upon this taxonomy.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Jun 2007 10:20:29 GMT"
            }
        ],
        "update_date": "2007-06-11",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0706.1151",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Steffen Rothkugel",
        "title": "A taxonomic Approach to Topology Control in Ad-hoc and Wireless Networks",
        "comments": "The Sixth International Conference on Networking, ICN 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.DC",
        "license": null,
        "abstract": "  Topology Control (TC) aims at tuning the topology of highly dynamic networks\nto provide better control over network resources and to increase the efficiency\nof communication. Recently, many TC protocols have been proposed. The protocols\nare designed for preserving connectivity, minimizing energy consumption,\nmaximizing the overall network coverage or network capacity. Each TC protocol\nmakes different assumptions about the network topology, environment detection\nresources, and control capacities. This circumstance makes it extremely\ndifficult to comprehend the role and purpose of each protocol. To tackle this\nsituation, a taxonomy for TC protocols is presented throughout this paper.\nAdditionally, some TC protocols are classified based upon this taxonomy.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Jun 2007 10:20:29 GMT"
            }
        ],
        "update_date": "2007-06-11",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0706.1162",
        "submitter": "Hichem Geryville",
        "authors": "Hichem Geryville (LIESP), Yacine Ouzrout (LIESP), Abdelaziz Bouras\n  (LIESP), Nikolaos Sapidis",
        "title": "The multiple viewpoints as approach to information retrieval within\n  collaborative development context",
        "comments": null,
        "journal-ref": "Dans Information and Communication Technologies International\n  Symposium - Information and Communication Technologies International\n  Symposium (Proceedings of IEEE), Fez : Maroc (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  Nowadays, to achieve competitive advantage, the industrial companies are\nconsidering that success is sustained to great product development. That is to\nmanage the product throughout its entire lifecycle. Achieving this goal\nrequires a tight collaboration between actors from a wide variety of domains,\nusing different software tools producing various product data types and\nformats. The actors' collaboration is mainly based on the exchange /share\nproduct information. The representation of the actors' viewpoints is the\nunderlying requirement of the collaborative product development. The multiple\nviewpoints approach was designed to provide an organizational framework\nfollowing the actors' perspectives in the collaboration, and their\nrelationships. The approach acknowledges the inevitability of multiple\nintegration of product information as different views, promotes gathering of\nactors' interest, and encourages retrieved adequate information while providing\nsupport for integration through PLM and/or SCM collaboration. In this paper, a\nmultiple viewpoints representation is proposed. The product, process,\norganization information models are discussed. A series of issues referring to\nthe viewpoints representation are discussed in detail. Based on XML standard,\ntaking electrical connector as an example, an application case of part of\nproduct information modeling is stated.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Jun 2007 11:29:24 GMT"
            }
        ],
        "update_date": "2008-02-19",
        "authors_parsed": [
            [
                "Geryville",
                "Hichem",
                "",
                "LIESP"
            ],
            [
                "Ouzrout",
                "Yacine",
                "",
                "LIESP"
            ],
            [
                "Bouras",
                "Abdelaziz",
                "",
                "LIESP"
            ],
            [
                "Sapidis",
                "Nikolaos",
                ""
            ]
        ]
    },
    {
        "id": "0706.1201",
        "submitter": "Matthias Brust R.",
        "authors": "Jose Eduardo M. Lobo, Jorge Luis Risco Becerra, Matthias R. Brust,\n  Steffen Rothkugel, Christian M. Adriano",
        "title": "Developing a Collaborative and Autonomous Training and Learning\n  Environment for Hybrid Wireless Networks",
        "comments": "Global Congress on Engineering and Technology Education (GCETE 2005)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CY cs.HC cs.NI",
        "license": null,
        "abstract": "  With larger memory capacities and the ability to link into wireless networks,\nmore and more students uses palmtop and handheld computers for learning\nactivities. However, existing software for Web-based learning is not\nwell-suited for such mobile devices, both due to constrained user interfaces as\nwell as communication effort required. A new generation of applications for the\nlearning domain that is explicitly designed to work on these kinds of small\nmobile devices has to be developed. For this purpose, we introduce CARLA, a\ncooperative learning system that is designed to act in hybrid wireless\nnetworks. As a cooperative environment, CARLA aims at disseminating teaching\nmaterial, notes, and even components of itself through both fixed and mobile\nnetworks to interested nodes. Due to the mobility of nodes, CARLA deals with\nupcoming problems such as network partitions and synchronization of teaching\nmaterial, resource dependencies, and time constraints.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Jun 2007 16:38:32 GMT"
            }
        ],
        "update_date": "2007-06-11",
        "authors_parsed": [
            [
                "Lobo",
                "Jose Eduardo M.",
                ""
            ],
            [
                "Becerra",
                "Jorge Luis Risco",
                ""
            ],
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ],
            [
                "Adriano",
                "Christian M.",
                ""
            ]
        ]
    },
    {
        "id": "0706.1201",
        "submitter": "Matthias Brust R.",
        "authors": "Jose Eduardo M. Lobo, Jorge Luis Risco Becerra, Matthias R. Brust,\n  Steffen Rothkugel, Christian M. Adriano",
        "title": "Developing a Collaborative and Autonomous Training and Learning\n  Environment for Hybrid Wireless Networks",
        "comments": "Global Congress on Engineering and Technology Education (GCETE 2005)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CY cs.HC cs.NI",
        "license": null,
        "abstract": "  With larger memory capacities and the ability to link into wireless networks,\nmore and more students uses palmtop and handheld computers for learning\nactivities. However, existing software for Web-based learning is not\nwell-suited for such mobile devices, both due to constrained user interfaces as\nwell as communication effort required. A new generation of applications for the\nlearning domain that is explicitly designed to work on these kinds of small\nmobile devices has to be developed. For this purpose, we introduce CARLA, a\ncooperative learning system that is designed to act in hybrid wireless\nnetworks. As a cooperative environment, CARLA aims at disseminating teaching\nmaterial, notes, and even components of itself through both fixed and mobile\nnetworks to interested nodes. Due to the mobility of nodes, CARLA deals with\nupcoming problems such as network partitions and synchronization of teaching\nmaterial, resource dependencies, and time constraints.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Jun 2007 16:38:32 GMT"
            }
        ],
        "update_date": "2007-06-11",
        "authors_parsed": [
            [
                "Lobo",
                "Jose Eduardo M.",
                ""
            ],
            [
                "Becerra",
                "Jorge Luis Risco",
                ""
            ],
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ],
            [
                "Adriano",
                "Christian M.",
                ""
            ]
        ]
    },
    {
        "id": "0706.1290",
        "submitter": "Sylviane Schwer",
        "authors": "Sylviane R. Schwer (LIPN)",
        "title": "Temporal Reasoning without Transitive Tables",
        "comments": "rapport interne",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  Representing and reasoning about qualitative temporal information is an\nessential part of many artificial intelligence tasks. Lots of models have been\nproposed in the litterature for representing such temporal information. All\nderive from a point-based or an interval-based framework. One fundamental\nreasoning task that arises in applications of these frameworks is given by the\nfollowing scheme: given possibly indefinite and incomplete knowledge of the\nbinary relationships between some temporal objects, find the consistent\nscenarii between all these objects. All these models require transitive tables\n-- or similarly inference rules-- for solving such tasks. We have defined an\nalternative model, S-languages - to represent qualitative temporal information,\nbased on the only two relations of \\emph{precedence} and \\emph{simultaneity}.\nIn this paper, we show how this model enables to avoid transitive tables or\ninference rules to handle this kind of problem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 9 Jun 2007 06:57:05 GMT"
            }
        ],
        "update_date": "2007-06-12",
        "authors_parsed": [
            [
                "Schwer",
                "Sylviane R.",
                "",
                "LIPN"
            ]
        ]
    },
    {
        "id": "0706.1395",
        "submitter": "Hulya Seferoglu",
        "authors": "Hulya Seferoglu, Athina Markopoulou",
        "title": "Opportunistic Network Coding for Video Streaming over Wireless",
        "comments": "11 pages, 8 figures (14 subfigures)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  In this paper, we study video streaming over wireless networks with network\ncoding capabilities. We build upon recent work, which demonstrated that network\ncoding can increase throughput over a broadcast medium, by mixing packets from\ndifferent flows into a single packet, thus increasing the information content\nper transmission. Our key insight is that, when the transmitted flows are video\nstreams, network codes should be selected so as to maximize not only the\nnetwork throughput but also the video quality. We propose video-aware\nopportunistic network coding schemes that take into account both (i) the\ndecodability of network codes by several receivers and (ii) the importance and\ndeadlines of video packets. Simulation results show that our schemes\nsignificantly improve both video quality and throughput.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 11 Jun 2007 17:33:25 GMT"
            }
        ],
        "update_date": "2007-06-12",
        "authors_parsed": [
            [
                "Seferoglu",
                "Hulya",
                ""
            ],
            [
                "Markopoulou",
                "Athina",
                ""
            ]
        ]
    },
    {
        "id": "0706.1402",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Steffen Rothkugel",
        "title": "Analyzing Design Process and Experiments on the AnITA Generic Tutoring\n  System",
        "comments": "Published in: Proceedings of International Conference on Education\n  and Information Systems (EISTA 04), 2004, ISBN 980-6560-11-6",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CY cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the field of tutoring systems, investigations have shown that there are\nmany tutoring systems specific to a specific domain that, because of their\nstatic architecture, cannot be adapted to other domains. As consequence, often\nneither methods nor knowledge can be reused. In addition, the knowledge\nengineer must have programming skills in order to enhance and evaluate the\nsystem. One particular challenge is to tackle these problems with the\ndevelopment of a generic tutoring system. AnITA, as a stand-alone application,\nhas been developed and implemented particularly for this purpose. However, in\nthe testing phase, we discovered that this architecture did not fully match the\nuser's intuitive understanding of the use of a learning tool. Therefore, AnITA\nhas been redesigned to exclusively work as a client/server application and\nrenamed to AnITA2. This paper discusses the evolvements made on the AnITA\ntutoring system, the goal of which is to use generic principles for system\nre-use in any domain. Two experiments were conducted, and the results are\npresented in this paper.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 11 Jun 2007 06:09:25 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 12 Jun 2018 19:54:33 GMT"
            }
        ],
        "update_date": "2018-06-14",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0706.1409",
        "submitter": "Bruno Salvy",
        "authors": "Jonathan M. Borwein, Bruno Salvy (INRIA Rocquencourt)",
        "title": "A Proof of a Recursion for Bessel Moments",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1080/10586458.2008.10129032",
        "report-no": null,
        "categories": "cs.SC math.CA",
        "license": null,
        "abstract": "  We provide a proof of a conjecture in (Bailey, Borwein, Borwein, Crandall\n2007) on the existence and form of linear recursions for moments of powers of\nthe Bessel function $K_0$.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 11 Jun 2007 06:45:39 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 1 Apr 2008 14:52:49 GMT"
            }
        ],
        "update_date": "2013-06-19",
        "authors_parsed": [
            [
                "Borwein",
                "Jonathan M.",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Salvy",
                "Bruno",
                "",
                "INRIA Rocquencourt"
            ]
        ]
    },
    {
        "id": "0706.1410",
        "submitter": "Marc Schoenauer",
        "authors": "Francis Sourd (CMAP), Marc Schoenauer (CMAP)",
        "title": "Evolutionary Mesh Numbering: Preliminary Results",
        "comments": null,
        "journal-ref": "Dans Adaptive Computing in Design and Manufacture, ACDM'98 (1998)\n  137-150",
        "doi": null,
        "report-no": null,
        "categories": "cs.NA cs.NE math.NA math.OC",
        "license": null,
        "abstract": "  Mesh numbering is a critical issue in Finite Element Methods, as the\ncomputational cost of one analysis is highly dependent on the order of the\nnodes of the mesh. This paper presents some preliminary investigations on the\nproblem of mesh numbering using Evolutionary Algorithms. Three conclusions can\nbe drawn from these experiments. First, the results of the up-to-date method\nused in all FEM softwares (Gibb's method) can be consistently improved; second,\nnone of the crossover operators tried so far (either general or problem\nspecific) proved useful; third, though the general tendency in Evolutionary\nComputation seems to be the hybridization with other methods (deterministic or\nheuristic), none of the presented attempt did encounter any success yet. The\ngood news, however, is that this algorithm allows an improvement over the\nstandard heuristic method between 12% and 20% for both the 1545 and 5453-nodes\nmeshes used as test-bed. Finally, some strange interaction between the\nselection scheme and the use of problem specific mutation operator was\nobserved, which appeals for further investigation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 11 Jun 2007 12:17:19 GMT"
            }
        ],
        "update_date": "2007-06-12",
        "authors_parsed": [
            [
                "Sourd",
                "Francis",
                "",
                "CMAP"
            ],
            [
                "Schoenauer",
                "Marc",
                "",
                "CMAP"
            ]
        ]
    },
    {
        "id": "0706.1456",
        "submitter": "Benoit Caillaud",
        "authors": "Albert Benveniste (IRISA), Benoit Caillaud (IRISA), Roberto Passerone",
        "title": "A Generic Model of Contracts for Embedded Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  We present the mathematical foundations of the contract-based model developed\nin the framework of the SPEEDS project. SPEEDS aims at developing methods and\ntools to support \"speculative design\", a design methodology in which\ndistributed designers develop different aspects of the overall system, in a\nconcurrent but controlled way. Our generic mathematical model of contract\nsupports this style of development. This is achieved by focusing on behaviors,\nby supporting the notion of \"rich component\" where diverse (functional and\nnon-functional) aspects of the system can be considered and combined, by\nrepresenting rich components via their set of associated contracts, and by\nformalizing the whole process of component composition.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 11 Jun 2007 12:22:15 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 13 Jun 2007 09:16:32 GMT"
            }
        ],
        "update_date": "2007-06-13",
        "authors_parsed": [
            [
                "Benveniste",
                "Albert",
                "",
                "IRISA"
            ],
            [
                "Caillaud",
                "Benoit",
                "",
                "IRISA"
            ],
            [
                "Passerone",
                "Roberto",
                ""
            ]
        ]
    },
    {
        "id": "0706.1563",
        "submitter": "Natalia Osipova",
        "authors": "Konstantin Avrachenkov (INRIA Sophia Antipolis), Patrick Brown (FT\n  R&D), Natalia Osipova (INRIA Sophia Antipolis)",
        "title": "Optimal Choice of Threshold in Two Level Processor Sharing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  We analyze the Two Level Processor Sharing (TLPS) scheduling discipline with\nthe hyper-exponential job size distribution and with the Poisson arrival\nprocess. TLPS is a convenient model to study the benefit of the file size based\ndifferentiation in TCP/IP networks. In the case of the hyper-exponential job\nsize distribution with two phases, we find a closed form analytic expression\nfor the expected sojourn time and an approximation for the optimal value of the\nthreshold that minimizes the expected sojourn time. In the case of the\nhyper-exponential job size distribution with more than two phases, we derive a\ntight upper bound for the expected sojourn time conditioned on the job size. We\nshow that when the variance of the job size distribution increases, the gain in\nsystem performance increases and the sensitivity to the choice of the threshold\nnear its optimal value decreases.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 12 Jun 2007 12:21:53 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 13 Jun 2007 09:45:28 GMT"
            }
        ],
        "update_date": "2007-06-13",
        "authors_parsed": [
            [
                "Avrachenkov",
                "Konstantin",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Brown",
                "Patrick",
                "",
                "FT\n  R&D"
            ],
            [
                "Osipova",
                "Natalia",
                "",
                "INRIA Sophia Antipolis"
            ]
        ]
    },
    {
        "id": "0706.1614",
        "submitter": "Corinne Touati",
        "authors": "Arnaud Legrand (INRIA Rh\\^one-Alpes / Id-Imag, Lig), Corinne Touati\n  (INRIA Rh\\^one-Alpes / Id-Imag, Lig)",
        "title": "Non-Cooperative Scheduling of Multiple Bag-of-Task Applications",
        "comments": null,
        "journal-ref": "Dans INFOCOM (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.GT",
        "license": null,
        "abstract": "  Multiple applications that execute concurrently on heterogeneous platforms\ncompete for CPU and network resources. In this paper we analyze the behavior of\n$K$ non-cooperative schedulers using the optimal strategy that maximize their\nefficiency while fairness is ensured at a system level ignoring applications\ncharacteristics. We limit our study to simple single-level master-worker\nplatforms and to the case where each scheduler is in charge of a single\napplication consisting of a large number of independent tasks. The tasks of a\ngiven application all have the same computation and communication requirements,\nbut these requirements can vary from one application to another. In this\ncontext, we assume that each scheduler aims at maximizing its throughput. We\ngive closed-form formula of the equilibrium reached by such a system and study\nits performance. We characterize the situations where this Nash equilibrium is\noptimal (in the Pareto sense) and show that even though no catastrophic\nsituation (Braess-like paradox) can occur, such an equilibrium can be\narbitrarily bad for any classical performance measure.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 12 Jun 2007 06:39:33 GMT"
            }
        ],
        "update_date": "2007-06-13",
        "authors_parsed": [
            [
                "Legrand",
                "Arnaud",
                "",
                "INRIA Rh\u00f4ne-Alpes / Id-Imag, Lig"
            ],
            [
                "Touati",
                "Corinne",
                "",
                "INRIA Rh\u00f4ne-Alpes / Id-Imag, Lig"
            ]
        ]
    },
    {
        "id": "0706.1755",
        "submitter": "Kirill Bolshakov",
        "authors": "Kirill Bolshakov (1), Elena Reshetova (1) ((1) Saint-Petersburg State\n  University of Aerospace Instrumentation)",
        "title": "FreeBSD Mandatory Access Control Usage for Implementing Enterprise\n  Security Policies",
        "comments": "6 pages, 3 figures, submitted to XI International Symposium \"Problems\n  of redundancy in information and control systems\"",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  FreeBSD was one of the first widely deployed free operating systems to\nprovide mandatory access control. It supports a number of classic MAC models.\nThis tutorial paper addresses exploiting this implementation to enforce typical\nenterprise security policies of varying complexities.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 12 Jun 2007 18:11:15 GMT"
            }
        ],
        "update_date": "2007-06-13",
        "authors_parsed": [
            [
                "Bolshakov",
                "Kirill",
                ""
            ],
            [
                "Reshetova",
                "Elena",
                ""
            ]
        ]
    },
    {
        "id": "0706.1780",
        "submitter": "Marie-France Ango-Obiang",
        "authors": "Marie-France Ango-Obiang (SITE, Loria)",
        "title": "Le travail collaboratif dans le cadre d'un projet architectural",
        "comments": null,
        "journal-ref": "Dans Innovation et tradition de l'association internationale\n  Management Strat\\'egique (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  The analysis of the practices and the tendencies of the users at the time of\nthe search for information on Internet makes it possible to highlight several\npoints. The search for information becomes powerful after knowledge of the\ntypology of the various systems of research. This typology supports the\nadoption of a methodology of research which one can characterize by pull\nsystems, intelligent agents, etc. In addition, the importance of the structure\nof the electronic document, correctly elaborated in advance, will support a\nhigher relevance ratio to find information. In our article, the problems turn\naround the study of the behavior of the users in situation of search for\ninformation, as well as the constitution of a pole of documentary resources\nwithin a framework of an architectural project. It is noted that the evolution\nof the documentary resources is related to information technologies.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 12 Jun 2007 20:06:38 GMT"
            }
        ],
        "update_date": "2007-06-14",
        "authors_parsed": [
            [
                "Ango-Obiang",
                "Marie-France",
                "",
                "SITE, Loria"
            ]
        ]
    },
    {
        "id": "0706.1860",
        "submitter": "Jordi Cucurull",
        "authors": "Jordi Cucurull, Ramon Marti, Sergi Robles, Joan Borrell, Guillermo\n  Navarro",
        "title": "FIPA-based Interoperable Agent Mobility Proposal",
        "comments": "10 pages, agent migration architecture proposal",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MA cs.NI",
        "license": null,
        "abstract": "  This paper presents a proposal for a flexible agent mobility architecture\nbased on IEEE-FIPA standards and intended to be one of them. This proposal is a\nfirst step towards interoperable mobility mechanisms, which are needed for\nfuture agent migration between different kinds of platforms. Our proposal is\npresented as a flexible and robust architecture that has been successfully\nimplemented in the JADE and AgentScape platforms. It is based on an open set of\nprotocols, allowing new protocols and future improvements to be accommodated in\nthe architecture. With this proposal we demonstrate that a standard\narchitecture for agent mobility capable of supporting several agent platforms\ncan be defined and implemented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 13 Jun 2007 14:37:58 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 31 Aug 2007 10:57:01 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Cucurull",
                "Jordi",
                ""
            ],
            [
                "Marti",
                "Ramon",
                ""
            ],
            [
                "Robles",
                "Sergi",
                ""
            ],
            [
                "Borrell",
                "Joan",
                ""
            ],
            [
                "Navarro",
                "Guillermo",
                ""
            ]
        ]
    },
    {
        "id": "0706.1926",
        "submitter": "Michele Bezzi",
        "authors": "Michele Bezzi, Robin Groenevelt",
        "title": "Towards understanding and modelling office daily life",
        "comments": "5 pages, ECHISE 2006 - 2nd International Workshop on Exploiting\n  Context Histories in Smart Environments - Infrastructures and Design, 8th\n  International Conference of Ubiquitous Computing (Ubicomp 2006), Orange\n  County, CA, 17-21 September 2006",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.CY",
        "license": null,
        "abstract": "  Measuring and modeling human behavior is a very complex task. In this paper\nwe present our initial thoughts on modeling and automatic recognition of some\nhuman activities in an office. We argue that to successfully model human\nactivities, we need to consider both individual behavior and group dynamics. To\ndemonstrate these theoretical approaches, we introduce an experimental system\nfor analyzing everyday activity in our office.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 13 Jun 2007 15:15:00 GMT"
            }
        ],
        "update_date": "2007-06-14",
        "authors_parsed": [
            [
                "Bezzi",
                "Michele",
                ""
            ],
            [
                "Groenevelt",
                "Robin",
                ""
            ]
        ]
    },
    {
        "id": "0706.2010",
        "submitter": "Anne Broadbent",
        "authors": "Anne Broadbent and Alain Tapp",
        "title": "Information-theoretic security without an honest majority",
        "comments": "15 pages, to appear in Proceedings of ASIACRYPT 2007",
        "journal-ref": "Proceedings of ASIACRYPT 2007 pp. 410-426",
        "doi": "10.1007/978-3-540-76900-2_25",
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  We present six multiparty protocols with information-theoretic security that\ntolerate an arbitrary number of corrupt participants. All protocols assume\npairwise authentic private channels and a broadcast channel (in a single case,\nwe require a simultaneous broadcast channel). We give protocols for veto, vote,\nanonymous bit transmission, collision detection, notification and anonymous\nmessage transmission. Not assuming an honest majority, in most cases, a single\ncorrupt participant can make the protocol abort. All protocols achieve\nfunctionality never obtained before without the use of either computational\nassumptions or of an honest majority.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 13 Jun 2007 23:05:40 GMT"
            }
        ],
        "update_date": "2016-01-06",
        "authors_parsed": [
            [
                "Broadbent",
                "Anne",
                ""
            ],
            [
                "Tapp",
                "Alain",
                ""
            ]
        ]
    },
    {
        "id": "0706.2025",
        "submitter": "Sapon Tanachaiwiwat",
        "authors": "Sapon Tanachaiwiwat, Ahmed Helmy",
        "title": "On the Performance Evaluation of Encounter-based Worm Interactions Based\n  on Node Characteristics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.NI",
        "license": null,
        "abstract": "  An encounter-based network is a frequently disconnected wireless ad-hoc\nnetwork requiring nearby neighbors to store and forward data utilizing mobility\nand encounters over time. Using traditional approaches such as gateways or\nfirewalls for deterring worm propagation in encounter-based networks is\ninappropriate. Because this type of network is highly dynamic and has no\nspecific boundary, a distributed counter-worm mechanism is needed. We propose\nmodels for the worm interaction approach that relies upon automated beneficial\nworm generation to alleviate problems of worm propagation in such networks. We\nstudy and analyze the impact of key mobile node characteristics including node\ncooperation, immunization, on-off behavior on the worm propagations and\ninteractions. We validate our proposed model using extensive simulations. We\nalso find that, in addition to immunization, cooperation can reduce the level\nof worm infection. Furthermore, on-off behavior linearly impacts only timing\naspect but not the overall infection. Using realistic mobile network\nmeasurements, we find that encounters are non-uniform, the trends are\nconsistent with the model but the magnitudes are drastically different.\nImmunization seems to be the most effective in such scenarios. These findings\nprovide insight that we hope would aid to develop counter-worm protocols in\nfuture encounter-based networks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 14 Jun 2007 02:23:01 GMT"
            }
        ],
        "update_date": "2007-06-15",
        "authors_parsed": [
            [
                "Tanachaiwiwat",
                "Sapon",
                ""
            ],
            [
                "Helmy",
                "Ahmed",
                ""
            ]
        ]
    },
    {
        "id": "0706.2025",
        "submitter": "Sapon Tanachaiwiwat",
        "authors": "Sapon Tanachaiwiwat, Ahmed Helmy",
        "title": "On the Performance Evaluation of Encounter-based Worm Interactions Based\n  on Node Characteristics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.NI",
        "license": null,
        "abstract": "  An encounter-based network is a frequently disconnected wireless ad-hoc\nnetwork requiring nearby neighbors to store and forward data utilizing mobility\nand encounters over time. Using traditional approaches such as gateways or\nfirewalls for deterring worm propagation in encounter-based networks is\ninappropriate. Because this type of network is highly dynamic and has no\nspecific boundary, a distributed counter-worm mechanism is needed. We propose\nmodels for the worm interaction approach that relies upon automated beneficial\nworm generation to alleviate problems of worm propagation in such networks. We\nstudy and analyze the impact of key mobile node characteristics including node\ncooperation, immunization, on-off behavior on the worm propagations and\ninteractions. We validate our proposed model using extensive simulations. We\nalso find that, in addition to immunization, cooperation can reduce the level\nof worm infection. Furthermore, on-off behavior linearly impacts only timing\naspect but not the overall infection. Using realistic mobile network\nmeasurements, we find that encounters are non-uniform, the trends are\nconsistent with the model but the magnitudes are drastically different.\nImmunization seems to be the most effective in such scenarios. These findings\nprovide insight that we hope would aid to develop counter-worm protocols in\nfuture encounter-based networks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 14 Jun 2007 02:23:01 GMT"
            }
        ],
        "update_date": "2007-06-15",
        "authors_parsed": [
            [
                "Tanachaiwiwat",
                "Sapon",
                ""
            ],
            [
                "Helmy",
                "Ahmed",
                ""
            ]
        ]
    },
    {
        "id": "0706.2040",
        "submitter": "Edoardo Airoldi",
        "authors": "Edoardo M Airoldi",
        "title": "Getting started in probabilistic graphical models",
        "comments": "12 pages, 1 figure",
        "journal-ref": "Airoldi EM (2007) Getting started in probabilistic graphical\n  models. PLoS Comput Biol 3(12): e252",
        "doi": "10.1371/journal.pcbi.0030252",
        "report-no": null,
        "categories": "q-bio.QM cs.LG physics.soc-ph stat.ME stat.ML",
        "license": null,
        "abstract": "  Probabilistic graphical models (PGMs) have become a popular tool for\ncomputational analysis of biological data in a variety of domains. But, what\nexactly are they and how do they work? How can we use PGMs to discover patterns\nthat are biologically relevant? And to what extent can PGMs help us formulate\nnew hypotheses that are testable at the bench? This note sketches out some\nanswers and illustrates the main ideas behind the statistical approach to\nbiological pattern discovery.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 14 Jun 2007 14:52:06 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 10 Nov 2007 19:25:59 GMT"
            }
        ],
        "update_date": "2010-02-22",
        "authors_parsed": [
            [
                "Airoldi",
                "Edoardo M",
                ""
            ]
        ]
    },
    {
        "id": "0706.2069",
        "submitter": "Samuel Thibault",
        "authors": "Samuel Thibault (INRIA Futurs), Raymond Namyst (INRIA Futurs),\n  Pierre-Andr\\'e Wacrenier (INRIA Futurs)",
        "title": "Building Portable Thread Schedulers for Hierarchical Multiprocessors:\n  the BubbleSched Framework",
        "comments": null,
        "journal-ref": "Dans EuroPar (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Exploiting full computational power of current more and more hierarchical\nmultiprocessor machines requires a very careful distribution of threads and\ndata among the underlying non-uniform architecture. Unfortunately, most\noperating systems only provide a poor scheduling API that does not allow\napplications to transmit valuable scheduling hints to the system. In a previous\npaper, we showed that using a bubble-based thread scheduler can significantly\nimprove applications' performance in a portable way. However, since\nmultithreaded applications have various scheduling requirements, there is no\nuniversal scheduler that could meet all these needs. In this paper, we present\na framework that allows scheduling experts to implement and experiment with\ncustomized thread schedulers. It provides a powerful API for dynamically\ndistributing bubbles among the machine in a high-level, portable, and efficient\nway. Several examples show how experts can then develop, debug and tune their\nown portable bubble schedulers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 14 Jun 2007 09:35:30 GMT"
            }
        ],
        "update_date": "2007-06-15",
        "authors_parsed": [
            [
                "Thibault",
                "Samuel",
                "",
                "INRIA Futurs"
            ],
            [
                "Namyst",
                "Raymond",
                "",
                "INRIA Futurs"
            ],
            [
                "Wacrenier",
                "Pierre-Andr\u00e9",
                "",
                "INRIA Futurs"
            ]
        ]
    },
    {
        "id": "0706.2073",
        "submitter": "Samuel Thibault",
        "authors": "Samuel Thibault (INRIA Futurs), Fran\\c{c}ois Broquedis (INRIA Futurs),\n  Brice Goglin (INRIA Futurs), Raymond Namyst (INRIA Futurs), Pierre-Andr\\'e\n  Wacrenier (INRIA Futurs)",
        "title": "An Efficient OpenMP Runtime System for Hierarchical Arch",
        "comments": null,
        "journal-ref": "Dans International Workshop on OpenMP (IWOMP) (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  Exploiting the full computational power of always deeper hierarchical\nmultiprocessor machines requires a very careful distribution of threads and\ndata among the underlying non-uniform architecture. The emergence of multi-core\nchips and NUMA machines makes it important to minimize the number of remote\nmemory accesses, to favor cache affinities, and to guarantee fast completion of\nsynchronization steps. By using the BubbleSched platform as a threading backend\nfor the GOMP OpenMP compiler, we are able to easily transpose affinities of\nthread teams into scheduling hints using abstractions called bubbles. We then\npropose a scheduling strategy suited to nested OpenMP parallelism. The\nresulting preliminary performance evaluations show an important improvement of\nthe speedup on a typical NAS OpenMP benchmark application.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 14 Jun 2007 09:43:23 GMT"
            }
        ],
        "update_date": "2007-06-15",
        "authors_parsed": [
            [
                "Thibault",
                "Samuel",
                "",
                "INRIA Futurs"
            ],
            [
                "Broquedis",
                "Fran\u00e7ois",
                "",
                "INRIA Futurs"
            ],
            [
                "Goglin",
                "Brice",
                "",
                "INRIA Futurs"
            ],
            [
                "Namyst",
                "Raymond",
                "",
                "INRIA Futurs"
            ],
            [
                "Wacrenier",
                "Pierre-Andr\u00e9",
                "",
                "INRIA Futurs"
            ]
        ]
    },
    {
        "id": "0706.2146",
        "submitter": "Rajesh Sudarsan",
        "authors": "Rajesh Sudarsan and Calvin J. Ribbens",
        "title": "Efficient Multidimensional Data Redistribution for Resizable Parallel\n  Computations",
        "comments": "18 pages, 12 figures, 2 tables. A shorter version of this paper is\n  available in the proceedings of the The Fifth International Symposium on\n  Parallel and Distributed Processing and Applications (ISPA07)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Traditional parallel schedulers running on cluster supercomputers support\nonly static scheduling, where the number of processors allocated to an\napplication remains fixed throughout the execution of the job. This results in\nunder-utilization of idle system resources thereby decreasing overall system\nthroughput. In our research, we have developed a prototype framework called\nReSHAPE, which supports dynamic resizing of parallel MPI applications executing\non distributed memory platforms. The resizing library in ReSHAPE includes\nsupport for releasing and acquiring processors and efficiently redistributing\napplication state to a new set of processors. In this paper, we derive an\nalgorithm for redistributing two-dimensional block-cyclic arrays from $P$ to\n$Q$ processors, organized as 2-D processor grids. The algorithm ensures a\ncontention-free communication schedule for data redistribution if $P_r \\leq\nQ_r$ and $P_c \\leq Q_c$. In other cases, the algorithm implements circular row\nand column shifts on the communication schedule to minimize node contention.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 14 Jun 2007 15:54:10 GMT"
            }
        ],
        "update_date": "2007-06-15",
        "authors_parsed": [
            [
                "Sudarsan",
                "Rajesh",
                ""
            ],
            [
                "Ribbens",
                "Calvin J.",
                ""
            ]
        ]
    },
    {
        "id": "0706.2155",
        "submitter": "Greg Sepesi",
        "authors": "Greg Sepesi",
        "title": "Dualheap Selection Algorithm: Efficient, Inherently Parallel and\n  Somewhat Mysterious",
        "comments": "5 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.CC cs.DC",
        "license": null,
        "abstract": "  An inherently parallel algorithm is proposed that efficiently performs\nselection: finding the K-th largest member of a set of N members. Selection is\na common component of many more complex algorithms and therefore is a widely\nstudied problem.\n  Not much is new in the proposed dualheap selection algorithm: the heap data\nstructure is from J.W.J.Williams, the bottom-up heap construction is from R.W.\nFloyd, and the concept of a two heap data structure is from J.W.J. Williams and\nD.E. Knuth. The algorithm's novelty is limited to a few relatively minor\nimplementation twists: 1) the two heaps are oriented with their roots at the\npartition values rather than at the minimum and maximum values, 2)the coding of\none of the heaps (the heap of smaller values) employs negative indexing, and 3)\nthe exchange phase of the algorithm is similar to a bottom-up heap\nconstruction, but navigates the heap with a post-order tree traversal.\n  When run on a single processor, the dualheap selection algorithm's\nperformance is competitive with quickselect with median estimation, a common\nvariant of C.A.R. Hoare's quicksort algorithm. When run on parallel processors,\nthe dualheap selection algorithm is superior due to its subtasks that are\neasily partitioned and innately balanced.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 14 Jun 2007 16:11:24 GMT"
            }
        ],
        "update_date": "2007-06-15",
        "authors_parsed": [
            [
                "Sepesi",
                "Greg",
                ""
            ]
        ]
    },
    {
        "id": "0706.2293",
        "submitter": "Romain Pechoux",
        "authors": "Jean-Yves Marion and Romain Pechoux",
        "title": "Resource control of object-oriented programs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.LO",
        "license": null,
        "abstract": "  A sup-interpretation is a tool which provides an upper bound on the size of a\nvalue computed by some symbol of a program. Sup-interpretations have shown\ntheir interest to deal with the complexity of first order functional programs.\nFor instance, they allow to characterize all the functions bitwise computable\nin Alogtime. This paper is an attempt to adapt the framework of\nsup-interpretations to a fragment of oriented-object programs, including\ndistinct encodings of numbers through the use of constructor symbols, loop and\nwhile constructs and non recursive methods with side effects. We give a\ncriterion, called brotherly criterion, which ensures that each brotherly\nprogram computes objects whose size is polynomially bounded by the inputs\nsizes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 15 Jun 2007 13:39:12 GMT"
            }
        ],
        "update_date": "2007-06-18",
        "authors_parsed": [
            [
                "Marion",
                "Jean-Yves",
                ""
            ],
            [
                "Pechoux",
                "Romain",
                ""
            ]
        ]
    },
    {
        "id": "0706.2331",
        "submitter": "Erhan Bayraktar",
        "authors": "Erhan Bayraktar, Hao Xing",
        "title": "Pricing American Options for Jump Diffusions by Iterating Optimal\n  Stopping Problems for Diffusions",
        "comments": "Key Words: Pricing derivatives, American options, jump diffusions,\n  barrier options, finite difference methods",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We approximate the price of the American put for jump diffusions by a\nsequence of functions, which are computed iteratively. This sequence converges\nto the price function uniformly and exponentially fast. Each element of the\napproximating sequence solves an optimal stopping problem for geometric\nBrownian motion, and can be numerically computed using the classical finite\ndifference methods. We prove the convergence of this numerical scheme and\npresent examples to illustrate its performance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 15 Jun 2007 16:43:14 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 16 Jun 2007 15:40:41 GMT"
            },
            {
                "version": "v3",
                "created": "Sat, 14 Jun 2008 03:46:38 GMT"
            },
            {
                "version": "v4",
                "created": "Thu, 31 Jul 2008 03:43:12 GMT"
            },
            {
                "version": "v5",
                "created": "Wed, 3 Dec 2008 16:56:17 GMT"
            }
        ],
        "update_date": "2008-12-03",
        "authors_parsed": [
            [
                "Bayraktar",
                "Erhan",
                ""
            ],
            [
                "Xing",
                "Hao",
                ""
            ]
        ]
    },
    {
        "id": "0706.2520",
        "submitter": "Viktoria Rojkova",
        "authors": "Viktoria Rojkova, Mehmed Kantardzic",
        "title": "Analysis of Inter-Domain Traffic Correlations: Random Matrix Theory\n  Approach",
        "comments": "submitted to Internet Measurement Conference 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  The traffic behavior of University of Louisville network with the\ninterconnected backbone routers and the number of Virtual Local Area Network\n(VLAN) subnets is investigated using the Random Matrix Theory (RMT) approach.\nWe employ the system of equal interval time series of traffic counts at all\nrouter to router and router to subnet connections as a representation of the\ninter-VLAN traffic. The cross-correlation matrix C of the traffic rate changes\nbetween different traffic time series is calculated and tested against\nnull-hypothesis of random interactions.\n  The majority of the eigenvalues \\lambda_{i} of matrix C fall within the\nbounds predicted by the RMT for the eigenvalues of random correlation matrices.\nThe distribution of eigenvalues and eigenvectors outside of the RMT bounds\ndisplays prominent and systematic deviations from the RMT predictions.\nMoreover, these deviations are stable in time.\n  The method we use provides a unique possibility to accomplish three\nconcurrent tasks of traffic analysis. The method verifies the uncongested state\nof the network, by establishing the profile of random interactions. It\nrecognizes the system-specific large-scale interactions, by establishing the\nprofile of stable in time non-random interactions. Finally, by looking into the\neigenstatistics we are able to detect and allocate anomalies of network traffic\ninteractions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 18 Jun 2007 01:31:54 GMT"
            }
        ],
        "update_date": "2007-06-19",
        "authors_parsed": [
            [
                "Rojkova",
                "Viktoria",
                ""
            ],
            [
                "Kantardzic",
                "Mehmed",
                ""
            ]
        ]
    },
    {
        "id": "0706.2606",
        "submitter": "Christian Schaffner",
        "authors": "Serge Fehr, Christian Schaffner",
        "title": "Randomness Extraction via Delta-Biased Masking in the Presence of a\n  Quantum Attacker",
        "comments": "17 pages, v2: mainly editorial changes taking into account referee\n  comments",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "quant-ph cs.CR",
        "license": null,
        "abstract": "  Randomness extraction is of fundamental importance for information-theoretic\ncryptography. It allows to transform a raw key about which an attacker has some\nlimited knowledge into a fully secure random key, on which the attacker has\nessentially no information. Up to date, only very few randomness-extraction\ntechniques are known to work against an attacker holding quantum information on\nthe raw key. This is very much in contrast to the classical (non-quantum)\nsetting, which is much better understood and for which a vast amount of\ndifferent techniques are known and proven to work.\n  We prove a new randomness-extraction technique, which is known to work in the\nclassical setting, to be secure against a quantum attacker as well. Randomness\nextraction is done by XOR'ing a so-called delta-biased mask to the raw key. Our\nresult allows to extend the classical applications of this extractor to the\nquantum setting. We discuss the following two applications. We show how to\nencrypt a long message with a short key, information-theoretically secure\nagainst a quantum attacker, provided that the attacker has enough quantum\nuncertainty on the message. This generalizes the concept of entropically-secure\nencryption to the case of a quantum attacker. As second application, we show\nhow to do error-correction without leaking partial information to a quantum\nattacker. Such a technique is useful in settings where the raw key may contain\nerrors, since standard error-correction techniques may provide the attacker\nwith information on, say, a secret key that was used to obtain the raw key.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 18 Jun 2007 14:16:02 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 13 Dec 2007 09:19:49 GMT"
            }
        ],
        "update_date": "2007-12-13",
        "authors_parsed": [
            [
                "Fehr",
                "Serge",
                ""
            ],
            [
                "Schaffner",
                "Christian",
                ""
            ]
        ]
    },
    {
        "id": "0706.2748",
        "submitter": "Royon Yvan",
        "authors": "Yvan Royon (INRIA Rh\\^one-Alpes), St\\'ephane Fr\\'enot (INRIA\n  Rh\\^one-Alpes)",
        "title": "A Survey of Unix Init Schemes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": null,
        "abstract": "  In most modern operating systems, init (as in \"initialization\") is the\nprogram launched by the kernel at boot time. It runs as a daemon and typically\nhas PID 1. Init is responsible for spawning all other processes and scavenging\nzombies. It is also responsible for reboot and shutdown operations. This\ndocument describes existing solutions that implement the init process and/or\ninit scripts in Unix-like systems. These solutions range from the legacy and\nstill-in-use BSD and SystemV schemes, to recent and promising schemes from\nUbuntu, Apple, Sun and independent developers. Our goal is to highlight their\nfocus and compare their sets of features.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 19 Jun 2007 09:44:36 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 20 Jun 2007 07:31:38 GMT"
            }
        ],
        "update_date": "2021-08-23",
        "authors_parsed": [
            [
                "Royon",
                "Yvan",
                "",
                "INRIA Rh\u00f4ne-Alpes"
            ],
            [
                "Fr\u00e9not",
                "St\u00e9phane",
                "",
                "INRIA\n  Rh\u00f4ne-Alpes"
            ]
        ]
    },
    {
        "id": "0706.2839",
        "submitter": "Rajeev Raman",
        "authors": "Naila Rahman and Rajeev Raman",
        "title": "Cache Analysis of Non-uniform Distribution Sorting Algorithms",
        "comments": "The full version of our ESA 2000 paper (LNCS 1879) on this subject",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.PF",
        "license": null,
        "abstract": "  We analyse the average-case cache performance of distribution sorting\nalgorithms in the case when keys are independently but not necessarily\nuniformly distributed. The analysis is for both `in-place' and `out-of-place'\ndistribution sorting algorithms and is more accurate than the analysis\npresented in \\cite{RRESA00}. In particular, this new analysis yields tighter\nupper and lower bounds when the keys are drawn from a uniform distribution.\n  We use this analysis to tune the performance of the integer sorting algorithm\nMSB radix sort when it is used to sort independent uniform floating-point\nnumbers (floats). Our tuned MSB radix sort algorithm comfortably outperforms a\ncache-tuned implementations of bucketsort \\cite{RR99} and Quicksort when\nsorting uniform floats from $[0, 1)$.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 19 Jun 2007 17:12:47 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 13 Aug 2007 22:57:01 GMT"
            }
        ],
        "update_date": "2007-08-14",
        "authors_parsed": [
            [
                "Rahman",
                "Naila",
                ""
            ],
            [
                "Raman",
                "Rajeev",
                ""
            ]
        ]
    },
    {
        "id": "0706.2888",
        "submitter": "James Harold Thomas",
        "authors": "James Harold Thomas",
        "title": "Variations on Kak's Three Stage Quantum Cryptography Protocol",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  This paper introduces a variation on Kak's three-stage quanutm key\ndistribution protocol which allows for defence against the man in the middle\nattack. In addition, we introduce a new protocol, which also offers similar\nresiliance against such an attack.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 19 Jun 2007 23:04:18 GMT"
            }
        ],
        "update_date": "2007-06-21",
        "authors_parsed": [
            [
                "Thomas",
                "James Harold",
                ""
            ]
        ]
    },
    {
        "id": "0706.2893",
        "submitter": "Greg Sepesi",
        "authors": "Greg Sepesi",
        "title": "Dualheap Sort Algorithm: An Inherently Parallel Generalization of\n  Heapsort",
        "comments": "4 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.CC cs.DC",
        "license": null,
        "abstract": "  A generalization of the heapsort algorithm is proposed. At the expense of\nabout 50% more comparison and move operations for typical cases, the dualheap\nsort algorithm offers several advantages over heapsort: improved cache\nperformance, better performance if the input happens to be already sorted, and\neasier parallel implementations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 20 Jun 2007 14:42:45 GMT"
            }
        ],
        "update_date": "2007-06-21",
        "authors_parsed": [
            [
                "Sepesi",
                "Greg",
                ""
            ]
        ]
    },
    {
        "id": "0706.3008",
        "submitter": "Areski Flissi",
        "authors": "Areski Flissi (LIFL), Philippe Merle (INRIA Futurs)",
        "title": "A Generic Deployment Framework for Grid Computing and Distributed\n  Applications",
        "comments": "The original publication is available at http://www.springerlink.com",
        "journal-ref": "OTM 2006, LNCS 4276 (2006) 1402-1411",
        "doi": "10.1007/11914952_26",
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Deployment of distributed applications on large systems, and especially on\ngrid infrastructures, becomes a more and more complex task. Grid users spend a\nlot of time to prepare, install and configure middleware and application\nbinaries on nodes, and eventually start their applications. The problem is that\nthe deployment process is composed of many heterogeneous tasks that have to be\norchestrated in a specific correct order. As a consequence, the automatization\nof the deployment process is currently very difficult to reach. To address this\nproblem, we propose in this paper a generic deployment framework allowing to\nautomatize the execution of heterogeneous tasks composing the whole deployment\nprocess. Our approach is based on a reification as software components of all\nrequired deployment mechanisms or existing tools. Grid users only have to\ndescribe the configuration to deploy in a simple natural language instead of\nprogramming or scripting how the deployment process is executed. As a toy\nexample, this framework is used to deploy CORBA component-based applications\nand OpenCCM middleware on one thousand nodes of the French Grid5000\ninfrastructure.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 20 Jun 2007 15:17:47 GMT"
            }
        ],
        "update_date": "2007-06-21",
        "authors_parsed": [
            [
                "Flissi",
                "Areski",
                "",
                "LIFL"
            ],
            [
                "Merle",
                "Philippe",
                "",
                "INRIA Futurs"
            ]
        ]
    },
    {
        "id": "0706.3060",
        "submitter": "Eric Darve",
        "authors": "Erich Elsen, V. Vishal, Mike Houston, Vijay Pande, Pat Hanrahan, Eric\n  Darve",
        "title": "N-Body Simulations on GPUs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.DC",
        "license": null,
        "abstract": "  Commercial graphics processors (GPUs) have high compute capacity at very low\ncost, which makes them attractive for general purpose scientific computing. In\nthis paper we show how graphics processors can be used for N-body simulations\nto obtain improvements in performance over current generation CPUs. We have\ndeveloped a highly optimized algorithm for performing the O(N^2) force\ncalculations that constitute the major part of stellar and molecular dynamics\nsimulations. In some of the calculations, we achieve sustained performance of\nnearly 100 GFlops on an ATI X1900XTX. The performance on GPUs is comparable to\nspecialized processors such as GRAPE-6A and MDGRAPE-3, but at a fraction of the\ncost. Furthermore, the wide availability of GPUs has significant implications\nfor cluster computing and distributed computing efforts like Folding@Home.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 20 Jun 2007 21:02:14 GMT"
            }
        ],
        "update_date": "2007-06-22",
        "authors_parsed": [
            [
                "Elsen",
                "Erich",
                ""
            ],
            [
                "Vishal",
                "V.",
                ""
            ],
            [
                "Houston",
                "Mike",
                ""
            ],
            [
                "Pande",
                "Vijay",
                ""
            ],
            [
                "Hanrahan",
                "Pat",
                ""
            ],
            [
                "Darve",
                "Eric",
                ""
            ]
        ]
    },
    {
        "id": "0706.3060",
        "submitter": "Eric Darve",
        "authors": "Erich Elsen, V. Vishal, Mike Houston, Vijay Pande, Pat Hanrahan, Eric\n  Darve",
        "title": "N-Body Simulations on GPUs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.DC",
        "license": null,
        "abstract": "  Commercial graphics processors (GPUs) have high compute capacity at very low\ncost, which makes them attractive for general purpose scientific computing. In\nthis paper we show how graphics processors can be used for N-body simulations\nto obtain improvements in performance over current generation CPUs. We have\ndeveloped a highly optimized algorithm for performing the O(N^2) force\ncalculations that constitute the major part of stellar and molecular dynamics\nsimulations. In some of the calculations, we achieve sustained performance of\nnearly 100 GFlops on an ATI X1900XTX. The performance on GPUs is comparable to\nspecialized processors such as GRAPE-6A and MDGRAPE-3, but at a fraction of the\ncost. Furthermore, the wide availability of GPUs has significant implications\nfor cluster computing and distributed computing efforts like Folding@Home.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 20 Jun 2007 21:02:14 GMT"
            }
        ],
        "update_date": "2007-06-22",
        "authors_parsed": [
            [
                "Elsen",
                "Erich",
                ""
            ],
            [
                "Vishal",
                "V.",
                ""
            ],
            [
                "Houston",
                "Mike",
                ""
            ],
            [
                "Pande",
                "Vijay",
                ""
            ],
            [
                "Hanrahan",
                "Pat",
                ""
            ],
            [
                "Darve",
                "Eric",
                ""
            ]
        ]
    },
    {
        "id": "0706.3076",
        "submitter": "Shiguo Lian",
        "authors": "Shiguo Lian, Zhongxuan Liu, Zhen Ren, Haila Wang",
        "title": "On the Performance of Joint Fingerprint Embedding and Decryption Scheme",
        "comments": "10 pages,9 figures. To be submitted",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MM cs.CR",
        "license": null,
        "abstract": "  Till now, few work has been done to analyze the performances of joint\nfingerprint embedding and decryption schemes. In this paper, the security of\nthe joint fingerprint embedding and decryption scheme proposed by Kundur et al.\nis analyzed and improved. The analyses include the security against\nunauthorized customer, the security against authorized customer, the\nrelationship between security and robustness, the relationship between\nsecu-rity and imperceptibility and the perceptual security. Based these\nanalyses, some means are proposed to strengthen the system, such as multi-key\nencryp-tion and DC coefficient encryption. The method can be used to analyze\nother JFD schemes. It is expected to provide valuable information to design JFD\nschemes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 21 Jun 2007 02:44:44 GMT"
            }
        ],
        "update_date": "2007-06-22",
        "authors_parsed": [
            [
                "Lian",
                "Shiguo",
                ""
            ],
            [
                "Liu",
                "Zhongxuan",
                ""
            ],
            [
                "Ren",
                "Zhen",
                ""
            ],
            [
                "Wang",
                "Haila",
                ""
            ]
        ]
    },
    {
        "id": "0706.3132",
        "submitter": "Paulo Condado",
        "authors": "Paulo A. Condado and Fernando G. Lobo",
        "title": "EasyVoice: Integrating voice synthesis with Skype",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CY cs.HC",
        "license": null,
        "abstract": "  This paper presents EasyVoice, a system that integrates voice synthesis with\nSkype. EasyVoice allows a person with voice disabilities to talk with another\nperson located anywhere in the world, removing an important obstacle that\naffect these people during a phone or VoIP-based conversation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 21 Jun 2007 12:04:40 GMT"
            }
        ],
        "update_date": "2007-06-22",
        "authors_parsed": [
            [
                "Condado",
                "Paulo A.",
                ""
            ],
            [
                "Lobo",
                "Fernando G.",
                ""
            ]
        ]
    },
    {
        "id": "0706.3146",
        "submitter": "Hao Hu",
        "authors": "Hao Hu, Steven Myers, Vittoria Colizza, Alessandro Vespignani",
        "title": "WiFi Epidemiology: Can Your Neighbors' Router Make Yours Sick?",
        "comments": "22 pages, 1 table, 4 figures",
        "journal-ref": "Proceedings of the National Academy of Sciences, vol. 106, no. 5,\n  1318-1323 (2009)",
        "doi": "10.1073/pnas.0811973106",
        "report-no": null,
        "categories": "cs.CR physics.soc-ph",
        "license": null,
        "abstract": "  In densely populated urban areas WiFi routers form a tightly interconnected\nproximity network that can be exploited as a substrate for the spreading of\nmalware able to launch massive fraudulent attack and affect entire urban areas\nWiFi networks. In this paper we consider several scenarios for the deployment\nof malware that spreads solely over the wireless channel of major urban areas\nin the US. We develop an epidemiological model that takes into consideration\nprevalent security flaws on these routers. The spread of such a contagion is\nsimulated on real-world data for geo-referenced wireless routers. We uncover a\nmajor weakness of WiFi networks in that most of the simulated scenarios show\ntens of thousands of routers infected in as little time as two weeks, with the\nmajority of the infections occurring in the first 24 to 48 hours. We indicate\npossible containment and prevention measure to limit the eventual harm of such\nan attack.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 21 Jun 2007 19:36:21 GMT"
            }
        ],
        "update_date": "2009-02-11",
        "authors_parsed": [
            [
                "Hu",
                "Hao",
                ""
            ],
            [
                "Myers",
                "Steven",
                ""
            ],
            [
                "Colizza",
                "Vittoria",
                ""
            ],
            [
                "Vespignani",
                "Alessandro",
                ""
            ]
        ]
    },
    {
        "id": "0706.3159",
        "submitter": "Pierre Deransart",
        "authors": "Pierre Deransart (INRIA Rocquencourt), Mireille Ducass\\'e (IRISA),\n  G\\'erard Ferrand (LIFO)",
        "title": "Une s\\'emantique observationnelle du mod\\`ele des bo\\^ites pour la\n  r\\'esolution de programmes logiques (version \\'etendue)",
        "comments": "Project of research report used to produce the paper presented at\n  JFPC 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  This report specifies an observational semantics and gives an original\npresentation of the Byrd's box model. The approach accounts for the semantics\nof Prolog tracers independently of a particular implementation. Traces are, in\ngeneral, considered as rather obscure and difficult to use. The proposed formal\npresentation of a trace constitutes a simple and pedagogical approach for\nteaching Prolog or for implementing Prolog tracers. It constitutes a form of\ndeclarative specification for the tracers. Our approach highlights qualities of\nthe box model which made its success, but also its drawbacks and limits. As a\nmatter of fact, the presented semantics is only one example to illustrate\ngeneral problems relating to tracers and observing processes. Observing\nprocesses know, from observed processes, only their traces. The issue is then\nto be able to reconstitute by the sole analysis of the trace the main part of\nthe observed process, and if possible, without any loss of information.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 21 Jun 2007 14:20:30 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 25 Jun 2007 14:03:05 GMT"
            }
        ],
        "update_date": "2007-06-25",
        "authors_parsed": [
            [
                "Deransart",
                "Pierre",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Ducass\u00e9",
                "Mireille",
                "",
                "IRISA"
            ],
            [
                "Ferrand",
                "G\u00e9rard",
                "",
                "LIFO"
            ]
        ]
    },
    {
        "id": "0706.3159",
        "submitter": "Pierre Deransart",
        "authors": "Pierre Deransart (INRIA Rocquencourt), Mireille Ducass\\'e (IRISA),\n  G\\'erard Ferrand (LIFO)",
        "title": "Une s\\'emantique observationnelle du mod\\`ele des bo\\^ites pour la\n  r\\'esolution de programmes logiques (version \\'etendue)",
        "comments": "Project of research report used to produce the paper presented at\n  JFPC 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  This report specifies an observational semantics and gives an original\npresentation of the Byrd's box model. The approach accounts for the semantics\nof Prolog tracers independently of a particular implementation. Traces are, in\ngeneral, considered as rather obscure and difficult to use. The proposed formal\npresentation of a trace constitutes a simple and pedagogical approach for\nteaching Prolog or for implementing Prolog tracers. It constitutes a form of\ndeclarative specification for the tracers. Our approach highlights qualities of\nthe box model which made its success, but also its drawbacks and limits. As a\nmatter of fact, the presented semantics is only one example to illustrate\ngeneral problems relating to tracers and observing processes. Observing\nprocesses know, from observed processes, only their traces. The issue is then\nto be able to reconstitute by the sole analysis of the trace the main part of\nthe observed process, and if possible, without any loss of information.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 21 Jun 2007 14:20:30 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 25 Jun 2007 14:03:05 GMT"
            }
        ],
        "update_date": "2007-06-25",
        "authors_parsed": [
            [
                "Deransart",
                "Pierre",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Ducass\u00e9",
                "Mireille",
                "",
                "IRISA"
            ],
            [
                "Ferrand",
                "G\u00e9rard",
                "",
                "LIFO"
            ]
        ]
    },
    {
        "id": "0706.3165",
        "submitter": "Hichem Geryville",
        "authors": "Hichem Geryville (LIESP), Abdelaziz Bouras (LIESP), Yacine Ouzrout\n  (LIESP), Nikolaos Sapidis",
        "title": "A solution for actors' viewpoints representation with collaborative\n  product development",
        "comments": "ISBN: 2-287-48363-9",
        "journal-ref": "Research in Interactive Design (2007) 39-40",
        "doi": "10.1007/978-2-287-48370-7",
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  As product complexity and marketing competition increase, a collaborative\nproduct development is necessary for companies which develop high quality\nproducts in short lead-times. To support product actors from different fields,\ndisciplines, and locations, wishing to exchange and share information, the\nrepresentation of the actors' viewpoints is the underlying requirement of the\ncollaborative product development. The actors' viewpoints approach was designed\nto provide an organisational framework following the actors' perspectives in\nthe collaboration, and their relationships, could be explicitly gathered and\nformatted. The approach acknowledges the inevitability of multiple integration\nof product information as different views, promotes gathering of actors'\ninterests, and encourages retrieved adequate information while providing\nsupport for integration through PLM and/or SCM collaboration. In this paper, a\nsolution for neutral viewpoints representation is proposed. The product,\nprocess, and organisation information models are seriatim discussed. A series\nof issues referring to the viewpoints representation are discussed in detail.\nBased on XML standard, taking cyclone vessel as an example, an application case\nof part of product information modelling is stated.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 21 Jun 2007 14:40:25 GMT"
            }
        ],
        "update_date": "2007-06-22",
        "authors_parsed": [
            [
                "Geryville",
                "Hichem",
                "",
                "LIESP"
            ],
            [
                "Bouras",
                "Abdelaziz",
                "",
                "LIESP"
            ],
            [
                "Ouzrout",
                "Yacine",
                "",
                "LIESP"
            ],
            [
                "Sapidis",
                "Nikolaos",
                ""
            ]
        ]
    },
    {
        "id": "0706.3188",
        "submitter": "Vladimir Vovk",
        "authors": "Glenn Shafer and Vladimir Vovk",
        "title": "A tutorial on conformal prediction",
        "comments": "58 pages, 9 figures",
        "journal-ref": "Journal of Machine Learning Research 9 (2008) 371-421.\n  http://www.jmlr.org/papers/v9/shafer08a.html",
        "doi": null,
        "report-no": null,
        "categories": "cs.LG stat.ML",
        "license": null,
        "abstract": "  Conformal prediction uses past experience to determine precise levels of\nconfidence in new predictions. Given an error probability $\\epsilon$, together\nwith a method that makes a prediction $\\hat{y}$ of a label $y$, it produces a\nset of labels, typically containing $\\hat{y}$, that also contains $y$ with\nprobability $1-\\epsilon$. Conformal prediction can be applied to any method for\nproducing $\\hat{y}$: a nearest-neighbor method, a support-vector machine, ridge\nregression, etc.\n  Conformal prediction is designed for an on-line setting in which labels are\npredicted successively, each one being revealed before the next is predicted.\nThe most novel and valuable feature of conformal prediction is that if the\nsuccessive examples are sampled independently from the same distribution, then\nthe successive predictions will be right $1-\\epsilon$ of the time, even though\nthey are based on an accumulating dataset rather than on independent datasets.\n  In addition to the model under which successive examples are sampled\nindependently, other on-line compression models can also use conformal\nprediction. The widely used Gaussian linear model is one of these.\n  This tutorial presents a self-contained account of the theory of conformal\nprediction and works through several numerical examples. A more comprehensive\ntreatment of the topic is provided in \"Algorithmic Learning in a Random World\",\nby Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 21 Jun 2007 16:40:06 GMT"
            }
        ],
        "update_date": "2019-04-23",
        "authors_parsed": [
            [
                "Shafer",
                "Glenn",
                ""
            ],
            [
                "Vovk",
                "Vladimir",
                ""
            ]
        ]
    },
    {
        "id": "0706.3305",
        "submitter": "Ayan Mahalanobis",
        "authors": "Ayan Mahalanobis",
        "title": "A simple generalization of the ElGamal cryptosystem to non-abelian\n  groups II",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR math.GR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This is a study of the MOR cryptosystem using the special linear group over\nfinite fields. The automorphism group of the special linear group is analyzed\nfor this purpose. At our current state of knowledge, I show that the MOR\ncryptosystem has better security than the ElGamal cryptosystem over finite\nfields.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 22 Jun 2007 10:28:28 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 27 Jun 2007 16:41:57 GMT"
            },
            {
                "version": "v3",
                "created": "Sat, 29 Dec 2007 21:36:10 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 6 Mar 2009 19:09:55 GMT"
            },
            {
                "version": "v5",
                "created": "Tue, 25 Jan 2011 04:57:52 GMT"
            }
        ],
        "update_date": "2011-01-26",
        "authors_parsed": [
            [
                "Mahalanobis",
                "Ayan",
                ""
            ]
        ]
    },
    {
        "id": "0706.3350",
        "submitter": "Veronika Rehn-Sonigo",
        "authors": "Veronika Rehn-Sonigo (INRIA Rh\\^one-Alpes, LIP)",
        "title": "Optimal Replica Placement in Tree Networks with QoS and Bandwidth\n  Constraints and the Closest Allocation Policy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  This paper deals with the replica placement problem on fully homogeneous tree\nnetworks known as the Replica Placement optimization problem. The client\nrequests are known beforehand, while the number and location of the servers are\nto be determined. We investigate the latter problem using the Closest access\npolicy when adding QoS and bandwidth constraints. We propose an optimal\nalgorithm in two passes using dynamic programming.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 22 Jun 2007 15:01:35 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 27 Jun 2007 14:06:26 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 28 Jun 2007 15:24:39 GMT"
            }
        ],
        "update_date": "2007-06-28",
        "authors_parsed": [
            [
                "Rehn-Sonigo",
                "Veronika",
                "",
                "INRIA Rh\u00f4ne-Alpes, LIP"
            ]
        ]
    },
    {
        "id": "0706.3502",
        "submitter": "P Vijay Kumar",
        "authors": "Petros Elia and P. Vijay Kumar",
        "title": "Approximately-Universal Space-Time Codes for the Parallel, Multi-Block\n  and Cooperative-Dynamic-Decode-and-Forward Channels",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.DM cs.NI math.IT",
        "license": null,
        "abstract": "  Explicit codes are constructed that achieve the diversity-multiplexing gain\ntradeoff of the cooperative-relay channel under the dynamic decode-and-forward\nprotocol for any network size and for all numbers of transmit and receive\nantennas at the relays.\n  A particularly simple code construction that makes use of the Alamouti code\nas a basic building block is provided for the single relay case.\n  Along the way, we prove that space-time codes previously constructed in the\nliterature for the block-fading and parallel channels are approximately\nuniversal, i.e., they achieve the DMT for any fading distribution. It is shown\nhow approximate universality of these codes leads to the first DMT-optimum code\nconstruction for the general, MIMO-OFDM channel.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Jun 2007 07:47:57 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 13 Jul 2007 12:43:51 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Elia",
                "Petros",
                ""
            ],
            [
                "Kumar",
                "P. Vijay",
                ""
            ]
        ]
    },
    {
        "id": "0706.3546",
        "submitter": "Samer Al Kiswany",
        "authors": "Samer Al Kiswany, Matei Ripeanu, Sudharshan S. Vazhkudai, Abdullah\n  Gharaibeh",
        "title": "stdchk: A Checkpoint Storage System for Desktop Grid Computing",
        "comments": "13 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Checkpointing is an indispensable technique to provide fault tolerance for\nlong-running high-throughput applications like those running on desktop grids.\nThis paper argues that a dedicated checkpoint storage system, optimized to\noperate in these environments, can offer multiple benefits: reduce the load on\na traditional file system, offer high-performance through specialization, and,\nfinally, optimize data management by taking into account checkpoint application\nsemantics. Such a storage system can present a unifying abstraction to\ncheckpoint operations, while hiding the fact that there are no dedicated\nresources to store the checkpoint data. We prototype stdchk, a checkpoint\nstorage system that uses scavenged disk space from participating desktops to\nbuild a low-cost storage system, offering a traditional file system interface\nfor easy integration with applications. This paper presents the stdchk\narchitecture, key performance optimizations, support for incremental\ncheckpointing, and increased data availability. Our evaluation confirms that\nthe stdchk approach is viable in a desktop grid setting and offers a low cost\nstorage system with desirable performance characteristics: high write\nthroughput and reduced storage space and network effort to save checkpoint\nimages.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 25 Jun 2007 01:24:46 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 23 Nov 2007 20:51:06 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Kiswany",
                "Samer Al",
                ""
            ],
            [
                "Ripeanu",
                "Matei",
                ""
            ],
            [
                "Vazhkudai",
                "Sudharshan S.",
                ""
            ],
            [
                "Gharaibeh",
                "Abdullah",
                ""
            ]
        ]
    },
    {
        "id": "0706.3639",
        "submitter": "Marcus Hutter",
        "authors": "Shane Legg and Marcus Hutter",
        "title": "A Collection of Definitions of Intelligence",
        "comments": "12 LaTeX pages",
        "journal-ref": "Frontiers in Artificial Intelligence and Applications, Vol.157\n  (2007) 17-24",
        "doi": null,
        "report-no": "IDSIA-07-07",
        "categories": "cs.AI",
        "license": null,
        "abstract": "  This paper is a survey of a large number of informal definitions of\n``intelligence'' that the authors have collected over the years. Naturally,\ncompiling a complete list would be impossible as many definitions of\nintelligence are buried deep inside articles and books. Nevertheless, the\n70-odd definitions presented here are, to the authors' knowledge, the largest\nand most well referenced collection there is.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 25 Jun 2007 13:40:56 GMT"
            }
        ],
        "update_date": "2007-06-26",
        "authors_parsed": [
            [
                "Legg",
                "Shane",
                ""
            ],
            [
                "Hutter",
                "Marcus",
                ""
            ]
        ]
    },
    {
        "id": "0706.3679",
        "submitter": "Yann Guermeur",
        "authors": "Yann Guermeur (LORIA)",
        "title": "Scale-sensitive Psi-dimensions: the Capacity Measures for Classifiers\n  Taking Values in R^Q",
        "comments": null,
        "journal-ref": "ASMDA 2007 (2007) 1-8",
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  Bounds on the risk play a crucial role in statistical learning theory. They\nusually involve as capacity measure of the model studied the VC dimension or\none of its extensions. In classification, such \"VC dimensions\" exist for models\ntaking values in {0, 1}, {1,..., Q} and R. We introduce the generalizations\nappropriate for the missing case, the one of models with values in R^Q. This\nprovides us with a new guaranteed risk for M-SVMs which appears superior to the\nexisting one.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 25 Jun 2007 17:28:57 GMT"
            }
        ],
        "update_date": "2007-06-26",
        "authors_parsed": [
            [
                "Guermeur",
                "Yann",
                "",
                "LORIA"
            ]
        ]
    },
    {
        "id": "0706.3752",
        "submitter": "Ruoheng Liu",
        "authors": "Ruoheng Liu, Yingbin Liang, H. Vincent Poor, and Predrag Spasojevic",
        "title": "Secure Nested Codes for Type II Wiretap Channels",
        "comments": "To appear in the Proceedings of the 2007 IEEE Information Theory\n  Workshop on Frontiers in Coding Theory, Lake Tahoe, CA, September 2-6, 2007",
        "journal-ref": null,
        "doi": "10.1109/ITW.2007.4313097",
        "report-no": null,
        "categories": "cs.IT cs.CR math.IT",
        "license": null,
        "abstract": "  This paper considers the problem of secure coding design for a type II\nwiretap channel, where the main channel is noiseless and the eavesdropper\nchannel is a general binary-input symmetric-output memoryless channel. The\nproposed secure error-correcting code has a nested code structure. Two secure\nnested coding schemes are studied for a type II Gaussian wiretap channel. The\nnesting is based on cosets of a good code sequence for the first scheme and on\ncosets of the dual of a good code sequence for the second scheme. In each case,\nthe corresponding achievable rate-equivocation pair is derived based on the\nthreshold behavior of good code sequences. The two secure coding schemes\ntogether establish an achievable rate-equivocation region, which almost covers\nthe secrecy capacity-equivocation region in this case study. The proposed\nsecure coding scheme is extended to a type II binary symmetric wiretap channel.\nA new achievable perfect secrecy rate, which improves upon the previously\nreported result by Thangaraj et al., is derived for this channel.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 26 Jun 2007 03:57:34 GMT"
            }
        ],
        "update_date": "2016-11-18",
        "authors_parsed": [
            [
                "Liu",
                "Ruoheng",
                ""
            ],
            [
                "Liang",
                "Yingbin",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ],
            [
                "Spasojevic",
                "Predrag",
                ""
            ]
        ]
    },
    {
        "id": "0706.3768",
        "submitter": "Luca Dall'Asta",
        "authors": "Luca Dall'Asta",
        "title": "Dynamic Exploration of Networks: from general principles to the\n  traceroute process",
        "comments": "13 pages, 6 figures",
        "journal-ref": null,
        "doi": "10.1140/epjb/e2007-00326-9",
        "report-no": null,
        "categories": "physics.soc-ph cond-mat.dis-nn cs.NI physics.data-an",
        "license": null,
        "abstract": "  Dynamical processes taking place on real networks define on them evolving\nsubnetworks whose topology is not necessarily the same of the underlying one.\nWe investigate the problem of determining the emerging degree distribution,\nfocusing on a class of tree-like processes, such as those used to explore the\nInternet's topology. A general theory based on mean-field arguments is\nproposed, both for single-source and multiple-source cases, and applied to the\nspecific example of the traceroute exploration of networks. Our results provide\na qualitative improvement in the understanding of dynamical sampling and of the\ninterplay between dynamics and topology in large networks like the Internet.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 26 Jun 2007 08:00:09 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Dall'Asta",
                "Luca",
                ""
            ]
        ]
    },
    {
        "id": "0706.3812",
        "submitter": "Pierre Parrend",
        "authors": "Pierre Parrend (INRIA Rh\\^one-Alpes), St\\'ephane Fr\\'enot (INRIA\n  Rh\\^one-Alpes)",
        "title": "Java Components Vulnerabilities - An Experimental Classification\n  Targeted at the OSGi Platform",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.OS",
        "license": null,
        "abstract": "  The OSGi Platform finds a growing interest in two different applications\ndomains: embedded systems, and applications servers. However, the security\nproperties of this platform are hardly studied, which is likely to hinder its\nuse in production systems. This is all the more important that the dynamic\naspect of OSGi-based applications, that can be extended at runtime, make them\nvulnerable to malicious code injection. We therefore perform a systematic audit\nof the OSGi platform so as to build a vulnerability catalog that intends to\nreference OSGi Vulnerabilities originating in the Core Specification, and in\nbehaviors related to the use of the Java language. Standard Services are not\nconsidered. To support this audit, a Semi-formal Vulnerability Pattern is\ndefined, that enables to uniquely characterize fundamental properties for each\nvulnerability, to include verbose description in the pattern, to reference\nknown security protections, and to track the implementation status of the\nproof-of-concept OSGi Bundles that exploit the vulnerability. Based on the\nanalysis of the catalog, a robust OSGi Platform is built, and recommendations\nare made to enhance the OSGi Specifications.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 26 Jun 2007 12:36:37 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 27 Jun 2007 08:03:05 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 27 Jul 2007 13:50:46 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Parrend",
                "Pierre",
                "",
                "INRIA Rh\u00f4ne-Alpes"
            ],
            [
                "Fr\u00e9not",
                "St\u00e9phane",
                "",
                "INRIA\n  Rh\u00f4ne-Alpes"
            ]
        ]
    },
    {
        "id": "0706.3812",
        "submitter": "Pierre Parrend",
        "authors": "Pierre Parrend (INRIA Rh\\^one-Alpes), St\\'ephane Fr\\'enot (INRIA\n  Rh\\^one-Alpes)",
        "title": "Java Components Vulnerabilities - An Experimental Classification\n  Targeted at the OSGi Platform",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.OS",
        "license": null,
        "abstract": "  The OSGi Platform finds a growing interest in two different applications\ndomains: embedded systems, and applications servers. However, the security\nproperties of this platform are hardly studied, which is likely to hinder its\nuse in production systems. This is all the more important that the dynamic\naspect of OSGi-based applications, that can be extended at runtime, make them\nvulnerable to malicious code injection. We therefore perform a systematic audit\nof the OSGi platform so as to build a vulnerability catalog that intends to\nreference OSGi Vulnerabilities originating in the Core Specification, and in\nbehaviors related to the use of the Java language. Standard Services are not\nconsidered. To support this audit, a Semi-formal Vulnerability Pattern is\ndefined, that enables to uniquely characterize fundamental properties for each\nvulnerability, to include verbose description in the pattern, to reference\nknown security protections, and to track the implementation status of the\nproof-of-concept OSGi Bundles that exploit the vulnerability. Based on the\nanalysis of the catalog, a robust OSGi Platform is built, and recommendations\nare made to enhance the OSGi Specifications.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 26 Jun 2007 12:36:37 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 27 Jun 2007 08:03:05 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 27 Jul 2007 13:50:46 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Parrend",
                "Pierre",
                "",
                "INRIA Rh\u00f4ne-Alpes"
            ],
            [
                "Fr\u00e9not",
                "St\u00e9phane",
                "",
                "INRIA\n  Rh\u00f4ne-Alpes"
            ]
        ]
    },
    {
        "id": "0706.3984",
        "submitter": "Ali Mesbah",
        "authors": "Engin Bozdag, Ali Mesbah, Arie van Deursen",
        "title": "A Comparison of Push and Pull Techniques for Ajax",
        "comments": "Conference: WSE 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.PF",
        "license": null,
        "abstract": "  Ajax applications are designed to have high user interactivity and low\nuser-perceived latency. Real-time dynamic web data such as news headlines,\nstock tickers, and auction updates need to be propagated to the users as soon\nas possible. However, Ajax still suffers from the limitations of the Web's\nrequest/response architecture which prevents servers from pushing real-time\ndynamic web data. Such applications usually use a pull style to obtain the\nlatest updates, where the client actively requests the changes based on a\npredefined interval. It is possible to overcome this limitation by adopting a\npush style of interaction where the server broadcasts data when a change occurs\non the server side. Both these options have their own trade-offs. This paper\nexplores the fundamental limits of browser-based applications and analyzes push\nsolutions for Ajax technology. It also shows the results of an empirical study\ncomparing push and pull.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Jun 2007 09:14:40 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 16 Aug 2007 11:53:37 GMT"
            }
        ],
        "update_date": "2007-08-16",
        "authors_parsed": [
            [
                "Bozdag",
                "Engin",
                ""
            ],
            [
                "Mesbah",
                "Ali",
                ""
            ],
            [
                "van Deursen",
                "Arie",
                ""
            ]
        ]
    },
    {
        "id": "0706.3984",
        "submitter": "Ali Mesbah",
        "authors": "Engin Bozdag, Ali Mesbah, Arie van Deursen",
        "title": "A Comparison of Push and Pull Techniques for Ajax",
        "comments": "Conference: WSE 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.PF",
        "license": null,
        "abstract": "  Ajax applications are designed to have high user interactivity and low\nuser-perceived latency. Real-time dynamic web data such as news headlines,\nstock tickers, and auction updates need to be propagated to the users as soon\nas possible. However, Ajax still suffers from the limitations of the Web's\nrequest/response architecture which prevents servers from pushing real-time\ndynamic web data. Such applications usually use a pull style to obtain the\nlatest updates, where the client actively requests the changes based on a\npredefined interval. It is possible to overcome this limitation by adopting a\npush style of interaction where the server broadcasts data when a change occurs\non the server side. Both these options have their own trade-offs. This paper\nexplores the fundamental limits of browser-based applications and analyzes push\nsolutions for Ajax technology. It also shows the results of an empirical study\ncomparing push and pull.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Jun 2007 09:14:40 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 16 Aug 2007 11:53:37 GMT"
            }
        ],
        "update_date": "2007-08-16",
        "authors_parsed": [
            [
                "Bozdag",
                "Engin",
                ""
            ],
            [
                "Mesbah",
                "Ali",
                ""
            ],
            [
                "van Deursen",
                "Arie",
                ""
            ]
        ]
    },
    {
        "id": "0706.4004",
        "submitter": "Ahmed Ait Ali",
        "authors": "Ahmed Ait Ali (CRAN), Fabien Michaut (CRAN), Francis Lepage (CRAN)",
        "title": "End-to-End Available Bandwidth Measurement Tools : A Comparative\n  Evaluation of Performances",
        "comments": null,
        "journal-ref": "IPS-MoMe 2006 IEEE /ACM International workshop on Internet\n  Performance, Simulation, Monitoring and Measurement, Autriche (27/02/2005) 13",
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  In recent years, there has been a strong interest in measuring the available\nbandwidth of network paths. Several methods and techniques have been proposed\nand various measurement tools have been developed and evaluated. However, there\nhave been few comparative studies with regards to the actual performance of\nthese tools. This paper presents a study of available bandwidth measurement\ntechniques and undertakes a comparative analysis in terms of accuracy,\nintrusiveness and response time of active probing tools. Finally, measurement\nerrors and the uncertainty of the tools are analysed and overall conclusions\nmade.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Jun 2007 11:51:36 GMT"
            }
        ],
        "update_date": "2007-06-28",
        "authors_parsed": [
            [
                "Ali",
                "Ahmed Ait",
                "",
                "CRAN"
            ],
            [
                "Michaut",
                "Fabien",
                "",
                "CRAN"
            ],
            [
                "Lepage",
                "Francis",
                "",
                "CRAN"
            ]
        ]
    },
    {
        "id": "0706.4009",
        "submitter": "Veronika Rehn-Sonigo",
        "authors": "Anne Benoit (INRIA Rh\\^one-Alpes, LIP), Veronika Rehn-Sonigo (INRIA\n  Rh\\^one-Alpes, LIP), Yves Robert (INRIA Rh\\^one-Alpes, LIP)",
        "title": "Multi-criteria scheduling of pipeline workflows",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Mapping workflow applications onto parallel platforms is a challenging\nproblem, even for simple application patterns such as pipeline graphs. Several\nantagonist criteria should be optimized, such as throughput and latency (or a\ncombination). In this paper, we study the complexity of the bi-criteria mapping\nproblem for pipeline graphs on communication homogeneous platforms. In\nparticular, we assess the complexity of the well-known chains-to-chains problem\nfor different-speed processors, which turns out to be NP-hard. We provide\nseveral efficient polynomial bi-criteria heuristics, and their relative\nperformance is evaluated through extensive simulations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Jun 2007 13:43:16 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 28 Jun 2007 15:20:26 GMT"
            }
        ],
        "update_date": "2007-06-28",
        "authors_parsed": [
            [
                "Benoit",
                "Anne",
                "",
                "INRIA Rh\u00f4ne-Alpes, LIP"
            ],
            [
                "Rehn-Sonigo",
                "Veronika",
                "",
                "INRIA\n  Rh\u00f4ne-Alpes, LIP"
            ],
            [
                "Robert",
                "Yves",
                "",
                "INRIA Rh\u00f4ne-Alpes, LIP"
            ]
        ]
    },
    {
        "id": "0706.4015",
        "submitter": "Franck Petit",
        "authors": "Christian Boulinier (LaRIA), Franck Petit (LaRIA)",
        "title": "Self-Stabilizing Wavelets and r-Hops Coordination",
        "comments": null,
        "journal-ref": "Rapport Interne (01/04/2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  We introduce a simple tool called the wavelet (or, r-wavelet) scheme.\nWavelets deals with coordination among processes which are at most r hops away\nof each other. We present a selfstabilizing solution for this scheme. Our\nsolution requires no underlying structure and works in arbritrary anonymous\nnetworks, i.e., no process identifier is required. Moreover, our solution works\nunder any (even unfair) daemon. Next, we use the wavelet scheme to design\nself-stabilizing layer clocks. We show that they provide an efficient device in\nthe design of local coordination problems at distance r, i.e., r-barrier\nsynchronization and r-local resource allocation (LRA) such as r-local mutual\nexclusion (LME), r-group mutual exclusion (GME), and r-Reader/Writers. Some\nsolutions to the r-LRA problem (e.g., r-LME) also provide transformers to\ntransform algorithms written assuming any r-central daemon into algorithms\nworking with any distributed daemon.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Jun 2007 12:53:06 GMT"
            }
        ],
        "update_date": "2007-06-28",
        "authors_parsed": [
            [
                "Boulinier",
                "Christian",
                "",
                "LaRIA"
            ],
            [
                "Petit",
                "Franck",
                "",
                "LaRIA"
            ]
        ]
    },
    {
        "id": "0706.4035",
        "submitter": "Sapon Tanachaiwiwat",
        "authors": "Sapon Tanachaiwiwat, Ahmed Helmy",
        "title": "Encounter-based worms: Analysis and Defense",
        "comments": "Submitted to a journal",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.CR",
        "license": null,
        "abstract": "  Encounter-based network is a frequently-disconnected wireless ad-hoc network\nrequiring immediate neighbors to store and forward aggregated data for\ninformation disseminations. Using traditional approaches such as gateways or\nfirewalls for deterring worm propagation in encounter-based networks is\ninappropriate. We propose the worm interaction approach that relies upon\nautomated beneficial worm generation aiming to alleviate problems of worm\npropagations in such networks. To understand the dynamic of worm interactions\nand its performance, we mathematically model worm interactions based on major\nworm interaction factors including worm interaction types, network\ncharacteristics, and node characteristics using ordinary differential equations\nand analyze their effects on our proposed metrics. We validate our proposed\nmodel using extensive synthetic and trace-driven simulations. We find that, all\nworm interaction factors significantly affect the pattern of worm propagations.\nFor example, immunization linearly decreases the infection of susceptible nodes\nwhile on-off behavior only impacts the duration of infection. Using realistic\nmobile network measurements, we find that encounters are bursty, multi-group\nand non-uniform. The trends from the trace-driven simulations are consistent\nwith the model, in general. Immunization and timely deployment seem to be the\nmost effective to counter the worm attacks in such scenarios while cooperation\nmay help in a specific case. These findings provide insight that we hope would\naid to develop counter-worm protocols in future encounter-based networks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Jun 2007 18:01:50 GMT"
            }
        ],
        "update_date": "2007-06-28",
        "authors_parsed": [
            [
                "Tanachaiwiwat",
                "Sapon",
                ""
            ],
            [
                "Helmy",
                "Ahmed",
                ""
            ]
        ]
    },
    {
        "id": "0706.4035",
        "submitter": "Sapon Tanachaiwiwat",
        "authors": "Sapon Tanachaiwiwat, Ahmed Helmy",
        "title": "Encounter-based worms: Analysis and Defense",
        "comments": "Submitted to a journal",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.CR",
        "license": null,
        "abstract": "  Encounter-based network is a frequently-disconnected wireless ad-hoc network\nrequiring immediate neighbors to store and forward aggregated data for\ninformation disseminations. Using traditional approaches such as gateways or\nfirewalls for deterring worm propagation in encounter-based networks is\ninappropriate. We propose the worm interaction approach that relies upon\nautomated beneficial worm generation aiming to alleviate problems of worm\npropagations in such networks. To understand the dynamic of worm interactions\nand its performance, we mathematically model worm interactions based on major\nworm interaction factors including worm interaction types, network\ncharacteristics, and node characteristics using ordinary differential equations\nand analyze their effects on our proposed metrics. We validate our proposed\nmodel using extensive synthetic and trace-driven simulations. We find that, all\nworm interaction factors significantly affect the pattern of worm propagations.\nFor example, immunization linearly decreases the infection of susceptible nodes\nwhile on-off behavior only impacts the duration of infection. Using realistic\nmobile network measurements, we find that encounters are bursty, multi-group\nand non-uniform. The trends from the trace-driven simulations are consistent\nwith the model, in general. Immunization and timely deployment seem to be the\nmost effective to counter the worm attacks in such scenarios while cooperation\nmay help in a specific case. These findings provide insight that we hope would\naid to develop counter-worm protocols in future encounter-based networks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Jun 2007 18:01:50 GMT"
            }
        ],
        "update_date": "2007-06-28",
        "authors_parsed": [
            [
                "Tanachaiwiwat",
                "Sapon",
                ""
            ],
            [
                "Helmy",
                "Ahmed",
                ""
            ]
        ]
    },
    {
        "id": "0706.4038",
        "submitter": "Frederic Vivien",
        "authors": "Matthieu Gallet (LIP, INRIA Rh\\^one-Alpes), Yves Robert (LIP, INRIA\n  Rh\\^one-Alpes), Fr\\'ed\\'eric Vivien (LIP, INRIA Rh\\^one-Alpes)",
        "title": "Scheduling multiple divisible loads on a linear processor network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Min, Veeravalli, and Barlas have recently proposed strategies to minimize the\noverall execution time of one or several divisible loads on a heterogeneous\nlinear network, using one or more installments. We show on a very simple\nexample that their approach does not always produce a solution and that, when\nit does, the solution is often suboptimal. We also show how to find an optimal\nschedule for any instance, once the number of installments per load is given.\nThen, we formally state that any optimal schedule has an infinite number of\ninstallments under a linear cost model as the one assumed in the original\npapers. Therefore, such a cost model cannot be used to design practical\nmulti-installment strategies. Finally, through extensive simulations we\nconfirmed that the best solution is always produced by the linear programming\napproach, while solutions of the original papers can be far away from the\noptimal.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Jun 2007 14:43:13 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 28 Jun 2007 15:02:05 GMT"
            }
        ],
        "update_date": "2007-06-28",
        "authors_parsed": [
            [
                "Gallet",
                "Matthieu",
                "",
                "LIP, INRIA Rh\u00f4ne-Alpes"
            ],
            [
                "Robert",
                "Yves",
                "",
                "LIP, INRIA\n  Rh\u00f4ne-Alpes"
            ],
            [
                "Vivien",
                "Fr\u00e9d\u00e9ric",
                "",
                "LIP, INRIA Rh\u00f4ne-Alpes"
            ]
        ]
    },
    {
        "id": "0706.4048",
        "submitter": "Michael Noble S.",
        "authors": "Michael S. Noble",
        "title": "Getting More From Your Multicore: Exploiting OpenMP From An Open Source\n  Numerical Scripting Language",
        "comments": "15 pages, 16 figures; working paper, in preparation",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC astro-ph",
        "license": null,
        "abstract": "  We introduce SLIRP, a module generator for the S-Lang numerical scripting\nlanguage, with a focus on its vectorization capabilities. We demonstrate how\nboth SLIRP and S-Lang were easily adapted to exploit the inherent parallelism\nof high-level mathematical languages with OpenMP, allowing general users to\nemploy tightly-coupled multiprocessors in scriptable research calculations\nwhile requiring no special knowledge of parallel programming. Motivated by\nexamples in the ISIS astrophysical modeling & analysis tool, performance\nfigures are presented for several machine and compiler configurations,\ndemonstrating beneficial speedups for real-world operations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Jun 2007 15:21:28 GMT"
            }
        ],
        "update_date": "2007-06-28",
        "authors_parsed": [
            [
                "Noble",
                "Michael S.",
                ""
            ]
        ]
    },
    {
        "id": "0706.4175",
        "submitter": "Cedric Adjih",
        "authors": "Song Yean Cho (INRIA Rocquencourt), C\\'edric Adjih (INRIA\n  Rocquencourt), Philippe Jacquet (INRIA Rocquencourt)",
        "title": "Heuristics for Network Coding in Wireless Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  Multicast is a central challenge for emerging multi-hop wireless\narchitectures such as wireless mesh networks, because of its substantial cost\nin terms of bandwidth. In this report, we study one specific case of multicast:\nbroadcasting, sending data from one source to all nodes, in a multi-hop\nwireless network. The broadcast we focus on is based on network coding, a\npromising avenue for reducing cost; previous work of ours showed that the\nperformance of network coding with simple heuristics is asymptotically optimal:\neach transmission is beneficial to nearly every receiver. This is for\nhomogenous and large networks of the plan. But for small, sparse or for\ninhomogeneous networks, some additional heuristics are required. This report\nproposes such additional new heuristics (for selecting rates) for broadcasting\nwith network coding. Our heuristics are intended to use only simple local\ntopology information. We detail the logic of the heuristics, and with\nexperimental results, we illustrate the behavior of the heuristics, and\ndemonstrate their excellent performance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 28 Jun 2007 09:56:50 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 4 Jul 2007 12:24:12 GMT"
            }
        ],
        "update_date": "2009-04-16",
        "authors_parsed": [
            [
                "Cho",
                "Song Yean",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Adjih",
                "C\u00e9dric",
                "",
                "INRIA\n  Rocquencourt"
            ],
            [
                "Jacquet",
                "Philippe",
                "",
                "INRIA Rocquencourt"
            ]
        ]
    },
    {
        "id": "0706.4224",
        "submitter": "Sergey Andreyev",
        "authors": "Sergey Andreyev",
        "title": "User driven applications - new design paradigm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.GR cs.HC",
        "license": null,
        "abstract": "  Programs for complicated engineering and scientific tasks always have to deal\nwith a problem of showing numerous graphical results. The limits of the screen\nspace and often opposite requirements from different users are the cause of the\ninfinite discussions between designers and users, but the source of this\nongoing conflict is not in the level of interface design, but in the basic\nprinciple of current graphical output: user may change some views and details,\nbut in general the output view is absolutely defined and fixed by the\ndeveloper. Author was working for several years on the algorithm that will\nallow eliminating this problem thus allowing stepping from designer-driven\napplications to user-driven. Such type of applications in which user is\ndeciding what, when and how to show on the screen, is the dream of scientists\nand engineers working on the analysis of the most complicated tasks. The new\nparadigm is based on movable and resizable graphics, and such type of graphics\ncan be widely used not only for scientific and engineering applications.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 28 Jun 2007 13:19:04 GMT"
            }
        ],
        "update_date": "2007-06-29",
        "authors_parsed": [
            [
                "Andreyev",
                "Sergey",
                ""
            ]
        ]
    },
    {
        "id": "0706.4298",
        "submitter": "Christian Boulinier",
        "authors": "Christian Boulinier (LaRIA)",
        "title": "Unison as a Self-Stabilizing Wave Stream Algorithm in Asynchronous\n  Anonymous Networks",
        "comments": null,
        "journal-ref": "Rapport de recherche. (28/06/2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  How to pass from local to global scales in anonymous networks? How to\norganize a selfstabilizing propagation of information with feedback. From the\nAngluin impossibility results, we cannot elect a leader in a general anonymous\nnetwork. Thus, it is impossible to build a rooted spanning tree. Many problems\ncan only be solved by probabilistic methods. In this paper we show how to use\nUnison to design a self-stabilizing barrier synchronization in an anonymous\nnetwork. We show that the commuication structure of this barrier\nsynchronization designs a self-stabilizing wave-stream, or pipelining wave, in\nanonymous networks. We introduce two variants of Wave: the strong waves and the\nwavelets. A strong wave can be used to solve the idempotent r-operator\nparametrized computation problem. A wavelet deals with k-distance computation.\nWe show how to use Unison to design a self-stabilizing wave stream, a\nself-stabilizing strong wave stream and a self-stabilizing wavelet stream.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 28 Jun 2007 18:51:36 GMT"
            }
        ],
        "update_date": "2007-06-29",
        "authors_parsed": [
            [
                "Boulinier",
                "Christian",
                "",
                "LaRIA"
            ]
        ]
    },
    {
        "id": "0706.4323",
        "submitter": "Khalil Djelloul",
        "authors": "Khalil Djelloul, Thi-bich-hanh Dao and Thom Fruehwirth",
        "title": "Theory of Finite or Infinite Trees Revisited",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.AI",
        "license": null,
        "abstract": "  We present in this paper a first-order axiomatization of an extended theory\n$T$ of finite or infinite trees, built on a signature containing an infinite\nset of function symbols and a relation $\\fini(t)$ which enables to distinguish\nbetween finite or infinite trees. We show that $T$ has at least one model and\nprove its completeness by giving not only a decision procedure, but a full\nfirst-order constraint solver which gives clear and explicit solutions for any\nfirst-order constraint satisfaction problem in $T$. The solver is given in the\nform of 16 rewriting rules which transform any first-order constraint $\\phi$\ninto an equivalent disjunction $\\phi$ of simple formulas such that $\\phi$ is\neither the formula $\\true$ or the formula $\\false$ or a formula having at least\none free variable, being equivalent neither to $\\true$ nor to $\\false$ and\nwhere the solutions of the free variables are expressed in a clear and explicit\nway. The correctness of our rules implies the completeness of $T$. We also\ndescribe an implementation of our algorithm in CHR (Constraint Handling Rules)\nand compare the performance with an implementation in C++ and that of a recent\ndecision procedure for decomposable theories.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 28 Jun 2007 21:18:19 GMT"
            }
        ],
        "update_date": "2007-07-02",
        "authors_parsed": [
            [
                "Djelloul",
                "Khalil",
                ""
            ],
            [
                "Dao",
                "Thi-bich-hanh",
                ""
            ],
            [
                "Fruehwirth",
                "Thom",
                ""
            ]
        ]
    },
    {
        "id": "0706.4375",
        "submitter": "Thierry Hamon",
        "authors": "Thierry Hamon (LIPN), Adeline Nazarenko (LIPN), Thierry Poibeau\n  (LIPN), Sophie Aubin (LIPN), Julien Derivi\\`ere (LIPN)",
        "title": "A Robust Linguistic Platform for Efficient and Domain specific Web\n  Content Analysis",
        "comments": null,
        "journal-ref": "Proceedings of RIAO 2007 (30/05/2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  Web semantic access in specific domains calls for specialized search engines\nwith enhanced semantic querying and indexing capacities, which pertain both to\ninformation retrieval (IR) and to information extraction (IE). A rich\nlinguistic analysis is required either to identify the relevant semantic units\nto index and weight them according to linguistic specific statistical\ndistribution, or as the basis of an information extraction process. Recent\ndevelopments make Natural Language Processing (NLP) techniques reliable enough\nto process large collections of documents and to enrich them with semantic\nannotations. This paper focuses on the design and the development of a text\nprocessing platform, Ogmios, which has been developed in the ALVIS project. The\nOgmios platform exploits existing NLP modules and resources, which may be tuned\nto specific domains and produces linguistically annotated documents. We show\nhow the three constraints of genericity, domain semantic awareness and\nperformance can be handled all together.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 29 Jun 2007 08:58:02 GMT"
            }
        ],
        "update_date": "2007-07-02",
        "authors_parsed": [
            [
                "Hamon",
                "Thierry",
                "",
                "LIPN"
            ],
            [
                "Nazarenko",
                "Adeline",
                "",
                "LIPN"
            ],
            [
                "Poibeau",
                "Thierry",
                "",
                "LIPN"
            ],
            [
                "Aubin",
                "Sophie",
                "",
                "LIPN"
            ],
            [
                "Derivi\u00e8re",
                "Julien",
                "",
                "LIPN"
            ]
        ]
    },
    {
        "id": "0707.0181",
        "submitter": "Yuriy Bunyak",
        "authors": "Yu. Bunyak and O. Bunyak",
        "title": "Location and Spectral Estimation of Weak Wave Packets on Noise\n  Background",
        "comments": "7 pages, 8 figures. Extended version of presentation in the\n  conferences IMTC-2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": null,
        "abstract": "  The method of location and spectral estimation of weak signals on a noise\nbackground is being considered. The method is based on the optimized on order\nand noise dispersion autoregressive model of a sought signal. A new approach of\nmodel order determination is being offered. Available estimation of the noise\ndispersion is close to the real one. The optimized model allows to define\nfunction of empirical data spectral and dynamic features changes. The analysis\nof the signal as dynamic invariant in respect of the linear shift\ntransformation yields the function of model consistency. Use of these both\nfunctions enables to detect short-time and nonstationary wave packets at signal\nto noise ratio as from -20 dB and above.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 2 Jul 2007 09:47:33 GMT"
            }
        ],
        "update_date": "2007-07-03",
        "authors_parsed": [
            [
                "Bunyak",
                "Yu.",
                ""
            ],
            [
                "Bunyak",
                "O.",
                ""
            ]
        ]
    },
    {
        "id": "0707.0336",
        "submitter": "Erhan Bayraktar",
        "authors": "Erhan Bayraktar",
        "title": "Pricing Options on Defaultable Stocks",
        "comments": "Key Words: Option pricing, multiscale perturbation methods,\n  defaultable stocks, stochastic intensity of default, implied volatility skew",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": null,
        "abstract": "  In this note, we develop stock option price approximations for a model which\ntakes both the risk o default and the stochastic volatility into account. We\nalso let the intensity of defaults be influenced by the volatility. We show\nthat it might be possible to infer the risk neutral default intensity from the\nstock option prices. Our option price approximation has a rich implied\nvolatility surface structure and fits the data implied volatility well. Our\ncalibration exercise shows that an effective hazard rate from bonds issued by a\ncompany can be used to explain the implied volatility skew of the implied\nvolatility of the option prices issued by the same company.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Jul 2007 03:28:35 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 21 Dec 2007 01:08:18 GMT"
            }
        ],
        "update_date": "2007-12-21",
        "authors_parsed": [
            [
                "Bayraktar",
                "Erhan",
                ""
            ]
        ]
    },
    {
        "id": "0707.0365",
        "submitter": "Jean-Christophe Dubacq",
        "authors": "Heithem Abbes (UTIC), Christophe C\\'erin (LIPN), Jean-Christophe\n  Dubacq (LIPN), Mohamed Jemni (UTIC)",
        "title": "Performance Analysis of Publish/Subscribe Systems",
        "comments": null,
        "journal-ref": "Rapport Interne LIPN (15/05/2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  The Desktop Grid offers solutions to overcome several challenges and to\nanswer increasingly needs of scientific computing. Its technology consists\nmainly in exploiting resources, geographically dispersed, to treat complex\napplications needing big power of calculation and/or important storage\ncapacity. However, as resources number increases, the need for scalability,\nself-organisation, dynamic reconfigurations, decentralisation and performance\nbecomes more and more essential. Since such properties are exhibited by P2P\nsystems, the convergence of grid computing and P2P computing seems natural. In\nthis context, this paper evaluates the scalability and performance of P2P tools\nfor discovering and registering services. Three protocols are used for this\npurpose: Bonjour, Avahi and Free-Pastry. We have studied the behaviour of\ntheses protocols related to two criteria: the elapsed time for registrations\nservices and the needed time to discover new services. Our aim is to analyse\nthese results in order to choose the best protocol we can use in order to\ncreate a decentralised middleware for desktop grid.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Jul 2007 09:02:45 GMT"
            }
        ],
        "update_date": "2007-07-04",
        "authors_parsed": [
            [
                "Abbes",
                "Heithem",
                "",
                "UTIC"
            ],
            [
                "C\u00e9rin",
                "Christophe",
                "",
                "LIPN"
            ],
            [
                "Dubacq",
                "Jean-Christophe",
                "",
                "LIPN"
            ],
            [
                "Jemni",
                "Mohamed",
                "",
                "UTIC"
            ]
        ]
    },
    {
        "id": "0707.0397",
        "submitter": "Shijun Xiang jack-xiang",
        "authors": "Shijun Xiang and Jiwu Huang",
        "title": "Robust Audio Watermarking Against the D/A and A/D conversions",
        "comments": "Pages 29",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.MM",
        "license": null,
        "abstract": "  Audio watermarking has played an important role in multimedia security. In\nmany applications using audio watermarking, D/A and A/D conversions (denoted by\nDA/AD in this paper) are often involved. In previous works, however, the\nrobustness issue of audio watermarking against the DA/AD conversions has not\ndrawn sufficient attention yet. In our extensive investigation, it has been\nfound that the degradation of a watermarked audio signal caused by the DA/AD\nconversions manifests itself mainly in terms of wave magnitude distortion and\nlinear temporal scaling, making the watermark extraction failed. Accordingly, a\nDWT-based audio watermarking algorithm robust against the DA/AD conversions is\nproposed in this paper. To resist the magnitude distortion, the relative energy\nrelationships among different groups of the DWT coefficients in the\nlow-frequency sub-band are utilized in watermark embedding by adaptively\ncontrolling the embedding strength. Furthermore, the resynchronization is\ndesigned to cope with the linear temporal scaling. The time-frequency\nlocalization characteristics of DWT are exploited to save the computational\nload in the resynchronization. Consequently, the proposed audio watermarking\nalgorithm is robust against the DA/AD conversions, other common audio\nprocessing manipulations, and the attacks in StirMark Benchmark for Audio,\nwhich has been verified by experiments.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Jul 2007 11:49:03 GMT"
            }
        ],
        "update_date": "2007-07-04",
        "authors_parsed": [
            [
                "Xiang",
                "Shijun",
                ""
            ],
            [
                "Huang",
                "Jiwu",
                ""
            ]
        ]
    },
    {
        "id": "0707.0421",
        "submitter": "Riccardo Dondi",
        "authors": "Paola Bonizzoni, Gianluca Della Vedova, Riccardo Dondi",
        "title": "The $k$-anonymity Problem is Hard",
        "comments": "21 pages, A short version of this paper has been accepted in FCT 2009\n  - 17th International Symposium on Fundamentals of Computation Theory",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.CC cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The problem of publishing personal data without giving up privacy is becoming\nincreasingly important. An interesting formalization recently proposed is the\nk-anonymity. This approach requires that the rows in a table are clustered in\nsets of size at least k and that all the rows in a cluster become the same\ntuple, after the suppression of some records. The natural optimization problem,\nwhere the goal is to minimize the number of suppressed entries, is known to be\nNP-hard when the values are over a ternary alphabet, k = 3 and the rows length\nis unbounded. In this paper we give a lower bound on the approximation factor\nthat any polynomial-time algorithm can achive on two restrictions of the\nproblem,namely (i) when the records values are over a binary alphabet and k =\n3, and (ii) when the records have length at most 8 and k = 4, showing that\nthese restrictions of the problem are APX-hard.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Jul 2007 14:17:49 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 2 Jun 2009 16:40:37 GMT"
            }
        ],
        "update_date": "2009-06-02",
        "authors_parsed": [
            [
                "Bonizzoni",
                "Paola",
                ""
            ],
            [
                "Della Vedova",
                "Gianluca",
                ""
            ],
            [
                "Dondi",
                "Riccardo",
                ""
            ]
        ]
    },
    {
        "id": "0707.0459",
        "submitter": "Petar Popovski",
        "authors": "Petar Popovski and Hiroyuki Yomo",
        "title": "Physical Network Coding in Two-Way Wireless Relay Channels",
        "comments": null,
        "journal-ref": "Proc. of IEEE International Conference on Communications (ICC),\n  Glasgow, Scotland, 2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.NI math.IT",
        "license": null,
        "abstract": "  It has recently been recognized that the wireless networks represent a\nfertile ground for devising communication modes based on network coding. A\nparticularly suitable application of the network coding arises for the two--way\nrelay channels, where two nodes communicate with each other assisted by using a\nthird, relay node. Such a scenario enables application of \\emph{physical\nnetwork coding}, where the network coding is either done (a) jointly with the\nchannel coding or (b) through physical combining of the communication flows\nover the multiple access channel. In this paper we first group the existing\nschemes for physical network coding into two generic schemes, termed 3--step\nand 2--step scheme, respectively. We investigate the conditions for\nmaximization of the two--way rate for each individual scheme: (1) the\nDecode--and--Forward (DF) 3--step schemes (2) three different schemes with two\nsteps: Amplify--and--Forward (AF), JDF and Denoise--and--Forward (DNF). While\nthe DNF scheme has a potential to offer the best two--way rate, the most\ninteresting result of the paper is that, for some SNR configurations of the\nsource--relay links, JDF yields identical maximal two--way rate as the upper\nbound on the rate for DNF.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Jul 2007 16:40:41 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Popovski",
                "Petar",
                ""
            ],
            [
                "Yomo",
                "Hiroyuki",
                ""
            ]
        ]
    },
    {
        "id": "0707.0498",
        "submitter": "Roy Murphy Dr",
        "authors": "Roy E. Murphy",
        "title": "The Role of Time in the Creation of Knowledge",
        "comments": "Adaptive Processes",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI cs.IT math.IT",
        "license": null,
        "abstract": "  This paper I assume that in humans the creation of knowledge depends on a\ndiscrete time, or stage, sequential decision-making process subjected to a\nstochastic, information transmitting environment. For each time-stage, this\nenvironment randomly transmits Shannon type information-packets to the\ndecision-maker, who examines each of them for relevancy and then determines his\noptimal choices. Using this set of relevant information-packets, the\ndecision-maker adapts, over time, to the stochastic nature of his environment,\nand optimizes the subjective expected rate-of-growth of knowledge. The\ndecision-maker's optimal actions, lead to a decision function that involves,\nover time, his view of the subjective entropy of the environmental process and\nother important parameters at each time-stage of the process. Using this model\nof human behavior, one could create psychometric experiments using computer\nsimulation and real decision-makers, to play programmed games to measure the\nresulting human performance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Jul 2007 20:43:43 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Murphy",
                "Roy E.",
                ""
            ]
        ]
    },
    {
        "id": "0707.0498",
        "submitter": "Roy Murphy Dr",
        "authors": "Roy E. Murphy",
        "title": "The Role of Time in the Creation of Knowledge",
        "comments": "Adaptive Processes",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI cs.IT math.IT",
        "license": null,
        "abstract": "  This paper I assume that in humans the creation of knowledge depends on a\ndiscrete time, or stage, sequential decision-making process subjected to a\nstochastic, information transmitting environment. For each time-stage, this\nenvironment randomly transmits Shannon type information-packets to the\ndecision-maker, who examines each of them for relevancy and then determines his\noptimal choices. Using this set of relevant information-packets, the\ndecision-maker adapts, over time, to the stochastic nature of his environment,\nand optimizes the subjective expected rate-of-growth of knowledge. The\ndecision-maker's optimal actions, lead to a decision function that involves,\nover time, his view of the subjective entropy of the environmental process and\nother important parameters at each time-stage of the process. Using this model\nof human behavior, one could create psychometric experiments using computer\nsimulation and real decision-makers, to play programmed games to measure the\nresulting human performance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Jul 2007 20:43:43 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Murphy",
                "Roy E.",
                ""
            ]
        ]
    },
    {
        "id": "0707.0548",
        "submitter": "Sebastien Verel",
        "authors": "Michael Defoin Platel (I3S), Sebastien Verel (I3S), Manuel Clergue\n  (I3S), Philippe Collard (I3S)",
        "title": "From Royal Road to Epistatic Road for Variable Length Evolution\n  Algorithm",
        "comments": null,
        "journal-ref": "Lecture notes in computer science (Lect. notes comput. sci.) ISSN\n  0302-9743 (27/10/2003) 3-14",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  Although there are some real world applications where the use of variable\nlength representation (VLR) in Evolutionary Algorithm is natural and suitable,\nan academic framework is lacking for such representations. In this work we\npropose a family of tunable fitness landscapes based on VLR of genotypes. The\nfitness landscapes we propose possess a tunable degree of both neutrality and\nepistasis; they are inspired, on the one hand by the Royal Road fitness\nlandscapes, and the other hand by the NK fitness landscapes. So these\nlandscapes offer a scale of continuity from Royal Road functions, with\nneutrality and no epistasis, to landscapes with a large amount of epistasis and\nno redundancy. To gain insight into these fitness landscapes, we first use\nstandard tools such as adaptive walks and correlation length. Second, we\nevaluate the performances of evolutionary algorithms on these landscapes for\nvarious values of the neutral and the epistatic parameters; the results allow\nus to correlate the performances with the expected degrees of neutrality and\nepistasis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Jul 2007 06:57:52 GMT"
            }
        ],
        "update_date": "2007-07-05",
        "authors_parsed": [
            [
                "Platel",
                "Michael Defoin",
                "",
                "I3S"
            ],
            [
                "Verel",
                "Sebastien",
                "",
                "I3S"
            ],
            [
                "Clergue",
                "Manuel",
                "",
                "I3S"
            ],
            [
                "Collard",
                "Philippe",
                "",
                "I3S"
            ]
        ]
    },
    {
        "id": "0707.0641",
        "submitter": "Sebastien Verel",
        "authors": "S\\'ebastien Verel (I3S), Philippe Collard (I3S), Manuel Clergue (I3S)",
        "title": "Where are Bottlenecks in NK Fitness Landscapes?",
        "comments": null,
        "journal-ref": "Evolutionary Computation, 2003. CEC'03 (08/12/2003) 273--280",
        "doi": "10.1109/CEC.2003.1299585",
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  Usually the offspring-parent fitness correlation is used to visualize and\nanalyze some caracteristics of fitness landscapes such as evolvability. In this\npaper, we introduce a more general representation of this correlation, the\nFitness Cloud (FC). We use the bottleneck metaphor to emphasise fitness levels\nin landscape that cause local search process to slow down. For a local search\nheuristic such as hill-climbing or simulated annealing, FC allows to visualize\nbottleneck and neutrality of landscapes. To confirm the relevance of the FC\nrepresentation we show where the bottlenecks are in the well-know NK fitness\nlandscape and also how to use neutrality information from the FC to combine\nsome neutral operator with local search heuristic.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Jul 2007 15:30:54 GMT"
            }
        ],
        "update_date": "2007-07-05",
        "authors_parsed": [
            [
                "Verel",
                "S\u00e9bastien",
                "",
                "I3S"
            ],
            [
                "Collard",
                "Philippe",
                "",
                "I3S"
            ],
            [
                "Clergue",
                "Manuel",
                "",
                "I3S"
            ]
        ]
    },
    {
        "id": "0707.0643",
        "submitter": "Sebastien Verel",
        "authors": "S\\'ebastien Verel (I3S), Philippe Collard (I3S), Manuel Clergue (I3S)",
        "title": "Scuba Search : when selection meets innovation",
        "comments": null,
        "journal-ref": "Evolutionary Computation, 2004. CEC2004 (23/06/2004) 924 - 931",
        "doi": "10.1109/CEC.2004.1330960",
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  We proposed a new search heuristic using the scuba diving metaphor. This\napproach is based on the concept of evolvability and tends to exploit\nneutrality in fitness landscape. Despite the fact that natural evolution does\nnot directly select for evolvability, the basic idea behind the scuba search\nheuristic is to explicitly push the evolvability to increase. The search\nprocess switches between two phases: Conquest-of-the-Waters and\nInvasion-of-the-Land. A comparative study of the new algorithm and standard\nlocal search heuristics on the NKq-landscapes has shown advantage and limit of\nthe scuba search. To enlighten qualitative differences between neutral search\nprocesses, the space is changed into a connected graph to visualize the\npathways that the search is likely to follow.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Jul 2007 15:36:35 GMT"
            }
        ],
        "update_date": "2007-07-05",
        "authors_parsed": [
            [
                "Verel",
                "S\u00e9bastien",
                "",
                "I3S"
            ],
            [
                "Collard",
                "Philippe",
                "",
                "I3S"
            ],
            [
                "Clergue",
                "Manuel",
                "",
                "I3S"
            ]
        ]
    },
    {
        "id": "0707.0652",
        "submitter": "Sebastien Verel",
        "authors": "Philippe Collard (I3S), S\\'ebastien Verel (I3S), Manuel Clergue (I3S)",
        "title": "How to use the Scuba Diving metaphor to solve problem with neutrality ?",
        "comments": null,
        "journal-ref": "ECAI'2004 (27/08/2004) 166-170",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  We proposed a new search heuristic using the scuba diving metaphor. This\napproach is based on the concept of evolvability and tends to exploit\nneutrality which exists in many real-world problems. Despite the fact that\nnatural evolution does not directly select for evolvability, the basic idea\nbehind the scuba search heuristic is to explicitly push evolvability to\nincrease. A comparative study of the scuba algorithm and standard local search\nheuristics has shown the advantage and the limitation of the scuba search. In\norder to tune neutrality, we use the NKq fitness landscapes and a family of\ntravelling salesman problems (TSP) where cities are randomly placed on a\nlattice and where travel distance between cities is computed with the Manhattan\nmetric. In this last problem the amount of neutrality varies with the city\nconcentration on the grid ; assuming the concentration below one, this TSP\nreasonably remains a NP-hard problem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Jul 2007 16:12:17 GMT"
            }
        ],
        "update_date": "2007-07-05",
        "authors_parsed": [
            [
                "Collard",
                "Philippe",
                "",
                "I3S"
            ],
            [
                "Verel",
                "S\u00e9bastien",
                "",
                "I3S"
            ],
            [
                "Clergue",
                "Manuel",
                "",
                "I3S"
            ]
        ]
    },
    {
        "id": "0707.0701",
        "submitter": "Alexandre d'Aspremont",
        "authors": "Ronny Luss, Alexandre d'Aspremont",
        "title": "Clustering and Feature Selection using Sparse Principal Component\n  Analysis",
        "comments": "More experiments",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LG cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the application of sparse principal component\nanalysis (PCA) to clustering and feature selection problems. Sparse PCA seeks\nsparse factors, or linear combinations of the data variables, explaining a\nmaximum amount of variance in the data while having only a limited number of\nnonzero coefficients. PCA is often used as a simple clustering technique and\nsparse factors allow us here to interpret the clusters in terms of a reduced\nset of variables. We begin with a brief introduction and motivation on sparse\nPCA and detail our implementation of the algorithm in d'Aspremont et al.\n(2005). We then apply these results to some classic clustering and feature\nselection problems arising in biology.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Jul 2007 21:53:11 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 8 Oct 2008 18:41:53 GMT"
            }
        ],
        "update_date": "2008-10-08",
        "authors_parsed": [
            [
                "Luss",
                "Ronny",
                ""
            ],
            [
                "d'Aspremont",
                "Alexandre",
                ""
            ]
        ]
    },
    {
        "id": "0707.0701",
        "submitter": "Alexandre d'Aspremont",
        "authors": "Ronny Luss, Alexandre d'Aspremont",
        "title": "Clustering and Feature Selection using Sparse Principal Component\n  Analysis",
        "comments": "More experiments",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LG cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the application of sparse principal component\nanalysis (PCA) to clustering and feature selection problems. Sparse PCA seeks\nsparse factors, or linear combinations of the data variables, explaining a\nmaximum amount of variance in the data while having only a limited number of\nnonzero coefficients. PCA is often used as a simple clustering technique and\nsparse factors allow us here to interpret the clusters in terms of a reduced\nset of variables. We begin with a brief introduction and motivation on sparse\nPCA and detail our implementation of the algorithm in d'Aspremont et al.\n(2005). We then apply these results to some classic clustering and feature\nselection problems arising in biology.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Jul 2007 21:53:11 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 8 Oct 2008 18:41:53 GMT"
            }
        ],
        "update_date": "2008-10-08",
        "authors_parsed": [
            [
                "Luss",
                "Ronny",
                ""
            ],
            [
                "d'Aspremont",
                "Alexandre",
                ""
            ]
        ]
    },
    {
        "id": "0707.0701",
        "submitter": "Alexandre d'Aspremont",
        "authors": "Ronny Luss, Alexandre d'Aspremont",
        "title": "Clustering and Feature Selection using Sparse Principal Component\n  Analysis",
        "comments": "More experiments",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LG cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the application of sparse principal component\nanalysis (PCA) to clustering and feature selection problems. Sparse PCA seeks\nsparse factors, or linear combinations of the data variables, explaining a\nmaximum amount of variance in the data while having only a limited number of\nnonzero coefficients. PCA is often used as a simple clustering technique and\nsparse factors allow us here to interpret the clusters in terms of a reduced\nset of variables. We begin with a brief introduction and motivation on sparse\nPCA and detail our implementation of the algorithm in d'Aspremont et al.\n(2005). We then apply these results to some classic clustering and feature\nselection problems arising in biology.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Jul 2007 21:53:11 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 8 Oct 2008 18:41:53 GMT"
            }
        ],
        "update_date": "2008-10-08",
        "authors_parsed": [
            [
                "Luss",
                "Ronny",
                ""
            ],
            [
                "d'Aspremont",
                "Alexandre",
                ""
            ]
        ]
    },
    {
        "id": "0707.0704",
        "submitter": "Alexandre d'Aspremont",
        "authors": "Onureena Banerjee, Laurent El Ghaoui, Alexandre d'Aspremont",
        "title": "Model Selection Through Sparse Maximum Likelihood Estimation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LG",
        "license": null,
        "abstract": "  We consider the problem of estimating the parameters of a Gaussian or binary\ndistribution in such a way that the resulting undirected graphical model is\nsparse. Our approach is to solve a maximum likelihood problem with an added\nl_1-norm penalty term. The problem as formulated is convex but the memory\nrequirements and complexity of existing interior point methods are prohibitive\nfor problems with more than tens of nodes. We present two new algorithms for\nsolving problems with at least a thousand nodes in the Gaussian case. Our first\nalgorithm uses block coordinate descent, and can be interpreted as recursive\nl_1-norm penalized regression. Our second algorithm, based on Nesterov's first\norder method, yields a complexity estimate with a better dependence on problem\nsize than existing interior point methods. Using a log determinant relaxation\nof the log partition function (Wainwright & Jordan (2006)), we show that these\nsame algorithms can be used to solve an approximate sparse maximum likelihood\nproblem for the binary case. We test our algorithms on synthetic data, as well\nas on gene expression and senate voting records data.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Jul 2007 22:13:42 GMT"
            }
        ],
        "update_date": "2007-07-06",
        "authors_parsed": [
            [
                "Banerjee",
                "Onureena",
                ""
            ],
            [
                "Ghaoui",
                "Laurent El",
                ""
            ],
            [
                "d'Aspremont",
                "Alexandre",
                ""
            ]
        ]
    },
    {
        "id": "0707.0704",
        "submitter": "Alexandre d'Aspremont",
        "authors": "Onureena Banerjee, Laurent El Ghaoui, Alexandre d'Aspremont",
        "title": "Model Selection Through Sparse Maximum Likelihood Estimation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LG",
        "license": null,
        "abstract": "  We consider the problem of estimating the parameters of a Gaussian or binary\ndistribution in such a way that the resulting undirected graphical model is\nsparse. Our approach is to solve a maximum likelihood problem with an added\nl_1-norm penalty term. The problem as formulated is convex but the memory\nrequirements and complexity of existing interior point methods are prohibitive\nfor problems with more than tens of nodes. We present two new algorithms for\nsolving problems with at least a thousand nodes in the Gaussian case. Our first\nalgorithm uses block coordinate descent, and can be interpreted as recursive\nl_1-norm penalized regression. Our second algorithm, based on Nesterov's first\norder method, yields a complexity estimate with a better dependence on problem\nsize than existing interior point methods. Using a log determinant relaxation\nof the log partition function (Wainwright & Jordan (2006)), we show that these\nsame algorithms can be used to solve an approximate sparse maximum likelihood\nproblem for the binary case. We test our algorithms on synthetic data, as well\nas on gene expression and senate voting records data.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Jul 2007 22:13:42 GMT"
            }
        ],
        "update_date": "2007-07-06",
        "authors_parsed": [
            [
                "Banerjee",
                "Onureena",
                ""
            ],
            [
                "Ghaoui",
                "Laurent El",
                ""
            ],
            [
                "d'Aspremont",
                "Alexandre",
                ""
            ]
        ]
    },
    {
        "id": "0707.0705",
        "submitter": "Alexandre d'Aspremont",
        "authors": "Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui",
        "title": "Optimal Solutions for Sparse Principal Component Analysis",
        "comments": "Revised journal version. More efficient optimality conditions and new\n  examples in subset selection and sparse recovery. Original version is in ICML\n  proceedings",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LG",
        "license": null,
        "abstract": "  Given a sample covariance matrix, we examine the problem of maximizing the\nvariance explained by a linear combination of the input variables while\nconstraining the number of nonzero coefficients in this combination. This is\nknown as sparse principal component analysis and has a wide array of\napplications in machine learning and engineering. We formulate a new\nsemidefinite relaxation to this problem and derive a greedy algorithm that\ncomputes a full set of good solutions for all target numbers of non zero\ncoefficients, with total complexity O(n^3), where n is the number of variables.\nWe then use the same relaxation to derive sufficient conditions for global\noptimality of a solution, which can be tested in O(n^3) per pattern. We discuss\napplications in subset selection and sparse recovery and show on artificial\nexamples and biological data that our algorithm does provide globally optimal\nsolutions in many cases.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Jul 2007 22:28:28 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 3 Aug 2007 22:30:22 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 24 Aug 2007 00:49:31 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 9 Nov 2007 17:27:11 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "d'Aspremont",
                "Alexandre",
                ""
            ],
            [
                "Bach",
                "Francis",
                ""
            ],
            [
                "Ghaoui",
                "Laurent El",
                ""
            ]
        ]
    },
    {
        "id": "0707.0705",
        "submitter": "Alexandre d'Aspremont",
        "authors": "Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui",
        "title": "Optimal Solutions for Sparse Principal Component Analysis",
        "comments": "Revised journal version. More efficient optimality conditions and new\n  examples in subset selection and sparse recovery. Original version is in ICML\n  proceedings",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LG",
        "license": null,
        "abstract": "  Given a sample covariance matrix, we examine the problem of maximizing the\nvariance explained by a linear combination of the input variables while\nconstraining the number of nonzero coefficients in this combination. This is\nknown as sparse principal component analysis and has a wide array of\napplications in machine learning and engineering. We formulate a new\nsemidefinite relaxation to this problem and derive a greedy algorithm that\ncomputes a full set of good solutions for all target numbers of non zero\ncoefficients, with total complexity O(n^3), where n is the number of variables.\nWe then use the same relaxation to derive sufficient conditions for global\noptimality of a solution, which can be tested in O(n^3) per pattern. We discuss\napplications in subset selection and sparse recovery and show on artificial\nexamples and biological data that our algorithm does provide globally optimal\nsolutions in many cases.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Jul 2007 22:28:28 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 3 Aug 2007 22:30:22 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 24 Aug 2007 00:49:31 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 9 Nov 2007 17:27:11 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "d'Aspremont",
                "Alexandre",
                ""
            ],
            [
                "Bach",
                "Francis",
                ""
            ],
            [
                "Ghaoui",
                "Laurent El",
                ""
            ]
        ]
    },
    {
        "id": "0707.0740",
        "submitter": "Richard McClatchey",
        "authors": "A. Ali, A. Anjum, J. Bunn, F. Khan, R.McClatchey, H. Newman, C.\n  Steenberg, M. Thomas, Ian Willers",
        "title": "A Multi Interface Grid Discovery System",
        "comments": "2 pages, 4 figures. Presented at the Grid 2006 conference, Barcelona\n  Spain",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Discovery Systems (DS) can be considered as entry points for global loosely\ncoupled distributed systems. An efficient Discovery System in essence increases\nthe performance, reliability and decision making capability of distributed\nsystems. With the rapid increase in scale of distributed applications, existing\nsolutions for discovery systems are fast becoming either obsolete or incapable\nof handling such complexity. They are particularly ineffective when handling\nservice lifetimes and providing up-to-date information, poor at enabling\ndynamic service access and they can also impose unwanted restrictions on\ninterfaces to widely available information repositories. In this paper we\npresent essential the design characteristics, an implementation and a\nperformance analysis for a discovery system capable of overcoming these\ndeficiencies in large, globally distributed environments.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 09:22:45 GMT"
            }
        ],
        "update_date": "2007-07-06",
        "authors_parsed": [
            [
                "Ali",
                "A.",
                ""
            ],
            [
                "Anjum",
                "A.",
                ""
            ],
            [
                "Bunn",
                "J.",
                ""
            ],
            [
                "Khan",
                "F.",
                ""
            ],
            [
                "McClatchey",
                "R.",
                ""
            ],
            [
                "Newman",
                "H.",
                ""
            ],
            [
                "Steenberg",
                "C.",
                ""
            ],
            [
                "Thomas",
                "M.",
                ""
            ],
            [
                "Willers",
                "Ian",
                ""
            ]
        ]
    },
    {
        "id": "0707.0742",
        "submitter": "Richard McClatchey",
        "authors": "A. Ali, A. Anjum, T. Azim, J. Bunn, A. Ikram, R. McClatchey, H.\n  Newman, C. Steenberg, M. Thomas, I. Willers",
        "title": "Mobile Computing in Physics Analysis - An Indicator for eScience",
        "comments": "8 pages, 7 figures. Presented at the 3rd Int Conf on Mobile Computing\n  & Ubiquitous Networking (ICMU06. London October 2006",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  This paper presents the design and implementation of a Grid-enabled physics\nanalysis environment for handheld and other resource-limited computing devices\nas one example of the use of mobile devices in eScience. Handheld devices offer\ngreat potential because they provide ubiquitous access to data and\nround-the-clock connectivity over wireless links. Our solution aims to provide\nusers of handheld devices the capability to launch heavy computational tasks on\ncomputational and data Grids, monitor the jobs status during execution, and\nretrieve results after job completion. Users carry their jobs on their handheld\ndevices in the form of executables (and associated libraries). Users can\ntransparently view the status of their jobs and get back their outputs without\nhaving to know where they are being executed. In this way, our system is able\nto act as a high-throughput computing environment where devices ranging from\npowerful desktop machines to small handhelds can employ the power of the Grid.\nThe results shown in this paper are readily applicable to the wider eScience\ncommunity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 09:32:29 GMT"
            }
        ],
        "update_date": "2007-07-06",
        "authors_parsed": [
            [
                "Ali",
                "A.",
                ""
            ],
            [
                "Anjum",
                "A.",
                ""
            ],
            [
                "Azim",
                "T.",
                ""
            ],
            [
                "Bunn",
                "J.",
                ""
            ],
            [
                "Ikram",
                "A.",
                ""
            ],
            [
                "McClatchey",
                "R.",
                ""
            ],
            [
                "Newman",
                "H.",
                ""
            ],
            [
                "Steenberg",
                "C.",
                ""
            ],
            [
                "Thomas",
                "M.",
                ""
            ],
            [
                "Willers",
                "I.",
                ""
            ]
        ]
    },
    {
        "id": "0707.0743",
        "submitter": "Richard McClatchey",
        "authors": "A. Anjum, R. McClatchey, H. Stockinger, A. Ali, I. Willers, M. Thomas,\n  M. Sagheer, K. Hasham, O. Alvi",
        "title": "DIANA Scheduling Hierarchies for Optimizing Bulk Job Scheduling",
        "comments": "8 pages, 9 figures. Presented at the 2nd IEEE Int Conference on\n  eScience & Grid Computing. Amsterdam Netherlands, December 2006",
        "journal-ref": null,
        "doi": "10.1109/E-SCIENCE.2006.261173",
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  The use of meta-schedulers for resource management in large-scale distributed\nsystems often leads to a hierarchy of schedulers. In this paper, we discuss why\nexisting meta-scheduling hierarchies are sometimes not sufficient for Grid\nsystems due to their inability to re-organise jobs already scheduled locally.\nSuch a job re-organisation is required to adapt to evolving loads which are\ncommon in heavily used Grid infrastructures. We propose a peer-to-peer\nscheduling model and evaluate it using case studies and mathematical modelling.\nWe detail the DIANA (Data Intensive and Network Aware) scheduling algorithm and\nits queue management system for coping with the load distribution and for\nsupporting bulk job scheduling. We demonstrate that such a system is beneficial\nfor dynamic, distributed and self-organizing resource management and can assist\nin optimizing load or job distribution in complex Grid infrastructures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 09:36:18 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Anjum",
                "A.",
                ""
            ],
            [
                "McClatchey",
                "R.",
                ""
            ],
            [
                "Stockinger",
                "H.",
                ""
            ],
            [
                "Ali",
                "A.",
                ""
            ],
            [
                "Willers",
                "I.",
                ""
            ],
            [
                "Thomas",
                "M.",
                ""
            ],
            [
                "Sagheer",
                "M.",
                ""
            ],
            [
                "Hasham",
                "K.",
                ""
            ],
            [
                "Alvi",
                "O.",
                ""
            ]
        ]
    },
    {
        "id": "0707.0745",
        "submitter": "Richard McClatchey",
        "authors": "K. Munir, M. Odeh, R. McClatchey, S. Khan, I. Habib",
        "title": "Semantic Information Retrieval from Distributed Heterogeneous Data\n  Sources",
        "comments": "6 pages, 3 figures. Presented at the 4th International Workshop on\n  Frontiers of Information Technology -- FIT 2006. Islamabad, Pakistan December\n  2006",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  Information retrieval from distributed heterogeneous data sources remains a\nchallenging issue. As the number of data sources increases more intelligent\nretrieval techniques, focusing on information content and semantics, are\nrequired. Currently ontologies are being widely used for managing semantic\nknowledge, especially in the field of bioinformatics. In this paper we describe\nan ontology assisted system that allows users to query distributed\nheterogeneous data sources by hiding details like location, information\nstructure, access pattern and semantic structure of the data. Our goal is to\nprovide an integrated view on biomedical information sources for the\nHealth-e-Child project with the aim to overcome the lack of sufficient\nsemantic-based reformulation techniques for querying distributed data sources.\nIn particular, this paper examines the problem of query reformulation across\nbiomedical data sources, based on merged ontologies and the underlying\nheterogeneous descriptions of the respective data sources.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 09:40:19 GMT"
            }
        ],
        "update_date": "2007-07-06",
        "authors_parsed": [
            [
                "Munir",
                "K.",
                ""
            ],
            [
                "Odeh",
                "M.",
                ""
            ],
            [
                "McClatchey",
                "R.",
                ""
            ],
            [
                "Khan",
                "S.",
                ""
            ],
            [
                "Habib",
                "I.",
                ""
            ]
        ]
    },
    {
        "id": "0707.0748",
        "submitter": "Richard McClatchey",
        "authors": "F. Estrella, T. Hauer, R. McClatchey, M. Odeh, D Rogulin, T.\n  Solomonides",
        "title": "Experiences of Engineering Grid-Based Medical Software",
        "comments": "18 pages, 2 tables, 5 figures. In press International Journal of\n  Medical Informatics. Elsevier publishers",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Objectives: Grid-based technologies are emerging as potential solutions for\nmanaging and collaborating distributed resources in the biomedical domain. Few\nexamples exist, however, of successful implementations of Grid-enabled medical\nsystems and even fewer have been deployed for evaluation in practice. The\nobjective of this paper is to evaluate the use in clinical practice of a\nGrid-based imaging prototype and to establish directions for engineering future\nmedical Grid developments and their subsequent deployment. Method: The\nMammoGrid project has deployed a prototype system for clinicians using the Grid\nas its information infrastructure. To assist in the specification of the system\nrequirements (and for the first time in healthgrid applications), use-case\nmodelling has been carried out in close collaboration with clinicians and\nradiologists who had no prior experience of this modelling technique. A\ncritical qualitative and, where possible, quantitative analysis of the\nMammoGrid prototype is presented leading to a set of recommendations from the\ndelivery of the first deployed Grid-based medical imaging application. Results:\nWe report critically on the application of software engineering techniques in\nthe specification and implementation of the MammoGrid project and show that\nuse-case modelling is a suitable vehicle for representing medical requirements\nand for communicating effectively with the clinical community. This paper also\ndiscusses the practical advantages and limitations of applying the Grid to\nreal-life clinical applications and presents the consequent lessons learned.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 10:06:41 GMT"
            }
        ],
        "update_date": "2007-07-06",
        "authors_parsed": [
            [
                "Estrella",
                "F.",
                ""
            ],
            [
                "Hauer",
                "T.",
                ""
            ],
            [
                "McClatchey",
                "R.",
                ""
            ],
            [
                "Odeh",
                "M.",
                ""
            ],
            [
                "Rogulin",
                "D",
                ""
            ],
            [
                "Solomonides",
                "T.",
                ""
            ]
        ]
    },
    {
        "id": "0707.0761",
        "submitter": "Richard McClatchey",
        "authors": "David Manset, Herve Verjus, Richard McClatchey",
        "title": "Managing Separation of Concerns in Grid Applications Through\n  Architectural Model Transformations",
        "comments": "4 pages, 2 figures. Presented at the First European Conference on\n  Software Architectures (ECSA 2007). Madrid, Spain September 24-26, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.DC",
        "license": null,
        "abstract": "  Grids enable the aggregation, virtualization and sharing of massive\nheterogeneous and geographically dispersed resources, using files, applications\nand storage devices, to solve computation and data intensive problems, across\ninstitutions and countries via temporary collaborations called virtual\norganizations (VO). Most implementations result in complex superposition of\nsoftware layers, often delivering low quality of service and quality of\napplications. As a consequence, Grid-based applications design and development\nis increasingly complex, and the use of most classical engineering practices is\nunsuccessful. Not only is the development of such applications a\ntime-consuming, error prone and expensive task, but also the resulting\napplications are often hard-coded for specific Grid configurations, platforms\nand infra-structures. Having neither guidelines nor rules in the design of a\nGrid-based application is a paradox since there are many existing architectural\napproaches for distributed computing, which could ease and promote rigorous\nengineering methods based on the re-use of software components. It is our\nbelief that ad-hoc and semi-formal engineer-ing approaches, in current use, are\ninsufficient to tackle tomorrows Grid develop-ments requirements. Because\nGrid-based applications address multi-disciplinary and complex domains (health,\nmilitary, scientific computation), their engineering requires rigor and\ncontrol. This paper therefore advocates a formal model-driven engineering\nprocess and corresponding design framework and tools for building the next\ngeneration of Grids.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 11:10:03 GMT"
            }
        ],
        "update_date": "2007-07-06",
        "authors_parsed": [
            [
                "Manset",
                "David",
                ""
            ],
            [
                "Verjus",
                "Herve",
                ""
            ],
            [
                "McClatchey",
                "Richard",
                ""
            ]
        ]
    },
    {
        "id": "0707.0761",
        "submitter": "Richard McClatchey",
        "authors": "David Manset, Herve Verjus, Richard McClatchey",
        "title": "Managing Separation of Concerns in Grid Applications Through\n  Architectural Model Transformations",
        "comments": "4 pages, 2 figures. Presented at the First European Conference on\n  Software Architectures (ECSA 2007). Madrid, Spain September 24-26, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.DC",
        "license": null,
        "abstract": "  Grids enable the aggregation, virtualization and sharing of massive\nheterogeneous and geographically dispersed resources, using files, applications\nand storage devices, to solve computation and data intensive problems, across\ninstitutions and countries via temporary collaborations called virtual\norganizations (VO). Most implementations result in complex superposition of\nsoftware layers, often delivering low quality of service and quality of\napplications. As a consequence, Grid-based applications design and development\nis increasingly complex, and the use of most classical engineering practices is\nunsuccessful. Not only is the development of such applications a\ntime-consuming, error prone and expensive task, but also the resulting\napplications are often hard-coded for specific Grid configurations, platforms\nand infra-structures. Having neither guidelines nor rules in the design of a\nGrid-based application is a paradox since there are many existing architectural\napproaches for distributed computing, which could ease and promote rigorous\nengineering methods based on the re-use of software components. It is our\nbelief that ad-hoc and semi-formal engineer-ing approaches, in current use, are\ninsufficient to tackle tomorrows Grid develop-ments requirements. Because\nGrid-based applications address multi-disciplinary and complex domains (health,\nmilitary, scientific computation), their engineering requires rigor and\ncontrol. This paper therefore advocates a formal model-driven engineering\nprocess and corresponding design framework and tools for building the next\ngeneration of Grids.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 11:10:03 GMT"
            }
        ],
        "update_date": "2007-07-06",
        "authors_parsed": [
            [
                "Manset",
                "David",
                ""
            ],
            [
                "Verjus",
                "Herve",
                ""
            ],
            [
                "McClatchey",
                "Richard",
                ""
            ]
        ]
    },
    {
        "id": "0707.0762",
        "submitter": "Richard McClatchey",
        "authors": "Irfan Habib, Kamran Soomro, Ashiq Anjum, Richard McClatchey, Arshad\n  Ali, Peter Bloodsworth",
        "title": "PhantomOS: A Next Generation Grid Operating System",
        "comments": "8 pages, 6 figures. Presented at the UK eScience All Hands Meeting\n  2007 (AHM07). Nottingham, UK. September 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Grid Computing has made substantial advances in the past decade; these are\nprimarily due to the adoption of standardized Grid middleware. However Grid\ncomputing has not yet become pervasive because of some barriers that we believe\nhave been caused by the adoption of middleware centric approaches. These\nbarriers include: scant support for major types of applications such as\ninteractive applications; lack of flexible, autonomic and scalable Grid\narchitectures; lack of plug-and-play Grid computing and, most importantly, no\nstraightforward way to setup and administer Grids. PhantomOS is a project which\naims to address many of these barriers. Its goal is the creation of a user\nfriendly pervasive Grid computing platform that facilitates the rapid\ndeployment and easy maintenance of Grids whilst providing support for major\ntypes of applications on Grids of almost any topology. In this paper we present\nthe detailed system architecture and an overview of its implementation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 11:14:45 GMT"
            }
        ],
        "update_date": "2007-07-06",
        "authors_parsed": [
            [
                "Habib",
                "Irfan",
                ""
            ],
            [
                "Soomro",
                "Kamran",
                ""
            ],
            [
                "Anjum",
                "Ashiq",
                ""
            ],
            [
                "McClatchey",
                "Richard",
                ""
            ],
            [
                "Ali",
                "Arshad",
                ""
            ],
            [
                "Bloodsworth",
                "Peter",
                ""
            ]
        ]
    },
    {
        "id": "0707.0763",
        "submitter": "Richard McClatchey",
        "authors": "Ashiq Anjum, Peter Bloodsworth, Andrew Branson, Tamas Hauer, Richard\n  McClatchey, Kamran Munir, Dmitry Rogulin, Jetendr Shamdasani",
        "title": "The Requirements for Ontologies in Medical Data Integration: A Case\n  Study",
        "comments": "6 pages, 1 figure. Presented at the 11th International Database\n  Engineering & Applications Symposium (Ideas2007). Banff, Canada September\n  2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  Evidence-based medicine is critically dependent on three sources of\ninformation: a medical knowledge base, the patients medical record and\nknowledge of available resources, including where appropriate, clinical\nprotocols. Patient data is often scattered in a variety of databases and may,\nin a distributed model, be held across several disparate repositories.\nConsequently addressing the needs of an evidence-based medicine community\npresents issues of biomedical data integration, clinical interpretation and\nknowledge management. This paper outlines how the Health-e-Child project has\napproached the challenge of requirements specification for (bio-) medical data\nintegration, from the level of cellular data, through disease to that of\npatient and population. The approach is illuminated through the requirements\nelicitation and analysis of Juvenile Idiopathic Arthritis (JIA), one of three\ndiseases being studied in the EC-funded Health-e-Child project.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 11:21:39 GMT"
            }
        ],
        "update_date": "2007-07-06",
        "authors_parsed": [
            [
                "Anjum",
                "Ashiq",
                ""
            ],
            [
                "Bloodsworth",
                "Peter",
                ""
            ],
            [
                "Branson",
                "Andrew",
                ""
            ],
            [
                "Hauer",
                "Tamas",
                ""
            ],
            [
                "McClatchey",
                "Richard",
                ""
            ],
            [
                "Munir",
                "Kamran",
                ""
            ],
            [
                "Rogulin",
                "Dmitry",
                ""
            ],
            [
                "Shamdasani",
                "Jetendr",
                ""
            ]
        ]
    },
    {
        "id": "0707.0802",
        "submitter": "Patricia Reynier",
        "authors": "Dinu Coltuc, Jean-Marc Chassery (GIPSA-lab)",
        "title": "Very fast watermarking by reversible contrast mapping",
        "comments": null,
        "journal-ref": "IEEE Signal Processing Letters 14, 4 (04/2007) pp 255-258",
        "doi": "10.1109/LSP.2006.884895",
        "report-no": null,
        "categories": "cs.MM cs.CR cs.CV cs.IT math.IT",
        "license": null,
        "abstract": "  Reversible contrast mapping (RCM) is a simple integer transform that applies\nto pairs of pixels. For some pairs of pixels, RCM is invertible, even if the\nleast significant bits (LSBs) of the transformed pixels are lost. The data\nspace occupied by the LSBs is suitable for data hiding. The embedded\ninformation bit-rates of the proposed spatial domain reversible watermarking\nscheme are close to the highest bit-rates reported so far. The scheme does not\nneed additional data compression, and, in terms of mathematical complexity, it\nappears to be the lowest complexity one proposed up to now. A very fast lookup\ntable implementation is proposed. Robustness against cropping can be ensured as\nwell.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 15:11:24 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Coltuc",
                "Dinu",
                "",
                "GIPSA-lab"
            ],
            [
                "Chassery",
                "Jean-Marc",
                "",
                "GIPSA-lab"
            ]
        ]
    },
    {
        "id": "0707.0802",
        "submitter": "Patricia Reynier",
        "authors": "Dinu Coltuc, Jean-Marc Chassery (GIPSA-lab)",
        "title": "Very fast watermarking by reversible contrast mapping",
        "comments": null,
        "journal-ref": "IEEE Signal Processing Letters 14, 4 (04/2007) pp 255-258",
        "doi": "10.1109/LSP.2006.884895",
        "report-no": null,
        "categories": "cs.MM cs.CR cs.CV cs.IT math.IT",
        "license": null,
        "abstract": "  Reversible contrast mapping (RCM) is a simple integer transform that applies\nto pairs of pixels. For some pairs of pixels, RCM is invertible, even if the\nleast significant bits (LSBs) of the transformed pixels are lost. The data\nspace occupied by the LSBs is suitable for data hiding. The embedded\ninformation bit-rates of the proposed spatial domain reversible watermarking\nscheme are close to the highest bit-rates reported so far. The scheme does not\nneed additional data compression, and, in terms of mathematical complexity, it\nappears to be the lowest complexity one proposed up to now. A very fast lookup\ntable implementation is proposed. Robustness against cropping can be ensured as\nwell.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 15:11:24 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Coltuc",
                "Dinu",
                "",
                "GIPSA-lab"
            ],
            [
                "Chassery",
                "Jean-Marc",
                "",
                "GIPSA-lab"
            ]
        ]
    },
    {
        "id": "0707.0805",
        "submitter": "Xinjia Chen",
        "authors": "Xinjia Chen",
        "title": "A New Generalization of Chebyshev Inequality for Random Vectors",
        "comments": "7 pages, 1 figure; added some references",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.ST cs.LG math.PR stat.AP stat.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we derive a new generalization of Chebyshev inequality for\nrandom vectors. We demonstrate that the new generalization is much less\nconservative than the classical generalization.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 15:28:05 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 24 Jun 2011 15:08:17 GMT"
            }
        ],
        "update_date": "2013-11-05",
        "authors_parsed": [
            [
                "Chen",
                "Xinjia",
                ""
            ]
        ]
    },
    {
        "id": "0707.0808",
        "submitter": "Patrick C. McGuire",
        "authors": "Alexandra Bartolo, Patrick C. McGuire, Kenneth P. Camilleri,\n  Christopher Spiteri, Jonathan C. Borg, Philip J. Farrugia, Jens Ormo, Javier\n  Gomez-Elvira, Jose Antonio Rodriguez-Manfredi, Enrique Diaz-Martinez, Helge\n  Ritter, Robert Haschke, Markus Oesker, Joerg Ontrup",
        "title": "The Cyborg Astrobiologist: Porting from a wearable computer to the\n  Astrobiology Phone-cam",
        "comments": "15 pages, 4 figures, accepted for publication in the International\n  Journal of Astrobiology",
        "journal-ref": "International Journal of Astrobiology, vol. 6, issue 4, pp.\n  255-261 (2007)",
        "doi": "10.1017/S1473550407003862",
        "report-no": null,
        "categories": "cs.CV astro-ph cs.AI cs.CE cs.HC cs.NI cs.RO cs.SE",
        "license": null,
        "abstract": "  We have used a simple camera phone to significantly improve an `exploration\nsystem' for astrobiology and geology. This camera phone will make it much\neasier to develop and test computer-vision algorithms for future planetary\nexploration. We envision that the `Astrobiology Phone-cam' exploration system\ncan be fruitfully used in other problem domains as well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 15:19:37 GMT"
            }
        ],
        "update_date": "2010-01-08",
        "authors_parsed": [
            [
                "Bartolo",
                "Alexandra",
                ""
            ],
            [
                "McGuire",
                "Patrick C.",
                ""
            ],
            [
                "Camilleri",
                "Kenneth P.",
                ""
            ],
            [
                "Spiteri",
                "Christopher",
                ""
            ],
            [
                "Borg",
                "Jonathan C.",
                ""
            ],
            [
                "Farrugia",
                "Philip J.",
                ""
            ],
            [
                "Ormo",
                "Jens",
                ""
            ],
            [
                "Gomez-Elvira",
                "Javier",
                ""
            ],
            [
                "Rodriguez-Manfredi",
                "Jose Antonio",
                ""
            ],
            [
                "Diaz-Martinez",
                "Enrique",
                ""
            ],
            [
                "Ritter",
                "Helge",
                ""
            ],
            [
                "Haschke",
                "Robert",
                ""
            ],
            [
                "Oesker",
                "Markus",
                ""
            ],
            [
                "Ontrup",
                "Joerg",
                ""
            ]
        ]
    },
    {
        "id": "0707.0808",
        "submitter": "Patrick C. McGuire",
        "authors": "Alexandra Bartolo, Patrick C. McGuire, Kenneth P. Camilleri,\n  Christopher Spiteri, Jonathan C. Borg, Philip J. Farrugia, Jens Ormo, Javier\n  Gomez-Elvira, Jose Antonio Rodriguez-Manfredi, Enrique Diaz-Martinez, Helge\n  Ritter, Robert Haschke, Markus Oesker, Joerg Ontrup",
        "title": "The Cyborg Astrobiologist: Porting from a wearable computer to the\n  Astrobiology Phone-cam",
        "comments": "15 pages, 4 figures, accepted for publication in the International\n  Journal of Astrobiology",
        "journal-ref": "International Journal of Astrobiology, vol. 6, issue 4, pp.\n  255-261 (2007)",
        "doi": "10.1017/S1473550407003862",
        "report-no": null,
        "categories": "cs.CV astro-ph cs.AI cs.CE cs.HC cs.NI cs.RO cs.SE",
        "license": null,
        "abstract": "  We have used a simple camera phone to significantly improve an `exploration\nsystem' for astrobiology and geology. This camera phone will make it much\neasier to develop and test computer-vision algorithms for future planetary\nexploration. We envision that the `Astrobiology Phone-cam' exploration system\ncan be fruitfully used in other problem domains as well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 15:19:37 GMT"
            }
        ],
        "update_date": "2010-01-08",
        "authors_parsed": [
            [
                "Bartolo",
                "Alexandra",
                ""
            ],
            [
                "McGuire",
                "Patrick C.",
                ""
            ],
            [
                "Camilleri",
                "Kenneth P.",
                ""
            ],
            [
                "Spiteri",
                "Christopher",
                ""
            ],
            [
                "Borg",
                "Jonathan C.",
                ""
            ],
            [
                "Farrugia",
                "Philip J.",
                ""
            ],
            [
                "Ormo",
                "Jens",
                ""
            ],
            [
                "Gomez-Elvira",
                "Javier",
                ""
            ],
            [
                "Rodriguez-Manfredi",
                "Jose Antonio",
                ""
            ],
            [
                "Diaz-Martinez",
                "Enrique",
                ""
            ],
            [
                "Ritter",
                "Helge",
                ""
            ],
            [
                "Haschke",
                "Robert",
                ""
            ],
            [
                "Oesker",
                "Markus",
                ""
            ],
            [
                "Ontrup",
                "Joerg",
                ""
            ]
        ]
    },
    {
        "id": "0707.0808",
        "submitter": "Patrick C. McGuire",
        "authors": "Alexandra Bartolo, Patrick C. McGuire, Kenneth P. Camilleri,\n  Christopher Spiteri, Jonathan C. Borg, Philip J. Farrugia, Jens Ormo, Javier\n  Gomez-Elvira, Jose Antonio Rodriguez-Manfredi, Enrique Diaz-Martinez, Helge\n  Ritter, Robert Haschke, Markus Oesker, Joerg Ontrup",
        "title": "The Cyborg Astrobiologist: Porting from a wearable computer to the\n  Astrobiology Phone-cam",
        "comments": "15 pages, 4 figures, accepted for publication in the International\n  Journal of Astrobiology",
        "journal-ref": "International Journal of Astrobiology, vol. 6, issue 4, pp.\n  255-261 (2007)",
        "doi": "10.1017/S1473550407003862",
        "report-no": null,
        "categories": "cs.CV astro-ph cs.AI cs.CE cs.HC cs.NI cs.RO cs.SE",
        "license": null,
        "abstract": "  We have used a simple camera phone to significantly improve an `exploration\nsystem' for astrobiology and geology. This camera phone will make it much\neasier to develop and test computer-vision algorithms for future planetary\nexploration. We envision that the `Astrobiology Phone-cam' exploration system\ncan be fruitfully used in other problem domains as well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 15:19:37 GMT"
            }
        ],
        "update_date": "2010-01-08",
        "authors_parsed": [
            [
                "Bartolo",
                "Alexandra",
                ""
            ],
            [
                "McGuire",
                "Patrick C.",
                ""
            ],
            [
                "Camilleri",
                "Kenneth P.",
                ""
            ],
            [
                "Spiteri",
                "Christopher",
                ""
            ],
            [
                "Borg",
                "Jonathan C.",
                ""
            ],
            [
                "Farrugia",
                "Philip J.",
                ""
            ],
            [
                "Ormo",
                "Jens",
                ""
            ],
            [
                "Gomez-Elvira",
                "Javier",
                ""
            ],
            [
                "Rodriguez-Manfredi",
                "Jose Antonio",
                ""
            ],
            [
                "Diaz-Martinez",
                "Enrique",
                ""
            ],
            [
                "Ritter",
                "Helge",
                ""
            ],
            [
                "Haschke",
                "Robert",
                ""
            ],
            [
                "Oesker",
                "Markus",
                ""
            ],
            [
                "Ontrup",
                "Joerg",
                ""
            ]
        ]
    },
    {
        "id": "0707.0808",
        "submitter": "Patrick C. McGuire",
        "authors": "Alexandra Bartolo, Patrick C. McGuire, Kenneth P. Camilleri,\n  Christopher Spiteri, Jonathan C. Borg, Philip J. Farrugia, Jens Ormo, Javier\n  Gomez-Elvira, Jose Antonio Rodriguez-Manfredi, Enrique Diaz-Martinez, Helge\n  Ritter, Robert Haschke, Markus Oesker, Joerg Ontrup",
        "title": "The Cyborg Astrobiologist: Porting from a wearable computer to the\n  Astrobiology Phone-cam",
        "comments": "15 pages, 4 figures, accepted for publication in the International\n  Journal of Astrobiology",
        "journal-ref": "International Journal of Astrobiology, vol. 6, issue 4, pp.\n  255-261 (2007)",
        "doi": "10.1017/S1473550407003862",
        "report-no": null,
        "categories": "cs.CV astro-ph cs.AI cs.CE cs.HC cs.NI cs.RO cs.SE",
        "license": null,
        "abstract": "  We have used a simple camera phone to significantly improve an `exploration\nsystem' for astrobiology and geology. This camera phone will make it much\neasier to develop and test computer-vision algorithms for future planetary\nexploration. We envision that the `Astrobiology Phone-cam' exploration system\ncan be fruitfully used in other problem domains as well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 15:19:37 GMT"
            }
        ],
        "update_date": "2010-01-08",
        "authors_parsed": [
            [
                "Bartolo",
                "Alexandra",
                ""
            ],
            [
                "McGuire",
                "Patrick C.",
                ""
            ],
            [
                "Camilleri",
                "Kenneth P.",
                ""
            ],
            [
                "Spiteri",
                "Christopher",
                ""
            ],
            [
                "Borg",
                "Jonathan C.",
                ""
            ],
            [
                "Farrugia",
                "Philip J.",
                ""
            ],
            [
                "Ormo",
                "Jens",
                ""
            ],
            [
                "Gomez-Elvira",
                "Javier",
                ""
            ],
            [
                "Rodriguez-Manfredi",
                "Jose Antonio",
                ""
            ],
            [
                "Diaz-Martinez",
                "Enrique",
                ""
            ],
            [
                "Ritter",
                "Helge",
                ""
            ],
            [
                "Haschke",
                "Robert",
                ""
            ],
            [
                "Oesker",
                "Markus",
                ""
            ],
            [
                "Ontrup",
                "Joerg",
                ""
            ]
        ]
    },
    {
        "id": "0707.0808",
        "submitter": "Patrick C. McGuire",
        "authors": "Alexandra Bartolo, Patrick C. McGuire, Kenneth P. Camilleri,\n  Christopher Spiteri, Jonathan C. Borg, Philip J. Farrugia, Jens Ormo, Javier\n  Gomez-Elvira, Jose Antonio Rodriguez-Manfredi, Enrique Diaz-Martinez, Helge\n  Ritter, Robert Haschke, Markus Oesker, Joerg Ontrup",
        "title": "The Cyborg Astrobiologist: Porting from a wearable computer to the\n  Astrobiology Phone-cam",
        "comments": "15 pages, 4 figures, accepted for publication in the International\n  Journal of Astrobiology",
        "journal-ref": "International Journal of Astrobiology, vol. 6, issue 4, pp.\n  255-261 (2007)",
        "doi": "10.1017/S1473550407003862",
        "report-no": null,
        "categories": "cs.CV astro-ph cs.AI cs.CE cs.HC cs.NI cs.RO cs.SE",
        "license": null,
        "abstract": "  We have used a simple camera phone to significantly improve an `exploration\nsystem' for astrobiology and geology. This camera phone will make it much\neasier to develop and test computer-vision algorithms for future planetary\nexploration. We envision that the `Astrobiology Phone-cam' exploration system\ncan be fruitfully used in other problem domains as well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 15:19:37 GMT"
            }
        ],
        "update_date": "2010-01-08",
        "authors_parsed": [
            [
                "Bartolo",
                "Alexandra",
                ""
            ],
            [
                "McGuire",
                "Patrick C.",
                ""
            ],
            [
                "Camilleri",
                "Kenneth P.",
                ""
            ],
            [
                "Spiteri",
                "Christopher",
                ""
            ],
            [
                "Borg",
                "Jonathan C.",
                ""
            ],
            [
                "Farrugia",
                "Philip J.",
                ""
            ],
            [
                "Ormo",
                "Jens",
                ""
            ],
            [
                "Gomez-Elvira",
                "Javier",
                ""
            ],
            [
                "Rodriguez-Manfredi",
                "Jose Antonio",
                ""
            ],
            [
                "Diaz-Martinez",
                "Enrique",
                ""
            ],
            [
                "Ritter",
                "Helge",
                ""
            ],
            [
                "Haschke",
                "Robert",
                ""
            ],
            [
                "Oesker",
                "Markus",
                ""
            ],
            [
                "Ontrup",
                "Joerg",
                ""
            ]
        ]
    },
    {
        "id": "0707.0808",
        "submitter": "Patrick C. McGuire",
        "authors": "Alexandra Bartolo, Patrick C. McGuire, Kenneth P. Camilleri,\n  Christopher Spiteri, Jonathan C. Borg, Philip J. Farrugia, Jens Ormo, Javier\n  Gomez-Elvira, Jose Antonio Rodriguez-Manfredi, Enrique Diaz-Martinez, Helge\n  Ritter, Robert Haschke, Markus Oesker, Joerg Ontrup",
        "title": "The Cyborg Astrobiologist: Porting from a wearable computer to the\n  Astrobiology Phone-cam",
        "comments": "15 pages, 4 figures, accepted for publication in the International\n  Journal of Astrobiology",
        "journal-ref": "International Journal of Astrobiology, vol. 6, issue 4, pp.\n  255-261 (2007)",
        "doi": "10.1017/S1473550407003862",
        "report-no": null,
        "categories": "cs.CV astro-ph cs.AI cs.CE cs.HC cs.NI cs.RO cs.SE",
        "license": null,
        "abstract": "  We have used a simple camera phone to significantly improve an `exploration\nsystem' for astrobiology and geology. This camera phone will make it much\neasier to develop and test computer-vision algorithms for future planetary\nexploration. We envision that the `Astrobiology Phone-cam' exploration system\ncan be fruitfully used in other problem domains as well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 15:19:37 GMT"
            }
        ],
        "update_date": "2010-01-08",
        "authors_parsed": [
            [
                "Bartolo",
                "Alexandra",
                ""
            ],
            [
                "McGuire",
                "Patrick C.",
                ""
            ],
            [
                "Camilleri",
                "Kenneth P.",
                ""
            ],
            [
                "Spiteri",
                "Christopher",
                ""
            ],
            [
                "Borg",
                "Jonathan C.",
                ""
            ],
            [
                "Farrugia",
                "Philip J.",
                ""
            ],
            [
                "Ormo",
                "Jens",
                ""
            ],
            [
                "Gomez-Elvira",
                "Javier",
                ""
            ],
            [
                "Rodriguez-Manfredi",
                "Jose Antonio",
                ""
            ],
            [
                "Diaz-Martinez",
                "Enrique",
                ""
            ],
            [
                "Ritter",
                "Helge",
                ""
            ],
            [
                "Haschke",
                "Robert",
                ""
            ],
            [
                "Oesker",
                "Markus",
                ""
            ],
            [
                "Ontrup",
                "Joerg",
                ""
            ]
        ]
    },
    {
        "id": "0707.0860",
        "submitter": "Mohammad Asad Rehman Chaudhry",
        "authors": "Salim Y. El Rouayheb, Mohammad Asad R. Chaudhry, and Alex Sprintson",
        "title": "On the Minimum Number of Transmissions in Single-Hop Wireless Coding\n  Networks",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.NI math.IT",
        "license": null,
        "abstract": "  The advent of network coding presents promising opportunities in many areas\nof communication and networking. It has been recently shown that network coding\ntechnique can significantly increase the overall throughput of wireless\nnetworks by taking advantage of their broadcast nature. In wireless networks,\neach transmitted packet is broadcasted within a certain area and can be\noverheard by the neighboring nodes. When a node needs to transmit packets, it\nemploys the opportunistic coding approach that uses the knowledge of what the\nnode's neighbors have heard in order to reduce the number of transmissions.\nWith this approach, each transmitted packet is a linear combination of the\noriginal packets over a certain finite field.\n  In this paper, we focus on the fundamental problem of finding the optimal\nencoding for the broadcasted packets that minimizes the overall number of\ntransmissions. We show that this problem is NP-complete over GF(2) and\nestablish several fundamental properties of the optimal solution. We also\npropose a simple heuristic solution for the problem based on graph coloring and\npresent some empirical results for random settings.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 19:58:23 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Rouayheb",
                "Salim Y. El",
                ""
            ],
            [
                "Chaudhry",
                "Mohammad Asad R.",
                ""
            ],
            [
                "Sprintson",
                "Alex",
                ""
            ]
        ]
    },
    {
        "id": "0707.0862",
        "submitter": "Richard McClatchey",
        "authors": "Richard McClatchey, Ashiq Anjum, Heinz Stockinger, Arshad Ali, Ian\n  Willers, Michael Thomas",
        "title": "Scheduling in Data Intensive and Network Aware (DIANA) Grid Environments",
        "comments": "22 pages, 14 figures. Early draft of paper to be submitted to Journal\n  of Grid Computing",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  In Grids scheduling decisions are often made on the basis of jobs being\neither data or computation intensive: in data intensive situations jobs may be\npushed to the data and in computation intensive situations data may be pulled\nto the jobs. This kind of scheduling, in which there is no consideration of\nnetwork characteristics, can lead to performance degradation in a Grid\nenvironment and may result in large processing queues and job execution delays\ndue to site overloads. In this paper we describe a Data Intensive and Network\nAware (DIANA) meta-scheduling approach, which takes into account data,\nprocessing power and network characteristics when making scheduling decisions\nacross multiple sites. Through a practical implementation on a Grid testbed, we\ndemonstrate that queue and execution times of data-intensive jobs can be\nsignificantly improved when we introduce our proposed DIANA scheduler. The\nbasic scheduling decisions are dictated by a weighting factor for each\npotential target location which is a calculated function of network\ncharacteristics, processing cycles and data location and size. The job\nscheduler provides a global ranking of the computing resources and then selects\nan optimal one on the basis of this overall access and execution cost. The\nDIANA approach considers the Grid as a combination of active network elements\nand takes network characteristics as a first class criterion in the scheduling\ndecision matrix along with computation and data. The scheduler can then make\ninformed decisions by taking into account the changing state of the network,\nlocality and size of the data and the pool of available processing cycles.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 19:46:51 GMT"
            }
        ],
        "update_date": "2007-07-06",
        "authors_parsed": [
            [
                "McClatchey",
                "Richard",
                ""
            ],
            [
                "Anjum",
                "Ashiq",
                ""
            ],
            [
                "Stockinger",
                "Heinz",
                ""
            ],
            [
                "Ali",
                "Arshad",
                ""
            ],
            [
                "Willers",
                "Ian",
                ""
            ],
            [
                "Thomas",
                "Michael",
                ""
            ]
        ]
    },
    {
        "id": "0707.0878",
        "submitter": "Xinjia Chen",
        "authors": "Xinjia Chen, Jorge Aravena and Kemin Zhou",
        "title": "Risk Analysis in Robust Control -- Making the Case for Probabilistic\n  Robust Control",
        "comments": "22 pages, 2 figures",
        "journal-ref": "Proceedings of American Control Conference, pp. 1533-1538,\n  Portland, June 2005.",
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY math.ST stat.TH",
        "license": null,
        "abstract": "  This paper offers a critical view of the \"worst-case\" approach that is the\ncornerstone of robust control design. It is our contention that a blind\nacceptance of worst-case scenarios may lead to designs that are actually more\ndangerous than designs based on probabilistic techniques with a built-in risk\nfactor. The real issue is one of modeling. If one accepts that no mathematical\nmodel of uncertainties is perfect then a probabilistic approach can lead to\nmore reliable control even if it cannot guarantee stability for all possible\ncases. Our presentation is based on case analysis. We first establish that\nworst-case is not necessarily \"all-encompassing.\" In fact, we show that for\nsome uncertain control problems to have a conventional robust control solution\nit is necessary to make assumptions that leave out some feasible cases. Once we\nestablish that point, we argue that it is not uncommon for the risk of\nunaccounted cases in worst-case design to be greater than that of the accepted\nrisk in a probabilistic approach. With an example, we quantify the risks and\nshow that worst-case can be significantly more risky. Finally, we join our\nanalysis with existing results on computational complexity and probabilistic\nrobustness to argue that the deterministic worst-case analysis is not\nnecessarily the better tool.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jul 2007 21:09:00 GMT"
            }
        ],
        "update_date": "2013-11-05",
        "authors_parsed": [
            [
                "Chen",
                "Xinjia",
                ""
            ],
            [
                "Aravena",
                "Jorge",
                ""
            ],
            [
                "Zhou",
                "Kemin",
                ""
            ]
        ]
    },
    {
        "id": "0707.0926",
        "submitter": "Yves Bertot",
        "authors": "Yves Bertot (INRIA Sophia Antipolis)",
        "title": "Theorem proving support in programming language semantics",
        "comments": "Propos\\'e pour publication dans l'ouvrage \\`a la m\\'emoire de Gilles\n  Kahn",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.PL",
        "license": null,
        "abstract": "  We describe several views of the semantics of a simple programming language\nas formal documents in the calculus of inductive constructions that can be\nverified by the Coq proof system. Covered aspects are natural semantics,\ndenotational semantics, axiomatic semantics, and abstract interpretation.\nDescriptions as recursive functions are also provided whenever suitable, thus\nyielding a a verification condition generator and a static analyser that can be\nrun inside the theorem prover for use in reflective proofs. Extraction of an\ninterpreter from the denotational semantics is also described. All different\naspects are formally proved sound with respect to the natural semantics\nspecification.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Jul 2007 08:55:26 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 10 Jul 2007 08:09:49 GMT"
            }
        ],
        "update_date": "2007-07-10",
        "authors_parsed": [
            [
                "Bertot",
                "Yves",
                "",
                "INRIA Sophia Antipolis"
            ]
        ]
    },
    {
        "id": "0707.1059",
        "submitter": "Alban Ponse",
        "authors": "Jan A. Bergstra and Alban Ponse",
        "title": "Projection semantics for rigid loops",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": "PRG0604",
        "categories": "cs.PL",
        "license": null,
        "abstract": "  A rigid loop is a for-loop with a counter not accessible to the loop body or\nany other part of a program. Special instructions for rigid loops are\nintroduced on top of the syntax of the program algebra PGA. Two different\nsemantic projections are provided and proven equivalent. One of these is taken\nto have definitional status on the basis of two criteria: `normative semantic\nadequacy' and `indicative algorithmic adequacy'.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Jul 2007 23:58:45 GMT"
            }
        ],
        "update_date": "2007-07-10",
        "authors_parsed": [
            [
                "Bergstra",
                "Jan A.",
                ""
            ],
            [
                "Ponse",
                "Alban",
                ""
            ]
        ]
    },
    {
        "id": "0707.1083",
        "submitter": "Viktoria Rojkova",
        "authors": "Viktoria Rojkova, Mehmed Kantardzic",
        "title": "Delayed Correlations in Inter-Domain Network Traffic",
        "comments": "submitted to CoNext 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.IR",
        "license": null,
        "abstract": "  To observe the evolution of network traffic correlations we analyze the\neigenvalue spectra and eigenvectors statistics of delayed correlation matrices\nof network traffic counts time series. Delayed correlation matrix D is composed\nof the correlations between one variable in the multivariable time series and\nanother at a time delay \\tau . Inverse participation ratio (IPR) of\neigenvectors of D deviates substantially from the IPR of eigenvectors of the\nequal time correlation matrix C. We relate this finding to the localization and\ndiscuss its importance for network congestion control. The time-lagged\ncorrelation pattern between network time series is preserved over a long time,\nup to 100\\tau, where \\tau=300 sec. The largest eigenvalue \\lambda_{max} of D\nand the corresponding IPR oscillate with two characteristic periods of 3\\tau\nand 6\\tau . The existence of delayed correlations between network time series\nfits well into the long range dependence (LRD) property of the network traffic.\n  The ability to monitor and control the long memory processes is crucial since\nthey impact the network performance. Injecting the random traffic counts\nbetween non-randomly correlated time series, we were able to break the picture\nof periodicity of \\lambda_{max}. In addition, we investigated influence of the\nperiodic injections on both largest eigenvalue and the IPR, and addressed\nrelevance of these indicators for the LRD and self-similarity of the network\ntraffic.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 7 Jul 2007 19:01:51 GMT"
            }
        ],
        "update_date": "2007-07-10",
        "authors_parsed": [
            [
                "Rojkova",
                "Viktoria",
                ""
            ],
            [
                "Kantardzic",
                "Mehmed",
                ""
            ]
        ]
    },
    {
        "id": "0707.1288",
        "submitter": "Kamel Aouiche",
        "authors": "Riadh Ben Messaoud and Kamel Aouiche and C\\'ecile Favre",
        "title": "Espaces de repr\\'esentation multidimensionnels d\\'edi\\'es \\`a la\n  visualisation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  In decision-support systems, the visual component is important for On Line\nAnalysis Processing (OLAP). In this paper, we propose a new approach that faces\nthe visualization problem due to data sparsity. We use the results of a\nMultiple Correspondence Analysis (MCA) to reduce the negative effect of\nsparsity by organizing differently data cube cells. Our approach does not\nreduce sparsity, however it tries to build relevant representation spaces where\nfacts are efficiently gathered. In order to evaluate our approach, we propose\nan homogeneity criterion based on geometric neighborhood of cells. The obtained\nexperimental results have shown the efficiency of our method.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 9 Jul 2007 15:52:02 GMT"
            }
        ],
        "update_date": "2007-07-10",
        "authors_parsed": [
            [
                "Messaoud",
                "Riadh Ben",
                ""
            ],
            [
                "Aouiche",
                "Kamel",
                ""
            ],
            [
                "Favre",
                "C\u00e9cile",
                ""
            ]
        ]
    },
    {
        "id": "0707.1295",
        "submitter": "Riccardo Zecchina",
        "authors": "Carlo Baldassi, Alfredo Braunstein, Nicolas Brunel, Riccardo Zecchina",
        "title": "Efficient supervised learning in networks with binary synapses",
        "comments": "10 pages, 4 figures",
        "journal-ref": "PNAS 104, 11079-11084 (2007)",
        "doi": "10.1073/pnas.0700324104",
        "report-no": null,
        "categories": "q-bio.NC cond-mat.stat-mech cs.NE q-bio.QM",
        "license": null,
        "abstract": "  Recent experimental studies indicate that synaptic changes induced by\nneuronal activity are discrete jumps between a small number of stable states.\nLearning in systems with discrete synapses is known to be a computationally\nhard problem. Here, we study a neurobiologically plausible on-line learning\nalgorithm that derives from Belief Propagation algorithms. We show that it\nperforms remarkably well in a model neuron with binary synapses, and a finite\nnumber of `hidden' states per synapse, that has to learn a random\nclassification task. Such system is able to learn a number of associations\nclose to the theoretical limit, in time which is sublinear in system size. This\nis to our knowledge the first on-line algorithm that is able to achieve\nefficiently a finite number of patterns learned per binary synapse.\nFurthermore, we show that performance is optimal for a finite number of hidden\nstates which becomes very small for sparse coding. The algorithm is similar to\nthe standard `perceptron' learning algorithm, with an additional rule for\nsynaptic transitions which occur only if a currently presented pattern is\n`barely correct'. In this case, the synaptic changes are meta-plastic only\n(change in hidden states and not in actual synaptic state), stabilizing the\nsynapse in its current state. Finally, we show that a system with two visible\nstates and K hidden states is much more robust to noise than a system with K\nvisible states. We suggest this rule is sufficiently simple to be easily\nimplemented by neurobiological systems or in hardware.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 9 Jul 2007 16:23:55 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Baldassi",
                "Carlo",
                ""
            ],
            [
                "Braunstein",
                "Alfredo",
                ""
            ],
            [
                "Brunel",
                "Nicolas",
                ""
            ],
            [
                "Zecchina",
                "Riccardo",
                ""
            ]
        ]
    },
    {
        "id": "0707.1304",
        "submitter": "Kamel Aouiche",
        "authors": "Hadj Mahboubi and Kamel Aouiche and J\\'er\\^ome Darmont",
        "title": "Un index de jointure pour les entrep\\^ots de donn\\'ees XML",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  XML data warehouses form an interesting basis for decision-support\napplications that exploit heterogeneous data from multiple sources. However,\nXML-native database systems currently bear limited performances and it is\nnecessary to research ways to optimize them. In this paper, we propose a new\nindex that is specifically adapted to the multidimensional architecture of XML\nwarehouses and eliminates join operations, while preserving the information\ncontained in the original warehouse. A theoretical study and experimental\nresults demonstrate the efficiency of our index, even when queries are complex.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 9 Jul 2007 16:58:14 GMT"
            }
        ],
        "update_date": "2007-07-10",
        "authors_parsed": [
            [
                "Mahboubi",
                "Hadj",
                ""
            ],
            [
                "Aouiche",
                "Kamel",
                ""
            ],
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                ""
            ]
        ]
    },
    {
        "id": "0707.1306",
        "submitter": "Kamel Aouiche",
        "authors": "Nora Maiz and Kamel Aouiche and J\\'er\\^ome Darmont",
        "title": "S\\'election simultan\\'ee d'index et de vues mat\\'erialis\\'ees",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  Indices and materialized views are physical structures that accelerate data\naccess in data warehouses. However, these data structures generate some\nmaintenance overhead. They also share the same storage space. The existing\nstudies about index and materialized view selection consider these structures\nseparately. In this paper, we adopt the opposite stance and couple index and\nmaterialized view selection to take into account the interactions between them\nand achieve an efficient storage space sharing. We develop cost models that\nevaluate the respective benefit of indexing and view materialization. These\ncost models are then exploited by a greedy algorithm to select a relevant\nconfiguration of indices and materialized views. Experimental results show that\nour strategy performs better than the independent selection of indices and\nmaterialized views.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 9 Jul 2007 17:23:31 GMT"
            }
        ],
        "update_date": "2007-07-10",
        "authors_parsed": [
            [
                "Maiz",
                "Nora",
                ""
            ],
            [
                "Aouiche",
                "Kamel",
                ""
            ],
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                ""
            ]
        ]
    },
    {
        "id": "0707.1432",
        "submitter": "Jean-Guillaume Dumas",
        "authors": "Jean-Guillaume Dumas (LJK), Dominique Duval (LJK), Jean-Claude Reynaud\n  (RC)",
        "title": "Sequential products in effect categories",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.CT cs.PL",
        "license": null,
        "abstract": "  A new categorical framework is provided for dealing with multiple arguments\nin a programming language with effects, for example in a language with\nimperative features. Like related frameworks (Monads, Arrows, Freyd\ncategories), we distinguish two kinds of functions. In addition, we also\ndistinguish two kinds of equations. Then, we are able to define a kind of\nproduct, that generalizes the usual categorical product. This yields a powerful\ntool for deriving many results about languages with effects.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Jul 2007 12:50:35 GMT"
            }
        ],
        "update_date": "2007-07-11",
        "authors_parsed": [
            [
                "Dumas",
                "Jean-Guillaume",
                "",
                "LJK"
            ],
            [
                "Duval",
                "Dominique",
                "",
                "LJK"
            ],
            [
                "Reynaud",
                "Jean-Claude",
                "",
                "RC"
            ]
        ]
    },
    {
        "id": "0707.1452",
        "submitter": "Xavier Polanco",
        "authors": "Xavier Polanco (INIST)",
        "title": "Clusters, Graphs, and Networks for Analysing Internet-Web-Supported\n  Communication within a Virtual Community",
        "comments": null,
        "journal-ref": "Advances in Knowledge Organization (2002) 364-371",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LG",
        "license": null,
        "abstract": "  The proposal is to use clusters, graphs and networks as models in order to\nanalyse the Web structure. Clusters, graphs and networks provide knowledge\nrepresentation and organization. Clusters were generated by co-site analysis.\nThe sample is a set of academic Web sites from the countries belonging to the\nEuropean Union. These clusters are here revisited from the point of view of\ngraph theory and social network analysis. This is a quantitative and structural\nanalysis. In fact, the Internet is a computer network that connects people and\norganizations. Thus we may consider it to be a social network. The set of Web\nacademic sites represents an empirical social network, and is viewed as a\nvirtual community. The network structural properties are here analysed applying\ntogether cluster analysis, graph theory and social network analysis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Jul 2007 13:47:32 GMT"
            }
        ],
        "update_date": "2007-07-11",
        "authors_parsed": [
            [
                "Polanco",
                "Xavier",
                "",
                "INIST"
            ]
        ]
    },
    {
        "id": "0707.1452",
        "submitter": "Xavier Polanco",
        "authors": "Xavier Polanco (INIST)",
        "title": "Clusters, Graphs, and Networks for Analysing Internet-Web-Supported\n  Communication within a Virtual Community",
        "comments": null,
        "journal-ref": "Advances in Knowledge Organization (2002) 364-371",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LG",
        "license": null,
        "abstract": "  The proposal is to use clusters, graphs and networks as models in order to\nanalyse the Web structure. Clusters, graphs and networks provide knowledge\nrepresentation and organization. Clusters were generated by co-site analysis.\nThe sample is a set of academic Web sites from the countries belonging to the\nEuropean Union. These clusters are here revisited from the point of view of\ngraph theory and social network analysis. This is a quantitative and structural\nanalysis. In fact, the Internet is a computer network that connects people and\norganizations. Thus we may consider it to be a social network. The set of Web\nacademic sites represents an empirical social network, and is viewed as a\nvirtual community. The network structural properties are here analysed applying\ntogether cluster analysis, graph theory and social network analysis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Jul 2007 13:47:32 GMT"
            }
        ],
        "update_date": "2007-07-11",
        "authors_parsed": [
            [
                "Polanco",
                "Xavier",
                "",
                "INIST"
            ]
        ]
    },
    {
        "id": "0707.1480",
        "submitter": "Rene Chalon",
        "authors": "Ren\\'e Chalon (ICTT), Bertrand T. David (ICTT)",
        "title": "IRVO: an Interaction Model for designing Collaborative Mixed Reality\n  systems",
        "comments": "10 pages",
        "journal-ref": "Human Computer International 2005, U.S. CD (11/08/2005) 1-10",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  This paper presents an interaction model adapted to mixed reality\nenvironments known as IRVO (Interacting with Real and Virtual Objects). IRVO\naims at modeling the interaction between one or more users and the Mixed\nReality system by representing explicitly the objects and tools involved and\ntheir relationship. IRVO covers the design phase of the life cycle and models\nthe intended use of the system. In a first part, we present a brief review of\nrelated HCI models. The second part is devoted to the IRVO model, its notation\nand some examples. In the third part, we present how IRVO is used for designing\napplications and in particular we show how this model can be integrated in a\nModel-Based Approach (CoCSys) which is currently designed at our lab.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Jul 2007 16:01:30 GMT"
            }
        ],
        "update_date": "2007-07-11",
        "authors_parsed": [
            [
                "Chalon",
                "Ren\u00e9",
                "",
                "ICTT"
            ],
            [
                "David",
                "Bertrand T.",
                "",
                "ICTT"
            ]
        ]
    },
    {
        "id": "0707.1490",
        "submitter": "Gianluca Argentini",
        "authors": "Gianluca Argentini",
        "title": "Fast computing of velocity field for flows in industrial burners and\n  pumps",
        "comments": "14 pages, 5 figures; paper accepted for Special Issue \"Application of\n  distributed and grid computing\", Future Generation Computer Systems journal,\n  2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.NA cs.MS",
        "license": null,
        "abstract": "  In this work we present a technique of fast numerical computation for\nsolutions of Navier-Stokes equations in the case of flows of industrial\ninterest. At first the partial differential equations are translated into a set\nof nonlinear ordinary differential equations using the geometrical shape of the\ndomain where the flow is developing, then these ODEs are numerically resolved\nusing a set of computations distributed among the available processors. We\npresent some results from simulations on a parallel hardware architecture using\nnative multithreads software and simulating a shared-memory or a\ndistributed-memory environment.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Jul 2007 16:23:43 GMT"
            }
        ],
        "update_date": "2007-07-11",
        "authors_parsed": [
            [
                "Argentini",
                "Gianluca",
                ""
            ]
        ]
    },
    {
        "id": "0707.1501",
        "submitter": "Alexander Ushakov",
        "authors": "Alexei G. Myasnikov, Alexander Ushakov",
        "title": "Random subgroups and analysis of the length-based and quotient attacks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.GR cs.CR",
        "license": null,
        "abstract": "  In this paper we discuss generic properties of \"random subgroups\" of a given\ngroup G. It turns out that in many groups G (even in most exotic of them) the\nrandom subgroups have a simple algebraic structure and they \"sit\" inside G in a\nvery particular way. This gives a strong mathematical foundation for\ncryptanalysis of several group-based cryptosystems and indicates on how to\nchose \"strong keys\". To illustrate our technique we analyze the\nAnshel-Anshel-Goldfeld (AAG) cryptosystem and give a mathematical explanation\nof recent success of some heuristic length-based attacks on it. Furthermore, we\ndesign and analyze a new type of attacks, which we term the quotient attacks.\nMathematical methods we develop here also indicate how one can try to choose\n\"parameters\" in AAG to foil the attacks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Jul 2007 17:38:26 GMT"
            }
        ],
        "update_date": "2007-07-12",
        "authors_parsed": [
            [
                "Myasnikov",
                "Alexei G.",
                ""
            ],
            [
                "Ushakov",
                "Alexander",
                ""
            ]
        ]
    },
    {
        "id": "0707.1527",
        "submitter": "Anya Tafliovich",
        "authors": "Anya Tafliovich, Eric C.R. Hehner",
        "title": "Programming Telepathy: Implementing Quantum Non-Locality Games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "quant-ph cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantum pseudo-telepathy is an intriguing phenomenon which results from the\napplication of quantum information theory to communication complexity. To\ndemonstrate this phenomenon researchers in the field of quantum communication\ncomplexity devised a number of quantum non-locality games. The setting of these\ngames is as follows: the players are separated so that no communication between\nthem is possible and are given a certain computational task. When the players\nhave access to a quantum resource called entanglement, they can accomplish the\ntask: something that is impossible in a classical setting. To an observer who\nis unfamiliar with the laws of quantum mechanics it seems that the players\nemploy some sort of telepathy; that is, they somehow exchange information\nwithout sharing a communication channel. This paper provides a formal framework\nfor specifying, implementing, and analysing quantum non-locality games.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Jul 2007 19:50:25 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 29 Jul 2009 15:25:07 GMT"
            }
        ],
        "update_date": "2009-07-29",
        "authors_parsed": [
            [
                "Tafliovich",
                "Anya",
                ""
            ],
            [
                "Hehner",
                "Eric C. R.",
                ""
            ]
        ]
    },
    {
        "id": "0707.1534",
        "submitter": "Kamel Aouiche",
        "authors": "J\\'er\\^ome Darmont, Omar Boussaid, Jean-Christian Ralaivao and Kamel\n  Aouiche",
        "title": "An Architecture Framework for Complex Data Warehouses",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  Nowadays, many decision support applications need to exploit data that are\nnot only numerical or symbolic, but also multimedia, multistructure,\nmultisource, multimodal, and/or multiversion. We term such data complex data.\nManaging and analyzing complex data involves a lot of different issues\nregarding their structure, storage and processing, and metadata are a key\nelement in all these processes. Such problems have been addressed by classical\ndata warehousing (i.e., applied to \"simple\" data). However, data warehousing\napproaches need to be adapted for complex data. In this paper, we first propose\na precise, though open, definition of complex data. Then we present a general\narchitecture framework for warehousing complex data. This architecture heavily\nrelies on metadata and domain-related knowledge, and rests on the XML language,\nwhich helps storing data, metadata and domain-specific knowledge altogether,\nand facilitates communication between the various warehousing processes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Jul 2007 22:01:40 GMT"
            }
        ],
        "update_date": "2007-07-12",
        "authors_parsed": [
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                ""
            ],
            [
                "Boussaid",
                "Omar",
                ""
            ],
            [
                "Ralaivao",
                "Jean-Christian",
                ""
            ],
            [
                "Aouiche",
                "Kamel",
                ""
            ]
        ]
    },
    {
        "id": "0707.1548",
        "submitter": "Kamel Aouiche",
        "authors": "Kamel Aouiche and J\\'er\\^ome Darmont",
        "title": "Data Mining-based Materialized View and Index Selection in Data\n  Warehouses",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  Materialized views and indexes are physical structures for accelerating data\naccess that are casually used in data warehouses. However, these data\nstructures generate some maintenance overhead. They also share the same storage\nspace. Most existing studies about materialized view and index selection\nconsider these structures separately. In this paper, we adopt the opposite\nstance and couple materialized view and index selection to take view-index\ninteractions into account and achieve efficient storage space sharing.\nCandidate materialized views and indexes are selected through a data mining\nprocess. We also exploit cost models that evaluate the respective benefit of\nindexing and view materialization, and help select a relevant configuration of\nindexes and materialized views among the candidates. Experimental results show\nthat our strategy performs better than an independent selection of materialized\nviews and indexes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Jul 2007 02:45:10 GMT"
            }
        ],
        "update_date": "2007-07-12",
        "authors_parsed": [
            [
                "Aouiche",
                "Kamel",
                ""
            ],
            [
                "Darmont",
                "J\u00e9r\u00f4me",
                ""
            ]
        ]
    },
    {
        "id": "0707.1607",
        "submitter": "Erik Schnetter",
        "authors": "Erik Schnetter, Christian D. Ott, Gabrielle Allen, Peter Diener, Tom\n  Goodale, Thomas Radke, Edward Seidel, John Shalf",
        "title": "Cactus Framework: Black Holes to Gamma Ray Bursts",
        "comments": "16 pages, 4 figures. To appear in Petascale Computing: Algorithms and\n  Applications, Ed. D. Bader, CRC Press LLC (2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Gamma Ray Bursts (GRBs) are intense narrowly-beamed flashes of gamma-rays of\ncosmological origin. They are among the most scientifically interesting\nastrophysical systems, and the riddle concerning their central engines and\nemission mechanisms is one of the most complex and challenging problems of\nastrophysics today. In this article we outline our petascale approach to the\nGRB problem and discuss the computational toolkits and numerical codes that are\ncurrently in use and that will be scaled up to run on emerging petaflop scale\ncomputing platforms in the near future.\n  Petascale computing will require additional ingredients over conventional\nparallelism. We consider some of the challenges which will be caused by future\npetascale architectures, and discuss our plans for the future development of\nthe Cactus framework and its applications to meet these challenges in order to\nprofit from these new architectures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Jul 2007 13:01:50 GMT"
            }
        ],
        "update_date": "2007-07-12",
        "authors_parsed": [
            [
                "Schnetter",
                "Erik",
                ""
            ],
            [
                "Ott",
                "Christian D.",
                ""
            ],
            [
                "Allen",
                "Gabrielle",
                ""
            ],
            [
                "Diener",
                "Peter",
                ""
            ],
            [
                "Goodale",
                "Tom",
                ""
            ],
            [
                "Radke",
                "Thomas",
                ""
            ],
            [
                "Seidel",
                "Edward",
                ""
            ],
            [
                "Shalf",
                "John",
                ""
            ]
        ]
    },
    {
        "id": "0707.1618",
        "submitter": "Per Ola Kristensson",
        "authors": "Per Ola Kristensson, Nils Dahlback, Daniel Anundi, Marius Bjornstad,\n  Hanna Gillberg, Jonas Haraldsson, Ingrid Martensson, Matttias Nordvall,\n  Josefin Stahl",
        "title": "The Trade-offs with Space Time Cube Representation of Spatiotemporal\n  Patterns",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.GR",
        "license": null,
        "abstract": "  Space time cube representation is an information visualization technique\nwhere spatiotemporal data points are mapped into a cube. Fast and correct\nanalysis of such information is important in for instance geospatial and social\nvisualization applications. Information visualization researchers have\npreviously argued that space time cube representation is beneficial in\nrevealing complex spatiotemporal patterns in a dataset to users. The argument\nis based on the fact that both time and spatial information are displayed\nsimultaneously to users, an effect difficult to achieve in other\nrepresentations. However, to our knowledge the actual usefulness of space time\ncube representation in conveying complex spatiotemporal patterns to users has\nnot been empirically validated. To fill this gap we report on a\nbetween-subjects experiment comparing novice users error rates and response\ntimes when answering a set of questions using either space time cube or a\nbaseline 2D representation. For some simple questions the error rates were\nlower when using the baseline representation. For complex questions where the\nparticipants needed an overall understanding of the spatiotemporal structure of\nthe dataset, the space time cube representation resulted in on average twice as\nfast response times with no difference in error rates compared to the baseline.\nThese results provide an empirical foundation for the hypothesis that space\ntime cube representation benefits users when analyzing complex spatiotemporal\npatterns.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Jul 2007 13:39:34 GMT"
            }
        ],
        "update_date": "2007-07-12",
        "authors_parsed": [
            [
                "Kristensson",
                "Per Ola",
                ""
            ],
            [
                "Dahlback",
                "Nils",
                ""
            ],
            [
                "Anundi",
                "Daniel",
                ""
            ],
            [
                "Bjornstad",
                "Marius",
                ""
            ],
            [
                "Gillberg",
                "Hanna",
                ""
            ],
            [
                "Haraldsson",
                "Jonas",
                ""
            ],
            [
                "Martensson",
                "Ingrid",
                ""
            ],
            [
                "Nordvall",
                "Matttias",
                ""
            ],
            [
                "Stahl",
                "Josefin",
                ""
            ]
        ]
    },
    {
        "id": "0707.1639",
        "submitter": "Alban Ponse",
        "authors": "Jan A. Bergstra and Alban Ponse",
        "title": "Interface groups and financial transfer architectures",
        "comments": "22 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": "PRG0702",
        "categories": "cs.SE",
        "license": null,
        "abstract": "  Analytic execution architectures have been proposed by the same authors as a\nmeans to conceptualize the cooperation between heterogeneous collectives of\ncomponents such as programs, threads, states and services. Interface groups\nhave been proposed as a means to formalize interface information concerning\nanalytic execution architectures. These concepts are adapted to organization\narchitectures with a focus on financial transfers. Interface groups (and\nmonoids) now provide a technique to combine interface elements into interfaces\nwith the flexibility to distinguish between directions of flow dependent on\nentity naming.\n  The main principle exploiting interface groups is that when composing a\nclosed system of a collection of interacting components, the sum of their\ninterfaces must vanish in the interface group modulo reflection. This certainly\nmatters for financial transfer interfaces.\n  As an example of this, we specify an interface group and within it some\nspecific interfaces concerning the financial transfer architecture for a part\nof our local academic organization.\n  Financial transfer interface groups arise as a special case of more general\nservice architecture interfaces.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Jul 2007 14:55:26 GMT"
            }
        ],
        "update_date": "2007-07-12",
        "authors_parsed": [
            [
                "Bergstra",
                "Jan A.",
                ""
            ],
            [
                "Ponse",
                "Alban",
                ""
            ]
        ]
    },
    {
        "id": "0707.1644",
        "submitter": "Dan Olteanu",
        "authors": "Lyublena Antova, Thomas Jansen, Christoph Koch, Dan Olteanu",
        "title": "Fast and Simple Relational Processing of Uncertain Data",
        "comments": "12 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.PF",
        "license": null,
        "abstract": "  This paper introduces U-relations, a succinct and purely relational\nrepresentation system for uncertain databases. U-relations support\nattribute-level uncertainty using vertical partitioning. If we consider\npositive relational algebra extended by an operation for computing possible\nanswers, a query on the logical level can be translated into, and evaluated as,\na single relational algebra query on the U-relation representation. The\ntranslation scheme essentially preserves the size of the query in terms of\nnumber of operations and, in particular, number of joins. Standard techniques\nemployed in off-the-shelf relational database management systems are effective\nfor optimizing and processing queries on U-relations. In our experiments we\nshow that query evaluation on U-relations scales to large amounts of data with\nhigh degrees of uncertainty.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Jul 2007 15:13:39 GMT"
            }
        ],
        "update_date": "2007-07-12",
        "authors_parsed": [
            [
                "Antova",
                "Lyublena",
                ""
            ],
            [
                "Jansen",
                "Thomas",
                ""
            ],
            [
                "Koch",
                "Christoph",
                ""
            ],
            [
                "Olteanu",
                "Dan",
                ""
            ]
        ]
    },
    {
        "id": "0707.1644",
        "submitter": "Dan Olteanu",
        "authors": "Lyublena Antova, Thomas Jansen, Christoph Koch, Dan Olteanu",
        "title": "Fast and Simple Relational Processing of Uncertain Data",
        "comments": "12 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.PF",
        "license": null,
        "abstract": "  This paper introduces U-relations, a succinct and purely relational\nrepresentation system for uncertain databases. U-relations support\nattribute-level uncertainty using vertical partitioning. If we consider\npositive relational algebra extended by an operation for computing possible\nanswers, a query on the logical level can be translated into, and evaluated as,\na single relational algebra query on the U-relation representation. The\ntranslation scheme essentially preserves the size of the query in terms of\nnumber of operations and, in particular, number of joins. Standard techniques\nemployed in off-the-shelf relational database management systems are effective\nfor optimizing and processing queries on U-relations. In our experiments we\nshow that query evaluation on U-relations scales to large amounts of data with\nhigh degrees of uncertainty.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Jul 2007 15:13:39 GMT"
            }
        ],
        "update_date": "2007-07-12",
        "authors_parsed": [
            [
                "Antova",
                "Lyublena",
                ""
            ],
            [
                "Jansen",
                "Thomas",
                ""
            ],
            [
                "Koch",
                "Christoph",
                ""
            ],
            [
                "Olteanu",
                "Dan",
                ""
            ]
        ]
    },
    {
        "id": "0707.1716",
        "submitter": "Luis Antonio da Mota",
        "authors": "B.O.Rodrigues, L.A.C.P.da Mota and L.G.S.Duarte",
        "title": "Numerical Calculation With Arbitrary Precision",
        "comments": "contribution to X hadron Physics - 2007 to appear on International\n  Journal of Modern Physics E",
        "journal-ref": null,
        "doi": "10.1142/S0218301307009014",
        "report-no": null,
        "categories": "cs.NA cs.MS",
        "license": null,
        "abstract": "  The vast use of computers on scientific numerical computation makes the\nawareness of the limited precision that these machines are able to provide us\nan essential matter. A limited and insufficient precision allied to the\ntruncation and rounding errors may induce the user to incorrect interpretation\nof his/hers answer. In this work, we have developed a computational package to\nminimize this kind of error by offering arbitrary precision numbers and\ncalculation. This is very important in Physics where we can work with numbers\ntoo small and too big simultaneously.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Jul 2007 22:24:48 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Rodrigues",
                "B. O.",
                ""
            ],
            [
                "da Mota",
                "L. A. C. P.",
                ""
            ],
            [
                "Duarte",
                "L. G. S.",
                ""
            ]
        ]
    },
    {
        "id": "0707.1820",
        "submitter": "Arnaud Legout",
        "authors": "Anwar Al Hamra (INRIA Sophia Antipolis / INRIA Rh\\^one-Alpes), Arnaud\n  Legout (INRIA Sophia Antipolis / INRIA Rh\\^one-Alpes), Chadi Barakat (INRIA\n  Sophia Antipolis / INRIA Rh\\^one-Alpes)",
        "title": "Understanding the Properties of the BitTorrent Overlay",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  In this paper, we conduct extensive simulations to understand the properties\nof the overlay generated by BitTorrent. We start by analyzing how the overlay\nproperties impact the efficiency of BitTorrent. We focus on the average peer\nset size (i.e., average number of neighbors), the time for a peer to reach its\nmaximum peer set size, and the diameter of the overlay. In particular, we show\nthat the later a peer arrives in a torrent, the longer it takes to reach its\nmaximum peer set size. Then, we evaluate the impact of the maximum peer set\nsize, the maximum number of outgoing connections per peer, and the number of\nNATed peers on the overlay properties. We show that BitTorrent generates a\nrobust overlay, but that this overlay is not a random graph. In particular, the\nconnectivity of a peer to its neighbors depends on its arriving order in the\ntorrent. We also show that a large number of NATed peers significantly\ncompromise the robustness of the overlay to attacks. Finally, we evaluate the\nimpact of peer exchange on the overlay properties, and we show that it\ngenerates a chain-like overlay with a large diameter, which will adversely\nimpact the efficiency of large torrents.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 12 Jul 2007 15:38:14 GMT"
            }
        ],
        "update_date": "2007-07-13",
        "authors_parsed": [
            [
                "Hamra",
                "Anwar Al",
                "",
                "INRIA Sophia Antipolis / INRIA Rh\u00f4ne-Alpes"
            ],
            [
                "Legout",
                "Arnaud",
                "",
                "INRIA Sophia Antipolis / INRIA Rh\u00f4ne-Alpes"
            ],
            [
                "Barakat",
                "Chadi",
                "",
                "INRIA\n  Sophia Antipolis / INRIA Rh\u00f4ne-Alpes"
            ]
        ]
    },
    {
        "id": "0707.2291",
        "submitter": "Marius Marin",
        "authors": "Marius Marin, Leon Moonen, Arie van Deursen",
        "title": "An Integrated Crosscutting Concern Migration Strategy and its\n  Application to JHotDraw",
        "comments": "10+ 4 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": "TUD-SERG-2007-019",
        "categories": "cs.SE",
        "license": null,
        "abstract": "  In this paper we propose a systematic strategy for migrating crosscutting\nconcerns in existing object-oriented systems to aspect-based solutions. The\nproposed strategy consists of four steps: mining, exploration, documentation\nand refactoring of crosscutting concerns. We discuss in detail a new approach\nto aspect refactoring that is fully integrated with our strategy, and apply the\nwhole strategy to an object-oriented system, namely the JHotDraw framework. The\nresult of this migration is made available as an open-source project, which is\nthe largest aspect refactoring available to date. We report on our experiences\nwith conducting this case study and reflect on the success and challenges of\nthe migration process, as well as on the feasibility of automatic aspect\nrefactoring.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 16 Jul 2007 09:38:23 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 22 Jul 2007 22:13:50 GMT"
            }
        ],
        "update_date": "2007-07-23",
        "authors_parsed": [
            [
                "Marin",
                "Marius",
                ""
            ],
            [
                "Moonen",
                "Leon",
                ""
            ],
            [
                "van Deursen",
                "Arie",
                ""
            ]
        ]
    },
    {
        "id": "0707.2293",
        "submitter": "Maziar Nekovee",
        "authors": "Maziar Nekovee",
        "title": "Worm Epidemics in Wireless Adhoc Networks",
        "comments": null,
        "journal-ref": "Published in New J. Phys. 9 189, 2007",
        "doi": "10.1088/1367-2630/9/6/189",
        "report-no": null,
        "categories": "cs.NI cond-mat.stat-mech cs.CR physics.soc-ph",
        "license": null,
        "abstract": "  A dramatic increase in the number of computing devices with wireless\ncommunication capability has resulted in the emergence of a new class of\ncomputer worms which specifically target such devices. The most striking\nfeature of these worms is that they do not require Internet connectivity for\ntheir propagation but can spread directly from device to device using a\nshort-range radio communication technology, such as WiFi or Bluetooth. In this\npaper, we develop a new model for epidemic spreading of these worms and\ninvestigate their spreading in wireless ad hoc networks via extensive Monte\nCarlo simulations. Our studies show that the threshold behaviour and dynamics\nof worm epidemics in these networks are greatly affected by a combination of\nspatial and temporal correlations which characterize these networks, and are\nsignificantly different from the previously studied epidemics in the Internet.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 16 Jul 2007 09:58:18 GMT"
            }
        ],
        "update_date": "2008-07-10",
        "authors_parsed": [
            [
                "Nekovee",
                "Maziar",
                ""
            ]
        ]
    },
    {
        "id": "0707.2293",
        "submitter": "Maziar Nekovee",
        "authors": "Maziar Nekovee",
        "title": "Worm Epidemics in Wireless Adhoc Networks",
        "comments": null,
        "journal-ref": "Published in New J. Phys. 9 189, 2007",
        "doi": "10.1088/1367-2630/9/6/189",
        "report-no": null,
        "categories": "cs.NI cond-mat.stat-mech cs.CR physics.soc-ph",
        "license": null,
        "abstract": "  A dramatic increase in the number of computing devices with wireless\ncommunication capability has resulted in the emergence of a new class of\ncomputer worms which specifically target such devices. The most striking\nfeature of these worms is that they do not require Internet connectivity for\ntheir propagation but can spread directly from device to device using a\nshort-range radio communication technology, such as WiFi or Bluetooth. In this\npaper, we develop a new model for epidemic spreading of these worms and\ninvestigate their spreading in wireless ad hoc networks via extensive Monte\nCarlo simulations. Our studies show that the threshold behaviour and dynamics\nof worm epidemics in these networks are greatly affected by a combination of\nspatial and temporal correlations which characterize these networks, and are\nsignificantly different from the previously studied epidemics in the Internet.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 16 Jul 2007 09:58:18 GMT"
            }
        ],
        "update_date": "2008-07-10",
        "authors_parsed": [
            [
                "Nekovee",
                "Maziar",
                ""
            ]
        ]
    },
    {
        "id": "0707.2347",
        "submitter": "Jean-Guillaume Dumas",
        "authors": "Brice Boyer (LJK), Jean-Guillaume Dumas (LJK), Cl\\'ement Pernet (INRIA\n  Rh\\^one-Alpes / LIG Laboratoire d'Informatique de Grenoble), Wei Zhou\n  (Symbolic Computation Group)",
        "title": "Memory efficient scheduling of Strassen-Winograd's matrix multiplication\n  algorithm",
        "comments": null,
        "journal-ref": "(International Symposium on Symbolic and Algebraic Computation\n  2009), S\\'eoul : Cor\\'ee, R\\'epublique de (2009)",
        "doi": null,
        "report-no": null,
        "categories": "cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose several new schedules for Strassen-Winograd's matrix\nmultiplication algorithm, they reduce the extra memory allocation requirements\nby three different means: by introducing a few pre-additions, by overwriting\nthe input matrices, or by using a first recursive level of classical\nmultiplication. In particular, we show two fully in-place schedules: one having\nthe same number of operations, if the input matrices can be overwritten; the\nother one, slightly increasing the constant of the leading term of the\ncomplexity, if the input matrices are read-only. Many of these schedules have\nbeen found by an implementation of an exhaustive search algorithm based on a\npebble game.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 16 Jul 2007 16:02:50 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 31 Aug 2007 07:31:58 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 23 Nov 2007 16:05:16 GMT"
            },
            {
                "version": "v4",
                "created": "Tue, 27 Jan 2009 09:20:56 GMT"
            },
            {
                "version": "v5",
                "created": "Mon, 18 May 2009 13:49:23 GMT"
            }
        ],
        "update_date": "2009-05-18",
        "authors_parsed": [
            [
                "Boyer",
                "Brice",
                "",
                "LJK"
            ],
            [
                "Dumas",
                "Jean-Guillaume",
                "",
                "LJK"
            ],
            [
                "Pernet",
                "Cl\u00e9ment",
                "",
                "INRIA\n  Rh\u00f4ne-Alpes / LIG Laboratoire d'Informatique de Grenoble"
            ],
            [
                "Zhou",
                "Wei",
                "",
                "Symbolic Computation Group"
            ]
        ]
    },
    {
        "id": "0707.2376",
        "submitter": "Francesc Rossell\\'o",
        "authors": "Gabriel Cardona, Francesc Rossello, Gabriel Valiente",
        "title": "Tripartitions do not always discriminate phylogenetic networks",
        "comments": "26 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-bio.PE cs.CE cs.DM",
        "license": null,
        "abstract": "  Phylogenetic networks are a generalization of phylogenetic trees that allow\nfor the representation of non-treelike evolutionary events, like recombination,\nhybridization, or lateral gene transfer. In a recent series of papers devoted\nto the study of reconstructibility of phylogenetic networks, Moret, Nakhleh,\nWarnow and collaborators introduced the so-called {tripartition metric for\nphylogenetic networks. In this paper we show that, in fact, this tripartition\nmetric does not satisfy the separation axiom of distances (zero distance means\nisomorphism, or, in a more relaxed version, zero distance means\nindistinguishability in some specific sense) in any of the subclasses of\nphylogenetic networks where it is claimed to do so. We also present a subclass\nof phylogenetic networks whose members can be singled out by means of their\nsets of tripartitions (or even clusters), and hence where the latter can be\nused to define a meaningful metric.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 16 Jul 2007 19:59:42 GMT"
            }
        ],
        "update_date": "2007-07-17",
        "authors_parsed": [
            [
                "Cardona",
                "Gabriel",
                ""
            ],
            [
                "Rossello",
                "Francesc",
                ""
            ],
            [
                "Valiente",
                "Gabriel",
                ""
            ]
        ]
    },
    {
        "id": "0707.2432",
        "submitter": "Erhan Bayraktar",
        "authors": "Erhan Bayraktar, Hao Xing",
        "title": "Pricing Asian Options for Jump Diffusions",
        "comments": "Key Words: Pricing Asian Options, Jump diffusions, an Iterative\n  Numerical Scheme, Classical Solutions of Integro-PDEs",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We construct a sequence of functions that uniformly converge (on compact\nsets) to the price of Asian option, which is written on a stock whose dynamics\nfollows a jump diffusion, exponentially fast. Each of the element in this\nsequence solves a parabolic partial differen- tial equation (not an\nintegro-differential equation). As a result we obtain a fast numerical\napproximation scheme whose accuracy versus speed characteristics can be\ncontrolled. We analyze the performance of our numerical algorithm on several\nexamples.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 17 Jul 2007 04:55:18 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 22 Oct 2007 17:35:45 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 6 Dec 2007 03:04:13 GMT"
            },
            {
                "version": "v4",
                "created": "Sat, 12 Jan 2008 18:28:58 GMT"
            },
            {
                "version": "v5",
                "created": "Thu, 22 May 2008 02:58:57 GMT"
            },
            {
                "version": "v6",
                "created": "Sun, 15 Jun 2008 21:36:58 GMT"
            },
            {
                "version": "v7",
                "created": "Wed, 29 Oct 2008 13:49:51 GMT"
            }
        ],
        "update_date": "2008-10-29",
        "authors_parsed": [
            [
                "Bayraktar",
                "Erhan",
                ""
            ],
            [
                "Xing",
                "Hao",
                ""
            ]
        ]
    },
    {
        "id": "0707.2506",
        "submitter": "Alain Dutech",
        "authors": "Raghav Aras (INRIA Lorraine - LORIA), Alain Dutech (INRIA Lorraine -\n  LORIA), Fran\\c{c}ois Charpillet (INRIA Lorraine - LORIA)",
        "title": "Mixed Integer Linear Programming For Exact Finite-Horizon Planning In\n  Decentralized Pomdps",
        "comments": null,
        "journal-ref": "Dans The International Conference on Automated Planning and\n  Scheduling (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  We consider the problem of finding an n-agent joint-policy for the optimal\nfinite-horizon control of a decentralized Pomdp (Dec-Pomdp). This is a problem\nof very high complexity (NEXP-hard in n >= 2). In this paper, we propose a new\nmathematical programming approach for the problem. Our approach is based on two\nideas: First, we represent each agent's policy in the sequence-form and not in\nthe tree-form, thereby obtaining a very compact representation of the set of\njoint-policies. Second, using this compact representation, we solve this\nproblem as an instance of combinatorial optimization for which we formulate a\nmixed integer linear program (MILP). The optimal solution of the MILP directly\nyields an optimal joint-policy for the Dec-Pomdp. Computational experience\nshows that formulating and solving the MILP requires significantly less time to\nsolve benchmark Dec-Pomdp problems than existing algorithms. For example, the\nmulti-agent tiger problem for horizon 4 is solved in 72 secs with the MILP\nwhereas existing algorithms require several hours to solve it.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 17 Jul 2007 12:49:30 GMT"
            }
        ],
        "update_date": "2016-03-28",
        "authors_parsed": [
            [
                "Aras",
                "Raghav",
                "",
                "INRIA Lorraine - LORIA"
            ],
            [
                "Dutech",
                "Alain",
                "",
                "INRIA Lorraine -\n  LORIA"
            ],
            [
                "Charpillet",
                "Fran\u00e7ois",
                "",
                "INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0707.2630",
        "submitter": "Toshiya Takami",
        "authors": "Toshiya Takami, Jun Maki, Jun'ichi Ooba, Yuuichi Inadomi, Hiroaki\n  Honda, Ryutaro Susukita, Koji Inoue, Taizo Kobayashi, Rie Nogita, and Mutsumi\n  Aoyagi",
        "title": "Multi-physics Extension of OpenFMO Framework",
        "comments": "4 pages with 11 figure files, to appear in the Proceedings of ICCMSE\n  2007",
        "journal-ref": "AIP Conf. Proc. 963, 122-125 (2007)",
        "doi": "10.1063/1.2835969",
        "report-no": null,
        "categories": "cs.DC physics.comp-ph",
        "license": null,
        "abstract": "  OpenFMO framework, an open-source software (OSS) platform for Fragment\nMolecular Orbital (FMO) method, is extended to multi-physics simulations (MPS).\nAfter reviewing the several FMO implementations on distributed computer\nenvironments, the subsequent development planning corresponding to MPS is\npresented. It is discussed which should be selected as a scientific software,\nlightweight and reconfigurable form or large and self-contained form.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Jul 2007 05:34:08 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Takami",
                "Toshiya",
                ""
            ],
            [
                "Maki",
                "Jun",
                ""
            ],
            [
                "Ooba",
                "Jun'ichi",
                ""
            ],
            [
                "Inadomi",
                "Yuuichi",
                ""
            ],
            [
                "Honda",
                "Hiroaki",
                ""
            ],
            [
                "Susukita",
                "Ryutaro",
                ""
            ],
            [
                "Inoue",
                "Koji",
                ""
            ],
            [
                "Kobayashi",
                "Taizo",
                ""
            ],
            [
                "Nogita",
                "Rie",
                ""
            ],
            [
                "Aoyagi",
                "Mutsumi",
                ""
            ]
        ]
    },
    {
        "id": "0707.3030",
        "submitter": "Matthias Brust R.",
        "authors": "Gregoire Danoy, Pascal Bouvry, Matthias R. Brust, Enrique Alba",
        "title": "Optimal Design of Ad Hoc Injection Networks by Using Genetic Algorithms",
        "comments": "1 page, 1 figure",
        "journal-ref": "Genetic and Evolutionary Computation Conference (GECCO 2007), ISBN\n  978-1-59593-697-4",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI cs.NI",
        "license": null,
        "abstract": "  This work aims at optimizing injection networks, which consist in adding a\nset of long-range links (called bypass links) in mobile multi-hop ad hoc\nnetworks so as to improve connectivity and overcome network partitioning. To\nthis end, we rely on small-world network properties, that comprise a high\nclustering coefficient and a low characteristic path length. We investigate the\nuse of two genetic algorithms (generational and steady-state) to optimize three\ninstances of this topology control problem and present results that show\ninitial evidence of their capacity to solve it.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 20 Jul 2007 10:07:27 GMT"
            }
        ],
        "update_date": "2007-07-23",
        "authors_parsed": [
            [
                "Danoy",
                "Gregoire",
                ""
            ],
            [
                "Bouvry",
                "Pascal",
                ""
            ],
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Alba",
                "Enrique",
                ""
            ]
        ]
    },
    {
        "id": "0707.3030",
        "submitter": "Matthias Brust R.",
        "authors": "Gregoire Danoy, Pascal Bouvry, Matthias R. Brust, Enrique Alba",
        "title": "Optimal Design of Ad Hoc Injection Networks by Using Genetic Algorithms",
        "comments": "1 page, 1 figure",
        "journal-ref": "Genetic and Evolutionary Computation Conference (GECCO 2007), ISBN\n  978-1-59593-697-4",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI cs.NI",
        "license": null,
        "abstract": "  This work aims at optimizing injection networks, which consist in adding a\nset of long-range links (called bypass links) in mobile multi-hop ad hoc\nnetworks so as to improve connectivity and overcome network partitioning. To\nthis end, we rely on small-world network properties, that comprise a high\nclustering coefficient and a low characteristic path length. We investigate the\nuse of two genetic algorithms (generational and steady-state) to optimize three\ninstances of this topology control problem and present results that show\ninitial evidence of their capacity to solve it.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 20 Jul 2007 10:07:27 GMT"
            }
        ],
        "update_date": "2007-07-23",
        "authors_parsed": [
            [
                "Danoy",
                "Gregoire",
                ""
            ],
            [
                "Bouvry",
                "Pascal",
                ""
            ],
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Alba",
                "Enrique",
                ""
            ]
        ]
    },
    {
        "id": "0707.3030",
        "submitter": "Matthias Brust R.",
        "authors": "Gregoire Danoy, Pascal Bouvry, Matthias R. Brust, Enrique Alba",
        "title": "Optimal Design of Ad Hoc Injection Networks by Using Genetic Algorithms",
        "comments": "1 page, 1 figure",
        "journal-ref": "Genetic and Evolutionary Computation Conference (GECCO 2007), ISBN\n  978-1-59593-697-4",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI cs.NI",
        "license": null,
        "abstract": "  This work aims at optimizing injection networks, which consist in adding a\nset of long-range links (called bypass links) in mobile multi-hop ad hoc\nnetworks so as to improve connectivity and overcome network partitioning. To\nthis end, we rely on small-world network properties, that comprise a high\nclustering coefficient and a low characteristic path length. We investigate the\nuse of two genetic algorithms (generational and steady-state) to optimize three\ninstances of this topology control problem and present results that show\ninitial evidence of their capacity to solve it.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 20 Jul 2007 10:07:27 GMT"
            }
        ],
        "update_date": "2007-07-23",
        "authors_parsed": [
            [
                "Danoy",
                "Gregoire",
                ""
            ],
            [
                "Bouvry",
                "Pascal",
                ""
            ],
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Alba",
                "Enrique",
                ""
            ]
        ]
    },
    {
        "id": "0707.3087",
        "submitter": "Ciamac Moallemi",
        "authors": "Vivek F. Farias, Ciamac C. Moallemi, Tsachy Weissman, Benjamin Van Roy",
        "title": "Universal Reinforcement Learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.LG math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider an agent interacting with an unmodeled environment. At each time,\nthe agent makes an observation, takes an action, and incurs a cost. Its actions\ncan influence future observations and costs. The goal is to minimize the\nlong-term average cost. We propose a novel algorithm, known as the active LZ\nalgorithm, for optimal control based on ideas from the Lempel-Ziv scheme for\nuniversal data compression and prediction. We establish that, under the active\nLZ algorithm, if there exists an integer $K$ such that the future is\nconditionally independent of the past given a window of $K$ consecutive actions\nand observations, then the average cost converges to the optimum. Experimental\nresults involving the game of Rock-Paper-Scissors illustrate merits of the\nalgorithm.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 20 Jul 2007 14:51:39 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 9 Jun 2009 19:41:57 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 22 Jul 2009 00:58:34 GMT"
            }
        ],
        "update_date": "2009-07-22",
        "authors_parsed": [
            [
                "Farias",
                "Vivek F.",
                ""
            ],
            [
                "Moallemi",
                "Ciamac C.",
                ""
            ],
            [
                "Weissman",
                "Tsachy",
                ""
            ],
            [
                "Van Roy",
                "Benjamin",
                ""
            ]
        ]
    },
    {
        "id": "0707.3205",
        "submitter": "Andrew Schumann",
        "authors": "Andrew Schumann, Florentin Smarandache",
        "title": "Neutrality and Many-Valued Logics",
        "comments": "119 pages",
        "journal-ref": "A. Schumann, F. Smarandache, Neutrality and Many-Valued Logics.\n  American Research Press, 2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.AI",
        "license": null,
        "abstract": "  In this book, we consider various many-valued logics: standard, linear,\nhyperbolic, parabolic, non-Archimedean, p-adic, interval, neutrosophic, etc. We\nsurvey also results which show the tree different proof-theoretic frameworks\nfor many-valued logics, e.g. frameworks of the following deductive calculi:\nHilbert's style, sequent, and hypersequent. We present a general way that\nallows to construct systematically analytic calculi for a large family of\nnon-Archimedean many-valued logics: hyperrational-valued, hyperreal-valued, and\np-adic valued logics characterized by a special format of semantics with an\nappropriate rejection of Archimedes' axiom. These logics are built as different\nextensions of standard many-valued logics (namely, Lukasiewicz's, Goedel's,\nProduct, and Post's logics). The informal sense of Archimedes' axiom is that\nanything can be measured by a ruler. Also logical multiple-validity without\nArchimedes' axiom consists in that the set of truth values is infinite and it\nis not well-founded and well-ordered. On the base of non-Archimedean valued\nlogics, we construct non-Archimedean valued interval neutrosophic logic INL by\nwhich we can describe neutrality phenomena.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 21 Jul 2007 10:35:37 GMT"
            }
        ],
        "update_date": "2007-07-24",
        "authors_parsed": [
            [
                "Schumann",
                "Andrew",
                ""
            ],
            [
                "Smarandache",
                "Florentin",
                ""
            ]
        ]
    },
    {
        "id": "0707.3263",
        "submitter": "Wojciech Wislicki",
        "authors": "Wojciech Wislicki",
        "title": "Autonomous tools for Grid management, monitoring and optimization",
        "comments": "The original version of this proposal was created on 22nd March 2006,\n  published as the ICM UW preprint and registered in the bibliographic database\n  of the University of Warsaw on the following Internet address:\n  http://bibliografia.icm.edu.pl/g2/main.pl?mod=p&id=51470&t=1&tytul=Autonomous&lim=100&ord=1",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.CE hep-ex",
        "license": null,
        "abstract": "  We outline design and lines of development of autonomous tools for the\ncomputing Grid management, monitoring and optimization. The management is\nproposed to be based on the notion of utility. Grid optimization is considered\nto be application-oriented. A generic Grid simulator is proposed as an\noptimization tool for Grid structure and functionality.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 22 Jul 2007 14:02:21 GMT"
            }
        ],
        "update_date": "2009-07-09",
        "authors_parsed": [
            [
                "Wislicki",
                "Wojciech",
                ""
            ]
        ]
    },
    {
        "id": "0707.3263",
        "submitter": "Wojciech Wislicki",
        "authors": "Wojciech Wislicki",
        "title": "Autonomous tools for Grid management, monitoring and optimization",
        "comments": "The original version of this proposal was created on 22nd March 2006,\n  published as the ICM UW preprint and registered in the bibliographic database\n  of the University of Warsaw on the following Internet address:\n  http://bibliografia.icm.edu.pl/g2/main.pl?mod=p&id=51470&t=1&tytul=Autonomous&lim=100&ord=1",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.CE hep-ex",
        "license": null,
        "abstract": "  We outline design and lines of development of autonomous tools for the\ncomputing Grid management, monitoring and optimization. The management is\nproposed to be based on the notion of utility. Grid optimization is considered\nto be application-oriented. A generic Grid simulator is proposed as an\noptimization tool for Grid structure and functionality.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 22 Jul 2007 14:02:21 GMT"
            }
        ],
        "update_date": "2009-07-09",
        "authors_parsed": [
            [
                "Wislicki",
                "Wojciech",
                ""
            ]
        ]
    },
    {
        "id": "0707.3390",
        "submitter": "Francis Bach",
        "authors": "Francis Bach (WILLOW Project - Inria/Ens)",
        "title": "Consistency of the group Lasso and multiple kernel learning",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  We consider the least-square regression problem with regularization by a\nblock 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger\nthan one. This problem, referred to as the group Lasso, extends the usual\nregularization by the 1-norm where all spaces have dimension one, where it is\ncommonly referred to as the Lasso. In this paper, we study the asymptotic model\nconsistency of the group Lasso. We derive necessary and sufficient conditions\nfor the consistency of group Lasso under practical assumptions, such as model\nmisspecification. When the linear predictors and Euclidean norms are replaced\nby functions and reproducing kernel Hilbert norms, the problem is usually\nreferred to as multiple kernel learning and is commonly used for learning from\nheterogeneous data sources and for non linear variable selection. Using tools\nfrom functional analysis, and in particular covariance operators, we extend the\nconsistency results to this infinite dimensional case and also propose an\nadaptive scheme to obtain a consistent model estimate, even when the necessary\ncondition required for the non adaptive scheme is not satisfied.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Jul 2007 14:35:20 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 28 Jan 2008 10:10:31 GMT"
            }
        ],
        "update_date": "2008-01-28",
        "authors_parsed": [
            [
                "Bach",
                "Francis",
                "",
                "WILLOW Project - Inria/Ens"
            ]
        ]
    },
    {
        "id": "0707.3409",
        "submitter": "Alexander Tiskin",
        "authors": "Alexander Tiskin",
        "title": "Faster exon assembly by sparse spliced alignment",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.CC cs.CE q-bio.QM",
        "license": null,
        "abstract": "  Assembling a gene from candidate exons is an important problem in\ncomputational biology. Among the most successful approaches to this problem is\n\\emph{spliced alignment}, proposed by Gelfand et al., which scores different\ncandidate exon chains within a DNA sequence of length $m$ by comparing them to\na known related gene sequence of length n, $m = \\Theta(n)$. Gelfand et al.\\\ngave an algorithm for spliced alignment running in time O(n^3). Kent et al.\\\nconsidered sparse spliced alignment, where the number of candidate exons is\nO(n), and proposed an algorithm for this problem running in time O(n^{2.5}). We\nimprove on this result, by proposing an algorithm for sparse spliced alignment\nrunning in time O(n^{2.25}). Our approach is based on a new framework of\n\\emph{quasi-local string comparison}.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Jul 2007 16:35:54 GMT"
            }
        ],
        "update_date": "2007-07-24",
        "authors_parsed": [
            [
                "Tiskin",
                "Alexander",
                ""
            ]
        ]
    },
    {
        "id": "0707.3435",
        "submitter": "Joseph Y. Halpern",
        "authors": "Joseph Y. Halpern and Sabina Petride",
        "title": "A Knowledge-Based Analysis of Global Function Computation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.LO",
        "license": null,
        "abstract": "  Consider a distributed system N in which each agent has an input value and\neach communication link has a weight. Given a global function, that is, a\nfunction f whose value depends on the whole network, the goal is for every\nagent to eventually compute the value f(N). We call this problem global\nfunction computation. Various solutions for instances of this problem, such as\nBoolean function computation, leader election, (minimum) spanning tree\nconstruction, and network determination, have been proposed, each under\nparticular assumptions about what processors know about the system and how this\nknowledge can be acquired. We give a necessary and sufficient condition for the\nproblem to be solvable that generalizes a number of well-known results. We then\nprovide a knowledge-based (kb) program (like those of Fagin, Halpern, Moses,\nand Vardi) that solves global function computation whenever possible. Finally,\nwe improve the message overhead inherent in our initial kb program by giving a\ncounterfactual belief-based program that also solves the global function\ncomputation whenever possible, but where agents send messages only when they\nbelieve it is necessary to do so. The latter program is shown to be implemented\nby a number of well-known algorithms for solving leader election.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Jul 2007 18:49:28 GMT"
            }
        ],
        "update_date": "2007-08-08",
        "authors_parsed": [
            [
                "Halpern",
                "Joseph Y.",
                ""
            ],
            [
                "Petride",
                "Sabina",
                ""
            ]
        ]
    },
    {
        "id": "0707.3457",
        "submitter": "Chenguang Lu",
        "authors": "Chenguang Lu",
        "title": "A Generalized Information Formula as the Bridge between Shannon and\n  Popper",
        "comments": "8 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.AI math.IT",
        "license": null,
        "abstract": "  A generalized information formula related to logical probability and fuzzy\nset is deduced from the classical information formula. The new information\nmeasure accords with to Popper's criterion for knowledge evolution very much.\nIn comparison with square error criterion, the information criterion does not\nonly reflect error of a proposition, but also reflects the particularity of the\nevent described by the proposition. It gives a proposition with less logical\nprobability higher evaluation. The paper introduces how to select a prediction\nor sentence from many for forecasts and language translations according to the\ngeneralized information criterion. It also introduces the rate fidelity theory,\nwhich comes from the improvement of the rate distortion theory in the classical\ninformation theory by replacing distortion (i.e. average error) criterion with\nthe generalized mutual information criterion, for data compression and\ncommunication efficiency. Some interesting conclusions are obtained from the\nrate-fidelity function in relation to image communication. It also discusses\nhow to improve Popper's theory.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jul 2007 00:04:32 GMT"
            }
        ],
        "update_date": "2007-07-25",
        "authors_parsed": [
            [
                "Lu",
                "Chenguang",
                ""
            ]
        ]
    },
    {
        "id": "0707.3479",
        "submitter": "Alp Atici",
        "authors": "Alp Atici, Rocco A. Servedio",
        "title": "Quantum Algorithms for Learning and Testing Juntas",
        "comments": "15 pages, 1 figure. Uses synttree package. To appear in Quantum\n  Information Processing",
        "journal-ref": "Quantum Information Processing, Vol. 6, No. 5, 323 - 348 (2007)",
        "doi": "10.1007/s11128-007-0061-6",
        "report-no": null,
        "categories": "quant-ph cs.LG",
        "license": null,
        "abstract": "  In this article we develop quantum algorithms for learning and testing\njuntas, i.e. Boolean functions which depend only on an unknown set of k out of\nn input variables. Our aim is to develop efficient algorithms:\n  - whose sample complexity has no dependence on n, the dimension of the domain\nthe Boolean functions are defined over;\n  - with no access to any classical or quantum membership (\"black-box\")\nqueries. Instead, our algorithms use only classical examples generated\nuniformly at random and fixed quantum superpositions of such classical\nexamples;\n  - which require only a few quantum examples but possibly many classical\nrandom examples (which are considered quite \"cheap\" relative to quantum\nexamples).\n  Our quantum algorithms are based on a subroutine FS which enables sampling\naccording to the Fourier spectrum of f; the FS subroutine was used in earlier\nwork of Bshouty and Jackson on quantum learning. Our results are as follows:\n  - We give an algorithm for testing k-juntas to accuracy $\\epsilon$ that uses\n$O(k/\\epsilon)$ quantum examples. This improves on the number of examples used\nby the best known classical algorithm.\n  - We establish the following lower bound: any FS-based k-junta testing\nalgorithm requires $\\Omega(\\sqrt{k})$ queries.\n  - We give an algorithm for learning $k$-juntas to accuracy $\\epsilon$ that\nuses $O(\\epsilon^{-1} k\\log k)$ quantum examples and $O(2^k \\log(1/\\epsilon))$\nrandom examples. We show that this learning algorithms is close to optimal by\ngiving a related lower bound.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jul 2007 13:17:55 GMT"
            }
        ],
        "update_date": "2007-10-16",
        "authors_parsed": [
            [
                "Atici",
                "Alp",
                ""
            ],
            [
                "Servedio",
                "Rocco A.",
                ""
            ]
        ]
    },
    {
        "id": "0707.3482",
        "submitter": "Kenton K. Yee",
        "authors": "Kenton K. Yee",
        "title": "A Bayesian Framework for Combining Valuation Estimates",
        "comments": "Citations at\n  http://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=240309 Review of\n  Quantitative Finance and Accounting, 30.3 (2008) forthcoming",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.ST cs.CE nlin.AO nlin.CD nlin.SI physics.pop-ph physics.soc-ph stat.AP",
        "license": null,
        "abstract": "  Obtaining more accurate equity value estimates is the starting point for\nstock selection, value-based indexing in a noisy market, and beating benchmark\nindices through tactical style rotation. Unfortunately, discounted cash flow,\nmethod of comparables, and fundamental analysis typically yield discrepant\nvaluation estimates. Moreover, the valuation estimates typically disagree with\nmarket price. Can one form a superior valuation estimate by averaging over the\nindividual estimates, including market price? This article suggests a Bayesian\nframework for combining two or more estimates into a superior valuation\nestimate. The framework justifies the common practice of averaging over several\nestimates to arrive at a final point estimate.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jul 2007 05:04:53 GMT"
            }
        ],
        "update_date": "2008-12-02",
        "authors_parsed": [
            [
                "Yee",
                "Kenton K.",
                ""
            ]
        ]
    },
    {
        "id": "0707.3509",
        "submitter": "Laurent Decreusefond",
        "authors": "Laurent Decreusefond (LTCI), Eduardo Ferraz (LTCI), Philippe Martins\n  (LTCI)",
        "title": "Upper bound of loss probability in an OFDMA system with randomly located\n  users",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.PR cs.IT cs.NI math.IT",
        "license": null,
        "abstract": "  For OFDMA systems, we find a rough but easily computed upper bound for the\nprobability of loosing communications by insufficient number of sub-channels on\ndownlink. We consider as random the positions of receiving users in the system\nas well as the number of sub-channels dedicated to each one. We use recent\nresults of the theory of point processes which reduce our calculations to the\nfirst and second moments of the total required number of sub-carriers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jul 2007 15:12:13 GMT"
            }
        ],
        "update_date": "2007-07-25",
        "authors_parsed": [
            [
                "Decreusefond",
                "Laurent",
                "",
                "LTCI"
            ],
            [
                "Ferraz",
                "Eduardo",
                "",
                "LTCI"
            ],
            [
                "Martins",
                "Philippe",
                "",
                "LTCI"
            ]
        ]
    },
    {
        "id": "0707.3531",
        "submitter": "Luis Nunez A",
        "authors": "J. L. Chaves, G. Diaz, V. Hamar, R. Isea, F. Rojas, N. Ruiz, R.\n  Torrens, M. Uzcategui, J. Florez-Lopez, H. Hoeger, C. Mendoza, L. A. Nunez",
        "title": "e-Science initiatives in Venezuela",
        "comments": "9 pages, 4 figures",
        "journal-ref": "Procceedings Spanish Conference on e-Science Grid Computing, J.\n  Casado, R. Mayo y R. Munoz (Editors) CIEMAT, Madrid Spain (2007), pp 45 - 52",
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.DC",
        "license": null,
        "abstract": "  Within the context of the nascent e-Science infrastructure in Venezuela, we\ndescribe several web-based scientific applications developed at the Centro\nNacional de Calculo Cientifico Universidad de Los Andes (CeCalCULA), Merida,\nand at the Instituto Venezolano de Investigaciones Cientificas (IVIC), Caracas.\nThe different strategies that have been followed for implementing quantum\nchemistry and atomic physics applications are presented. We also briefly\ndiscuss a damage portal based on dynamic, nonlinear, finite elements of lumped\ndamage mechanics and a biomedical portal developed within the framework of the\n\\textit{E-Infrastructure shared between Europe and Latin America} (EELA)\ninitiative for searching common sequences and inferring their functions in\nparasitic diseases such as leishmaniasis, chagas and malaria.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jul 2007 12:00:43 GMT"
            }
        ],
        "update_date": "2007-07-25",
        "authors_parsed": [
            [
                "Chaves",
                "J. L.",
                ""
            ],
            [
                "Diaz",
                "G.",
                ""
            ],
            [
                "Hamar",
                "V.",
                ""
            ],
            [
                "Isea",
                "R.",
                ""
            ],
            [
                "Rojas",
                "F.",
                ""
            ],
            [
                "Ruiz",
                "N.",
                ""
            ],
            [
                "Torrens",
                "R.",
                ""
            ],
            [
                "Uzcategui",
                "M.",
                ""
            ],
            [
                "Florez-Lopez",
                "J.",
                ""
            ],
            [
                "Hoeger",
                "H.",
                ""
            ],
            [
                "Mendoza",
                "C.",
                ""
            ],
            [
                "Nunez",
                "L. A.",
                ""
            ]
        ]
    },
    {
        "id": "0707.3531",
        "submitter": "Luis Nunez A",
        "authors": "J. L. Chaves, G. Diaz, V. Hamar, R. Isea, F. Rojas, N. Ruiz, R.\n  Torrens, M. Uzcategui, J. Florez-Lopez, H. Hoeger, C. Mendoza, L. A. Nunez",
        "title": "e-Science initiatives in Venezuela",
        "comments": "9 pages, 4 figures",
        "journal-ref": "Procceedings Spanish Conference on e-Science Grid Computing, J.\n  Casado, R. Mayo y R. Munoz (Editors) CIEMAT, Madrid Spain (2007), pp 45 - 52",
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.DC",
        "license": null,
        "abstract": "  Within the context of the nascent e-Science infrastructure in Venezuela, we\ndescribe several web-based scientific applications developed at the Centro\nNacional de Calculo Cientifico Universidad de Los Andes (CeCalCULA), Merida,\nand at the Instituto Venezolano de Investigaciones Cientificas (IVIC), Caracas.\nThe different strategies that have been followed for implementing quantum\nchemistry and atomic physics applications are presented. We also briefly\ndiscuss a damage portal based on dynamic, nonlinear, finite elements of lumped\ndamage mechanics and a biomedical portal developed within the framework of the\n\\textit{E-Infrastructure shared between Europe and Latin America} (EELA)\ninitiative for searching common sequences and inferring their functions in\nparasitic diseases such as leishmaniasis, chagas and malaria.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jul 2007 12:00:43 GMT"
            }
        ],
        "update_date": "2007-07-25",
        "authors_parsed": [
            [
                "Chaves",
                "J. L.",
                ""
            ],
            [
                "Diaz",
                "G.",
                ""
            ],
            [
                "Hamar",
                "V.",
                ""
            ],
            [
                "Isea",
                "R.",
                ""
            ],
            [
                "Rojas",
                "F.",
                ""
            ],
            [
                "Ruiz",
                "N.",
                ""
            ],
            [
                "Torrens",
                "R.",
                ""
            ],
            [
                "Uzcategui",
                "M.",
                ""
            ],
            [
                "Florez-Lopez",
                "J.",
                ""
            ],
            [
                "Hoeger",
                "H.",
                ""
            ],
            [
                "Mendoza",
                "C.",
                ""
            ],
            [
                "Nunez",
                "L. A.",
                ""
            ]
        ]
    },
    {
        "id": "0707.3559",
        "submitter": "Wilson Wong",
        "authors": "Wilson Wong",
        "title": "Practical Approach to Knowledge-based Question Answering with Natural\n  Language Understanding and Advanced Reasoning",
        "comments": "Master of Science thesis, National Technical University College of\n  Malaysia, 2005",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CL cs.AI cs.HC cs.IR",
        "license": null,
        "abstract": "  This research hypothesized that a practical approach in the form of a\nsolution framework known as Natural Language Understanding and Reasoning for\nIntelligence (NaLURI), which combines full-discourse natural language\nunderstanding, powerful representation formalism capable of exploiting\nontological information and reasoning approach with advanced features, will\nsolve the following problems without compromising practicality factors: 1)\nrestriction on the nature of question and response, and 2) limitation to scale\nacross domains and to real-life natural language text.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jul 2007 14:30:27 GMT"
            }
        ],
        "update_date": "2007-07-25",
        "authors_parsed": [
            [
                "Wong",
                "Wilson",
                ""
            ]
        ]
    },
    {
        "id": "0707.3559",
        "submitter": "Wilson Wong",
        "authors": "Wilson Wong",
        "title": "Practical Approach to Knowledge-based Question Answering with Natural\n  Language Understanding and Advanced Reasoning",
        "comments": "Master of Science thesis, National Technical University College of\n  Malaysia, 2005",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CL cs.AI cs.HC cs.IR",
        "license": null,
        "abstract": "  This research hypothesized that a practical approach in the form of a\nsolution framework known as Natural Language Understanding and Reasoning for\nIntelligence (NaLURI), which combines full-discourse natural language\nunderstanding, powerful representation formalism capable of exploiting\nontological information and reasoning approach with advanced features, will\nsolve the following problems without compromising practicality factors: 1)\nrestriction on the nature of question and response, and 2) limitation to scale\nacross domains and to real-life natural language text.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jul 2007 14:30:27 GMT"
            }
        ],
        "update_date": "2007-07-25",
        "authors_parsed": [
            [
                "Wong",
                "Wilson",
                ""
            ]
        ]
    },
    {
        "id": "0707.3638",
        "submitter": "Vita Hinze-Hoare",
        "authors": "V. Hinze-Hoare",
        "title": "The Review and Analysis of Human Computer Interaction (HCI) Principles",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  The History of HCI is briefly reviewed together with three HCI models and\nstructure including CSCW, CSCL and CSCR. It is shown that a number of\nauthorities consider HCI to be a fragmented discipline with no agreed set of\nunifying design principles. An analysis of usability criteria based upon\ncitation frequency of authors is performed in order to discover the eight most\nrecognised HCI principles.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jul 2007 20:19:47 GMT"
            }
        ],
        "update_date": "2007-07-26",
        "authors_parsed": [
            [
                "Hinze-Hoare",
                "V.",
                ""
            ]
        ]
    },
    {
        "id": "0707.3670",
        "submitter": "Xu Cheng",
        "authors": "Xu Cheng and Cameron Dale and Jiangchuan Liu",
        "title": "Understanding the Characteristics of Internet Short Video Sharing:\n  YouTube as a Case Study",
        "comments": "IEEE format, 9 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.MM",
        "license": null,
        "abstract": "  Established in 2005, YouTube has become the most successful Internet site\nproviding a new generation of short video sharing service. Today, YouTube alone\ncomprises approximately 20% of all HTTP traffic, or nearly 10% of all traffic\non the Internet. Understanding the features of YouTube and similar video\nsharing sites is thus crucial to their sustainable development and to network\ntraffic engineering. In this paper, using traces crawled in a 3-month period,\nwe present an in-depth and systematic measurement study on the characteristics\nof YouTube videos. We find that YouTube videos have noticeably different\nstatistics compared to traditional streaming videos, ranging from length and\naccess pattern, to their active life span, ratings, and comments. The series of\ndatasets also allows us to identify the growth trend of this fast evolving\nInternet site in various aspects, which has seldom been explored before. We\nalso look closely at the social networking aspect of YouTube, as this is a key\ndriving force toward its success. In particular, we find that the links to\nrelated videos generated by uploaders' choices form a small-world network. This\nsuggests that the videos have strong correlations with each other, and creates\nopportunities for developing novel caching or peer-to-peer distribution schemes\nto efficiently deliver videos to end users.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Jul 2007 05:39:44 GMT"
            }
        ],
        "update_date": "2007-07-26",
        "authors_parsed": [
            [
                "Cheng",
                "Xu",
                ""
            ],
            [
                "Dale",
                "Cameron",
                ""
            ],
            [
                "Liu",
                "Jiangchuan",
                ""
            ]
        ]
    },
    {
        "id": "0707.3717",
        "submitter": "Yann Busnel",
        "authors": "Yann Busnel (IRISA, DIT), Marin Bertier (IRISA), Eric Fleury (INRIA\n  Rh\\^one-Alpes), Anne-Marie Kermarrec (IRISA)",
        "title": "GCP: Gossip-based Code Propagation for Large-scale Mobile Wireless\n  Sensor Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  Wireless sensor networks (WSN) have recently received an increasing interest.\nThey are now expected to be deployed for long periods of time, thus requiring\nsoftware updates. Updating the software code automatically on a huge number of\nsensors is a tremendous task, as ''by hand'' updates can obviously not be\nconsidered, especially when all participating sensors are embedded on mobile\nentities. In this paper, we investigate an approach to automatically update\nsoftware in mobile sensor-based application when no localization mechanism is\navailable. We leverage the peer-to-peer cooperation paradigm to achieve a good\ntrade-off between reliability and scalability of code propagation. More\nspecifically, we present the design and evaluation of GCP ({\\emph Gossip-based\nCode Propagation}), a distributed software update algorithm for mobile wireless\nsensor networks. GCP relies on two different mechanisms (piggy-backing and\nforwarding control) to improve significantly the load balance without\nsacrificing on the propagation speed. We compare GCP against traditional\ndissemination approaches. Simulation results based on both synthetic and\nrealistic workloads show that GCP achieves a good convergence speed while\nbalancing the load evenly between sensors.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Jul 2007 11:52:09 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 30 Jul 2007 12:21:45 GMT"
            }
        ],
        "update_date": "2007-07-30",
        "authors_parsed": [
            [
                "Busnel",
                "Yann",
                "",
                "IRISA, DIT"
            ],
            [
                "Bertier",
                "Marin",
                "",
                "IRISA"
            ],
            [
                "Fleury",
                "Eric",
                "",
                "INRIA\n  Rh\u00f4ne-Alpes"
            ],
            [
                "Kermarrec",
                "Anne-Marie",
                "",
                "IRISA"
            ]
        ]
    },
    {
        "id": "0707.3750",
        "submitter": "Guillaume Collet",
        "authors": "Rumen Andonov (IRISA), Guillaume Collet (IRISA), Jean-Fran\\c{c}ois\n  Gibrat (MIG), Antoine Marin (MIG), Vincent Poirriez (LAMIH), Nikola Yanev\n  (IRISA)",
        "title": "Recent Advances in Solving the Protein Threading Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-bio.QM cs.DC",
        "license": null,
        "abstract": "  The fold recognition methods are promissing tools for capturing the structure\nof a protein by its amino acid residues sequence but their use is still\nrestricted by the needs of huge computational resources and suitable efficient\nalgorithms as well. In the recent version of FROST (Fold Recognition Oriented\nSearch Tool) package the most efficient algorithm for solving the Protein\nThreading Problem (PTP) is implemented due to the strong collaboration between\nthe SYMBIOSE group in IRISA and MIG in Jouy-en-Josas. In this paper, we present\nthe diverse components of FROST, emphasizing on the recent advances in\nformulating and solving new versions of the PTP and on the way of solving on a\ncomputer cluster a million of instances in a easonable time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Jul 2007 14:05:59 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 30 Jul 2007 12:45:26 GMT"
            }
        ],
        "update_date": "2007-07-30",
        "authors_parsed": [
            [
                "Andonov",
                "Rumen",
                "",
                "IRISA"
            ],
            [
                "Collet",
                "Guillaume",
                "",
                "IRISA"
            ],
            [
                "Gibrat",
                "Jean-Fran\u00e7ois",
                "",
                "MIG"
            ],
            [
                "Marin",
                "Antoine",
                "",
                "MIG"
            ],
            [
                "Poirriez",
                "Vincent",
                "",
                "LAMIH"
            ],
            [
                "Yanev",
                "Nikola",
                "",
                "IRISA"
            ]
        ]
    },
    {
        "id": "0707.3781",
        "submitter": "Paolo Liberatore",
        "authors": "Paolo Liberatore",
        "title": "Bijective Faithful Translations among Default Logics",
        "comments": "Removed one useless section",
        "journal-ref": null,
        "doi": "10.1093/logcom/ext073",
        "report-no": null,
        "categories": "cs.AI cs.LO",
        "license": null,
        "abstract": "  In this article, we study translations between variants of defaults logics\nsuch that the extensions of the theories that are the input and the output of\nthe translation are in a bijective correspondence. We assume that a translation\ncan introduce new variables and that the result of translating a theory can\neither be produced in time polynomial in the size of the theory or its output\nis polynomial in that size; we however restrict to the case in which the\noriginal theory has extensions. This study fills a gap between two previous\npieces of work, one studying bijective translations among restrictions of\ndefault logics, and the other one studying non-bijective translations between\ndefault logics variants.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Jul 2007 17:03:57 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 30 Jul 2007 13:46:43 GMT"
            }
        ],
        "update_date": "2021-04-12",
        "authors_parsed": [
            [
                "Liberatore",
                "Paolo",
                ""
            ]
        ]
    },
    {
        "id": "0707.3807",
        "submitter": "Catherine Recanati",
        "authors": "Catherine Recanati (LIPN)",
        "title": "How to be correct, lazy and efficient ?",
        "comments": "21 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  This paper is an introduction to Lambdix, a lazy Lisp interpreter implemented\nat the Research Laboratory of Paris XI University (Laboratoire de Recherche en\nInformatique, Orsay). Lambdix was devised in the course of an investigation\ninto the relationship between the semantics of programming languages and their\nimplementation; it was used to demonstrate that in the Lisp domain, semantic\ncorrectness is consistent with efficiency, contrary to what has often been\nclaimed. The first part of the paper is an overview of well-known semantic\ndifficulties encountered by Lisp as well as an informal presentation of\nLambdix; it is shown that the difficulties which Lisp encouters do not arise in\nLambdix. The second part is about efficiency in implementation models. It\nexplains why Lambdix is better suited for lazy evaluation than previous models.\nThe section ends by giving comparative execution time tables.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Jul 2007 19:33:07 GMT"
            }
        ],
        "update_date": "2007-07-26",
        "authors_parsed": [
            [
                "Recanati",
                "Catherine",
                "",
                "LIPN"
            ]
        ]
    },
    {
        "id": "0707.3936",
        "submitter": "Konstantin Avrachenkov",
        "authors": "Eitan Altman (INRIA Sophia Antipolis), Konstantin Avrachenkov (INRIA\n  Sophia Antipolis), Andrey Garnaev",
        "title": "Closed form solutions for symmetric water filling games",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.GT",
        "license": null,
        "abstract": "  We study power control in optimization and game frameworks. In the\noptimization framework there is a single decision maker who assigns network\nresources and in the game framework users share the network resources according\nto Nash equilibrium. The solution of these problems is based on so-called\nwater-filling technique, which in turn uses bisection method for solution of\nnon-linear equations for Lagrange multiplies. Here we provide a closed form\nsolution to the water-filling problem, which allows us to solve it in a finite\nnumber of operations. Also, we produce a closed form solution for the Nash\nequilibrium in symmetric Gaussian interference game with an arbitrary number of\nusers. Even though the game is symmetric, there is an intrinsic hierarchical\nstructure induced by the quantity of the resources available to the users. We\nuse this hierarchical structure to perform a successive reduction of the game.\nIn addition, to its mathematical beauty, the explicit solution allows one to\nstudy limiting cases when the crosstalk coefficient is either small or large.\nWe provide an alternative simple proof of the convergence of the Iterative\nWater Filling Algorithm. Furthermore, it turns out that the convergence of\nIterative Water Filling Algorithm slows down when the crosstalk coefficient is\nlarge. Using the closed form solution, we can avoid this problem. Finally, we\ncompare the non-cooperative approach with the cooperative approach and show\nthat the non-cooperative approach results in a more fair resource distribution.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 26 Jul 2007 14:30:50 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 30 Jul 2007 12:40:05 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Altman",
                "Eitan",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Avrachenkov",
                "Konstantin",
                "",
                "INRIA\n  Sophia Antipolis"
            ],
            [
                "Garnaev",
                "Andrey",
                ""
            ]
        ]
    },
    {
        "id": "0707.3972",
        "submitter": "Ted Pedersen",
        "authors": "Ted Pedersen",
        "title": "Learning Probabilistic Models of Word Sense Disambiguation",
        "comments": "195 pages",
        "journal-ref": "PhD dissertation, May 1998, Department of Computer Science and\n  Engineering, Southern Methodist University",
        "doi": null,
        "report-no": null,
        "categories": "cs.CL cs.AI",
        "license": null,
        "abstract": "  This dissertation presents several new methods of supervised and unsupervised\nlearning of word sense disambiguation models. The supervised methods focus on\nperforming model searches through a space of probabilistic models, and the\nunsupervised methods rely on the use of Gibbs Sampling and the Expectation\nMaximization (EM) algorithm. In both the supervised and unsupervised case, the\nNaive Bayesian model is found to perform well. An explanation for this success\nis presented in terms of learning rates and bias-variance decompositions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 26 Jul 2007 17:02:40 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Pedersen",
                "Ted",
                ""
            ]
        ]
    },
    {
        "id": "0707.3979",
        "submitter": "Refugio Vallejo",
        "authors": "Isidro B. Nieto and J. Refugio Vallejo",
        "title": "Clifford Algebra of the Vector Space of Conics for decision boundary\n  Hyperplanes in m-Euclidean Space",
        "comments": "12 pages, 2 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CG",
        "license": null,
        "abstract": "  In this paper we embed $m$-dimensional Euclidean space in the geometric\nalgebra $Cl_m $ to extend the operators of incidence in ${R^m}$ to operators of\nincidence in the geometric algebra to generalize the notion of separator to a\ndecision boundary hyperconic in the Clifford algebra of hyperconic sections\ndenoted as ${Cl}({Co}_{2})$. This allows us to extend the concept of a linear\nperceptron or the spherical perceptron in conformal geometry and introduce the\nmore general conic perceptron, namely the {elliptical perceptron}. Using\nClifford duality a vector orthogonal to the decision boundary hyperplane is\ndetermined. Experimental results are shown in 2-dimensional Euclidean space\nwhere we separate data that are naturally separated by some typical plane conic\nseparators by this procedure. This procedure is more general in the sense that\nit is independent of the dimension of the input data and hence we can speak of\nthe hyperconic elliptic perceptron.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 26 Jul 2007 18:03:23 GMT"
            }
        ],
        "update_date": "2007-07-27",
        "authors_parsed": [
            [
                "Nieto",
                "Isidro B.",
                ""
            ],
            [
                "Vallejo",
                "J. Refugio",
                ""
            ]
        ]
    },
    {
        "id": "0707.4032",
        "submitter": "Shiguo Lian",
        "authors": "Shiguo Lian, Jinsheng Sun, Zhiquan Wang",
        "title": "One-way Hash Function Based on Neural Network",
        "comments": "7 pages,5 figures,submitted",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.NE",
        "license": null,
        "abstract": "  A hash function is constructed based on a three-layer neural network. The\nthree neuron-layers are used to realize data confusion, diffusion and\ncompression respectively, and the multi-block hash mode is presented to support\nthe plaintext with variable length. Theoretical analysis and experimental\nresults show that this hash function is one-way, with high key sensitivity and\nplaintext sensitivity, and secure against birthday attacks or\nmeet-in-the-middle attacks. Additionally, the neural network's property makes\nit practical to realize in a parallel way. These properties make it a suitable\nchoice for data signature or authentication.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Jul 2007 02:38:56 GMT"
            }
        ],
        "update_date": "2007-07-30",
        "authors_parsed": [
            [
                "Lian",
                "Shiguo",
                ""
            ],
            [
                "Sun",
                "Jinsheng",
                ""
            ],
            [
                "Wang",
                "Zhiquan",
                ""
            ]
        ]
    },
    {
        "id": "0707.4032",
        "submitter": "Shiguo Lian",
        "authors": "Shiguo Lian, Jinsheng Sun, Zhiquan Wang",
        "title": "One-way Hash Function Based on Neural Network",
        "comments": "7 pages,5 figures,submitted",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.NE",
        "license": null,
        "abstract": "  A hash function is constructed based on a three-layer neural network. The\nthree neuron-layers are used to realize data confusion, diffusion and\ncompression respectively, and the multi-block hash mode is presented to support\nthe plaintext with variable length. Theoretical analysis and experimental\nresults show that this hash function is one-way, with high key sensitivity and\nplaintext sensitivity, and secure against birthday attacks or\nmeet-in-the-middle attacks. Additionally, the neural network's property makes\nit practical to realize in a parallel way. These properties make it a suitable\nchoice for data signature or authentication.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Jul 2007 02:38:56 GMT"
            }
        ],
        "update_date": "2007-07-30",
        "authors_parsed": [
            [
                "Lian",
                "Shiguo",
                ""
            ],
            [
                "Sun",
                "Jinsheng",
                ""
            ],
            [
                "Wang",
                "Zhiquan",
                ""
            ]
        ]
    },
    {
        "id": "0707.4166",
        "submitter": "Todd Veldhuizen",
        "authors": "Todd L. Veldhuizen",
        "title": "Parsimony Principles for Software Components and Metalanguages",
        "comments": "Generative Programming and Component Engineering 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  Software is a communication system. The usual topic of communication is\nprogram behavior, as encoded by programs. Domain-specific libraries are\ncodebooks, domain-specific languages are coding schemes, and so forth. To turn\nmetaphor into method, we adapt toolsfrom information theory--the study of\nefficient communication--to probe the efficiency with which languages and\nlibraries let us communicate programs. In previous work we developed an\ninformation-theoretic analysis of software reuse in problem domains. This new\npaper uses information theory to analyze tradeoffs in the design of components,\ngenerators, and metalanguages. We seek answers to two questions: (1) How can we\njudge whether a component is over- or under-generalized? Drawing on minimum\ndescription length principles, we propose that the best component yields the\nmost succinct representation of the use cases. (2) If we view a programming\nlanguage as an assemblage of metalanguages, each providing a complementary\nstyle of abstraction, how can these metalanguages aid or hinder us in\nefficiently describing software? We describe a complex triangle of interactions\nbetween the power of an abstraction mechanism, the amount of reuse it enables,\nand the cognitive difficulty of its use.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Jul 2007 19:29:29 GMT"
            }
        ],
        "update_date": "2007-07-30",
        "authors_parsed": [
            [
                "Veldhuizen",
                "Todd L.",
                ""
            ]
        ]
    },
    {
        "id": "0707.4289",
        "submitter": "Sheng Bao",
        "authors": "Stephen Gang Wu, Forrest Sheng Bao, Eric You Xu, Yu-Xuan Wang, Yi-Fan\n  Chang and Qiao-Liang Xiang",
        "title": "A Leaf Recognition Algorithm for Plant Classification Using\n  Probabilistic Neural Network",
        "comments": "6 pages, 3 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  In this paper, we employ Probabilistic Neural Network (PNN) with image and\ndata processing techniques to implement a general purpose automated leaf\nrecognition algorithm. 12 leaf features are extracted and orthogonalized into 5\nprincipal variables which consist the input vector of the PNN. The PNN is\ntrained by 1800 leaves to classify 32 kinds of plants with an accuracy greater\nthan 90%. Compared with other approaches, our algorithm is an accurate\nartificial intelligence approach which is fast in execution and easy in\nimplementation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 29 Jul 2007 12:31:40 GMT"
            }
        ],
        "update_date": "2007-07-31",
        "authors_parsed": [
            [
                "Wu",
                "Stephen Gang",
                ""
            ],
            [
                "Bao",
                "Forrest Sheng",
                ""
            ],
            [
                "Xu",
                "Eric You",
                ""
            ],
            [
                "Wang",
                "Yu-Xuan",
                ""
            ],
            [
                "Chang",
                "Yi-Fan",
                ""
            ],
            [
                "Xiang",
                "Qiao-Liang",
                ""
            ]
        ]
    },
    {
        "id": "0707.4304",
        "submitter": "Alejandro Vaisman Prof.",
        "authors": "Leticia Gomez, Sofie Haesevoets, Bart Kuijpers and Alejandro Vaisman",
        "title": "Spatial Aggregation: Data Model and Implementation",
        "comments": "56 pages, 28 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  Data aggregation in Geographic Information Systems (GIS) is only marginally\npresent in commercial systems nowadays, mostly through ad-hoc solutions. In\nthis paper, we first present a formal model for representing spatial data. This\nmodel integrates geographic data and information contained in data warehouses\nexternal to the GIS. We define the notion of geometric aggregation, a general\nframework for aggregate queries in a GIS setting. We also identify the class of\nsummable queries, which can be efficiently evaluated by precomputing the\noverlay of two or more of the thematic layers involved in the query. We also\nsketch a language, denoted GISOLAP-QL, for expressing queries that involve GIS\nand OLAP features. In addition, we introduce Piet, an implementation of our\nproposal, that makes use of overlay precomputation for answering spatial\nqueries (aggregate or not). Our experimental evaluation showed that for a\ncertain class of geometric queries with or without aggregation, overlay\nprecomputation outperforms R-tree-based techniques. Finally, as a particular\napplication of our proposal, we study topological queries.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 29 Jul 2007 21:46:50 GMT"
            }
        ],
        "update_date": "2007-07-31",
        "authors_parsed": [
            [
                "Gomez",
                "Leticia",
                ""
            ],
            [
                "Haesevoets",
                "Sofie",
                ""
            ],
            [
                "Kuijpers",
                "Bart",
                ""
            ],
            [
                "Vaisman",
                "Alejandro",
                ""
            ]
        ]
    },
    {
        "id": "0707.4389",
        "submitter": "Sandrine Blazy",
        "authors": "Andrew W. Appel (INRIA Rocquencourt), Sandrine Blazy (CEDRIC, INRIA\n  Rocquencourt)",
        "title": "Separation Logic for Small-step Cminor",
        "comments": "Version courte du rapport de recherche RR-6138",
        "journal-ref": "Dans 20th Int. Conference on Theorem Proving in Higher Order\n  Logics (TPHOLs 2007) 4732 (2007) 5-21",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  Cminor is a mid-level imperative programming language; there are\nproved-correct optimizing compilers from C to Cminor and from Cminor to machine\nlanguage. We have redesigned Cminor so that it is suitable for Hoare Logic\nreasoning and we have designed a Separation Logic for Cminor. In this paper, we\ngive a small-step semantics (instead of the big-step of the proved-correct\ncompiler) that is motivated by the need to support future concurrent\nextensions. We detail a machine-checked proof of soundness of our Separation\nLogic. This is the first large-scale machine-checked proof of a Separation\nLogic w.r.t. a small-step semantics. The work presented in this paper has been\ncarried out in the Coq proof assistant. It is a first step towards an\nenvironment in which concurrent Cminor programs can be verified using\nSeparation Logic and also compiled by a proved-correct compiler with formal\nend-to-end correctness guarantees.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 30 Jul 2007 12:09:16 GMT"
            }
        ],
        "update_date": "2007-07-31",
        "authors_parsed": [
            [
                "Appel",
                "Andrew W.",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Blazy",
                "Sandrine",
                "",
                "CEDRIC, INRIA\n  Rocquencourt"
            ]
        ]
    },
    {
        "id": "0707.4524",
        "submitter": "Shiguo Lian",
        "authors": "Shiguo Lian",
        "title": "Image Authentication Based on Neural Networks",
        "comments": "16 pages,10 figures, submitted",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MM cs.NE",
        "license": null,
        "abstract": "  Neural network has been attracting more and more researchers since the past\ndecades. The properties, such as parameter sensitivity, random similarity,\nlearning ability, etc., make it suitable for information protection, such as\ndata encryption, data authentication, intrusion detection, etc. In this paper,\nby investigating neural networks' properties, the low-cost authentication\nmethod based on neural networks is proposed and used to authenticate images or\nvideos. The authentication method can detect whether the images or videos are\nmodified maliciously. Firstly, this chapter introduces neural networks'\nproperties, such as parameter sensitivity, random similarity, diffusion\nproperty, confusion property, one-way property, etc. Secondly, the chapter\ngives an introduction to neural network based protection methods. Thirdly, an\nimage or video authentication scheme based on neural networks is presented, and\nits performances, including security, robustness and efficiency, are analyzed.\nFinally, conclusions are drawn, and some open issues in this field are\npresented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 31 Jul 2007 02:27:10 GMT"
            }
        ],
        "update_date": "2007-08-01",
        "authors_parsed": [
            [
                "Lian",
                "Shiguo",
                ""
            ]
        ]
    },
    {
        "id": "0707.4651",
        "submitter": "Alan Rufty",
        "authors": "Alan Rufty",
        "title": "Comments on the Reliability of Lawson and Hanson's Linear Distance\n  Programming Algorithm: Subroutine LDP",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS",
        "license": null,
        "abstract": "  This brief paper: (1) Discusses strategies to generate random test cases that\ncan be used to extensively test any Linear Distance Program (LDP) software. (2)\nGives three numerical examples of input cases generated by this strategy that\ncause problems in the Lawson and Hanson LDP module. (3) Proposes, as a standard\nmatter of acceptable implementation procedures, that (unless it is done\ninternally in the software itself, but, in general, this seems to be much rarer\nthan one would expect) all users should test the returned output from any LDP\nmodule for self-consistency since it incurs only a small amount of added\ncomputational overhead and it is not hard to do.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 31 Jul 2007 17:04:34 GMT"
            }
        ],
        "update_date": "2007-08-01",
        "authors_parsed": [
            [
                "Rufty",
                "Alan",
                ""
            ]
        ]
    },
    {
        "id": "0707.4659",
        "submitter": "Johannes Bluemlein",
        "authors": "I. Bierenbaum, J. Bl\\\"umlein, S. Klein, and C. Schneider",
        "title": "Difference Equations in Massive Higher Order Calculations",
        "comments": "15 pages latex, Proceedings ACAT 2007",
        "journal-ref": "PoSACAT2007:082,2007",
        "doi": null,
        "report-no": "DESY 07-111, SFB/CPP-07-041",
        "categories": "math-ph cs.MS hep-ph math.MP",
        "license": null,
        "abstract": "  The calculation of massive 2--loop operator matrix elements, required for the\nhigher order Wilson coefficients for heavy flavor production in deeply\ninelastic scattering, leads to new types of multiple infinite sums over\nharmonic sums and related functions, which depend on the Mellin parameter $N$.\nWe report on the solution of these sums through higher order difference\nequations using the summation package {\\tt Sigma}.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 31 Jul 2007 16:54:33 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Bierenbaum",
                "I.",
                ""
            ],
            [
                "Bl\u00fcmlein",
                "J.",
                ""
            ],
            [
                "Klein",
                "S.",
                ""
            ],
            [
                "Schneider",
                "C.",
                ""
            ]
        ]
    },
    {
        "id": "0708.0171",
        "submitter": "Jean-Philippe Vert",
        "authors": "Pierre Mah\\'e (XRCE), Jean-Philippe Vert (CB)",
        "title": "Virtual screening with support vector machines and structure kernels",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-bio.QM cs.LG",
        "license": null,
        "abstract": "  Support vector machines and kernel methods have recently gained considerable\nattention in chemoinformatics. They offer generally good performance for\nproblems of supervised classification or regression, and provide a flexible and\ncomputationally efficient framework to include relevant information and prior\nknowledge about the data and problems to be handled. In particular, with kernel\nmethods molecules do not need to be represented and stored explicitly as\nvectors or fingerprints, but only to be compared to each other through a\ncomparison function technically called a kernel. While classical kernels can be\nused to compare vector or fingerprint representations of molecules, completely\nnew kernels were developed in the recent years to directly compare the 2D or 3D\nstructures of molecules, without the need for an explicit vectorization step\nthrough the extraction of molecular descriptors. While still in their infancy,\nthese approaches have already demonstrated their relevance on several toxicity\nprediction and structure-activity relationship problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 1 Aug 2007 19:13:52 GMT"
            }
        ],
        "update_date": "2007-08-02",
        "authors_parsed": [
            [
                "Mah\u00e9",
                "Pierre",
                "",
                "XRCE"
            ],
            [
                "Vert",
                "Jean-Philippe",
                "",
                "CB"
            ]
        ]
    },
    {
        "id": "0708.0353",
        "submitter": "Dariusz Grech",
        "authors": "D. Grech, G. Pamu{\\l}a (University of Wroclaw, ITP)",
        "title": "The Local Fractal Properties of the Financial Time Series on the Polish\n  Stock Exchange Market",
        "comments": "LaTeX, 14 pages, 12 figures included",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-fin.ST cs.CE physics.data-an",
        "license": null,
        "abstract": "  We investigate the local fractal properties of the financial time series\nbased on the evolution of the Warsaw Stock Exchange Index (WIG) connected with\nthe largest developing financial market in Europe. Calculating the local Hurst\nexponent for the WIG time series we find an interesting dependence between the\nbehavior of the local fractal properties of the WIG time series and the crashes\nappearance on the financial market.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 2 Aug 2007 14:22:30 GMT"
            }
        ],
        "update_date": "2008-12-02",
        "authors_parsed": [
            [
                "Grech",
                "D.",
                "",
                "University of Wroclaw, ITP"
            ],
            [
                "Pamu\u0142a",
                "G.",
                "",
                "University of Wroclaw, ITP"
            ]
        ]
    },
    {
        "id": "0708.0361",
        "submitter": "Grigoriev Evgeniy",
        "authors": "Evgeniy Grigoriev",
        "title": "Why the relational data model can be considered as a formal basis for\n  group operations in object-oriented systems",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  Relational data model defines a specification of a type \"relation\". However,\nits simplicity does not mean that the system implementing this model must\noperate with structures having the same simplicity. We consider two principles\nallowing create a system which combines object-oriented paradigm (OOP) and\nrelational data model (RDM) in one framework. The first principle -- \"complex\ndata in encapsulated domains\" -- is well known from The Third Manifesto by Date\nand Darwen. The second principle --\"data complexity in names\"-- is the basis\nfor a system where data are described as complex objects and uniquely\nrepresented as a set of relations. Names of these relations and names of their\nattributes are combinations of names entered in specifications of the complex\nobjects. Below, we consider the main properties of such a system.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 2 Aug 2007 15:24:29 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 3 Aug 2007 06:11:33 GMT"
            },
            {
                "version": "v3",
                "created": "Sun, 5 Aug 2007 22:38:53 GMT"
            },
            {
                "version": "v4",
                "created": "Tue, 7 Aug 2007 21:39:14 GMT"
            },
            {
                "version": "v5",
                "created": "Thu, 9 Aug 2007 04:56:59 GMT"
            },
            {
                "version": "v6",
                "created": "Thu, 9 Aug 2007 22:15:20 GMT"
            },
            {
                "version": "v7",
                "created": "Wed, 5 Sep 2007 20:18:51 GMT"
            }
        ],
        "update_date": "2007-09-05",
        "authors_parsed": [
            [
                "Grigoriev",
                "Evgeniy",
                ""
            ]
        ]
    },
    {
        "id": "0708.0505",
        "submitter": "Luca Di Gaspero PhD",
        "authors": "Luca Di Gaspero, Andrea Roli",
        "title": "A preliminary analysis on metaheuristics methods applied to the\n  Haplotype Inference Problem",
        "comments": "22 pages, 4 figures Technical Report: DEIS - Alma Mater Studiorum,\n  University of Bologna no. DEIS-LIA-006-07",
        "journal-ref": null,
        "doi": null,
        "report-no": "DEIS-LIA-006-07",
        "categories": "cs.AI cs.CE cs.DM q-bio.QM",
        "license": null,
        "abstract": "  Haplotype Inference is a challenging problem in bioinformatics that consists\nin inferring the basic genetic constitution of diploid organisms on the basis\nof their genotype. This information allows researchers to perform association\nstudies for the genetic variants involved in diseases and the individual\nresponses to therapeutic agents.\n  A notable approach to the problem is to encode it as a combinatorial problem\n(under certain hypotheses, such as the pure parsimony criterion) and to solve\nit using off-the-shelf combinatorial optimization techniques. The main methods\napplied to Haplotype Inference are either simple greedy heuristic or exact\nmethods (Integer Linear Programming, Semidefinite Programming, SAT encoding)\nthat, at present, are adequate only for moderate size instances.\n  We believe that metaheuristic and hybrid approaches could provide a better\nscalability. Moreover, metaheuristics can be very easily combined with problem\nspecific heuristics and they can also be integrated with tree-based search\ntechniques, thus providing a promising framework for hybrid systems in which a\ngood trade-off between effectiveness and efficiency can be reached.\n  In this paper we illustrate a feasibility study of the approach and discuss\nsome relevant design issues, such as modeling and design of approximate solvers\nthat combine constructive heuristics, local search-based improvement strategies\nand learning mechanisms. Besides the relevance of the Haplotype Inference\nproblem itself, this preliminary analysis is also an interesting case study\nbecause the formulation of the problem poses some challenges in modeling and\nhybrid metaheuristic solver design that can be generalized to other problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 3 Aug 2007 12:49:21 GMT"
            }
        ],
        "update_date": "2007-08-06",
        "authors_parsed": [
            [
                "Di Gaspero",
                "Luca",
                ""
            ],
            [
                "Roli",
                "Andrea",
                ""
            ]
        ]
    },
    {
        "id": "0708.0505",
        "submitter": "Luca Di Gaspero PhD",
        "authors": "Luca Di Gaspero, Andrea Roli",
        "title": "A preliminary analysis on metaheuristics methods applied to the\n  Haplotype Inference Problem",
        "comments": "22 pages, 4 figures Technical Report: DEIS - Alma Mater Studiorum,\n  University of Bologna no. DEIS-LIA-006-07",
        "journal-ref": null,
        "doi": null,
        "report-no": "DEIS-LIA-006-07",
        "categories": "cs.AI cs.CE cs.DM q-bio.QM",
        "license": null,
        "abstract": "  Haplotype Inference is a challenging problem in bioinformatics that consists\nin inferring the basic genetic constitution of diploid organisms on the basis\nof their genotype. This information allows researchers to perform association\nstudies for the genetic variants involved in diseases and the individual\nresponses to therapeutic agents.\n  A notable approach to the problem is to encode it as a combinatorial problem\n(under certain hypotheses, such as the pure parsimony criterion) and to solve\nit using off-the-shelf combinatorial optimization techniques. The main methods\napplied to Haplotype Inference are either simple greedy heuristic or exact\nmethods (Integer Linear Programming, Semidefinite Programming, SAT encoding)\nthat, at present, are adequate only for moderate size instances.\n  We believe that metaheuristic and hybrid approaches could provide a better\nscalability. Moreover, metaheuristics can be very easily combined with problem\nspecific heuristics and they can also be integrated with tree-based search\ntechniques, thus providing a promising framework for hybrid systems in which a\ngood trade-off between effectiveness and efficiency can be reached.\n  In this paper we illustrate a feasibility study of the approach and discuss\nsome relevant design issues, such as modeling and design of approximate solvers\nthat combine constructive heuristics, local search-based improvement strategies\nand learning mechanisms. Besides the relevance of the Haplotype Inference\nproblem itself, this preliminary analysis is also an interesting case study\nbecause the formulation of the problem poses some challenges in modeling and\nhybrid metaheuristic solver design that can be generalized to other problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 3 Aug 2007 12:49:21 GMT"
            }
        ],
        "update_date": "2007-08-06",
        "authors_parsed": [
            [
                "Di Gaspero",
                "Luca",
                ""
            ],
            [
                "Roli",
                "Andrea",
                ""
            ]
        ]
    },
    {
        "id": "0708.0522",
        "submitter": "Konstantin Avrachenkov",
        "authors": "Konstantin Avrachenkov (INRIA Sophia Antipolis), Vivek Borkar, Danil\n  Nemirovsky (INRIA Sophia Antipolis)",
        "title": "Quasi-stationary distributions as centrality measures of reducible\n  graphs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  Random walk can be used as a centrality measure of a directed graph. However,\nif the graph is reducible the random walk will be absorbed in some subset of\nnodes and will never visit the rest of the graph. In Google PageRank the\nproblem was solved by introduction of uniform random jumps with some\nprobability. Up to the present, there is no clear criterion for the choice this\nparameter. We propose to use parameter-free centrality measure which is based\non the notion of quasi-stationary distribution. Specifically we suggest four\nquasi-stationary based centrality measures, analyze them and conclude that they\nproduce approximately the same ranking. The new centrality measures can be\napplied in spam detection to detect ``link farms'' and in image search to find\nphoto albums.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 3 Aug 2007 14:19:21 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 8 Aug 2007 08:32:44 GMT"
            }
        ],
        "update_date": "2009-04-17",
        "authors_parsed": [
            [
                "Avrachenkov",
                "Konstantin",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Borkar",
                "Vivek",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Nemirovsky",
                "Danil",
                "",
                "INRIA Sophia Antipolis"
            ]
        ]
    },
    {
        "id": "0708.0598",
        "submitter": "Matthew McCool Dr",
        "authors": "Matthew McCool",
        "title": "An Application of Chromatic Prototypes",
        "comments": "This paper has been withdrawn",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.MM",
        "license": null,
        "abstract": "  This paper has been withdrawn.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 4 Aug 2007 02:38:19 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 3 Feb 2008 19:41:44 GMT"
            }
        ],
        "update_date": "2008-02-03",
        "authors_parsed": [
            [
                "McCool",
                "Matthew",
                ""
            ]
        ]
    },
    {
        "id": "0708.0603",
        "submitter": "L.T. Handoko",
        "authors": "Z. Akbar, Slamet, B. I. Ajinagoro, G.I. Ohara, I. Firmansyah, B.\n  Hermanto and L.T. Handoko",
        "title": "Public Cluster : parallel machine with multi-block approach",
        "comments": "4 pages, Proceeding of the International Conference on Electrical\n  Engineering and Informatics 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": "FISIKALIPI-07002",
        "categories": "cs.DC cs.CY",
        "license": null,
        "abstract": "  We introduce a new approach to enable an open and public parallel machine\nwhich is accessible for multi users with multi jobs belong to different blocks\nrunning at the same time. The concept is required especially for parallel\nmachines which are dedicated for public use as implemented at the LIPI Public\nCluster. We have deployed the simplest technique by running multi daemons of\nparallel processing engine with different configuration files specified for\neach user assigned to access the system, and also developed an integrated\nsystem to fully control and monitor the whole system over web. A brief\nperformance analysis is also given for Message Parsing Interface (MPI) engine.\nIt is shown that the proposed approach is quite reliable and affect the whole\nperformances only slightly.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 4 Aug 2007 05:18:21 GMT"
            }
        ],
        "update_date": "2007-08-07",
        "authors_parsed": [
            [
                "Akbar",
                "Z.",
                ""
            ],
            [
                "Slamet",
                "",
                ""
            ],
            [
                "Ajinagoro",
                "B. I.",
                ""
            ],
            [
                "Ohara",
                "G. I.",
                ""
            ],
            [
                "Firmansyah",
                "I.",
                ""
            ],
            [
                "Hermanto",
                "B.",
                ""
            ],
            [
                "Handoko",
                "L. T.",
                ""
            ]
        ]
    },
    {
        "id": "0708.0605",
        "submitter": "L.T. Handoko",
        "authors": "Z. Akbar, Slamet, B. I. Ajinagoro, G.I. Ohara, I. Firmansyah, B.\n  Hermanto and L.T. Handoko",
        "title": "Open and Free Cluster for Public",
        "comments": "4 pages, Proceeding of the International Conference on Rural\n  Information and Communication Technology 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": "FISIKALIPI-07003",
        "categories": "cs.DC cs.CY",
        "license": null,
        "abstract": "  We introduce the LIPI Public Cluster, the first parallel machine facility\nfully open for public and for free in Indonesia and surrounding countries. In\nthis paper, we focus on explaining our globally new concept on open cluster,\nand how to realize and manage it to meet the users needs. We show that after 2\nyears trial running and several upgradings, the Public Cluster performs well\nand is able to fulfil all requirements as expected.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 4 Aug 2007 05:17:34 GMT"
            }
        ],
        "update_date": "2007-08-07",
        "authors_parsed": [
            [
                "Akbar",
                "Z.",
                ""
            ],
            [
                "Slamet",
                "",
                ""
            ],
            [
                "Ajinagoro",
                "B. I.",
                ""
            ],
            [
                "Ohara",
                "G. I.",
                ""
            ],
            [
                "Firmansyah",
                "I.",
                ""
            ],
            [
                "Hermanto",
                "B.",
                ""
            ],
            [
                "Handoko",
                "L. T.",
                ""
            ]
        ]
    },
    {
        "id": "0708.0607",
        "submitter": "L.T. Handoko",
        "authors": "I. Firmansyah, B. Hermanto, Hadiyanto and L.T. Handoko",
        "title": "Real-time control and monitoring system for LIPI's Public Cluster",
        "comments": "4 pages, Proceeding of the International Conference on\n  Instrumentation, Communication and Information Technology 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": "FISIKALIPI-07004",
        "categories": "cs.DC cs.RO",
        "license": null,
        "abstract": "  We have developed a monitoring and control system for LIPI's Public Cluster.\nThe system consists of microcontrollers and full web-based user interfaces for\ndaily operation. It is argued that, due to its special natures, the cluster\nrequires fully dedicated and self developed control and monitoring system. We\ndiscuss the implementation of using parallel port and dedicated\nmicro-controller for this purpose. We also show that integrating such systems\nenables an autonomous control system based on the real time monitoring, for\ninstance an autonomous power supply control based on the actual temperature,\netc.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 4 Aug 2007 05:16:43 GMT"
            }
        ],
        "update_date": "2007-08-07",
        "authors_parsed": [
            [
                "Firmansyah",
                "I.",
                ""
            ],
            [
                "Hermanto",
                "B.",
                ""
            ],
            [
                "Hadiyanto",
                "",
                ""
            ],
            [
                "Handoko",
                "L. T.",
                ""
            ]
        ]
    },
    {
        "id": "0708.0608",
        "submitter": "L.T. Handoko",
        "authors": "Z. Akbar and L.T. Handoko",
        "title": "Resource Allocation in Public Cluster with Extended Optimization\n  Algorithm",
        "comments": "4 pages, Proceeding of the International Conference on\n  Instrumentation, Communication and Information Technology 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": "FISIKALIPI-07005",
        "categories": "cs.DC",
        "license": null,
        "abstract": "  We introduce an optimization algorithm for resource allocation in the LIPI\nPublic Cluster to optimize its usage according to incoming requests from users.\nThe tool is an extended and modified genetic algorithm developed to match\nspecific natures of public cluster. We present a detail analysis of\noptimization, and compare the results with the exact calculation. We show that\nit would be very useful and could realize an automatic decision making system\nfor public clusters.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 4 Aug 2007 05:15:05 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 9 Aug 2007 00:36:19 GMT"
            }
        ],
        "update_date": "2007-08-09",
        "authors_parsed": [
            [
                "Akbar",
                "Z.",
                ""
            ],
            [
                "Handoko",
                "L. T.",
                ""
            ]
        ]
    },
    {
        "id": "0708.0624",
        "submitter": "Matthias Brust R.",
        "authors": "Christian Hutter, Matthias R. Brust, Steffen Rothkugel",
        "title": "ADS-Directory Services for Mobile Ad-Hoc Networks Based on an\n  Information Market Model",
        "comments": "9 pages, in the 1st International Workshop on Ubiquitous Computing\n  (IWUC-2004) held in conjunction with the 6th International Conference on\n  Enterprise Information Systems (ICEIS 2004)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.DC",
        "license": null,
        "abstract": "  Ubiquitous computing based on small mobile devices using wireless\ncommunication links is becoming very attractive. The computational power and\nstorage capacities provided allow the execution of sophisticated applications.\nDue to the fact that sharing of information is a central problem for\ndistributed applications, the development of self organizing middleware\nservices providing high level interfaces for information managing is essential.\nADS is a directory service for mobile ad-hoc networks dealing with local and\nnearby information as well as providing access to distant information. The\napproach discussed throughout this paper is based upon the concept of\ninformation markets.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 4 Aug 2007 13:21:55 GMT"
            }
        ],
        "update_date": "2007-08-07",
        "authors_parsed": [
            [
                "Hutter",
                "Christian",
                ""
            ],
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0708.0624",
        "submitter": "Matthias Brust R.",
        "authors": "Christian Hutter, Matthias R. Brust, Steffen Rothkugel",
        "title": "ADS-Directory Services for Mobile Ad-Hoc Networks Based on an\n  Information Market Model",
        "comments": "9 pages, in the 1st International Workshop on Ubiquitous Computing\n  (IWUC-2004) held in conjunction with the 6th International Conference on\n  Enterprise Information Systems (ICEIS 2004)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.DC",
        "license": null,
        "abstract": "  Ubiquitous computing based on small mobile devices using wireless\ncommunication links is becoming very attractive. The computational power and\nstorage capacities provided allow the execution of sophisticated applications.\nDue to the fact that sharing of information is a central problem for\ndistributed applications, the development of self organizing middleware\nservices providing high level interfaces for information managing is essential.\nADS is a directory service for mobile ad-hoc networks dealing with local and\nnearby information as well as providing access to distant information. The\napproach discussed throughout this paper is based upon the concept of\ninformation markets.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 4 Aug 2007 13:21:55 GMT"
            }
        ],
        "update_date": "2007-08-07",
        "authors_parsed": [
            [
                "Hutter",
                "Christian",
                ""
            ],
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0708.0627",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Daniel Goergen, Christian Hutter, Steffen Rothkugel",
        "title": "ADS as Information Management Service in an M-Learning Environment",
        "comments": "8th International Conference on Knowledge-Based Intelligent\n  Information & Engineering Systems (KES2004)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.DC",
        "license": null,
        "abstract": "  Leveraging the potential power of even small handheld devices able to\ncommunicate wirelessly requires dedicated support. In particular, collaborative\napplications need sophisticated assistance in terms of querying and exchanging\ndifferent kinds of data. Using a concrete example from the domain of mobile\nlearning, the general need for information dissemination is motivated.\nSubsequently, and driven by infrastructural conditions, realization strategies\nof an appropriate middleware service are discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 4 Aug 2007 13:32:07 GMT"
            }
        ],
        "update_date": "2007-08-07",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Goergen",
                "Daniel",
                ""
            ],
            [
                "Hutter",
                "Christian",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0708.0627",
        "submitter": "Matthias Brust R.",
        "authors": "Matthias R. Brust, Daniel Goergen, Christian Hutter, Steffen Rothkugel",
        "title": "ADS as Information Management Service in an M-Learning Environment",
        "comments": "8th International Conference on Knowledge-Based Intelligent\n  Information & Engineering Systems (KES2004)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.DC",
        "license": null,
        "abstract": "  Leveraging the potential power of even small handheld devices able to\ncommunicate wirelessly requires dedicated support. In particular, collaborative\napplications need sophisticated assistance in terms of querying and exchanging\ndifferent kinds of data. Using a concrete example from the domain of mobile\nlearning, the general need for information dissemination is motivated.\nSubsequently, and driven by infrastructural conditions, realization strategies\nof an appropriate middleware service are discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 4 Aug 2007 13:32:07 GMT"
            }
        ],
        "update_date": "2007-08-07",
        "authors_parsed": [
            [
                "Brust",
                "Matthias R.",
                ""
            ],
            [
                "Goergen",
                "Daniel",
                ""
            ],
            [
                "Hutter",
                "Christian",
                ""
            ],
            [
                "Rothkugel",
                "Steffen",
                ""
            ]
        ]
    },
    {
        "id": "0708.0654",
        "submitter": "James P. Crutchfield",
        "authors": "Susanne Still, James P. Crutchfield",
        "title": "Structure or Noise?",
        "comments": "6 pages, 2 figures;\n  http://cse.ucdavis.edu/~cmg/compmech/pubs/son.html",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "physics.data-an cond-mat.stat-mech cs.IT cs.LG math-ph math.IT math.MP math.ST nlin.CD stat.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how rate-distortion theory provides a mechanism for automated theory\nbuilding by naturally distinguishing between regularity and randomness. We\nstart from the simple principle that model variables should, as much as\npossible, render the future and past conditionally independent. From this, we\nconstruct an objective function for model making whose extrema embody the\ntrade-off between a model's structural complexity and its predictive power. The\nsolutions correspond to a hierarchy of models that, at each level of\ncomplexity, achieve optimal predictive power at minimal cost. In the limit of\nmaximal prediction the resulting optimal model identifies a process's intrinsic\norganization by extracting the underlying causal states. In this limit, the\nmodel's complexity is given by the statistical complexity, which is known to be\nminimal for achieving maximum prediction. Examples show how theory building can\nprofit from analyzing a process's causal compressibility, which is reflected in\nthe optimal models' rate-distortion curve--the process's characteristic for\noptimally balancing structure and noise at different levels of representation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 5 Aug 2007 01:37:53 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 29 Jun 2008 23:52:26 GMT"
            }
        ],
        "update_date": "2016-09-08",
        "authors_parsed": [
            [
                "Still",
                "Susanne",
                ""
            ],
            [
                "Crutchfield",
                "James P.",
                ""
            ]
        ]
    },
    {
        "id": "0708.0660",
        "submitter": "Guanrong Chen",
        "authors": "Zhisheng Duan, Chao Liu and Guanrong Chen",
        "title": "Network synchronizability analysis: the theory of subgraphs and\n  complementary graphs",
        "comments": "13 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1016/j.physd.2007.12.003",
        "report-no": null,
        "categories": "cs.NI cs.GR",
        "license": null,
        "abstract": "  In this paper, subgraphs and complementary graphs are used to analyze the\nnetwork synchronizability. Some sharp and attainable bounds are provided for\nthe eigenratio of the network structural matrix, which characterizes the\nnetwork synchronizability, especially when the network's corresponding graph\nhas cycles, chains, bipartite graphs or product graphs as its subgraphs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 5 Aug 2007 05:25:45 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Duan",
                "Zhisheng",
                ""
            ],
            [
                "Liu",
                "Chao",
                ""
            ],
            [
                "Chen",
                "Guanrong",
                ""
            ]
        ]
    },
    {
        "id": "0708.0877",
        "submitter": "Vita Hinze-Hoare",
        "authors": "V. Hinze-Hoare",
        "title": "A Portal Analysis for the Design of a Collaborative Research Environment\n  for Students and Supervisors (CRESS) within the CSCR Domain",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  In a previous paper the CSCR domain was defined. Here this is taken to the\nnext stage where we consider the design of a particular Collaborative Research\nEnvironment to support Students and Supervisors CRESS. Following the CSCR\nstructure a preliminary design for CRESS has been established and a portal\nframework analysis is undertaken in order to determine the most appropriate set\nof tools for its implementation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 7 Aug 2007 19:53:24 GMT"
            }
        ],
        "update_date": "2007-08-08",
        "authors_parsed": [
            [
                "Hinze-Hoare",
                "V.",
                ""
            ]
        ]
    },
    {
        "id": "0708.0909",
        "submitter": "Sebastien Tixeuil",
        "authors": "L\\'elia Blin (IBISC), Maria Gradinariu Potop-Butucaru (INRIA\n  Rocquencourt, LIP6), S\\'ebastien Tixeuil (INRIA Futurs, LRI)",
        "title": "On the Self-stabilization of Mobile Robots in Graphs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.DC",
        "license": null,
        "abstract": "  Self-stabilization is a versatile technique to withstand any transient fault\nin a distributed system. Mobile robots (or agents) are one of the emerging\ntrends in distributed computing as they mimic autonomous biologic entities. The\ncontribution of this paper is threefold. First, we present a new model for\nstudying mobile entities in networks subject to transient faults. Our model\ndiffers from the classical robot model because robots have constraints about\nthe paths they are allowed to follow, and from the classical agent model\nbecause the number of agents remains fixed throughout the execution of the\nprotocol. Second, in this model, we study the possibility of designing\nself-stabilizing algorithms when those algorithms are run by mobile robots (or\nagents) evolving on a graph. We concentrate on the core building blocks of\nrobot and agents problems: naming and leader election. Not surprisingly, when\nno constraints are given on the network graph topology and local execution\nmodel, both problems are impossible to solve. Finally, using minimal hypothesis\nwith respect to impossibility results, we provide deterministic and\nprobabilistic solutions to both problems, and show equivalence of these\nproblems by an algorithmic reduction mechanism.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 7 Aug 2007 09:34:14 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 8 Aug 2007 09:11:22 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Blin",
                "L\u00e9lia",
                "",
                "IBISC"
            ],
            [
                "Potop-Butucaru",
                "Maria Gradinariu",
                "",
                "INRIA\n  Rocquencourt, LIP6"
            ],
            [
                "Tixeuil",
                "S\u00e9bastien",
                "",
                "INRIA Futurs, LRI"
            ]
        ]
    },
    {
        "id": "0708.0927",
        "submitter": "Emanuel Diamant",
        "authors": "Emanuel Diamant",
        "title": "Modeling Visual Information Processing in Brain: A Computer Vision Point\n  of View and Approach",
        "comments": "That is a journal version of a paper that in 2007 has been submitted\n  to 15 computer vision conferences and was discarded by 11 of them",
        "journal-ref": "Signal Processing: Image Communication, vol. 22, issue 6, pp.\n  583-590, July 2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.CV",
        "license": null,
        "abstract": "  We live in the Information Age, and information has become a critically\nimportant component of our life. The success of the Internet made huge amounts\nof it easily available and accessible to everyone. To keep the flow of this\ninformation manageable, means for its faultless circulation and effective\nhandling have become urgently required. Considerable research efforts are\ndedicated today to address this necessity, but they are seriously hampered by\nthe lack of a common agreement about \"What is information?\" In particular, what\nis \"visual information\" - human's primary input from the surrounding world. The\nproblem is further aggravated by a long-lasting stance borrowed from the\nbiological vision research that assumes human-like information processing as an\nenigmatic mix of perceptual and cognitive vision faculties. I am trying to find\na remedy for this bizarre situation. Relying on a new definition of\n\"information\", which can be derived from Kolmogorov's compexity theory and\nChaitin's notion of algorithmic information, I propose a unifying framework for\nvisual information processing, which explicitly accounts for the perceptual and\ncognitive image processing peculiarities. I believe that this framework will be\nuseful to overcome the difficulties that are impeding our attempts to develop\nthe right model of human-like intelligent image processing.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 7 Aug 2007 11:16:15 GMT"
            }
        ],
        "update_date": "2007-08-08",
        "authors_parsed": [
            [
                "Diamant",
                "Emanuel",
                ""
            ]
        ]
    },
    {
        "id": "0708.0927",
        "submitter": "Emanuel Diamant",
        "authors": "Emanuel Diamant",
        "title": "Modeling Visual Information Processing in Brain: A Computer Vision Point\n  of View and Approach",
        "comments": "That is a journal version of a paper that in 2007 has been submitted\n  to 15 computer vision conferences and was discarded by 11 of them",
        "journal-ref": "Signal Processing: Image Communication, vol. 22, issue 6, pp.\n  583-590, July 2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.CV",
        "license": null,
        "abstract": "  We live in the Information Age, and information has become a critically\nimportant component of our life. The success of the Internet made huge amounts\nof it easily available and accessible to everyone. To keep the flow of this\ninformation manageable, means for its faultless circulation and effective\nhandling have become urgently required. Considerable research efforts are\ndedicated today to address this necessity, but they are seriously hampered by\nthe lack of a common agreement about \"What is information?\" In particular, what\nis \"visual information\" - human's primary input from the surrounding world. The\nproblem is further aggravated by a long-lasting stance borrowed from the\nbiological vision research that assumes human-like information processing as an\nenigmatic mix of perceptual and cognitive vision faculties. I am trying to find\na remedy for this bizarre situation. Relying on a new definition of\n\"information\", which can be derived from Kolmogorov's compexity theory and\nChaitin's notion of algorithmic information, I propose a unifying framework for\nvisual information processing, which explicitly accounts for the perceptual and\ncognitive image processing peculiarities. I believe that this framework will be\nuseful to overcome the difficulties that are impeding our attempts to develop\nthe right model of human-like intelligent image processing.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 7 Aug 2007 11:16:15 GMT"
            }
        ],
        "update_date": "2007-08-08",
        "authors_parsed": [
            [
                "Diamant",
                "Emanuel",
                ""
            ]
        ]
    },
    {
        "id": "0708.0975",
        "submitter": "Cedric Adjih",
        "authors": "C\\'edric Adjih (INRIA Rocquencourt), Song Yean Cho (INRIA\n  Rocquencourt), Philippe Jacquet (INRIA Rocquencourt)",
        "title": "Near Optimal Broadcast with Network Coding in Large Sensor Networks",
        "comments": "Dans First International Workshop on Information Theory for Sensor\n  Netwoks (WITS 2007) (2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  We study efficient broadcasting for wireless sensor networks, with network\ncoding. We address this issue for homogeneous sensor networks in the plane. Our\nresults are based on a simple principle (IREN/IRON), which sets the same rate\non most of the nodes (wireless links) of the network. With this rate selection,\nwe give a value of the maximum achievable broadcast rate of the source: our\ncentral result is a proof of the value of the min-cut for such networks, viewed\nas hypergraphs. Our metric for efficiency is the number of transmissions\nnecessary to transmit one packet from the source to every destination: we show\nthat IREN/IRON achieves near optimality for large networks; that is,\nasymptotically, nearly every transmission brings new information from the\nsource to the receiver. As a consequence, network coding asymptotically\noutperforms any scheme that does not use network coding.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 7 Aug 2007 15:13:17 GMT"
            }
        ],
        "update_date": "2009-04-16",
        "authors_parsed": [
            [
                "Adjih",
                "C\u00e9dric",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Cho",
                "Song Yean",
                "",
                "INRIA\n  Rocquencourt"
            ],
            [
                "Jacquet",
                "Philippe",
                "",
                "INRIA Rocquencourt"
            ]
        ]
    },
    {
        "id": "0708.1116",
        "submitter": "Florian Simatos",
        "authors": "Florian Simatos",
        "title": "A variant of the Recoil Growth algorithm to generate multi-polymer\n  systems",
        "comments": "Title changed",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cond-mat.stat-mech",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Recoil Growth algorithm, proposed in 1999 by Consta et al., is one of the\nmost efficient algorithm available in the literature to sample from a\nmulti-polymer system. Such problems are closely related to the generation of\nself-avoiding paths. In this paper, we study a variant of the original Recoil\nGrowth algorithm, where we constrain the generation of a new polymer to take\nplace on a specific class of graphs. This makes it possible to make a fine\ntrade-off between computational cost and success rate. We moreover give a\nsimple proof for a lower bound on the irreducibility of this new algorithm,\nwhich applies to the original algorithm as well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 8 Aug 2007 15:07:53 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 12 Sep 2008 10:02:19 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 2 Jul 2009 12:31:15 GMT"
            }
        ],
        "update_date": "2009-07-02",
        "authors_parsed": [
            [
                "Simatos",
                "Florian",
                ""
            ]
        ]
    },
    {
        "id": "0708.1150",
        "submitter": "Marko Antonio Rodriguez",
        "authors": "Marko A. Rodriguez, Johah Bollen, Herbert Van de Sompel",
        "title": "A Practical Ontology for the Large-Scale Modeling of Scholarly Artifacts\n  and their Usage",
        "comments": null,
        "journal-ref": "Proceedings of the IEEE/ACM Joint Conference on Digital Libraries\n  (JCDL'07), pp. 278-287, 2007",
        "doi": "10.1145/1255175.1255229",
        "report-no": null,
        "categories": "cs.DL cs.AI",
        "license": null,
        "abstract": "  The large-scale analysis of scholarly artifact usage is constrained primarily\nby current practices in usage data archiving, privacy issues concerned with the\ndissemination of usage data, and the lack of a practical ontology for modeling\nthe usage domain. As a remedy to the third constraint, this article presents a\nscholarly ontology that was engineered to represent those classes for which\nlarge-scale bibliographic and usage data exists, supports usage research, and\nwhose instantiation is scalable to the order of 50 million articles along with\ntheir associated artifacts (e.g. authors and journals) and an accompanying 1\nbillion usage events. The real world instantiation of the presented abstract\nontology is a semantic network model of the scholarly community which lends the\nscholarly process to statistical analysis and computational support. We present\nthe ontology, discuss its instantiation, and provide some example inference\nrules for calculating various scholarly artifact metrics.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 8 Aug 2007 17:06:55 GMT"
            }
        ],
        "update_date": "2007-08-09",
        "authors_parsed": [
            [
                "Rodriguez",
                "Marko A.",
                ""
            ],
            [
                "Bollen",
                "Johah",
                ""
            ],
            [
                "Van de Sompel",
                "Herbert",
                ""
            ]
        ]
    },
    {
        "id": "0708.1242",
        "submitter": "Christos Dimitrakakis",
        "authors": "Christos Dimitrakakis and Christian Savu-Krohn",
        "title": "Cost-minimising strategies for data labelling : optimal stopping and\n  active learning",
        "comments": "17 pages, 4 figures. Corrected some errors and changed the flow of\n  the text",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  Supervised learning deals with the inference of a distribution over an output\nor label space $\\CY$ conditioned on points in an observation space $\\CX$, given\na training dataset $D$ of pairs in $\\CX \\times \\CY$. However, in a lot of\napplications of interest, acquisition of large amounts of observations is easy,\nwhile the process of generating labels is time-consuming or costly. One way to\ndeal with this problem is {\\em active} learning, where points to be labelled\nare selected with the aim of creating a model with better performance than that\nof an model trained on an equal number of randomly sampled points. In this\npaper, we instead propose to deal with the labelling cost directly: The\nlearning goal is defined as the minimisation of a cost which is a function of\nthe expected model performance and the total cost of the labels used. This\nallows the development of general strategies and specific algorithms for (a)\noptimal stopping, where the expected cost dictates whether label acquisition\nshould continue (b) empirical evaluation, where the cost is used as a\nperformance metric for a given combination of inference, stopping and sampling\nmethods. Though the main focus of the paper is optimal stopping, we also aim to\nprovide the background for further developments and discussion in the related\nfield of active learning.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 9 Aug 2007 10:21:34 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 27 Aug 2007 22:05:57 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 15 Nov 2007 16:37:51 GMT"
            }
        ],
        "update_date": "2007-11-15",
        "authors_parsed": [
            [
                "Dimitrakakis",
                "Christos",
                ""
            ],
            [
                "Savu-Krohn",
                "Christian",
                ""
            ]
        ]
    },
    {
        "id": "0708.1411",
        "submitter": "Sajad Sadough",
        "authors": "Sajad Sadough (LSS), Pablo Piantanida (LSS), Pierre Duhamel (LSS)",
        "title": "Achievable Outage Rates with Improved Decoding of Bicm Multiband Ofdm\n  Under Channel Estimation Errors",
        "comments": null,
        "journal-ref": "Dans 40th Asilomar Conference on Signals, Systems, and Computers -\n  40th Asilomar Conference on Signals, Systems, and Computers, Monterey :\n  \\'Etats-Unis d'Am\\'erique (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  We consider the decoding of bit interleaved coded modulation (BICM) applied\nto multiband OFDM for practical scenarios where only a noisy (possibly very\nbad) estimate of the channel is available at the receiver. First, a decoding\nmetric based on the channel it a posteriori probability density, conditioned on\nthe channel estimate is derived and used for decoding BICM multiband OFDM.\nThen, we characterize the limits of reliable information rates in terms of the\nmaximal achievable outage rates associated to the proposed metric. We also\ncompare our results with the outage rates of a system using a theoretical\ndecoder. Our results are useful for designing a communication system where a\nprescribed quality of service (QoS), in terms of achievable target rates with\nsmall error probability, must be satisfied even in the presence of imperfect\nchannel estimation. Numerical results over both realistic UWB and theoretical\nRayleigh fading channels show that the proposed method provides significant\ngain in terms of BER and outage rates compared to the classical mismatched\ndetector, without introducing any additional complexity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 10 Aug 2007 12:13:51 GMT"
            }
        ],
        "update_date": "2007-08-13",
        "authors_parsed": [
            [
                "Sadough",
                "Sajad",
                "",
                "LSS"
            ],
            [
                "Piantanida",
                "Pablo",
                "",
                "LSS"
            ],
            [
                "Duhamel",
                "Pierre",
                "",
                "LSS"
            ]
        ]
    },
    {
        "id": "0708.1413",
        "submitter": "Sajad Sadough",
        "authors": "Sajad Sadough (LSS), Pierre Duhamel (LSS)",
        "title": "On Optimal Turbo Decoding of Wideband MIMO-OFDM Systems Under Imperfect\n  Channel State Information",
        "comments": null,
        "journal-ref": "COST2100 Meeting, Lisbon : Portugal (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  We consider the decoding of bit interleaved coded modulation (BICM) applied\nto both multiband and MIMO OFDM systems for typical scenarios where only a\nnoisy (possibly very bad) estimate of the channel is provided by sending a\nlimited number of pilot symbols. First, by using a Bayesian framework involving\nthe channel a posteriori density, we adopt a practical decoding metric that is\nrobust to the presence of channel estimation errors. Then this metric is used\nin the demapping part of BICM multiband and MIMO OFDM receivers. We also\ncompare our results with the performance of a mismatched decoder that replaces\nthe channel by its estimate in the decoding metric. Numerical results over both\nrealistic UWB and theoretical Rayleigh fading channels show that the proposed\nmethod provides significant gain in terms of bit error rate compared to the\nclassical mismatched detector, without introducing any additional complexity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 10 Aug 2007 12:16:55 GMT"
            }
        ],
        "update_date": "2007-08-13",
        "authors_parsed": [
            [
                "Sadough",
                "Sajad",
                "",
                "LSS"
            ],
            [
                "Duhamel",
                "Pierre",
                "",
                "LSS"
            ]
        ]
    },
    {
        "id": "0708.1414",
        "submitter": "Sajad Sadough",
        "authors": "Sajad Sadough (LSS), Mahieddine Ichir (LSS), Emmanuel Jaffrot, Pierre\n  Duhamel (LSS)",
        "title": "Wavelet Based Semi-blind Channel Estimation For Multiband OFDM",
        "comments": null,
        "journal-ref": "Dans European Wireless - European Wireless, Paris : France (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  This paper introduces an expectation-maximization (EM) algorithm within a\nwavelet domain Bayesian framework for semi-blind channel estimation of\nmultiband OFDM based UWB communications. A prior distribution is chosen for the\nwavelet coefficients of the unknown channel impulse response in order to model\na sparseness property of the wavelet representation. This prior yields, in\nmaximum a posteriori estimation, a thresholding rule within the EM algorithm.\nWe particularly focus on reducing the number of estimated parameters by\niteratively discarding ``unsignificant'' wavelet coefficients from the\nestimation process. Simulation results using UWB channels issued from both\nmodels and measurements show that under sparsity conditions, the proposed\nalgorithm outperforms pilot based channel estimation in terms of mean square\nerror and bit error rate and enhances the estimation accuracy with less\ncomputational complexity than traditional semi-blind methods.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 10 Aug 2007 12:18:36 GMT"
            }
        ],
        "update_date": "2007-08-13",
        "authors_parsed": [
            [
                "Sadough",
                "Sajad",
                "",
                "LSS"
            ],
            [
                "Ichir",
                "Mahieddine",
                "",
                "LSS"
            ],
            [
                "Jaffrot",
                "Emmanuel",
                "",
                "LSS"
            ],
            [
                "Duhamel",
                "Pierre",
                "",
                "LSS"
            ]
        ]
    },
    {
        "id": "0708.1416",
        "submitter": "Sajad Sadough",
        "authors": "Sajad Sadough (LSS), Pablo Piantanida (LSS), Pierre Duhamel (LSS)",
        "title": "MIMO-OFDM Optimal Decoding and Achievable Information Rates Under\n  Imperfect Channel Estimation",
        "comments": null,
        "journal-ref": "Dans The VIII IEEE Workshop on Signal Processing Advances in\n  Wireless Communications - The VIII IEEE Workshop on Signal Processing\n  Advances in Wireless Communications, Finlande (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  Optimal decoding of bit interleaved coded modulation (BICM) MIMO-OFDM where\nan imperfect channel estimate is available at the receiver is investigated.\nFirst, by using a Bayesian approach involving the channel a posteriori density,\nwe derive a practical decoding metric for general memoryless channels that is\nrobust to the presence of channel estimation errors. Then, we evaluate the\noutage rates achieved by a decoder that uses our proposed metric. The\nperformance of the proposed decoder is compared to the classical mismatched\ndecoder and a theoretical decoder defined as the best decoder in the presence\nof imperfect channel estimation. Numerical results over Rayleigh block fading\nMIMO-OFDM channels show that the proposed decoder outperforms mismatched\ndecoding in terms of bit error rate and outage capacity without introducing any\nadditional complexity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 10 Aug 2007 12:40:08 GMT"
            }
        ],
        "update_date": "2007-08-13",
        "authors_parsed": [
            [
                "Sadough",
                "Sajad",
                "",
                "LSS"
            ],
            [
                "Piantanida",
                "Pablo",
                "",
                "LSS"
            ],
            [
                "Duhamel",
                "Pierre",
                "",
                "LSS"
            ]
        ]
    },
    {
        "id": "0708.1496",
        "submitter": "Mihai Oltean",
        "authors": "Mihai Oltean",
        "title": "A Light-Based Device for Solving the Hamiltonian Path Problem",
        "comments": "11 pages, Unconventional Computation conference, 2006",
        "journal-ref": "LNCS 4135, Unconventional Computation conference, pp. 217-227,\n  2006",
        "doi": "10.1007/11839132",
        "report-no": null,
        "categories": "cs.AR cs.DC",
        "license": null,
        "abstract": "  In this paper we suggest the use of light for performing useful computations.\nNamely, we propose a special device which uses light rays for solving the\nHamiltonian path problem on a directed graph. The device has a graph-like\nrepresentation and the light is traversing it following the routes given by the\nconnections between nodes. In each node the rays are uniquely marked so that\nthey can be easily identified. At the destination node we will search only for\nparticular rays that have passed only once through each node. We show that the\nproposed device can solve small and medium instances of the problem in\nreasonable time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 10 Aug 2007 18:12:52 GMT"
            }
        ],
        "update_date": "2007-08-13",
        "authors_parsed": [
            [
                "Oltean",
                "Mihai",
                ""
            ]
        ]
    },
    {
        "id": "0708.1503",
        "submitter": "Vladimir Vovk",
        "authors": "Vladimir Vovk",
        "title": "Defensive forecasting for optimal prediction with expert advice",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  The method of defensive forecasting is applied to the problem of prediction\nwith expert advice for binary outcomes. It turns out that defensive forecasting\nis not only competitive with the Aggregating Algorithm but also handles the\ncase of \"second-guessing\" experts, whose advice depends on the learner's\nprediction; this paper assumes that the dependence on the learner's prediction\nis continuous.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 10 Aug 2007 19:19:54 GMT"
            }
        ],
        "update_date": "2007-08-13",
        "authors_parsed": [
            [
                "Vovk",
                "Vladimir",
                ""
            ]
        ]
    },
    {
        "id": "0708.1512",
        "submitter": "Mihai Oltean",
        "authors": "Mihai Oltean",
        "title": "Solving the Hamiltonian path problem with a light-based computer",
        "comments": "17 pages, Natural Computing journal",
        "journal-ref": "Natural Computing, Springer, Vol 6, 2007",
        "doi": "10.1007/s11047-007-9042-z",
        "report-no": null,
        "categories": "cs.AR cs.DC",
        "license": null,
        "abstract": "  In this paper we propose a special computational device which uses light rays\nfor solving the Hamiltonian path problem on a directed graph. The device has a\ngraph-like representation and the light is traversing it by following the\nroutes given by the connections between nodes. In each node the rays are\nuniquely marked so that they can be easily identified. At the destination node\nwe will search only for particular rays that have passed only once through each\nnode. We show that the proposed device can solve small and medium instances of\nthe problem in reasonable time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 10 Aug 2007 20:01:24 GMT"
            }
        ],
        "update_date": "2007-08-14",
        "authors_parsed": [
            [
                "Oltean",
                "Mihai",
                ""
            ]
        ]
    },
    {
        "id": "0708.1527",
        "submitter": "Stasinos Konstantopoulos",
        "authors": "Stasinos Konstantopoulos",
        "title": "A Data-Parallel Version of Aleph",
        "comments": "Proceedings of Parallel and Distributed Computing for Machine\n  Learning workshop, held in conjunction with the 14th European Conference on\n  Machine Learning. Cavtat, Croatia, 2003",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.DC",
        "license": null,
        "abstract": "  This is to present work on modifying the Aleph ILP system so that it\nevaluates the hypothesised clauses in parallel by distributing the data-set\namong the nodes of a parallel or distributed machine. The paper briefly\ndiscusses MPI, the interface used to access message- passing libraries for\nparallel computers and clusters. It then proceeds to describe an extension of\nYAP Prolog with an MPI interface and an implementation of data-parallel clause\nevaluation for Aleph through this interface. The paper concludes by testing the\ndata-parallel Aleph on artificially constructed data-sets.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 10 Aug 2007 23:32:16 GMT"
            }
        ],
        "update_date": "2007-08-14",
        "authors_parsed": [
            [
                "Konstantopoulos",
                "Stasinos",
                ""
            ]
        ]
    },
    {
        "id": "0708.1527",
        "submitter": "Stasinos Konstantopoulos",
        "authors": "Stasinos Konstantopoulos",
        "title": "A Data-Parallel Version of Aleph",
        "comments": "Proceedings of Parallel and Distributed Computing for Machine\n  Learning workshop, held in conjunction with the 14th European Conference on\n  Machine Learning. Cavtat, Croatia, 2003",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.DC",
        "license": null,
        "abstract": "  This is to present work on modifying the Aleph ILP system so that it\nevaluates the hypothesised clauses in parallel by distributing the data-set\namong the nodes of a parallel or distributed machine. The paper briefly\ndiscusses MPI, the interface used to access message- passing libraries for\nparallel computers and clusters. It then proceeds to describe an extension of\nYAP Prolog with an MPI interface and an implementation of data-parallel clause\nevaluation for Aleph through this interface. The paper concludes by testing the\ndata-parallel Aleph on artificially constructed data-sets.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 10 Aug 2007 23:32:16 GMT"
            }
        ],
        "update_date": "2007-08-14",
        "authors_parsed": [
            [
                "Konstantopoulos",
                "Stasinos",
                ""
            ]
        ]
    },
    {
        "id": "0708.1579",
        "submitter": "Vicen\\c{c} G\\'omez Cerd\\`a",
        "authors": "Andreas Kaltenbrunner, Vicen\\c{c} G\\'omez, Ayman Moghnieh, Rodrigo\n  Meza, Josep Blat, Vicente L\\'opez",
        "title": "Homogeneous temporal activity patterns in a large online communication\n  space",
        "comments": "17 pages, 10 figures, to be published in International Journal on\n  WWW/Internet",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  The many-to-many social communication activity on the popular technology-news\nwebsite Slashdot has been studied. We have concentrated on the dynamics of\nmessage production without considering semantic relations and have found\nregular temporal patterns in the reaction time of the community to a news-post\nas well as in single user behavior. The statistics of these activities follow\nlog-normal distributions. Daily and weekly oscillatory cycles, which cause\nslight variations of this simple behavior, are identified. A superposition of\ntwo log-normal distributions can account for these variations. The findings are\nremarkable since the distribution of the number of comments per users, which is\nalso analyzed, indicates a great amount of heterogeneity in the community. The\nreader may find surprising that only a few parameters allow a detailed\ndescription, or even prediction, of social many-to-many information exchange in\nthis kind of popular public spaces.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 12 Aug 2007 00:29:03 GMT"
            }
        ],
        "update_date": "2007-08-14",
        "authors_parsed": [
            [
                "Kaltenbrunner",
                "Andreas",
                ""
            ],
            [
                "G\u00f3mez",
                "Vicen\u00e7",
                ""
            ],
            [
                "Moghnieh",
                "Ayman",
                ""
            ],
            [
                "Meza",
                "Rodrigo",
                ""
            ],
            [
                "Blat",
                "Josep",
                ""
            ],
            [
                "L\u00f3pez",
                "Vicente",
                ""
            ]
        ]
    },
    {
        "id": "0708.1580",
        "submitter": "James P. Crutchfield",
        "authors": "Susanne Still, James P. Crutchfield, Christopher J. Ellison",
        "title": "Optimal Causal Inference: Estimating Stored Information and\n  Approximating Causal Architecture",
        "comments": "14 pages, 13 figures;\n  http://cse.ucdavis.edu/~cmg/compmech/pubs/oci.htm; Updated figures and\n  citations; added corrections and clarifications",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cond-mat.stat-mech cs.LG math.IT math.ST stat.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce an approach to inferring the causal architecture of stochastic\ndynamical systems that extends rate distortion theory to use causal\nshielding---a natural principle of learning. We study two distinct cases of\ncausal inference: optimal causal filtering and optimal causal estimation.\n  Filtering corresponds to the ideal case in which the probability distribution\nof measurement sequences is known, giving a principled method to approximate a\nsystem's causal structure at a desired level of representation. We show that,\nin the limit in which a model complexity constraint is relaxed, filtering finds\nthe exact causal architecture of a stochastic dynamical system, known as the\ncausal-state partition. From this, one can estimate the amount of historical\ninformation the process stores. More generally, causal filtering finds a graded\nmodel-complexity hierarchy of approximations to the causal architecture. Abrupt\nchanges in the hierarchy, as a function of approximation, capture distinct\nscales of structural organization.\n  For nonideal cases with finite data, we show how the correct number of\nunderlying causal states can be found by optimal causal estimation. A\npreviously derived model complexity control term allows us to correct for the\neffect of statistical fluctuations in probability estimates and thereby avoid\nover-fitting.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 11 Aug 2007 19:13:29 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 19 Aug 2010 23:46:24 GMT"
            }
        ],
        "update_date": "2010-08-23",
        "authors_parsed": [
            [
                "Still",
                "Susanne",
                ""
            ],
            [
                "Crutchfield",
                "James P.",
                ""
            ],
            [
                "Ellison",
                "Christopher J.",
                ""
            ]
        ]
    },
    {
        "id": "0708.1624",
        "submitter": "Vita Hinze-Hoare",
        "authors": "V. Hinze-Hoare",
        "title": "Designing a Collaborative Research Environment for Students and their\n  Supervisors (CRESS)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  In a previous paper the CSCR domain was defined. Here this is taken to the\nnext stage where the design of a particular Collaborative Research Environment\nto support Students and Supervisors (CRESS) is considered. Following the CSCR\nstructure this paper deals with an analysis of 13 collaborative working\nenvironments to determine a preliminary design for CRESS in order to discover\nthe most appropriate set of tools for its implementation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 12 Aug 2007 19:58:37 GMT"
            }
        ],
        "update_date": "2007-08-14",
        "authors_parsed": [
            [
                "Hinze-Hoare",
                "V.",
                ""
            ]
        ]
    },
    {
        "id": "0708.1725",
        "submitter": "Willemien Visser",
        "authors": "Willemien Visser (LTCI)",
        "title": "Design: One, but in different forms",
        "comments": null,
        "journal-ref": "Design Studies 30, 3 (2009) 187-223",
        "doi": "10.1016/j.destud.2008.11.004",
        "report-no": null,
        "categories": "cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This overview paper defends an augmented cognitively oriented generic-design\nhypothesis: there are both significant similarities between the design\nactivities implemented in different situations and crucial differences between\nthese and other cognitive activities; yet, characteristics of a design\nsituation (related to the design process, the designers, and the artefact)\nintroduce specificities in the corresponding cognitive activities and\nstructures that are used, and in the resulting designs. We thus augment the\nclassical generic-design hypothesis with that of different forms of designing.\nWe review the data available in the cognitive design research literature and\npropose a series of candidates underlying such forms of design, outlining a\nnumber of directions requiring further elaboration.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 13 Aug 2007 15:00:37 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 20 Aug 2009 14:55:41 GMT"
            }
        ],
        "update_date": "2009-08-20",
        "authors_parsed": [
            [
                "Visser",
                "Willemien",
                "",
                "LTCI"
            ]
        ]
    },
    {
        "id": "0708.1768",
        "submitter": "Alexander Ushakov",
        "authors": "Jonathan Longrigg and Alexander Ushakov",
        "title": "Cryptanalysis of shifted conjugacy authentication protocol",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.GR cs.CR",
        "license": null,
        "abstract": "  In this paper we present the first practical attack on the shifted\nconjugacy-based authentication protocol proposed by P. Dehornoy. We discuss the\nweaknesses of that primitive and propose ways to improve the protocol.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 14 Aug 2007 19:11:59 GMT"
            }
        ],
        "update_date": "2007-08-15",
        "authors_parsed": [
            [
                "Longrigg",
                "Jonathan",
                ""
            ],
            [
                "Ushakov",
                "Alexander",
                ""
            ]
        ]
    },
    {
        "id": "0708.1818",
        "submitter": "Francoise Heres-Renzetti",
        "authors": "L.-V. Bochkareva, M.-V. Kireitseu, G. R. Tomlinson, H. Altenbach, V.\n  Kompis, D. Hui",
        "title": "Computational Simulation and 3D Virtual Reality Engineering Tools for\n  Dynamical Modeling and Imaging of Composite Nanomaterials",
        "comments": "Submitted on behalf of TIMA Editions\n  (http://irevues.inist.fr/tima-editions)",
        "journal-ref": "Dans European Nano Systems Worshop - ENS 2005, Paris : France\n  (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cond-mat.other",
        "license": null,
        "abstract": "  An adventure at engineering design and modeling is possible with a Virtual\nReality Environment (VRE) that uses multiple computer-generated media to let a\nuser experience situations that are temporally and spatially prohibiting. In\nthis paper, an approach to developing some advanced architecture and modeling\ntools is presented to allow multiple frameworks work together while being\nshielded from the application program. This architecture is being developed in\na framework of workbench interactive tools for next generation\nnanoparticle-reinforced damping/dynamic systems. Through the use of system, an\nengineer/programmer can respectively concentrate on tailoring an engineering\ndesign concept of novel system and the application software design while using\nexisting databases/software outputs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 14 Aug 2007 08:17:45 GMT"
            }
        ],
        "update_date": "2007-08-15",
        "authors_parsed": [
            [
                "Bochkareva",
                "L. -V.",
                ""
            ],
            [
                "Kireitseu",
                "M. -V.",
                ""
            ],
            [
                "Tomlinson",
                "G. R.",
                ""
            ],
            [
                "Altenbach",
                "H.",
                ""
            ],
            [
                "Kompis",
                "V.",
                ""
            ],
            [
                "Hui",
                "D.",
                ""
            ]
        ]
    },
    {
        "id": "0708.1962",
        "submitter": "Mihai Oltean",
        "authors": "Mihai Oltean, Oana Muntean",
        "title": "Exact Cover with light",
        "comments": "20 pages, 4 figures, New Generation Computing, accepted, 2007",
        "journal-ref": "New Generation Computing, Springer-Verlag, Vol. 26, Issue 4, pp.\n  327-344, 2008",
        "doi": "10.1007/s00354-008-0049-5",
        "report-no": null,
        "categories": "cs.AR cs.DC",
        "license": null,
        "abstract": "  We suggest a new optical solution for solving the YES/NO version of the Exact\nCover problem by using the massive parallelism of light. The idea is to build\nan optical device which can generate all possible solutions of the problem and\nthen to pick the correct one. In our case the device has a graph-like\nrepresentation and the light is traversing it by following the routes given by\nthe connections between nodes. The nodes are connected by arcs in a special way\nwhich lets us to generate all possible covers (exact or not) of the given set.\nFor selecting the correct solution we assign to each item, from the set to be\ncovered, a special integer number. These numbers will actually represent delays\ninduced to light when it passes through arcs. The solution is represented as a\nsubray arriving at a certain moment in the destination node. This will tell us\nif an exact cover does exist or not.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 14 Aug 2007 21:41:55 GMT"
            }
        ],
        "update_date": "2009-02-07",
        "authors_parsed": [
            [
                "Oltean",
                "Mihai",
                ""
            ],
            [
                "Muntean",
                "Oana",
                ""
            ]
        ]
    },
    {
        "id": "0708.1964",
        "submitter": "Mihai Oltean",
        "authors": "Mihai Oltean, Oana Muntean",
        "title": "Solving the subset-sum problem with a light-based device",
        "comments": "14 pages, 6 figures, Natural Computing, 2007",
        "journal-ref": "Natural Computing, Springer-Verlag, Vol 8, Issue 2, pp. 321-331,\n  2009",
        "doi": "10.1007/s11047-007-9059-3",
        "report-no": null,
        "categories": "cs.AR cs.AI cs.DC",
        "license": null,
        "abstract": "  We propose a special computational device which uses light rays for solving\nthe subset-sum problem. The device has a graph-like representation and the\nlight is traversing it by following the routes given by the connections between\nnodes. The nodes are connected by arcs in a special way which lets us to\ngenerate all possible subsets of the given set. To each arc we assign either a\nnumber from the given set or a predefined constant. When the light is passing\nthrough an arc it is delayed by the amount of time indicated by the number\nplaced in that arc. At the destination node we will check if there is a ray\nwhose total delay is equal to the target value of the subset sum problem (plus\nsome constants).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 14 Aug 2007 21:46:32 GMT"
            }
        ],
        "update_date": "2015-09-11",
        "authors_parsed": [
            [
                "Oltean",
                "Mihai",
                ""
            ],
            [
                "Muntean",
                "Oana",
                ""
            ]
        ]
    },
    {
        "id": "0708.1964",
        "submitter": "Mihai Oltean",
        "authors": "Mihai Oltean, Oana Muntean",
        "title": "Solving the subset-sum problem with a light-based device",
        "comments": "14 pages, 6 figures, Natural Computing, 2007",
        "journal-ref": "Natural Computing, Springer-Verlag, Vol 8, Issue 2, pp. 321-331,\n  2009",
        "doi": "10.1007/s11047-007-9059-3",
        "report-no": null,
        "categories": "cs.AR cs.AI cs.DC",
        "license": null,
        "abstract": "  We propose a special computational device which uses light rays for solving\nthe subset-sum problem. The device has a graph-like representation and the\nlight is traversing it by following the routes given by the connections between\nnodes. The nodes are connected by arcs in a special way which lets us to\ngenerate all possible subsets of the given set. To each arc we assign either a\nnumber from the given set or a predefined constant. When the light is passing\nthrough an arc it is delayed by the amount of time indicated by the number\nplaced in that arc. At the destination node we will check if there is a ray\nwhose total delay is equal to the target value of the subset sum problem (plus\nsome constants).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 14 Aug 2007 21:46:32 GMT"
            }
        ],
        "update_date": "2015-09-11",
        "authors_parsed": [
            [
                "Oltean",
                "Mihai",
                ""
            ],
            [
                "Muntean",
                "Oana",
                ""
            ]
        ]
    },
    {
        "id": "0708.2021",
        "submitter": "Juan J. Merelo Pr.",
        "authors": "Juan J. Merelo and Carlos Cotta",
        "title": "Who is the best connected EC researcher? Centrality analysis of the\n  complex network of authors in evolutionary computation",
        "comments": "Abstract accepted as poster to the GECCO 2007 conference",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CY cs.NE",
        "license": null,
        "abstract": "  Co-authorship graphs (that is, the graph of authors linked by co-authorship\nof papers) are complex networks, which expresses the dynamics of a complex\nsystem. Only recently its study has started to draw interest from the EC\ncommunity, the first paper dealing with it having been published two years ago.\nIn this paper we will study the co-authorship network of EC at a microscopic\nlevel. Our objective is ascertaining which are the most relevant nodes (i.e.\nauthors) in it. For this purpose, we examine several metrics defined in the\ncomplex-network literature, and analyze them both in isolation and combined\nwithin a Pareto-dominance approach. The result of our analysis indicates that\nthere are some well-known researchers that appear systematically in top\nrankings. This also provides some hints on the social behavior of our\ncommunity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 15 Aug 2007 10:13:03 GMT"
            }
        ],
        "update_date": "2007-08-16",
        "authors_parsed": [
            [
                "Merelo",
                "Juan J.",
                ""
            ],
            [
                "Cotta",
                "Carlos",
                ""
            ]
        ]
    },
    {
        "id": "0708.2076",
        "submitter": "Loreto Bravo",
        "authors": "Loreto Bravo, James Cheney and Irini Fundulaki",
        "title": "Repairing Inconsistent XML Write-Access Control Policies",
        "comments": "25 pages. To appear in Proceedings of DBPL 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  XML access control policies involving updates may contain security flaws,\nhere called inconsistencies, in which a forbidden operation may be simulated by\nperforming a sequence of allowed operations. This paper investigates the\nproblem of deciding whether a policy is consistent, and if not, how its\ninconsistencies can be repaired. We consider policies expressed in terms of\nannotated DTDs defining which operations are allowed or denied for the XML\ntrees that are instances of the DTD. We show that consistency is decidable in\nPTIME for such policies and that consistent partial policies can be extended to\nunique \"least-privilege\" consistent total policies. We also consider repair\nproblems based on deleting privileges to restore consistency, show that finding\nminimal repairs is NP-complete, and give heuristics for finding repairs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 15 Aug 2007 18:31:48 GMT"
            }
        ],
        "update_date": "2007-08-16",
        "authors_parsed": [
            [
                "Bravo",
                "Loreto",
                ""
            ],
            [
                "Cheney",
                "James",
                ""
            ],
            [
                "Fundulaki",
                "Irini",
                ""
            ]
        ]
    },
    {
        "id": "0708.2078",
        "submitter": "Viktor Levandovskyy",
        "authors": "Viktor Levandovskyy and Eva Zerz",
        "title": "Obstructions to Genericity in Study of Parametric Problems in Control\n  Theory",
        "comments": "20 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SC math.RA",
        "license": null,
        "abstract": "  We investigate systems of equations, involving parameters from the point of\nview of both control theory and computer algebra. The equations might involve\nlinear operators such as partial (q-)differentiation, (q-)shift, (q-)difference\nas well as more complicated ones, which act trivially on the parameters. Such a\nsystem can be identified algebraically with a certain left module over a\nnon-commutative algebra, where the operators commute with the parameters. We\ndevelop, implement and use in practice the algorithm for revealing all the\nexpressions in parameters, for which e.g. homological properties of a system\ndiffer from the generic properties. We use Groebner bases and Groebner basics\nin rings of solvable type as main tools. In particular, we demonstrate an\noptimized algorithm for computing the left inverse of a matrix over a ring of\nsolvable type. We illustrate the article with interesting examples. In\nparticular, we provide a complete solution to the \"two pendula, mounted on a\ncart\" problem from the classical book of Polderman and Willems, including the\ncase, where the friction at the joints is essential . To the best of our\nknowledge, the latter example has not been solved before in a complete way.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 15 Aug 2007 19:12:15 GMT"
            }
        ],
        "update_date": "2010-03-22",
        "authors_parsed": [
            [
                "Levandovskyy",
                "Viktor",
                ""
            ],
            [
                "Zerz",
                "Eva",
                ""
            ]
        ]
    },
    {
        "id": "0708.2173",
        "submitter": "James Cheney",
        "authors": "James Cheney, Amal Ahmed, and Umut Acar",
        "title": "Provenance as Dependency Analysis",
        "comments": "Long version of paper in 2007 Symposium on Database Programming\n  Languages; revised November 2009",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Provenance is information recording the source, derivation, or history of\nsome information. Provenance tracking has been studied in a variety of\nsettings; however, although many design points have been explored, the\nmathematical or semantic foundations of data provenance have received\ncomparatively little attention. In this paper, we argue that dependency\nanalysis techniques familiar from program analysis and program slicing provide\na formal foundation for forms of provenance that are intended to show how (part\nof) the output of a query depends on (parts of) its input. We introduce a\nsemantic characterization of such dependency provenance, show that this form of\nprovenance is not computable, and provide dynamic and static approximation\ntechniques.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 16 Aug 2007 11:11:43 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 22 Dec 2009 15:25:13 GMT"
            }
        ],
        "update_date": "2009-12-22",
        "authors_parsed": [
            [
                "Cheney",
                "James",
                ""
            ],
            [
                "Ahmed",
                "Amal",
                ""
            ],
            [
                "Acar",
                "Umut",
                ""
            ]
        ]
    },
    {
        "id": "0708.2173",
        "submitter": "James Cheney",
        "authors": "James Cheney, Amal Ahmed, and Umut Acar",
        "title": "Provenance as Dependency Analysis",
        "comments": "Long version of paper in 2007 Symposium on Database Programming\n  Languages; revised November 2009",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Provenance is information recording the source, derivation, or history of\nsome information. Provenance tracking has been studied in a variety of\nsettings; however, although many design points have been explored, the\nmathematical or semantic foundations of data provenance have received\ncomparatively little attention. In this paper, we argue that dependency\nanalysis techniques familiar from program analysis and program slicing provide\na formal foundation for forms of provenance that are intended to show how (part\nof) the output of a query depends on (parts of) its input. We introduce a\nsemantic characterization of such dependency provenance, show that this form of\nprovenance is not computable, and provide dynamic and static approximation\ntechniques.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 16 Aug 2007 11:11:43 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 22 Dec 2009 15:25:13 GMT"
            }
        ],
        "update_date": "2009-12-22",
        "authors_parsed": [
            [
                "Cheney",
                "James",
                ""
            ],
            [
                "Ahmed",
                "Amal",
                ""
            ],
            [
                "Acar",
                "Umut",
                ""
            ]
        ]
    },
    {
        "id": "0708.2213",
        "submitter": "Luai Jaff",
        "authors": "Lua\\\"i Jaff (LIPN), G\\'erard H.E. Duchamp (LIPN), Hatem Hadj Kacem,\n  Cyrille Bertelle (LITIS)",
        "title": "Moderate Growth Time Series for Dynamic Combinatorics Modelisation",
        "comments": null,
        "journal-ref": "ECELM-2, Tirgu-Mures : Roumanie (2006)",
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.MA math.CO",
        "license": null,
        "abstract": "  Here, we present a family of time series with a simple growth constraint.\nThis family can be the basis of a model to apply to emerging computation in\nbusiness and micro-economy where global functions can be expressed from local\nrules. We explicit a double statistics on these series which allows to\nestablish a one-to-one correspondence between three other ballot-like\nstrunctures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 16 Aug 2007 14:58:35 GMT"
            }
        ],
        "update_date": "2007-08-17",
        "authors_parsed": [
            [
                "Jaff",
                "Lua\u00ef",
                "",
                "LIPN"
            ],
            [
                "Duchamp",
                "G\u00e9rard H. E.",
                "",
                "LIPN"
            ],
            [
                "Kacem",
                "Hatem Hadj",
                "",
                "LITIS"
            ],
            [
                "Bertelle",
                "Cyrille",
                "",
                "LITIS"
            ]
        ]
    },
    {
        "id": "0708.2255",
        "submitter": "Jeremy Siek",
        "authors": "Jeremy G. Siek and Andrew Lumsdaine",
        "title": "A Language for Generic Programming in the Large",
        "comments": "50 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  Generic programming is an effective methodology for developing reusable\nsoftware libraries. Many programming languages provide generics and have\nfeatures for describing interfaces, but none completely support the idioms used\nin generic programming. To address this need we developed the language G. The\ncentral feature of G is the concept, a mechanism for organizing constraints on\ngenerics that is inspired by the needs of modern C++ libraries. G provides\nmodular type checking and separate compilation (even of generics). These\ncharacteristics support modular software development, especially the smooth\nintegration of independently developed components. In this article we present\nthe rationale for the design of G and demonstrate the expressiveness of G with\ntwo case studies: porting the Standard Template Library and the Boost Graph\nLibrary from C++ to G. The design of G shares much in common with the concept\nextension proposed for the next C++ Standard (the authors participated in its\ndesign) but there are important differences described in this article.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 16 Aug 2007 18:06:18 GMT"
            }
        ],
        "update_date": "2007-08-17",
        "authors_parsed": [
            [
                "Siek",
                "Jeremy G.",
                ""
            ],
            [
                "Lumsdaine",
                "Andrew",
                ""
            ]
        ]
    },
    {
        "id": "0708.2255",
        "submitter": "Jeremy Siek",
        "authors": "Jeremy G. Siek and Andrew Lumsdaine",
        "title": "A Language for Generic Programming in the Large",
        "comments": "50 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  Generic programming is an effective methodology for developing reusable\nsoftware libraries. Many programming languages provide generics and have\nfeatures for describing interfaces, but none completely support the idioms used\nin generic programming. To address this need we developed the language G. The\ncentral feature of G is the concept, a mechanism for organizing constraints on\ngenerics that is inspired by the needs of modern C++ libraries. G provides\nmodular type checking and separate compilation (even of generics). These\ncharacteristics support modular software development, especially the smooth\nintegration of independently developed components. In this article we present\nthe rationale for the design of G and demonstrate the expressiveness of G with\ntwo case studies: porting the Standard Template Library and the Boost Graph\nLibrary from C++ to G. The design of G shares much in common with the concept\nextension proposed for the next C++ Standard (the authors participated in its\ndesign) but there are important differences described in this article.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 16 Aug 2007 18:06:18 GMT"
            }
        ],
        "update_date": "2007-08-17",
        "authors_parsed": [
            [
                "Siek",
                "Jeremy G.",
                ""
            ],
            [
                "Lumsdaine",
                "Andrew",
                ""
            ]
        ]
    },
    {
        "id": "0708.2303",
        "submitter": "W Saba",
        "authors": "Walid S. Saba",
        "title": "Compositional Semantics Grounded in Commonsense Metaphysics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.CL",
        "license": null,
        "abstract": "  We argue for a compositional semantics grounded in a strongly typed ontology\nthat reflects our commonsense view of the world and the way we talk about it in\nordinary language. Assuming the existence of such a structure, we show that the\nsemantics of various natural language phenomena may become nearly trivial.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 17 Aug 2007 01:15:11 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 31 Aug 2007 17:48:14 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Saba",
                "Walid S.",
                ""
            ]
        ]
    },
    {
        "id": "0708.2309",
        "submitter": "Dmitri Krioukov",
        "authors": "Dmitri Krioukov, kc claffy, Kevin Fall, Arthur Brady",
        "title": "On Compact Routing for the Internet",
        "comments": "This is a significantly revised, journal version of cs/0508021",
        "journal-ref": "ACM SIGCOMM Computer Communication Review (CCR), v.37, n.3,\n  p.41-52, 2007",
        "doi": "10.1145/1273445.1273450",
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  While there exist compact routing schemes designed for grids, trees, and\nInternet-like topologies that offer routing tables of sizes that scale\nlogarithmically with the network size, we demonstrate in this paper that in\nview of recent results in compact routing research, such logarithmic scaling on\nInternet-like topologies is fundamentally impossible in the presence of\ntopology dynamics or topology-independent (flat) addressing. We use analytic\narguments to show that the number of routing control messages per topology\nchange cannot scale better than linearly on Internet-like topologies. We also\nemploy simulations to confirm that logarithmic routing table size scaling gets\nbroken by topology-independent addressing, a cornerstone of popular\nlocator-identifier split proposals aiming at improving routing scaling in the\npresence of network topology dynamics or host mobility. These pessimistic\nfindings lead us to the conclusion that a fundamental re-examination of\nassumptions behind routing models and abstractions is needed in order to find a\nrouting architecture that would be able to scale ``indefinitely.''\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 17 Aug 2007 02:37:20 GMT"
            }
        ],
        "update_date": "2008-04-16",
        "authors_parsed": [
            [
                "Krioukov",
                "Dmitri",
                ""
            ],
            [
                "claffy",
                "kc",
                ""
            ],
            [
                "Fall",
                "Kevin",
                ""
            ],
            [
                "Brady",
                "Arthur",
                ""
            ]
        ]
    },
    {
        "id": "0708.2319",
        "submitter": "Marcus Hutter",
        "authors": "Marcus Hutter and Andrej Muchnik",
        "title": "On Semimeasures Predicting Martin-Loef Random Sequences",
        "comments": "21 LaTeX pages",
        "journal-ref": "Theoretical Computer Science, 382 (2007) 247-261",
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.LG math.IT math.PR",
        "license": null,
        "abstract": "  Solomonoff's central result on induction is that the posterior of a universal\nsemimeasure M converges rapidly and with probability 1 to the true sequence\ngenerating posterior mu, if the latter is computable. Hence, M is eligible as a\nuniversal sequence predictor in case of unknown mu. Despite some nearby results\nand proofs in the literature, the stronger result of convergence for all\n(Martin-Loef) random sequences remained open. Such a convergence result would\nbe particularly interesting and natural, since randomness can be defined in\nterms of M itself. We show that there are universal semimeasures M which do not\nconverge for all random sequences, i.e. we give a partial negative answer to\nthe open problem. We also provide a positive answer for some non-universal\nsemimeasures. We define the incomputable measure D as a mixture over all\ncomputable measures and the enumerable semimeasure W as a mixture over all\nenumerable nearly-measures. We show that W converges to D and D to mu on all\nrandom sequences. The Hellinger distance measuring closeness of two\ndistributions plays a central role.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 17 Aug 2007 06:39:11 GMT"
            }
        ],
        "update_date": "2007-08-20",
        "authors_parsed": [
            [
                "Hutter",
                "Marcus",
                ""
            ],
            [
                "Muchnik",
                "Andrej",
                ""
            ]
        ]
    },
    {
        "id": "0708.2353",
        "submitter": "Vladimir Vovk",
        "authors": "Vladimir Vovk",
        "title": "Continuous and randomized defensive forecasting: unified view",
        "comments": "10 pages. The new version: (1) relaxes the assumption that the\n  outcome space is finite, and now it is only assumed to be compact; (2) shows\n  that in the case where the outcome space is finite of cardinality C, the\n  randomized forecasts can be chosen concentrated on a finite set of\n  cardinality at most C",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  Defensive forecasting is a method of transforming laws of probability (stated\nin game-theoretic terms as strategies for Sceptic) into forecasting algorithms.\nThere are two known varieties of defensive forecasting: \"continuous\", in which\nSceptic's moves are assumed to depend on the forecasts in a (semi)continuous\nmanner and which produces deterministic forecasts, and \"randomized\", in which\nthe dependence of Sceptic's moves on the forecasts is arbitrary and\nForecaster's moves are allowed to be randomized. This note shows that the\nrandomized variety can be obtained from the continuous variety by smearing\nSceptic's moves to make them continuous.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 17 Aug 2007 12:18:24 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 23 Aug 2007 12:44:34 GMT"
            }
        ],
        "update_date": "2007-08-23",
        "authors_parsed": [
            [
                "Vovk",
                "Vladimir",
                ""
            ]
        ]
    },
    {
        "id": "0708.2395",
        "submitter": "Milton Chowdhury",
        "authors": "M. M. Chowdhury",
        "title": "Key Agreement and Authentication Schemes Using Non-Commutative\n  Semigroups",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  We give a new two-pass authentication scheme, whichis a generalisation of an\nauthentication scheme of Sibert-Dehornoy-Girault based on the Diffie-Hellman\nconjugacy problem. Compared to the above scheme, for some parameters it is more\nefficient with respect to multiplications. We sketch a proof that our\nauthentication scheme is secure. We give a new key agreement protocols.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 17 Aug 2007 16:38:43 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 20 Aug 2007 00:15:01 GMT"
            }
        ],
        "update_date": "2007-08-20",
        "authors_parsed": [
            [
                "Chowdhury",
                "M. M.",
                ""
            ]
        ]
    },
    {
        "id": "0708.2397",
        "submitter": "Milton Chowdhury",
        "authors": "M. M. Chowdhury",
        "title": "On the AAGL Protocol",
        "comments": "This version differs from the first version because a new attack is\n  given",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  Recently the AAGL (Anshel-Anshel-Goldfeld-Lemieux) has been proposed which\ncan be used for RFID tags. We give algorithms for the problem (we call the\nMSCSPv) on which the security of the AAGL protocol is based upon. Hence we give\nvarious attacks for general parameters on the recent AAGL protocol proposed.\nOne of our attacks is a deterministic algorithm which has space complexity and\ntime complexity both atleast exponentialin the worst case. In a better case\nusing a probabilistic algorithm the time complexity canbe\nO(|XSS(ui')^L5*(n^(1+e)) and the space complexity can be O(|XSS(ui')|^L6),\nwhere the element ui' is part of a public key, n is the index of braid group,\nXSS is a summit type set and e is a constant in a limit. The above shows the\nAAGL protocol is potentially not significantly more secure as using key\nagreement protocols based on the conjugacy problem such as the AAG\n(Anshel-Anshel-Goldfeld) protocol because both protocols can be broken with\ncomplexity which do not significantly differ. We think our attacks can be\nimproved.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 17 Aug 2007 16:48:13 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 21 Aug 2007 00:44:02 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 1 Oct 2007 10:26:48 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 19 Nov 2007 16:11:45 GMT"
            },
            {
                "version": "v5",
                "created": "Thu, 22 Nov 2007 22:27:48 GMT"
            }
        ],
        "update_date": "2007-11-23",
        "authors_parsed": [
            [
                "Chowdhury",
                "M. M.",
                ""
            ]
        ]
    },
    {
        "id": "0708.2432",
        "submitter": "Oliver Knill",
        "authors": "Oliver Knill and Jose Ramirez-Herran",
        "title": "A structure from motion inequality",
        "comments": "15 pages, 22 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.AI",
        "license": null,
        "abstract": "  We state an elementary inequality for the structure from motion problem for m\ncameras and n points. This structure from motion inequality relates space\ndimension, camera parameter dimension, the number of cameras and number points\nand global symmetry properties and provides a rigorous criterion for which\nreconstruction is not possible with probability 1. Mathematically the\ninequality is based on Frobenius theorem which is a geometric incarnation of\nthe fundamental theorem of linear algebra. The paper also provides a general\nmathematical formalism for the structure from motion problem. It includes the\nsituation the points can move while the camera takes the pictures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 18 Aug 2007 14:36:28 GMT"
            }
        ],
        "update_date": "2007-08-21",
        "authors_parsed": [
            [
                "Knill",
                "Oliver",
                ""
            ],
            [
                "Ramirez-Herran",
                "Jose",
                ""
            ]
        ]
    },
    {
        "id": "0708.2432",
        "submitter": "Oliver Knill",
        "authors": "Oliver Knill and Jose Ramirez-Herran",
        "title": "A structure from motion inequality",
        "comments": "15 pages, 22 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.AI",
        "license": null,
        "abstract": "  We state an elementary inequality for the structure from motion problem for m\ncameras and n points. This structure from motion inequality relates space\ndimension, camera parameter dimension, the number of cameras and number points\nand global symmetry properties and provides a rigorous criterion for which\nreconstruction is not possible with probability 1. Mathematically the\ninequality is based on Frobenius theorem which is a geometric incarnation of\nthe fundamental theorem of linear algebra. The paper also provides a general\nmathematical formalism for the structure from motion problem. It includes the\nsituation the points can move while the camera takes the pictures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 18 Aug 2007 14:36:28 GMT"
            }
        ],
        "update_date": "2007-08-21",
        "authors_parsed": [
            [
                "Knill",
                "Oliver",
                ""
            ],
            [
                "Ramirez-Herran",
                "Jose",
                ""
            ]
        ]
    },
    {
        "id": "0708.2438",
        "submitter": "Oliver Knill",
        "authors": "Oliver Knill and Jose Ramirez-Herran",
        "title": "On Ullman's theorem in computer vision",
        "comments": "16 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.AI",
        "license": null,
        "abstract": "  Both in the plane and in space, we invert the nonlinear Ullman transformation\nfor 3 points and 3 orthographic cameras. While Ullman's theorem assures a\nunique reconstruction modulo a reflection for 3 cameras and 4 points, we find a\nlocally unique reconstruction for 3 cameras and 3 points. Explicit\nreconstruction formulas allow to decide whether picture data of three cameras\nseeing three points can be realized as a point-camera configuration.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 17 Aug 2007 21:36:08 GMT"
            }
        ],
        "update_date": "2007-08-21",
        "authors_parsed": [
            [
                "Knill",
                "Oliver",
                ""
            ],
            [
                "Ramirez-Herran",
                "Jose",
                ""
            ]
        ]
    },
    {
        "id": "0708.2438",
        "submitter": "Oliver Knill",
        "authors": "Oliver Knill and Jose Ramirez-Herran",
        "title": "On Ullman's theorem in computer vision",
        "comments": "16 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.AI",
        "license": null,
        "abstract": "  Both in the plane and in space, we invert the nonlinear Ullman transformation\nfor 3 points and 3 orthographic cameras. While Ullman's theorem assures a\nunique reconstruction modulo a reflection for 3 cameras and 4 points, we find a\nlocally unique reconstruction for 3 cameras and 3 points. Explicit\nreconstruction formulas allow to decide whether picture data of three cameras\nseeing three points can be realized as a point-camera configuration.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 17 Aug 2007 21:36:08 GMT"
            }
        ],
        "update_date": "2007-08-21",
        "authors_parsed": [
            [
                "Knill",
                "Oliver",
                ""
            ],
            [
                "Ramirez-Herran",
                "Jose",
                ""
            ]
        ]
    },
    {
        "id": "0708.2442",
        "submitter": "Oliver Knill",
        "authors": "Oliver Knill and Jose Ramirez-Herran",
        "title": "Space and camera path reconstruction for omni-directional vision",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.AI",
        "license": null,
        "abstract": "  In this paper, we address the inverse problem of reconstructing a scene as\nwell as the camera motion from the image sequence taken by an omni-directional\ncamera. Our structure from motion results give sharp conditions under which the\nreconstruction is unique. For example, if there are three points in general\nposition and three omni-directional cameras in general position, a unique\nreconstruction is possible up to a similarity. We then look at the\nreconstruction problem with m cameras and n points, where n and m can be large\nand the over-determined system is solved by least square methods. The\nreconstruction is robust and generalizes to the case of a dynamic environment\nwhere landmarks can move during the movie capture. Possible applications of the\nresult are computer assisted scene reconstruction, 3D scanning, autonomous\nrobot navigation, medical tomography and city reconstructions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 17 Aug 2007 21:53:41 GMT"
            }
        ],
        "update_date": "2007-08-21",
        "authors_parsed": [
            [
                "Knill",
                "Oliver",
                ""
            ],
            [
                "Ramirez-Herran",
                "Jose",
                ""
            ]
        ]
    },
    {
        "id": "0708.2442",
        "submitter": "Oliver Knill",
        "authors": "Oliver Knill and Jose Ramirez-Herran",
        "title": "Space and camera path reconstruction for omni-directional vision",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.AI",
        "license": null,
        "abstract": "  In this paper, we address the inverse problem of reconstructing a scene as\nwell as the camera motion from the image sequence taken by an omni-directional\ncamera. Our structure from motion results give sharp conditions under which the\nreconstruction is unique. For example, if there are three points in general\nposition and three omni-directional cameras in general position, a unique\nreconstruction is possible up to a similarity. We then look at the\nreconstruction problem with m cameras and n points, where n and m can be large\nand the over-determined system is solved by least square methods. The\nreconstruction is robust and generalizes to the case of a dynamic environment\nwhere landmarks can move during the movie capture. Possible applications of the\nresult are computer assisted scene reconstruction, 3D scanning, autonomous\nrobot navigation, medical tomography and city reconstructions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 17 Aug 2007 21:53:41 GMT"
            }
        ],
        "update_date": "2007-08-21",
        "authors_parsed": [
            [
                "Knill",
                "Oliver",
                ""
            ],
            [
                "Ramirez-Herran",
                "Jose",
                ""
            ]
        ]
    },
    {
        "id": "0708.2571",
        "submitter": "Milton Chowdhury",
        "authors": "M. M. Chowdhury",
        "title": "On the Security of the Cha-Ko-Lee-Han-Cheon Braid Group Public Key\n  Cryptosystem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  We show that a number of cryptographic protocols using non-commutative\nsemigroups including the Cha-Ko-Lee-Han-Cheon braid group public-key\ncryptosystem and related public-key cryptosystems such as the Shpilrain-Ushakov\npublic-key cryptosystems are based on the MSCSP.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 20 Aug 2007 00:47:15 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 22 Aug 2007 12:58:05 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 23 Aug 2007 02:54:51 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 24 Aug 2007 00:56:05 GMT"
            },
            {
                "version": "v5",
                "created": "Thu, 15 Nov 2007 22:28:36 GMT"
            }
        ],
        "update_date": "2007-11-16",
        "authors_parsed": [
            [
                "Chowdhury",
                "M. M.",
                ""
            ]
        ]
    },
    {
        "id": "0708.2616",
        "submitter": "Sourav Dhar",
        "authors": "Sourav Dhar and Kabir Chakraborty",
        "title": "An Experimental Investigation of Secure Communication With Chaos Masking",
        "comments": "8 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  The most exciting recent development in nonlinear dynamics is realization\nthat chaos can be useful. One application involves \"Secure Communication\". Two\npiecewise linear systems with switching nonlinearities have been taken as chaos\ngenerators. In the present work the phenomenon of secure communication with\nchaos masking has been investigated experimentally. In this investigation chaos\nwhich is generated from two chaos generators is masked with the massage signal\nto be transmitted, thus makes communication is more secure.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 20 Aug 2007 09:39:18 GMT"
            }
        ],
        "update_date": "2007-08-21",
        "authors_parsed": [
            [
                "Dhar",
                "Sourav",
                ""
            ],
            [
                "Chakraborty",
                "Kabir",
                ""
            ]
        ]
    },
    {
        "id": "0708.2686",
        "submitter": "Darko Roglic",
        "authors": "D. Roglic",
        "title": "The universal evolutionary computer based on super-recursive algorithms\n  of evolvability",
        "comments": "7 pages, 1 table, 1 figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  This work exposes which mechanisms and procesess in the Nature of evolution\ncompute a function not computable by Turing machine. The computer with\nintelligence that is not higher than one bacteria population could have, but\nwith efficency to solve the problems that are non-computable by Turing machine\nis represented. This theoretical construction is called Universal Evolutinary\nComputer and it is based on the superecursive algorithms of evolvability.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jul 2007 18:50:22 GMT"
            }
        ],
        "update_date": "2007-08-21",
        "authors_parsed": [
            [
                "Roglic",
                "D.",
                ""
            ]
        ]
    },
    {
        "id": "0708.2717",
        "submitter": "Alejandro Vaisman Prof.",
        "authors": "Leticia Gomez, Bart Kuijpers, Alejandro Vaisman",
        "title": "Aggregation Languages for Moving Object and Places of Interest Data",
        "comments": "15 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  We address aggregate queries over GIS data and moving object data, where\nnon-spatial data are stored in a data warehouse. We propose a formal data model\nand query language to express complex aggregate queries. Next, we study the\ncompression of trajectory data, produced by moving objects, using the notions\nof stops and moves. We show that stops and moves are expressible in our query\nlanguage and we consider a fragment of this language, consisting of regular\nexpressions to talk about temporally ordered sequences of stops and moves. This\nfragment can be used to efficiently express data mining and pattern matching\ntasks over trajectory data.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 20 Aug 2007 20:08:53 GMT"
            }
        ],
        "update_date": "2007-08-22",
        "authors_parsed": [
            [
                "Gomez",
                "Leticia",
                ""
            ],
            [
                "Kuijpers",
                "Bart",
                ""
            ],
            [
                "Vaisman",
                "Alejandro",
                ""
            ]
        ]
    },
    {
        "id": "0708.2843",
        "submitter": "Roger Colbeck",
        "authors": "Roger Colbeck",
        "title": "The Impossibility Of Secure Two-Party Classical Computation",
        "comments": "10 pages",
        "journal-ref": "Physical Review A 76, 062308 (2007)",
        "doi": "10.1103/PhysRevA.76.062308",
        "report-no": null,
        "categories": "quant-ph cs.CR",
        "license": null,
        "abstract": "  We present attacks that show that unconditionally secure two-party classical\ncomputation is impossible for many classes of function. Our analysis applies to\nboth quantum and relativistic protocols. We illustrate our results by showing\nthe impossibility of oblivious transfer.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 21 Aug 2007 13:52:45 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 14 Dec 2007 08:59:41 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Colbeck",
                "Roger",
                ""
            ]
        ]
    },
    {
        "id": "0708.2974",
        "submitter": "Preda Mihailescu",
        "authors": "Preda Mihailescu",
        "title": "The Fuzzy Vault for fingerprints is Vulnerable to Brute Force Attack",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.CR",
        "license": null,
        "abstract": "  The \\textit{fuzzy vault} approach is one of the best studied and well\naccepted ideas for binding cryptographic security into biometric\nauthentication. The vault has been implemented in connection with fingerprint\ndata by Uludag and Jain. We show that this instance of the vault is vulnerable\nto brute force attack. An interceptor of the vault data can recover both secret\nand template data using only generally affordable computational resources. Some\npossible alternatives are then discussed and it is suggested that cryptographic\nsecurity may be preferable to the one - way function approach to biometric\nsecurity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 22 Aug 2007 08:28:02 GMT"
            }
        ],
        "update_date": "2007-08-23",
        "authors_parsed": [
            [
                "Mihailescu",
                "Preda",
                ""
            ]
        ]
    },
    {
        "id": "0708.2974",
        "submitter": "Preda Mihailescu",
        "authors": "Preda Mihailescu",
        "title": "The Fuzzy Vault for fingerprints is Vulnerable to Brute Force Attack",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.CR",
        "license": null,
        "abstract": "  The \\textit{fuzzy vault} approach is one of the best studied and well\naccepted ideas for binding cryptographic security into biometric\nauthentication. The vault has been implemented in connection with fingerprint\ndata by Uludag and Jain. We show that this instance of the vault is vulnerable\nto brute force attack. An interceptor of the vault data can recover both secret\nand template data using only generally affordable computational resources. Some\npossible alternatives are then discussed and it is suggested that cryptographic\nsecurity may be preferable to the one - way function approach to biometric\nsecurity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 22 Aug 2007 08:28:02 GMT"
            }
        ],
        "update_date": "2007-08-23",
        "authors_parsed": [
            [
                "Mihailescu",
                "Preda",
                ""
            ]
        ]
    },
    {
        "id": "0708.3014",
        "submitter": "Elisa Gorla",
        "authors": "Elisa Gorla, Christoph Puttmann, and Jamshid Shokrollahi",
        "title": "Explicit formulas for efficient multiplication in F_{3^{6m}}",
        "comments": "11 pages, to appear in the proceedings of SAC2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.CC",
        "license": null,
        "abstract": "  Efficient computation of the Tate pairing is an important part of\npairing-based cryptography. Recently with the introduction of the Duursma-Lee\nmethod special attention has been given to the fields of characteristic 3.\nEspecially multiplication in F_{3^{6m}}, where m is prime, is an important\noperation in the above method. In this paper we propose a new method to reduce\nthe number of F_{3^m} multiplications for multiplication in F_{3^{6m}} from 18\nin recent implementations to 15. The method is based on the fast Fourier\ntranmsform and explicit formulas are given. The execution times of our software\nimplementations for F_{3^{6m}} show the efficiency of our results.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 22 Aug 2007 13:52:09 GMT"
            }
        ],
        "update_date": "2007-08-23",
        "authors_parsed": [
            [
                "Gorla",
                "Elisa",
                ""
            ],
            [
                "Puttmann",
                "Christoph",
                ""
            ],
            [
                "Shokrollahi",
                "Jamshid",
                ""
            ]
        ]
    },
    {
        "id": "0708.3022",
        "submitter": "Elisa Gorla",
        "authors": "Jamshid Shokrollahi, Elisa Gorla, Christoph Puttmann",
        "title": "Efficient FPGA-based multipliers for F_{3^97} and F_{3^{6*97}}",
        "comments": "6 pages, 3 figures, to appear in the proceedings of FPL07",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  In this work we present a new structure for multiplication in finite fields.\nThis structure is based on a digit-level LFSR (Linear Feedback Shift Register)\nmultiplier in which the area of digit-multipliers are reduced using the\nKaratsuba method. We compare our results with the other works in the literature\nfor F_{3^97}. We also propose new formulas for multiplication in F_{3^{6*97}}.\nThese new formulas reduce the number of F_{3^97}-multiplications from 18 to 15.\nThe fields F_{3^{97}} and F_{3^{6*97}} are relevant in the context of\npairing-based cryptography.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 22 Aug 2007 14:17:08 GMT"
            }
        ],
        "update_date": "2007-08-23",
        "authors_parsed": [
            [
                "Shokrollahi",
                "Jamshid",
                ""
            ],
            [
                "Gorla",
                "Elisa",
                ""
            ],
            [
                "Puttmann",
                "Christoph",
                ""
            ]
        ]
    },
    {
        "id": "0708.3048",
        "submitter": "Alexandre d'Aspremont",
        "authors": "Alexandre d'Aspremont",
        "title": "Identifying Small Mean Reverting Portfolios",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": null,
        "abstract": "  Given multivariate time series, we study the problem of forming portfolios\nwith maximum mean reversion while constraining the number of assets in these\nportfolios. We show that it can be formulated as a sparse canonical correlation\nanalysis and study various algorithms to solve the corresponding sparse\ngeneralized eigenvalue problems. After discussing penalized parameter\nestimation procedures, we study the sparsity versus predictability tradeoff and\nthe impact of predictability in various markets.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 22 Aug 2007 16:25:17 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 26 Feb 2008 16:29:01 GMT"
            }
        ],
        "update_date": "2008-02-26",
        "authors_parsed": [
            [
                "d'Aspremont",
                "Alexandre",
                ""
            ]
        ]
    },
    {
        "id": "0708.3057",
        "submitter": "Hai Jiang",
        "authors": "Hai Jiang, Ping Wang, H. Vincent Poor, and Weihua Zhuang",
        "title": "Voice Service Support in Mobile Ad Hoc Networks",
        "comments": "To appear in the Proceedings of the IEEE Global Communications\n  Conference (GLOBECOM), Washington, DC, November 26 - 30, 2007",
        "journal-ref": null,
        "doi": "10.1109/GLOCOM.2007.186",
        "report-no": null,
        "categories": "cs.IT cs.NI math.IT",
        "license": null,
        "abstract": "  Mobile ad hoc networks are expected to support voice traffic. The requirement\nfor small delay and jitter of voice traffic poses a significant challenge for\nmedium access control (MAC) in such networks. User mobility makes it more\ncomplex due to the associated dynamic path attenuation. In this paper, a MAC\nscheme for mobile ad hoc networks supporting voice traffic is proposed. With\nthe aid of a low-power probe prior to DATA transmissions, resource reservation\nis achieved in a distributed manner, thus leading to small delay and jitter.\nThe proposed scheme can automatically adapt to dynamic path attenuation in a\nmobile environment. Simulation results demonstrate the effectiveness of the\nproposed scheme.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 22 Aug 2007 17:12:10 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Jiang",
                "Hai",
                ""
            ],
            [
                "Wang",
                "Ping",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ],
            [
                "Zhuang",
                "Weihua",
                ""
            ]
        ]
    },
    {
        "id": "0708.3166",
        "submitter": "Achmad Benny Mutiara",
        "authors": "A. B. Mutiara and T. A. Sabastian",
        "title": "Web Server Benchmark Application WiiBench using Erlang/OTP R11 and\n  Fedora-Core Linux 5.0",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  As the web grows and the amount of traffics on the web server increase,\nproblems related to performance begin to appear. Some of the problems, such as\nthe number of users that can access the server simultaneously, the number of\nrequests that can be handled by the server per second (requests per second) to\nbandwidth consumption and hardware utilization like memories and CPU. To give\nbetter quality of service (\\textbf{\\textit{QoS}}), web hosting providers and\nalso the system administrators and network administrators who manage the server\nneed a benchmark application to measure the capabilities of their servers.\nLater, the application intends to work under Linux/Unix -- like platforms and\nbuilt using Erlang/OTP R11 as a concurrent oriented language under Fedora Core\nLinux 5.0. \\textbf{\\textit{WiiBench}} is divided into two main parts, the\ncontroller section and the launcher section. Controller is the core of the\napplication. It has several duties, such as read the benchmark scenario file,\nconfigure the program based on the scenario, initialize the launcher section,\ngather the benchmark results from local and remote Erlang node where the\nlauncher runs and write them in a log file (later the log file will be used to\ngenerate a report page for the sysadmin). Controller also has function as a\ntimer which act as timing for user inters arrival to the server. Launcher\ngenerates a number of users based on the scenario, initialize them and start\nthe benchmark by sending requests to the web server. The clients also gather\nthe benchmark result and send them to the controller.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 23 Aug 2007 12:25:50 GMT"
            }
        ],
        "update_date": "2007-08-24",
        "authors_parsed": [
            [
                "Mutiara",
                "A. B.",
                ""
            ],
            [
                "Sabastian",
                "T. A.",
                ""
            ]
        ]
    },
    {
        "id": "0708.3220",
        "submitter": "Jean-Charles Delvenne",
        "authors": "Jean-Charles Delvenne, Ruggero Carli and Sandro Zampieri",
        "title": "Optimal strategies in the average consensus problem",
        "comments": "9 pages; extended preprint with proofs of a CDC 2007 (Conference on\n  decision and Control) paper",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MA cs.NI math.OC",
        "license": null,
        "abstract": "  We prove that for a set of communicating agents to compute the average of\ntheir initial positions (average consensus problem), the optimal topology of\ncommunication is given by a de Bruijn's graph. Consensus is then reached in a\nfinitely many steps. A more general family of strategies, constructed by block\nKronecker products, is investigated and compared to Cayley strategies.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 23 Aug 2007 17:53:54 GMT"
            }
        ],
        "update_date": "2013-09-18",
        "authors_parsed": [
            [
                "Delvenne",
                "Jean-Charles",
                ""
            ],
            [
                "Carli",
                "Ruggero",
                ""
            ],
            [
                "Zampieri",
                "Sandro",
                ""
            ]
        ]
    },
    {
        "id": "0708.3226",
        "submitter": "Rustem Takhanov",
        "authors": "Rustem Takhanov",
        "title": "A Dichotomy Theorem for General Minimum Cost Homomorphism Problem",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.CC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the constraint satisfaction problem ($CSP$), the aim is to find an\nassignment of values to a set of variables subject to specified constraints. In\nthe minimum cost homomorphism problem ($MinHom$), one is additionally given\nweights $c_{va}$ for every variable $v$ and value $a$, and the aim is to find\nan assignment $f$ to the variables that minimizes $\\sum_{v} c_{vf(v)}$. Let\n$MinHom(\\Gamma)$ denote the $MinHom$ problem parameterized by the set of\npredicates allowed for constraints. $MinHom(\\Gamma)$ is related to many\nwell-studied combinatorial optimization problems, and concrete applications can\nbe found in, for instance, defence logistics and machine learning. We show that\n$MinHom(\\Gamma)$ can be studied by using algebraic methods similar to those\nused for CSPs. With the aid of algebraic techniques, we classify the\ncomputational complexity of $MinHom(\\Gamma)$ for all choices of $\\Gamma$. Our\nresult settles a general dichotomy conjecture previously resolved only for\ncertain classes of directed graphs, [Gutin, Hell, Rafiey, Yeo, European J. of\nCombinatorics, 2008].\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 23 Aug 2007 18:26:21 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 31 Aug 2008 21:54:49 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 22 Jan 2009 13:53:56 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 23 Jan 2009 16:13:44 GMT"
            },
            {
                "version": "v5",
                "created": "Mon, 20 Apr 2009 15:18:35 GMT"
            },
            {
                "version": "v6",
                "created": "Thu, 16 Jul 2009 16:43:08 GMT"
            },
            {
                "version": "v7",
                "created": "Sun, 4 Apr 2010 20:39:03 GMT"
            }
        ],
        "update_date": "2010-04-06",
        "authors_parsed": [
            [
                "Takhanov",
                "Rustem",
                ""
            ]
        ]
    },
    {
        "id": "0708.3230",
        "submitter": "Kamil Kulesza",
        "authors": "Kamil Kulesza",
        "title": "Can Alice and Bob be random: a study on human playing zero knowledge\n  protocols",
        "comments": "An extended abstract of the paper submitted for publication, 3 pages\n  total",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.CY",
        "license": null,
        "abstract": "  The research described in this abstract was initiated by discussions between\nthe author and Giovanni Di Crescenzo in Barcelona in early 2004. It was during\nAdvanced Course on Contemporary Cryptology that Di Crescenzo gave a course on\nzero knowledge protocols (ZKP), see [1]. After that course we started to play\nwith unorthodox ideas for breaking ZKP, especially one based on graph\n3-coloring. It was chosen for investigation because it is being considered as a\n\"benchmark\" ZKP, see [2], [3]. At this point we briefly recall such a\nprotocol's description.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 23 Aug 2007 18:39:56 GMT"
            }
        ],
        "update_date": "2007-08-24",
        "authors_parsed": [
            [
                "Kulesza",
                "Kamil",
                ""
            ]
        ]
    },
    {
        "id": "0708.3259",
        "submitter": "Rasmus Pagh",
        "authors": "Philip Bille, Anna Pagh, Rasmus Pagh",
        "title": "Fast evaluation of union-intersection expressions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.DB cs.IR",
        "license": null,
        "abstract": "  We show how to represent sets in a linear space data structure such that\nexpressions involving unions and intersections of sets can be computed in a\nworst-case efficient way. This problem has applications in e.g. information\nretrieval and database systems. We mainly consider the RAM model of\ncomputation, and sets of machine words, but also state our results in the I/O\nmodel. On a RAM with word size $w$, a special case of our result is that the\nintersection of $m$ (preprocessed) sets, containing $n$ elements in total, can\nbe computed in expected time $O(n (\\log w)^2 / w + km)$, where $k$ is the\nnumber of elements in the intersection. If the first of the two terms\ndominates, this is a factor $w^{1-o(1)}$ faster than the standard solution of\nmerging sorted lists. We show a cell probe lower bound of time $\\Omega(n/(w m\n\\log m)+ (1-\\tfrac{\\log k}{w}) k)$, meaning that our upper bound is nearly\noptimal for small $m$. Our algorithm uses a novel combination of approximate\nset representations and word-level parallelism.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 23 Aug 2007 22:23:04 GMT"
            }
        ],
        "update_date": "2007-08-27",
        "authors_parsed": [
            [
                "Bille",
                "Philip",
                ""
            ],
            [
                "Pagh",
                "Anna",
                ""
            ],
            [
                "Pagh",
                "Rasmus",
                ""
            ]
        ]
    },
    {
        "id": "0708.3341",
        "submitter": "Noelle Carbonell",
        "authors": "Olivier Christmann (INRIA Rocquencourt / INRIA Lorraine - LORIA),\n  No\\\"elle Carbonell (INRIA Rocquencourt / INRIA Lorraine - LORIA)",
        "title": "Browsing through 3D representations of unstructured picture collections:\n  an empirical study",
        "comments": "4 pages",
        "journal-ref": "Dans Proceedings of ACM Working Conference on Advanced Visual\n  Interfaces (AVI 2006), Venezia, Italy, May 23-26, 2006 - ACM Working\n  Conference on Advanced Visual Interfaces (AVI 2006), Venezia : Italie (2006)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  The paper presents a 3D interactive representation of fairly large picture\ncollections which facilitates browsing through unstructured sets of icons or\npictures. Implementation of this representation implies choosing between two\nvisualization strategies: users may either manipulate the view (OV) or be\nimmersed in it (IV). The paper first presents this representation, then\ndescribes an empirical study (17 participants) aimed at assessing the utility\nand usability of each view. Subjective judgements in questionnaires and\ndebriefings were varied: 7 participants preferred the IV view, 4 the OV one,\nand 6 could not choose between the two. Visual acuity and visual exploration\nstrategies seem to have exerted a greater influence on participants'\npreferences than task performance or feeling of immersion.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 24 Aug 2007 13:33:38 GMT"
            }
        ],
        "update_date": "2007-08-27",
        "authors_parsed": [
            [
                "Christmann",
                "Olivier",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ],
            [
                "Carbonell",
                "No\u00eblle",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0708.3446",
        "submitter": "L.T. Handoko",
        "authors": "Z. Akbar and L.T. Handoko",
        "title": "Multi and Independent Block Approach in Public Cluster",
        "comments": "3 pages, Proceeding of the 3rd Information and Communication\n  Technology Seminar 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": "FISIKALIPI-07012",
        "categories": "cs.DC",
        "license": null,
        "abstract": "  We present extended multi block approach in the LIPI Public Cluster. The\nmulti block approach enables a cluster to be divided into several independent\nblocks which run jobs owned by different users simultaneously. Previously, we\nhave maintained the blocks using single master node for all blocks due to\nefficiency and resource limitations. Following recent advancements and\nexpansion of node\\'s number, we have modified the multi block approach with\nmultiple master nodes, each of them is responsible for a single block. We argue\nthat this approach improves the overall performance significantly, for\nespecially data intensive computational works.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 25 Aug 2007 19:46:52 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 14 Sep 2007 21:22:23 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Akbar",
                "Z.",
                ""
            ],
            [
                "Handoko",
                "L. T.",
                ""
            ]
        ]
    },
    {
        "id": "0708.3463",
        "submitter": "Sabatino Costanzo",
        "authors": "Sabatino Costanzo, Loren Trigo, Luis Jimenez, Juan Gonzalez",
        "title": "A Neural Networks Model of the Venezuelan Economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.NE",
        "license": null,
        "abstract": "  Besides an indicator of the GDP, the Central Bank of Venezuela generates the\nso called Monthly Economic Activity General Indicator. The a priori knowledge\nof this indicator, which represents and sometimes even anticipates the\neconomy's fluctuations, could be helpful in developing public policies and in\ninvestment decision making. The purpose of this study is forecasting the IGAEM\nthrough non parametric methods, an approach that has proven effective in a wide\nvariety of problems in economics and finance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 26 Aug 2007 05:10:29 GMT"
            }
        ],
        "update_date": "2007-08-28",
        "authors_parsed": [
            [
                "Costanzo",
                "Sabatino",
                ""
            ],
            [
                "Trigo",
                "Loren",
                ""
            ],
            [
                "Jimenez",
                "Luis",
                ""
            ],
            [
                "Gonzalez",
                "Juan",
                ""
            ]
        ]
    },
    {
        "id": "0708.3463",
        "submitter": "Sabatino Costanzo",
        "authors": "Sabatino Costanzo, Loren Trigo, Luis Jimenez, Juan Gonzalez",
        "title": "A Neural Networks Model of the Venezuelan Economy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.NE",
        "license": null,
        "abstract": "  Besides an indicator of the GDP, the Central Bank of Venezuela generates the\nso called Monthly Economic Activity General Indicator. The a priori knowledge\nof this indicator, which represents and sometimes even anticipates the\neconomy's fluctuations, could be helpful in developing public policies and in\ninvestment decision making. The purpose of this study is forecasting the IGAEM\nthrough non parametric methods, an approach that has proven effective in a wide\nvariety of problems in economics and finance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 26 Aug 2007 05:10:29 GMT"
            }
        ],
        "update_date": "2007-08-28",
        "authors_parsed": [
            [
                "Costanzo",
                "Sabatino",
                ""
            ],
            [
                "Trigo",
                "Loren",
                ""
            ],
            [
                "Jimenez",
                "Luis",
                ""
            ],
            [
                "Gonzalez",
                "Juan",
                ""
            ]
        ]
    },
    {
        "id": "0708.3464",
        "submitter": "Sabatino Costanzo",
        "authors": "Sabatino Costanzo, Loren Trigo, Ramses Dominguez, William Moreno",
        "title": "A Non Parametric Study of the Volatility of the Economy as a Country\n  Risk Predictor",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.NE",
        "license": null,
        "abstract": "  This paper intends to explain Venezuela's country spread behavior through the\nNeural Networks analysis of a monthly economic activity general index of\neconomic indicators constructed by the Central Bank of Venezuela, a measure of\nthe shocks affecting country risk of emerging markets and the U.S. short term\ninterest rate. The use of non parametric methods allowed the finding of non\nlinear relationship between these inputs and the country risk. The networks\nperformance was evaluated using the method of excess predictability.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 26 Aug 2007 05:30:18 GMT"
            }
        ],
        "update_date": "2007-08-28",
        "authors_parsed": [
            [
                "Costanzo",
                "Sabatino",
                ""
            ],
            [
                "Trigo",
                "Loren",
                ""
            ],
            [
                "Dominguez",
                "Ramses",
                ""
            ],
            [
                "Moreno",
                "William",
                ""
            ]
        ]
    },
    {
        "id": "0708.3464",
        "submitter": "Sabatino Costanzo",
        "authors": "Sabatino Costanzo, Loren Trigo, Ramses Dominguez, William Moreno",
        "title": "A Non Parametric Study of the Volatility of the Economy as a Country\n  Risk Predictor",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.NE",
        "license": null,
        "abstract": "  This paper intends to explain Venezuela's country spread behavior through the\nNeural Networks analysis of a monthly economic activity general index of\neconomic indicators constructed by the Central Bank of Venezuela, a measure of\nthe shocks affecting country risk of emerging markets and the U.S. short term\ninterest rate. The use of non parametric methods allowed the finding of non\nlinear relationship between these inputs and the country risk. The networks\nperformance was evaluated using the method of excess predictability.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 26 Aug 2007 05:30:18 GMT"
            }
        ],
        "update_date": "2007-08-28",
        "authors_parsed": [
            [
                "Costanzo",
                "Sabatino",
                ""
            ],
            [
                "Trigo",
                "Loren",
                ""
            ],
            [
                "Dominguez",
                "Ramses",
                ""
            ],
            [
                "Moreno",
                "William",
                ""
            ]
        ]
    },
    {
        "id": "0708.3465",
        "submitter": "Sabatino Costanzo",
        "authors": "Loren Trigo, Sabatino Costanzo, Felix Gonzalez, Jose Llamozas",
        "title": "An Early Warning System for Bankruptcy Prediction: lessons from the\n  Venezuelan Bank Crisis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": null,
        "abstract": "  During 1993-94 Venezuela experienced a severe banking crisis which ended up\nwith 18 commercial banks intervened by the government. Here we develop an early\nwarning system for detecting credit related bankruptcy through discriminant\nfunctions developed on financial and macroeconomic data predating the crisis. A\nrobustness test performed on these functions shows high precision in error\nestimation. The model calibrated on pre-crisis data could detect abnormal\nfinancial tension in the late Banco Capital many months before it was\nintervened and liquidated.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 26 Aug 2007 05:33:41 GMT"
            }
        ],
        "update_date": "2007-08-28",
        "authors_parsed": [
            [
                "Trigo",
                "Loren",
                ""
            ],
            [
                "Costanzo",
                "Sabatino",
                ""
            ],
            [
                "Gonzalez",
                "Felix",
                ""
            ],
            [
                "Llamozas",
                "Jose",
                ""
            ]
        ]
    },
    {
        "id": "0708.3499",
        "submitter": "Francesc Rossell\\'o",
        "authors": "Gabriel Cardona, Francesc Rossello, Gabriel Valiente",
        "title": "Comparison of Tree-Child Phylogenetic Networks",
        "comments": "37 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-bio.PE cs.CE cs.DM",
        "license": null,
        "abstract": "  Phylogenetic networks are a generalization of phylogenetic trees that allow\nfor the representation of non-treelike evolutionary events, like recombination,\nhybridization, or lateral gene transfer. In this paper, we present and study a\nnew class of phylogenetic networks, called tree-child phylogenetic networks,\nwhere every non-extant species has some descendant through mutation. We provide\nan injective representation of these networks as multisets of vectors of\nnatural numbers, their path multiplicity vectors, and we use this\nrepresentation to define a distance on this class and to give an alignment\nmethod for pairs of these networks. To the best of our knowledge, they are\nrespectively the first true distance and the first alignment method defined on\na meaningful class of phylogenetic networks strictly extending the class of\nphylogenetic trees. Simple, polynomial algorithms for reconstructing a\ntree-child phylogenetic network from its path multiplicity vectors, for\ncomputing the distance between two tree-child phylogenetic networks, and for\naligning a pair of tree-child phylogenetic networks, are provided, and they\nhave been implemented as a Perl package and a Java applet, and they are\navailable at http://bioinfo.uib.es/~recerca/phylonetworks/mudistance\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 27 Aug 2007 09:37:55 GMT"
            }
        ],
        "update_date": "2007-08-28",
        "authors_parsed": [
            [
                "Cardona",
                "Gabriel",
                ""
            ],
            [
                "Rossello",
                "Francesc",
                ""
            ],
            [
                "Valiente",
                "Gabriel",
                ""
            ]
        ]
    },
    {
        "id": "0708.3505",
        "submitter": "Noelle Carbonell",
        "authors": "Daniel Gepner (INRIA Rocquencourt / INRIA Lorraine - LORIA),\n  J\\'er\\^ome Simonin (INRIA Rocquencourt / INRIA Lorraine - LORIA), No\\\"elle\n  Carbonell (INRIA Rocquencourt / INRIA Lorraine - LORIA)",
        "title": "Gaze as a Supplementary Modality for Interacting with Ambient\n  Intelligence Environments",
        "comments": "10 pages",
        "journal-ref": "Dans Universal Access to Ambient Interaction, Springer-Verlag,\n  LNCS-LNAI Series, number 4555 - 12th International Conference on\n  Human-Computer Interaction (HCI Internatinal 2007), Beijing : China (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  We present our current research on the implementation of gaze as an efficient\nand usable pointing modality supplementary to speech, for interacting with\naugmented objects in our daily environment or large displays, especially\nimmersive virtual reality environments, such as reality centres and caves. We\nare also addressing issues relating to the use of gaze as the main interaction\ninput modality. We have designed and developed two operational user interfaces:\none for providing motor-disabled users with easy gaze-based access to map\napplications and graphical software; the other for iteratively testing and\nimproving the usability of gaze-contingent displays.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 26 Aug 2007 18:53:41 GMT"
            }
        ],
        "update_date": "2007-08-28",
        "authors_parsed": [
            [
                "Gepner",
                "Daniel",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ],
            [
                "Simonin",
                "J\u00e9r\u00f4me",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ],
            [
                "Carbonell",
                "No\u00eblle",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0708.3522",
        "submitter": "Saugata Basu",
        "authors": "Saugata Basu, Dmitrii V. Pasechnik, Marie-Francoise Roy",
        "title": "Bounding the Betti numbers and computing the Euler-Poincar\\'e\n  characteristic of semi-algebraic sets defined by partly quadratic systems of\n  polynomials",
        "comments": "23 pages, 1 figure. Shortened revised version to appear in the J.\n  Eur. Math. Soc",
        "journal-ref": "J. European Math. Soc. 12(2010), 529-553",
        "doi": "10.1137/070711141",
        "report-no": null,
        "categories": "math.AG cs.SC math.AT math.GT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $\\R$ be a real closed field, $ {\\mathcal Q} \\subset\n\\R[Y_1,...,Y_\\ell,X_1,...,X_k], $ with $ \\deg_{Y}(Q) \\leq 2, \\deg_{X}(Q) \\leq\nd, Q \\in {\\mathcal Q}, #({\\mathcal Q})=m,$ and $ {\\mathcal P} \\subset\n\\R[X_1,...,X_k] $ with $\\deg_{X}(P) \\leq d, P \\in {\\mathcal P}, #({\\mathcal\nP})=s$, and $S \\subset \\R^{\\ell+k}$ a semi-algebraic set defined by a Boolean\nformula without negations, with atoms $P=0, P \\geq 0, P \\leq 0, P \\in {\\mathcal\nP} \\cup {\\mathcal Q}$. We prove that the sum of the Betti numbers of $S$ is\nbounded by \\[ \\ell^2 (O(s+\\ell+m)\\ell d)^{k+2m}. \\] This is a common\ngeneralization of previous results on bounding the Betti numbers of closed\nsemi-algebraic sets defined by polynomials of degree $d$ and 2, respectively.\n  We also describe an algorithm for computing the Euler-Poincar\\'e\ncharacteristic of such sets, generalizing similar algorithms known before. The\ncomplexity of the algorithm is bounded by $(\\ell s m d)^{O(m(m+k))}$.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 27 Aug 2007 01:31:17 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 24 Jun 2008 14:04:51 GMT"
            }
        ],
        "update_date": "2010-10-21",
        "authors_parsed": [
            [
                "Basu",
                "Saugata",
                ""
            ],
            [
                "Pasechnik",
                "Dmitrii V.",
                ""
            ],
            [
                "Roy",
                "Marie-Francoise",
                ""
            ]
        ]
    },
    {
        "id": "0708.3564",
        "submitter": "Eugen Zalinescu",
        "authors": "Hubert Comon-Lundh and V\\'eronique Cortier and Eugen Zalinescu",
        "title": "Deciding security properties for cryptographic protocols. Application to\n  key cycles",
        "comments": "revised version (corrected small mistakes, improved presentation); to\n  be published in ACM Transactions on Computational Logic; 39 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.CR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is a large amount of work dedicated to the formal verification of\nsecurity protocols. In this paper, we revisit and extend the NP-complete\ndecision procedure for a bounded number of sessions. We use a, now standard,\ndeducibility constraints formalism for modeling security protocols. Our first\ncontribution is to give a simple set of constraint simplification rules, that\nallows to reduce any deducibility constraint system to a set of solved forms,\nrepresenting all solutions (within the bound on sessions).\n  As a consequence, we prove that deciding the existence of key cycles is\nNP-complete for a bounded number of sessions. The problem of key-cycles has\nbeen put forward by recent works relating computational and symbolic models.\nThe so-called soundness of the symbolic model requires indeed that no key cycle\n(e.g., enc(k,k)) ever occurs in the execution of the protocol. Otherwise,\nstronger security assumptions (such as KDM-security) are required.\n  We show that our decision procedure can also be applied to prove again the\ndecidability of authentication-like properties and the decidability of a\nsignificant fragment of protocols with timestamps.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 27 Aug 2007 11:20:33 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 20 Mar 2009 16:43:04 GMT"
            }
        ],
        "update_date": "2009-03-20",
        "authors_parsed": [
            [
                "Comon-Lundh",
                "Hubert",
                ""
            ],
            [
                "Cortier",
                "V\u00e9ronique",
                ""
            ],
            [
                "Zalinescu",
                "Eugen",
                ""
            ]
        ]
    },
    {
        "id": "0708.3575",
        "submitter": "Noelle Carbonell",
        "authors": "Suzanne Kieffer (INRIA Rocquencourt / INRIA Lorraine - LORIA),\n  No\\\"elle Carbonell (INRIA Rocquencourt / INRIA Lorraine - LORIA)",
        "title": "How really effective are Multimodal Hints in enhancing Visual Target\n  Spotting? Some evidence from a usability study",
        "comments": "9 pages",
        "journal-ref": "Journal on Multimodal Interaction (JMUI), 1 (2007) 1-9",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  The main aim of the work presented here is to contribute to computer science\nadvances in the multimodal usability area, in-as-much as it addresses one of\nthe major issues relating to the generation of effective oral system messages:\nhow to design messages which effectively help users to locate specific\ngraphical objects in information visualisations? An experimental study was\ncarried out to determine whether oral messages including coarse information on\nthe locations of graphical objects on the current display may facilitate target\ndetection tasks sufficiently for making it worth while to integrate such\nmessages in GUIs. The display spatial layout varied in order to test the\ninfluence of visual presentation structure on the contribution of these\nmessages to facilitating visual search on crowded displays. Finally, three\nlevels of task difficulty were defined, based mainly on the target visual\ncomplexity and the number of distractors in the scene. The findings suggest\nthat spatial information messages improve participants' visual search\nperformances significantly; they are more appropriate to radial structures than\nto matrix, random and elleptic structures; and, they are particularly useful\nfor performing difficult visual search tasks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 27 Aug 2007 11:53:35 GMT"
            }
        ],
        "update_date": "2007-08-28",
        "authors_parsed": [
            [
                "Kieffer",
                "Suzanne",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ],
            [
                "Carbonell",
                "No\u00eblle",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0708.3580",
        "submitter": "Noelle Carbonell",
        "authors": "No\\\"elle Carbonell (INRIA Rocquencourt / INRIA Lorraine - LORIA)",
        "title": "Ambient Multimodality: an Asset for Developing Universal Access to the\n  Information Society",
        "comments": null,
        "journal-ref": "3rd International Conference on Universal Access in Human-Computer\n  Interaction, Las Vegas : \\'Etats-Unis d'Am\\'erique (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  The paper tries to point out the benefits that can be derived from research\nadvances in the implementation of concepts such as ambient intelligence (AmI)\nand ubiquitous or pervasive computing for promoting Universal Access (UA) to\nthe Information Society, that is, for contributing to enable everybody,\nespecially Physically Disabled (PD) people, to have easy access to all\ncomputing resources and information services that the coming worldwide\nInformation Society will soon make available to the general public. Following\ndefinitions of basic concepts relating to multimodal interaction, the\nsignificant contribution of multimodality to developing UA is briefly argued.\nThen, a short state of the art in AmI research is presented. In the last\nsection we bring out the potential contribution of advances in AmI research and\ntechnology to the improvement of computer access for PD people. This claim is\nsupported by the following observations: (i) most projects aiming at\nimplementing AmI focus on the design of new interaction modalities and flexible\nmultimodal user interfaces which may facilitate PD users' computer access ;\n(ii) targeted applications will support users in a wide range of daily\nactivities which will be performed simultaneously with supporting computing\ntasks; therefore, users will be placed in contexts where they will be\nconfronted with similar difficulties to those encountered by PD users; (iii)\nAmI applications being intended for the general public, a wide range of new\ninteraction devices and flexible processing software will be available, making\nit possible to provide PD users with human-computer facilities tailored to\ntheir specific needs at reasonable expense..\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 27 Aug 2007 12:11:56 GMT"
            }
        ],
        "update_date": "2007-08-28",
        "authors_parsed": [
            [
                "Carbonell",
                "No\u00eblle",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0708.3721",
        "submitter": "Marc Daumas",
        "authors": "Marc Daumas (LIRMM, Eliaus), David Lester (UNIVERSITY of Manchester),\n  C\\'esar Mu\\~noz (NIA)",
        "title": "Verified Real Number Calculations: A Library for Interval Arithmetic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.LO",
        "license": null,
        "abstract": "  Real number calculations on elementary functions are remarkably difficult to\nhandle in mechanical proofs. In this paper, we show how these calculations can\nbe performed within a theorem prover or proof assistant in a convenient and\nhighly automated as well as interactive way. First, we formally establish upper\nand lower bounds for elementary functions. Then, based on these bounds, we\ndevelop a rational interval arithmetic where real number calculations take\nplace in an algebraic setting. In order to reduce the dependency effect of\ninterval arithmetic, we integrate two techniques: interval splitting and taylor\nseries expansions. This pragmatic approach has been developed, and formally\nverified, in a theorem prover. The formal development also includes a set of\ncustomizable strategies to automate proofs involving explicit calculations over\nreal numbers. Our ultimate goal is to provide guaranteed proofs of numerical\nproperties with minimal human theorem-prover interaction.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 28 Aug 2007 07:14:29 GMT"
            }
        ],
        "update_date": "2007-08-29",
        "authors_parsed": [
            [
                "Daumas",
                "Marc",
                "",
                "LIRMM, Eliaus"
            ],
            [
                "Lester",
                "David",
                "",
                "UNIVERSITY of Manchester"
            ],
            [
                "Mu\u00f1oz",
                "C\u00e9sar",
                "",
                "NIA"
            ]
        ]
    },
    {
        "id": "0708.3722",
        "submitter": "Marc Daumas",
        "authors": "Sylvie Boldo (INRIA Futurs), Marc Daumas (LIRMM, Eliaus), Ren Cang Li",
        "title": "Formally Verified Argument Reduction with a Fused-Multiply-Add",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.PF",
        "license": null,
        "abstract": "  Cody & Waite argument reduction technique works perfectly for reasonably\nlarge arguments but as the input grows there are no bit left to approximate the\nconstant with enough accuracy. Under mild assumptions, we show that the result\ncomputed with a fused-multiply-add provides a fully accurate result for many\npossible values of the input with a constant almost accurate to the full\nworking precision. We also present an algorithm for a fully accurate second\nreduction step to reach double full accuracy (all the significand bits of two\nnumbers are significant) even in the worst cases of argument reduction. Our\nwork recalls the common algorithms and presents proofs of correctness. All the\nproofs are formally verified using the Coq automatic proof checker.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 28 Aug 2007 07:15:08 GMT"
            }
        ],
        "update_date": "2007-08-29",
        "authors_parsed": [
            [
                "Boldo",
                "Sylvie",
                "",
                "INRIA Futurs"
            ],
            [
                "Daumas",
                "Marc",
                "",
                "LIRMM, Eliaus"
            ],
            [
                "Li",
                "Ren Cang",
                ""
            ]
        ]
    },
    {
        "id": "0708.3722",
        "submitter": "Marc Daumas",
        "authors": "Sylvie Boldo (INRIA Futurs), Marc Daumas (LIRMM, Eliaus), Ren Cang Li",
        "title": "Formally Verified Argument Reduction with a Fused-Multiply-Add",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.PF",
        "license": null,
        "abstract": "  Cody & Waite argument reduction technique works perfectly for reasonably\nlarge arguments but as the input grows there are no bit left to approximate the\nconstant with enough accuracy. Under mild assumptions, we show that the result\ncomputed with a fused-multiply-add provides a fully accurate result for many\npossible values of the input with a constant almost accurate to the full\nworking precision. We also present an algorithm for a fully accurate second\nreduction step to reach double full accuracy (all the significand bits of two\nnumbers are significant) even in the worst cases of argument reduction. Our\nwork recalls the common algorithms and presents proofs of correctness. All the\nproofs are formally verified using the Coq automatic proof checker.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 28 Aug 2007 07:15:08 GMT"
            }
        ],
        "update_date": "2007-08-29",
        "authors_parsed": [
            [
                "Boldo",
                "Sylvie",
                "",
                "INRIA Futurs"
            ],
            [
                "Daumas",
                "Marc",
                "",
                "LIRMM, Eliaus"
            ],
            [
                "Li",
                "Ren Cang",
                ""
            ]
        ]
    },
    {
        "id": "0708.3734",
        "submitter": "Rossano Venturini",
        "authors": "Igor Nitto and Rossano Venturini",
        "title": "Searching for a dangerous host: randomized vs. deterministic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  A Black Hole is an harmful host in a network that destroys incoming agents\nwithout leaving any trace of such event. The problem of locating the black hole\nin a network through a team of agent coordinated by a common protocol is\nusually referred in literature as the Black Hole Search problem (or BHS for\nbrevity) and it is a consolidated research topic in the area of distributed\nalgorithms. The aim of this paper is to extend the results for BHS by\nconsidering more general (and hence harder) classes of dangerous host. In\nparticular we introduce rB-hole as a probabilistic generalization of the Black\nHole, in which the destruction of an incoming agent is a purely random event\nhappening with some fixed probability (like flipping a biased coin). The main\nresult we present is that if we tolerate an arbitrarily small error probability\nin the result then the rB-hole Search problem, or RBS, is not harder than the\nusual BHS. We establish this result in two different communication model,\nspecifically both in presence or absence of whiteboards non-located at the\nhomebase. The core of our methods is a general reduction tool for transforming\nalgorithms for the black hole into algorithms for the rB-hole.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 28 Aug 2007 09:24:13 GMT"
            }
        ],
        "update_date": "2007-08-29",
        "authors_parsed": [
            [
                "Nitto",
                "Igor",
                ""
            ],
            [
                "Venturini",
                "Rossano",
                ""
            ]
        ]
    },
    {
        "id": "0708.3740",
        "submitter": "Noelle Carbonell",
        "authors": "J\\'er\\^ome Simonin (INRIA Rocquencourt / INRIA Lorraine - LORIA),\n  Marius Hategan (INRIA Rocquencourt / INRIA Lorraine - LORIA), No\\\"elle\n  Carbonell (INRIA Rocquencourt / INRIA Lorraine - LORIA)",
        "title": "Plate-forme Magicien d'Oz pour l'\\'etude de l'apport des ACAs \\`a\n  l'interaction",
        "comments": null,
        "journal-ref": "Dans Actes du Second Workshop sur les Agents Conversationnels\n  anim\\'es - Second Workshop sur les Agents Conversationnels anim\\'es, Toulouse\n  : France (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  In order to evaluate the contribution of Embodied (Animated) Conversational\nAgents (ECAs) to the effectiveness and usability of human-computer interaction,\nwe developed a software platform meant to collect usage data. This platform,\nwhich implements the wizard of Oz paradigm, makes it possible to simulate user\ninterfaces integrating ACAs for any Windows software application. It can also\nsave and \"replay\" a rich interaction trace including user and system events,\nscreen captures, users' speech and eye fixations. This platform has been used\nto assess users' subjective judgements and reactions to a multimodal online\nhelp system meant to facilitate the use of software for the general public\n(Flash). The online help system is embodied using a 3D talking head (developed\nby FT R&D) which \"says\" oral help messages illustrated with Flash screen\ncopies.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 28 Aug 2007 09:56:05 GMT"
            }
        ],
        "update_date": "2007-08-29",
        "authors_parsed": [
            [
                "Simonin",
                "J\u00e9r\u00f4me",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ],
            [
                "Hategan",
                "Marius",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ],
            [
                "Carbonell",
                "No\u00eblle",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0708.3742",
        "submitter": "Noelle Carbonell",
        "authors": "J\\'er\\^ome Simonin (INRIA Rocquencourt / INRIA Lorraine - LORIA),\n  No\\\"elle Carbonell (INRIA Rocquencourt / INRIA Lorraine - LORIA)",
        "title": "Interfaces adaptatives Adaptation dynamique \\`a l'utilisateur courant",
        "comments": null,
        "journal-ref": "Interfaces num\\'eriques, Herm\\`es Science (Ed.) (2007) 18 pages",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  We present a survey of recent research studies of the implementation of\nadaptive user models in human-computer interaction. A classification of\nresearch directions on adaptive user interfaces is first proposed; it takes\naccount of the user characteristics that are modelled, the distribution of\ninitiative and control of the system evolution between user and system, and the\nrole of dynamic adaptation. Then, a few representative research studies are\nbriefly presented to illustrate this classification. In the conclusion, some\nmajor issues regarding the utility and usability of adaptive user interfaces\nand the design of an appropriate methodology for assessing the ergonomic\nquality of this new form of interaction are mentioned.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 28 Aug 2007 09:57:43 GMT"
            }
        ],
        "update_date": "2007-08-29",
        "authors_parsed": [
            [
                "Simonin",
                "J\u00e9r\u00f4me",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ],
            [
                "Carbonell",
                "No\u00eblle",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0708.3829",
        "submitter": "Sabatino Costanzo",
        "authors": "Sabatino Costanzo, Loren Trigo, Wafaa Dehne, Hender Prato",
        "title": "A Non Parametric Model for the Forecasting of the Venezuelan Oil Prices",
        "comments": "17 pages, in Spanish",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.NE",
        "license": null,
        "abstract": "  A neural net model for forecasting the prices of Venezuelan crude oil is\nproposed. The inputs of the neural net are selected by reference to a dynamic\nsystem model of oil prices by Mashayekhi (1995, 2001) and its performance is\nevaluated using two criteria: the Excess Profitability test by Anatoliev and\nGerko (2005) and the characteristics of the equity curve generated by a trading\nstrategy based on the neural net predictions.\n  -----\n  Se introduce aqui un modelo no parametrico para pronosticar los precios del\npetroleo Venezolano cuyos insumos son seleccionados en base a un sistema\ndinamico que explica los precios en terminos de dichos insumos. Se describe el\nproceso de recoleccion y pre-procesamiento de datos y la corrida de la red y se\nevaluan sus pronosticos a traves de un test estadistico de predictibilidad y de\nlas caracteristicas del Equity Curve inducido por la estrategia de compraventa\nbursatil generada por dichos pronosticos.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 28 Aug 2007 18:29:55 GMT"
            }
        ],
        "update_date": "2007-08-29",
        "authors_parsed": [
            [
                "Costanzo",
                "Sabatino",
                ""
            ],
            [
                "Trigo",
                "Loren",
                ""
            ],
            [
                "Dehne",
                "Wafaa",
                ""
            ],
            [
                "Prato",
                "Hender",
                ""
            ]
        ]
    },
    {
        "id": "0708.3829",
        "submitter": "Sabatino Costanzo",
        "authors": "Sabatino Costanzo, Loren Trigo, Wafaa Dehne, Hender Prato",
        "title": "A Non Parametric Model for the Forecasting of the Venezuelan Oil Prices",
        "comments": "17 pages, in Spanish",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.NE",
        "license": null,
        "abstract": "  A neural net model for forecasting the prices of Venezuelan crude oil is\nproposed. The inputs of the neural net are selected by reference to a dynamic\nsystem model of oil prices by Mashayekhi (1995, 2001) and its performance is\nevaluated using two criteria: the Excess Profitability test by Anatoliev and\nGerko (2005) and the characteristics of the equity curve generated by a trading\nstrategy based on the neural net predictions.\n  -----\n  Se introduce aqui un modelo no parametrico para pronosticar los precios del\npetroleo Venezolano cuyos insumos son seleccionados en base a un sistema\ndinamico que explica los precios en terminos de dichos insumos. Se describe el\nproceso de recoleccion y pre-procesamiento de datos y la corrida de la red y se\nevaluan sus pronosticos a traves de un test estadistico de predictibilidad y de\nlas caracteristicas del Equity Curve inducido por la estrategia de compraventa\nbursatil generada por dichos pronosticos.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 28 Aug 2007 18:29:55 GMT"
            }
        ],
        "update_date": "2007-08-29",
        "authors_parsed": [
            [
                "Costanzo",
                "Sabatino",
                ""
            ],
            [
                "Trigo",
                "Loren",
                ""
            ],
            [
                "Dehne",
                "Wafaa",
                ""
            ],
            [
                "Prato",
                "Hender",
                ""
            ]
        ]
    },
    {
        "id": "0708.3879",
        "submitter": "Dmitri Krioukov",
        "authors": "Xenofontas Dimitropoulos, Dmitri Krioukov, Amin Vahdat, George Riley",
        "title": "Graph Annotations in Modeling Complex Network Topologies",
        "comments": null,
        "journal-ref": "ACM Transactions on Modeling and Computer Simulation (TOMACS),\n  v.19, n.4, p.17, 2009",
        "doi": "10.1145/1596519.1596522",
        "report-no": null,
        "categories": "cs.NI cond-mat.dis-nn physics.data-an physics.soc-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The coarsest approximation of the structure of a complex network, such as the\nInternet, is a simple undirected unweighted graph. This approximation, however,\nloses too much detail. In reality, objects represented by vertices and edges in\nsuch a graph possess some non-trivial internal structure that varies across and\ndifferentiates among distinct types of links or nodes. In this work, we\nabstract such additional information as network annotations. We introduce a\nnetwork topology modeling framework that treats annotations as an extended\ncorrelation profile of a network. Assuming we have this profile measured for a\ngiven network, we present an algorithm to rescale it in order to construct\nnetworks of varying size that still reproduce the original measured annotation\nprofile.\n  Using this methodology, we accurately capture the network properties\nessential for realistic simulations of network applications and protocols, or\nany other simulations involving complex network topologies, including modeling\nand simulation of network evolution. We apply our approach to the Autonomous\nSystem (AS) topology of the Internet annotated with business relationships\nbetween ASs. This topology captures the large-scale structure of the Internet.\nIn depth understanding of this structure and tools to model it are cornerstones\nof research on future Internet architectures and designs. We find that our\ntechniques are able to accurately capture the structure of annotation\ncorrelations within this topology, thus reproducing a number of its important\nproperties in synthetically-generated random graphs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 29 Aug 2007 03:23:56 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 31 Aug 2007 22:09:32 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 19 Sep 2008 02:44:12 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 2 Nov 2009 20:00:00 GMT"
            }
        ],
        "update_date": "2009-11-02",
        "authors_parsed": [
            [
                "Dimitropoulos",
                "Xenofontas",
                ""
            ],
            [
                "Krioukov",
                "Dmitri",
                ""
            ],
            [
                "Vahdat",
                "Amin",
                ""
            ],
            [
                "Riley",
                "George",
                ""
            ]
        ]
    },
    {
        "id": "0708.4082",
        "submitter": "Noelle Carbonell",
        "authors": "Antonio Capobianco (INRIA Rocquencourt / INRIA Lorraine - LORIA),\n  No\\\"elle Carbonell (INRIA Rocquencourt / INRIA Lorraine - LORIA)",
        "title": "Aides en ligne \\`a l'utilisation de logiciels grand public : probl\\`emes\n  sp\\'ecifiques de conception et solutions potentielles",
        "comments": null,
        "journal-ref": "Intellectica 2006/2, 44 (2006) 87-120",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  The observation that novice users seldom consult online help was made over\ntwenty years ago. This observation still holds nowadays, although online help\nto the use of software for the general public has greatly improved in usability\nduring this period. The paper first demonstrates the necessity of online help\nto the use of new software whatever the transparency of the user interface, as\nwhether online help systems are meant to compensate for interface design\nweaknesses or actually do provide necessary assistance to the discovery of a\nnew software package functionalities is still an unsolved issue. The discussion\nrelies on results of empirical and experimental studies and theoretical\narguments. In the second part, we analyse the specific difficulties raised by\nthe design of effective online help systems for current software intended for\nthe general public so as to try and understand the reluctance of novice users\nto use online help. In the last part, we present and discuss the possible\ncontributions of various approaches to solving this issue. Recent interaction\nparadigms and techniques are considered, such as, static and dynamic\npersonalisation, contextual online help and new forms of multimodality.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 30 Aug 2007 06:39:15 GMT"
            }
        ],
        "update_date": "2007-08-31",
        "authors_parsed": [
            [
                "Capobianco",
                "Antonio",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ],
            [
                "Carbonell",
                "No\u00eblle",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0708.4170",
        "submitter": "Paolo Liberatore",
        "authors": "Paolo Liberatore",
        "title": "Raising a Hardness Result",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.CC cs.LO",
        "license": null,
        "abstract": "  This article presents a technique for proving problems hard for classes of\nthe polynomial hierarchy or for PSPACE. The rationale of this technique is that\nsome problem restrictions are able to simulate existential or universal\nquantifiers. If this is the case, reductions from Quantified Boolean Formulae\n(QBF) to these restrictions can be transformed into reductions from QBFs having\none more quantifier in the front. This means that a proof of hardness of a\nproblem at level n in the polynomial hierarchy can be split into n separate\nproofs, which may be simpler than a proof directly showing a reduction from a\nclass of QBFs to the considered problem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 30 Aug 2007 14:42:50 GMT"
            }
        ],
        "update_date": "2007-08-31",
        "authors_parsed": [
            [
                "Liberatore",
                "Paolo",
                ""
            ]
        ]
    },
    {
        "id": "0708.4230",
        "submitter": "Laurent Buse",
        "authors": "Laurent Bus\\'e (INRIA Sophia Antipolis), Marc Dohm (JAD)",
        "title": "Implicitization of Bihomogeneous Parametrizations of Algebraic Surfaces\n  via Linear Syzygies",
        "comments": null,
        "journal-ref": "Dans International Conference on Symbolic and Algebraic\n  Computation (2007) 69 - 76",
        "doi": "10.1145/1277548.1277559",
        "report-no": null,
        "categories": "math.AG cs.SC math.AC",
        "license": null,
        "abstract": "  We show that the implicit equation of a surface in 3-dimensional projective\nspace parametrized by bi-homogeneous polynomials of bi-degree (d,d), for a\ngiven positive integer d, can be represented and computed from the linear\nsyzygies of its parametrization if the base points are isolated and form\nlocally a complete intersection.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 30 Aug 2007 20:31:00 GMT"
            }
        ],
        "update_date": "2007-09-04",
        "authors_parsed": [
            [
                "Bus\u00e9",
                "Laurent",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Dohm",
                "Marc",
                "",
                "JAD"
            ]
        ]
    },
    {
        "id": "0708.4311",
        "submitter": "Juergen Schmidhuber",
        "authors": "Juergen Schmidhuber",
        "title": "2006: Celebrating 75 years of AI - History and Outlook: the Next 25\n  Years",
        "comments": "14 pages; preprint of invited contribution to the Proceedings of the\n  ``50th Anniversary Summit of Artificial Intelligence'' at Monte Verita,\n  Ascona, Switzerland, 9-14 July 2006",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  When Kurt Goedel layed the foundations of theoretical computer science in\n1931, he also introduced essential concepts of the theory of Artificial\nIntelligence (AI). Although much of subsequent AI research has focused on\nheuristics, which still play a major role in many practical AI applications, in\nthe new millennium AI theory has finally become a full-fledged formal science,\nwith important optimality results for embodied agents living in unknown\nenvironments, obtained through a combination of theory a la Goedel and\nprobability theory. Here we look back at important milestones of AI history,\nmention essential recent results, and speculate about what we may expect from\nthe next 25 years, emphasizing the significance of the ongoing dramatic\nhardware speedups, and discussing Goedel-inspired, self-referential,\nself-improving universal problem solvers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 31 Aug 2007 11:12:26 GMT"
            }
        ],
        "update_date": "2007-09-03",
        "authors_parsed": [
            [
                "Schmidhuber",
                "Juergen",
                ""
            ]
        ]
    },
    {
        "id": "0709.0116",
        "submitter": "Fionn Murtagh",
        "authors": "Fionn Murtagh",
        "title": "On Ultrametric Algorithmic Information",
        "comments": "Forthcoming, Computer Journal. Minor corrections 29 Oct. 2007",
        "journal-ref": "Computer Journal, 53, 405-416, 2010",
        "doi": "10.1093/comjnl/bxm084",
        "report-no": null,
        "categories": "cs.AI cs.CL",
        "license": null,
        "abstract": "  How best to quantify the information of an object, whether natural or\nartifact, is a problem of wide interest. A related problem is the computability\nof an object. We present practical examples of a new way to address this\nproblem. By giving an appropriate representation to our objects, based on a\nhierarchical coding of information, we exemplify how it is remarkably easy to\ncompute complex objects. Our algorithmic complexity is related to the length of\nthe class of objects, rather than to the length of the object.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 2 Sep 2007 17:00:40 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 29 Sep 2007 11:21:51 GMT"
            }
        ],
        "update_date": "2011-06-14",
        "authors_parsed": [
            [
                "Murtagh",
                "Fionn",
                ""
            ]
        ]
    },
    {
        "id": "0709.0178",
        "submitter": "Yasmine B. Sanderson",
        "authors": "Yasmine B. Sanderson",
        "title": "Effective Generation of Subjectively Random Binary Sequences",
        "comments": "Introduction and Section 6 revised",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an algorithm for effectively generating binary sequences which\nwould be rated by people as highly likely to have been generated by a random\nprocess, such as flipping a fair coin.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Sep 2007 09:32:28 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 5 Oct 2008 10:19:14 GMT"
            }
        ],
        "update_date": "2008-10-05",
        "authors_parsed": [
            [
                "Sanderson",
                "Yasmine B.",
                ""
            ]
        ]
    },
    {
        "id": "0709.0178",
        "submitter": "Yasmine B. Sanderson",
        "authors": "Yasmine B. Sanderson",
        "title": "Effective Generation of Subjectively Random Binary Sequences",
        "comments": "Introduction and Section 6 revised",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an algorithm for effectively generating binary sequences which\nwould be rated by people as highly likely to have been generated by a random\nprocess, such as flipping a fair coin.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Sep 2007 09:32:28 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 5 Oct 2008 10:19:14 GMT"
            }
        ],
        "update_date": "2008-10-05",
        "authors_parsed": [
            [
                "Sanderson",
                "Yasmine B.",
                ""
            ]
        ]
    },
    {
        "id": "0709.0218",
        "submitter": "Debprakash Patnaik",
        "authors": "Debprakash Patnaik (Electrical Engg. Dept., Indian Institute of\n  Science), P. S. Sastry (Electrical Engg. Dept., Indian Institute of Science),\n  K. P. Unnikrishnan (General Motors R&D)",
        "title": "Inferring Neuronal Network Connectivity using Time-constrained Episodes",
        "comments": "9 pages. See also http://neural-code.cs.vt.edu/",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB q-bio.NC",
        "license": null,
        "abstract": "  Discovering frequent episodes in event sequences is an interesting data\nmining task. In this paper, we argue that this framework is very effective for\nanalyzing multi-neuronal spike train data. Analyzing spike train data is an\nimportant problem in neuroscience though there are no data mining approaches\nreported for this. Motivated by this application, we introduce different\ntemporal constraints on the occurrences of episodes. We present algorithms for\ndiscovering frequent episodes under temporal constraints. Through simulations,\nwe show that our method is very effective for analyzing spike train data for\nunearthing underlying connectivity patterns.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Sep 2007 13:23:33 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 26 Sep 2007 06:49:01 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Patnaik",
                "Debprakash",
                "",
                "Electrical Engg. Dept., Indian Institute of\n  Science"
            ],
            [
                "Sastry",
                "P. S.",
                "",
                "Electrical Engg. Dept., Indian Institute of Science"
            ],
            [
                "Unnikrishnan",
                "K. P.",
                "",
                "General Motors R&D"
            ]
        ]
    },
    {
        "id": "0709.0289",
        "submitter": "Christian Schaffner",
        "authors": "Christian Schaffner",
        "title": "Cryptography in the Bounded-Quantum-Storage Model",
        "comments": "PhD Thesis, BRICS, University of Aarhus, Denmark, 128 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "quant-ph cs.CR",
        "license": null,
        "abstract": "  This thesis initiates the study of cryptographic protocols in the\nbounded-quantum-storage model. On the practical side, simple protocols for\nRabin Oblivious Transfer, 1-2 Oblivious Transfer and Bit Commitment are\npresented. No quantum memory is required for honest players, whereas the\nprotocols can only be broken by an adversary controlling a large amount of\nquantum memory. The protocols are efficient, non-interactive and can be\nimplemented with today's technology.\n  On the theoretical side, new entropic uncertainty relations involving\nmin-entropy are established and used to prove the security of protocols\naccording to new strong security definitions. For instance, in the realistic\nsetting of Quantum Key Distribution (QKD) against quantum-memory-bounded\neavesdroppers, the uncertainty relation allows to prove the security of QKD\nprotocols while tolerating considerably higher error rates compared to the\nstandard model with unbounded adversaries.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Sep 2007 19:28:46 GMT"
            }
        ],
        "update_date": "2007-09-04",
        "authors_parsed": [
            [
                "Schaffner",
                "Christian",
                ""
            ]
        ]
    },
    {
        "id": "0709.0303",
        "submitter": "Marian Boguna",
        "authors": "Marian Boguna, Dmitri Krioukov, kc claffy",
        "title": "Navigability of Complex Networks",
        "comments": null,
        "journal-ref": "Nature Physics 5, 74-80 (2009)",
        "doi": "10.1038/NPHYS1130",
        "report-no": null,
        "categories": "physics.soc-ph cond-mat.dis-nn cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Routing information through networks is a universal phenomenon in both\nnatural and manmade complex systems. When each node has full knowledge of the\nglobal network connectivity, finding short communication paths is merely a\nmatter of distributed computation. However, in many real networks nodes\ncommunicate efficiently even without such global intelligence. Here we show\nthat the peculiar structural characteristics of many complex networks support\nefficient communication without global knowledge. We also describe a general\nmechanism that explains this connection between network structure and function.\nThis mechanism relies on the presence of a metric space hidden behind an\nobservable network. Our findings suggest that real networks in nature have\nunderlying metric spaces that remain undiscovered. Their discovery would have\npractical applications ranging from routing in the Internet and searching\nsocial networks, to studying information flows in neural, gene regulatory\nnetworks, or signaling pathways.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Sep 2007 18:57:26 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 10 Sep 2008 22:26:42 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 20 May 2009 13:00:20 GMT"
            }
        ],
        "update_date": "2009-05-20",
        "authors_parsed": [
            [
                "Boguna",
                "Marian",
                ""
            ],
            [
                "Krioukov",
                "Dmitri",
                ""
            ],
            [
                "claffy",
                "kc",
                ""
            ]
        ]
    },
    {
        "id": "0709.0355",
        "submitter": "Roland Bouffanais",
        "authors": "Nicolas Bodard, Roland Bouffanais, Michel O. Deville",
        "title": "Solution of moving-boundary problems by the spectral element method",
        "comments": "Applied Numerical Mathematics, In Press, 2008",
        "journal-ref": "Applied Numerial Mathematics 58 (2008) 968-984",
        "doi": "10.1016/j.apnum.2007.04.009",
        "report-no": null,
        "categories": "cs.CE cs.NA",
        "license": null,
        "abstract": "  This paper describes a novel numerical model aiming at solving\nmoving-boundary problems such as free-surface flows or fluid-structure\ninteraction. This model uses a moving-grid technique to solve the\nNavier--Stokes equations expressed in the arbitrary Lagrangian--Eulerian\nkinematics. The discretization in space is based on the spectral element\nmethod. The coupling of the fluid equations and the moving-grid equations is\nessentially done through the conditions on the moving boundaries. Two- and\nthree-dimensional simulations are presented: translation and rotation of a\ncylinder in a fluid, and large-amplitude sloshing in a rectangular tank. The\naccuracy and robustness of the present numerical model is studied and\ndiscussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 4 Sep 2007 14:51:56 GMT"
            }
        ],
        "update_date": "2022-09-29",
        "authors_parsed": [
            [
                "Bodard",
                "Nicolas",
                ""
            ],
            [
                "Bouffanais",
                "Roland",
                ""
            ],
            [
                "Deville",
                "Michel O.",
                ""
            ]
        ]
    },
    {
        "id": "0709.0370",
        "submitter": "Damien Chablat",
        "authors": "Ying Wang (DIE), Wei Zhang (DIE), Fouad Bennis (IRCCyN), Damien\n  Chablat (IRCCyN)",
        "title": "An Integrated Simulation System for Human Factors Study",
        "comments": null,
        "journal-ref": "Dans The Institute of Industrial Engineers Annual Conference - The\n  Institute of Industrial Engineers Annual Conference, Orlando : \\'Etats-Unis\n  d'Am\\'erique (2006)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  It has been reported that virtual reality can be a useful tool for ergonomics\nstudy. The proposed integrated simulation system aims at measuring operator's\nperformance in an interactive way for 2D control panel design. By incorporating\nsome sophisticated virtual reality hardware/software, the system allows natural\nhuman-system and/or human-human interaction in a simulated virtual environment;\nenables dynamic objective measurement of human performance; and evaluates the\nquality of the system design in human factors perspective based on the\nmeasurement. It can also be for operation training for some 2D control panels.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 4 Sep 2007 09:20:03 GMT"
            }
        ],
        "update_date": "2007-09-05",
        "authors_parsed": [
            [
                "Wang",
                "Ying",
                "",
                "DIE"
            ],
            [
                "Zhang",
                "Wei",
                "",
                "DIE"
            ],
            [
                "Bennis",
                "Fouad",
                "",
                "IRCCyN"
            ],
            [
                "Chablat",
                "Damien",
                "",
                "IRCCyN"
            ]
        ]
    },
    {
        "id": "0709.0426",
        "submitter": "Noelle Carbonell",
        "authors": "No\\\"elle Carbonell (INRIA Rocquencourt / INRIA Lorraine - LORIA),\n  Suzanne Kieffer (INRIA Rocquencourt / INRIA Lorraine - LORIA)",
        "title": "Do oral messages help visual search?",
        "comments": "26 pages",
        "journal-ref": "Advances in Natural Multimodal Dialogue Systems, Dordrecht (NL)\n  Springer (Ed.) (2005) pp. 131-157",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  A preliminary experimental study is presented, that aims at eliciting the\ncontribution of oral messages to facilitating visual search tasks on crowded\nvisual displays. Results of quantitative and qualitative analyses suggest that\nappropriate verbal messages can improve both target selection time and\naccuracy. In particular, multimodal messages including a visual presentation of\nthe isolated target together with absolute spatial oral information on its\nlocation in the displayed scene seem most effective. These messages also got\ntop-ranking ratings from most subjects.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 4 Sep 2007 13:23:40 GMT"
            }
        ],
        "update_date": "2007-09-05",
        "authors_parsed": [
            [
                "Carbonell",
                "No\u00eblle",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ],
            [
                "Kieffer",
                "Suzanne",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0709.0428",
        "submitter": "Noelle Carbonell",
        "authors": "Suzanne Kieffer (INRIA Rocquencourt / INRIA Lorraine - LORIA),\n  No\\\"elle Carbonell (INRIA Rocquencourt / INRIA Lorraine - LORIA)",
        "title": "Oral messages improve visual search",
        "comments": "4 pages",
        "journal-ref": "Dans Proceedings of ACM Working Conference on Advanced Visual\n  Interfaces - ACM Working Conference on Advanced Visual Interfaces (AVI 2006),\n  Venezia : Italie (2006)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  Input multimodality combining speech and hand gestures has motivated numerous\nusability studies. Contrastingly, issues relating to the design and ergonomic\nevaluation of multimodal output messages combining speech with visual\nmodalities have not yet been addressed extensively. The experimental study\npresented here addresses one of these issues. Its aim is to assess the actual\nefficiency and usability of oral system messages including brief spatial\ninformation for helping users to locate objects on crowded displays rapidly.\nTarget presentation mode, scene spatial structure and task difficulty were\nchosen as independent variables. Two conditions were defined: the visual target\npresentation mode (VP condition) and the multimodal target presentation mode\n(MP condition). Each participant carried out two blocks of visual search tasks\n(120 tasks per block, and one block per condition). Scene target presentation\nmode, scene structure and task difficulty were found to be significant factors.\nMultimodal target presentation proved to be more efficient than visual target\npresentation. In addition, participants expressed very positive judgments on\nmultimodal target presentations which were preferred to visual presentations by\na majority of participants. Besides, the contribution of spatial messages to\nvisual search speed and accuracy was influenced by scene spatial structure and\ntask difficulty: (i) messages improved search efficiency to a lesser extent for\n2D array layouts than for some other symmetrical layouts, although the use of\n2D arrays for displaying pictures is currently prevailing; (ii) message\nusefulness increased with task difficulty. Most of these results are\nstatistically significant.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 4 Sep 2007 13:27:33 GMT"
            }
        ],
        "update_date": "2007-09-05",
        "authors_parsed": [
            [
                "Kieffer",
                "Suzanne",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ],
            [
                "Carbonell",
                "No\u00eblle",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0709.0492",
        "submitter": "Stephanie Wehner",
        "authors": "Stephanie Wehner, J\\\"urg Wullschleger",
        "title": "Composable Security in the Bounded-Quantum-Storage Model",
        "comments": "21 pages",
        "journal-ref": "Proceedings of ICALP 2008, pages 604--615",
        "doi": null,
        "report-no": null,
        "categories": "quant-ph cs.CR",
        "license": null,
        "abstract": "  We present a simplified framework for proving sequential composability in the\nquantum setting. In particular, we give a new, simulation-based, definition for\nsecurity in the bounded-quantum-storage model, and show that this definition\nallows for sequential composition of protocols. Damgard et al. (FOCS '05,\nCRYPTO '07) showed how to securely implement bit commitment and oblivious\ntransfer in the bounded-quantum-storage model, where the adversary is only\nallowed to store a limited number of qubits. However, their security\ndefinitions did only apply to the standalone setting, and it was not clear if\ntheir protocols could be composed. Indeed, we first give a simple attack that\nshows that these protocols are not composable without a small refinement of the\nmodel. Finally, we prove the security of their randomized oblivious transfer\nprotocol in our refined model. Secure implementations of oblivious transfer and\nbit commitment then follow easily by a (classical) reduction to randomized\noblivious transfer.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 4 Sep 2007 18:15:13 GMT"
            }
        ],
        "update_date": "2009-01-20",
        "authors_parsed": [
            [
                "Wehner",
                "Stephanie",
                ""
            ],
            [
                "Wullschleger",
                "J\u00fcrg",
                ""
            ]
        ]
    },
    {
        "id": "0709.0509",
        "submitter": "Enrique ter Horst Dr",
        "authors": "Henryk Gzyl and Enrique ter Horst",
        "title": "Filtering Additive Measurement Noise with Maximum Entropy in the Mean",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  The purpose of this note is to show how the method of maximum entropy in the\nmean (MEM) may be used to improve parametric estimation when the measurements\nare corrupted by large level of noise. The method is developed in the context\non a concrete example: that of estimation of the parameter in an exponential\ndistribution. We compare the performance of our method with the bayesian and\nmaximum likelihood approaches.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 4 Sep 2007 19:36:22 GMT"
            }
        ],
        "update_date": "2021-08-23",
        "authors_parsed": [
            [
                "Gzyl",
                "Henryk",
                ""
            ],
            [
                "ter Horst",
                "Enrique",
                ""
            ]
        ]
    },
    {
        "id": "0709.0522",
        "submitter": "Florentin Smarandache",
        "authors": "Florentin Smarandache, Jean Dezert",
        "title": "Qualitative Belief Conditioning Rules (QBCR)",
        "comments": "13 pages. Presented at Fusion 2007 International Conference, Quebec\n  City, Canada, July 2007",
        "journal-ref": "Proceedings of Fusion 2007 International Conference, Quebec City,\n  Canada, July 2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  In this paper we extend the new family of (quantitative) Belief Conditioning\nRules (BCR) recently developed in the Dezert-Smarandache Theory (DSmT) to their\nqualitative counterpart for belief revision. Since the revision of quantitative\nas well as qualitative belief assignment given the occurrence of a new event\n(the conditioning constraint) can be done in many possible ways, we present\nhere only what we consider as the most appealing Qualitative Belief\nConditioning Rules (QBCR) which allow to revise the belief directly with words\nand linguistic labels and thus avoids the introduction of ad-hoc translations\nof quantitative beliefs into quantitative ones for solving the problem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 4 Sep 2007 20:03:04 GMT"
            }
        ],
        "update_date": "2007-09-06",
        "authors_parsed": [
            [
                "Smarandache",
                "Florentin",
                ""
            ],
            [
                "Dezert",
                "Jean",
                ""
            ]
        ]
    },
    {
        "id": "0709.0566",
        "submitter": "Debprakash Patnaik",
        "authors": "K.P.Unnikrishnan (General Motors R&D Center, Warren, MI), Debprakash\n  Patnaik (Dept. Elecetrical Engineering, Indian Institute of Science,\n  Bangalore), P.S.Sastry (Dept. Elecetrical Engineering, Indian Institute of\n  Science, Bangalore)",
        "title": "Discovering Patterns in Multi-neuronal Spike Trains using the Frequent\n  Episode Method",
        "comments": "Also see http://neural-code.cs.vt.edu/",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB q-bio.NC",
        "license": null,
        "abstract": "  Discovering the 'Neural Code' from multi-neuronal spike trains is an\nimportant task in neuroscience. For such an analysis, it is important to\nunearth interesting regularities in the spiking patterns. In this report, we\npresent an efficient method for automatically discovering synchrony, synfire\nchains, and more general sequences of neuronal firings. We use the Frequent\nEpisode Discovery framework of Laxman, Sastry, and Unnikrishnan (2005), in\nwhich the episodes are represented and recognized using finite-state automata.\nMany aspects of functional connectivity between neuronal populations can be\ninferred from the episodes. We demonstrate these using simulated multi-neuronal\ndata from a Poisson model. We also present a method to assess the statistical\nsignificance of the discovered episodes. Since the Temporal Data Mining (TDM)\nmethods used in this report can analyze data from hundreds and potentially\nthousands of neurons, we argue that this framework is appropriate for\ndiscovering the `Neural Code'.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 5 Sep 2007 14:46:59 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 26 Sep 2007 06:51:41 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Unnikrishnan",
                "K. P.",
                "",
                "General Motors R&D Center, Warren, MI"
            ],
            [
                "Patnaik",
                "Debprakash",
                "",
                "Dept. Elecetrical Engineering, Indian Institute of Science,\n  Bangalore"
            ],
            [
                "Sastry",
                "P. S.",
                "",
                "Dept. Elecetrical Engineering, Indian Institute of\n  Science, Bangalore"
            ]
        ]
    },
    {
        "id": "0709.0674",
        "submitter": "Juergen Schmidhuber",
        "authors": "Juergen Schmidhuber",
        "title": "Simple Algorithmic Principles of Discovery, Subjective Beauty, Selective\n  Attention, Curiosity & Creativity",
        "comments": "15 pages, 3 highly compressible low-complexity drawings. Joint\n  Invited Lecture for Algorithmic Learning Theory (ALT 2007) and Discovery\n  Science (DS 2007), Sendai, Japan, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.GR",
        "license": null,
        "abstract": "  I postulate that human or other intelligent agents function or should\nfunction as follows. They store all sensory observations as they come - the\ndata is holy. At any time, given some agent's current coding capabilities, part\nof the data is compressible by a short and hopefully fast program / description\n/ explanation / world model. In the agent's subjective eyes, such data is more\nregular and more \"beautiful\" than other data. It is well-known that knowledge\nof regularity and repeatability may improve the agent's ability to plan actions\nleading to external rewards. In absence of such rewards, however, known beauty\nis boring. Then \"interestingness\" becomes the first derivative of subjective\nbeauty: as the learning agent improves its compression algorithm, formerly\napparently random data parts become subjectively more regular and beautiful.\nSuch progress in compressibility is measured and maximized by the curiosity\ndrive: create action sequences that extend the observation history and yield\npreviously unknown / unpredictable but quickly learnable algorithmic\nregularity. We discuss how all of the above can be naturally implemented on\ncomputers, through an extension of passive unsupervised learning to the case of\nactive data selection: we reward a general reinforcement learner (with access\nto the adaptive compressor) for actions that improve the subjective\ncompressibility of the growing data. An unusually large breakthrough in\ncompressibility deserves the name \"discovery\". The \"creativity\" of artists,\ndancers, musicians, pure mathematicians can be viewed as a by-product of this\nprinciple. Several qualitative examples support this hypothesis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 5 Sep 2007 15:20:59 GMT"
            }
        ],
        "update_date": "2007-09-06",
        "authors_parsed": [
            [
                "Schmidhuber",
                "Juergen",
                ""
            ]
        ]
    },
    {
        "id": "0709.0787",
        "submitter": "Toshiya Takami",
        "authors": "Taizo Kobayashi, Toshiya Takami, Kin'ya Takahashi, Ryota Mibu, Mutsumi\n  Aoyagi",
        "title": "Sound Generation by a Turbulent Flow in Musical Instruments -\n  Multiphysics Simulation Approach -",
        "comments": "6 pages, 10 figure files, to appear in the proceedings of HPCAsia07",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "physics.comp-ph cs.CE physics.flu-dyn",
        "license": null,
        "abstract": "  Total computational costs of scientific simulations are analyzed between\ndirect numerical simulations (DNS) and multiphysics simulations (MPS) for sound\ngeneration in musical instruments. In order to produce acoustic sound by a\nturbulent flow in a simple recorder-like instrument, compressible fluid dynamic\ncalculations with a low Mach number are required around the edges and the\nresonator of the instrument in DNS, while incompressible fluid dynamic\ncalculations coupled with dynamics of sound propagation based on the\nLighthill's acoustic analogy are used in MPS. These strategies are evaluated\nnot only from the viewpoint of computational performances but also from the\ntheoretical points of view as tools for scientific simulations of complicated\nsystems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Sep 2007 13:47:31 GMT"
            }
        ],
        "update_date": "2007-09-07",
        "authors_parsed": [
            [
                "Kobayashi",
                "Taizo",
                ""
            ],
            [
                "Takami",
                "Toshiya",
                ""
            ],
            [
                "Takahashi",
                "Kin'ya",
                ""
            ],
            [
                "Mibu",
                "Ryota",
                ""
            ],
            [
                "Aoyagi",
                "Mutsumi",
                ""
            ]
        ]
    },
    {
        "id": "0709.0883",
        "submitter": "Joshua Herman J",
        "authors": "Joshua Jay Herman",
        "title": "Liquid State Machines in Adbiatic Quantum Computers for General\n  Computation",
        "comments": "Totally wrong",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CC cs.NE",
        "license": null,
        "abstract": "  Major mistakes do not read\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Sep 2007 16:04:42 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 7 Sep 2007 19:34:51 GMT"
            },
            {
                "version": "v3",
                "created": "Sun, 9 Sep 2007 14:29:14 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 21 Sep 2007 13:09:13 GMT"
            },
            {
                "version": "v5",
                "created": "Fri, 8 Jul 2011 01:54:35 GMT"
            }
        ],
        "update_date": "2011-07-11",
        "authors_parsed": [
            [
                "Herman",
                "Joshua Jay",
                ""
            ]
        ]
    },
    {
        "id": "0709.0929",
        "submitter": "Vladimir Gudkov",
        "authors": "V. Gudkov and V. Montealegre",
        "title": "Analysis of network by generalized mutual entropies",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.physa.2008.01.005",
        "report-no": null,
        "categories": "cond-mat.dis-nn cs.NI physics.comp-ph",
        "license": null,
        "abstract": "  Generalized mutual entropy is defined for networks and applied for analysis\nof complex network structures. The method is tested for the case of computer\nsimulated scale free networks, random networks, and their mixtures. The\npossible applications for real network analysis are discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Sep 2007 18:00:15 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Gudkov",
                "V.",
                ""
            ],
            [
                "Montealegre",
                "V.",
                ""
            ]
        ]
    },
    {
        "id": "0709.0961",
        "submitter": "Harish Sethu",
        "authors": "Harish Sethu and Thomas Gerety",
        "title": "A New Distributed Topology Control Algorithm for Wireless Environments\n  with Non-Uniform Path Loss and Multipath Propagation",
        "comments": "To appear in Ad Hoc Networks",
        "journal-ref": "Ad Hoc Networks, May 2010, volume 8, issue 3, pages 280-294.",
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Each node in a wireless multi-hop network can adjust the power level at which\nit transmits and thus change the topology of the network to save energy by\nchoosing the neighbors with which it directly communicates. Many previous\nalgorithms for distributed topology control have assumed an ability at each\nnode to deduce some location-based information such as the direction and the\ndistance of its neighbor nodes with respect to itself. Such a deduction of\nlocation-based information, however, cannot be relied upon in real environments\nwhere the path loss exponents vary greatly leading to significant errors in\ndistance estimates. Also, multipath effects may result in different signal\npaths with different loss characteristics, and none of these paths may be\nline-of-sight, making it difficult to estimate the direction of a neighboring\nnode. In this paper, we present Step Topology Control (STC), a simple\ndistributed topology control algorithm which reduces energy consumption while\npreserving the connectivity of a heterogeneous sensor network without use of\nany location-based information. We show that the STC algorithm achieves the\nsame or better order of communication and computational complexity when\ncompared to other known algorithms that also preserve connectivity without the\nuse of location-based information. We also present a detailed simulation-based\ncomparative analysis of the energy savings and interference reduction achieved\nby the algorithms. The results show that, in spite of not incurring a higher\ncommunication or computational complexity, the STC algorithm performs better\nthan other algorithms in uniform wireless environments and especially better\nwhen path loss characteristics are non-uniform.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Sep 2007 15:13:34 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 13 Nov 2009 22:36:40 GMT"
            }
        ],
        "update_date": "2010-03-26",
        "authors_parsed": [
            [
                "Sethu",
                "Harish",
                ""
            ],
            [
                "Gerety",
                "Thomas",
                ""
            ]
        ]
    },
    {
        "id": "0709.0965",
        "submitter": "Harish Sethu",
        "authors": "Harish Sethu",
        "title": "The Induced Bounded-Degree Subgraph Problem and Stream Control in MIMO\n  Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  In this report, we consider maximal solutions to the induced bounded-degree\nsubgraph problem and relate it to issues concerning stream control in\nmultiple-input multiple-output (MIMO) networks. We present a new distributed\nalgorithm that completes in logarithmic time with high probability and is\nguaranteed to complete in linear time. We conclude the report with simulation\nresults that address the effectiveness of stream control and the relative\nimpact of receiver overloading and flexible interference suppression.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Sep 2007 22:40:22 GMT"
            }
        ],
        "update_date": "2007-09-10",
        "authors_parsed": [
            [
                "Sethu",
                "Harish",
                ""
            ]
        ]
    },
    {
        "id": "0709.1024",
        "submitter": "Roland Bouffanais",
        "authors": "Roland Bouffanais, Vincent Keller, Ralf Gruber, Michel O. Deville",
        "title": "Computational performance of a parallelized high-order spectral and\n  mortar element toolbox",
        "comments": "Preprint submitted for publication to Parallel Computing",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.PF",
        "license": null,
        "abstract": "  In this paper, a comprehensive performance review of a MPI-based high-order\nspectral and mortar element method C++ toolbox is presented. The focus is put\non the performance evaluation of several aspects with a particular emphasis on\nthe parallel efficiency. The performance evaluation is analyzed and compared to\npredictions given by a heuristic model, the so-called Gamma model. A\ntailor-made CFD computation benchmark case is introduced and used to carry out\nthis review, stressing the particular interest for commodity clusters.\nConclusions are drawn from this extensive series of analyses and modeling\nleading to specific recommendations concerning such toolbox development and\nparallel implementation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Sep 2007 08:52:32 GMT"
            }
        ],
        "update_date": "2007-09-10",
        "authors_parsed": [
            [
                "Bouffanais",
                "Roland",
                ""
            ],
            [
                "Keller",
                "Vincent",
                ""
            ],
            [
                "Gruber",
                "Ralf",
                ""
            ],
            [
                "Deville",
                "Michel O.",
                ""
            ]
        ]
    },
    {
        "id": "0709.1024",
        "submitter": "Roland Bouffanais",
        "authors": "Roland Bouffanais, Vincent Keller, Ralf Gruber, Michel O. Deville",
        "title": "Computational performance of a parallelized high-order spectral and\n  mortar element toolbox",
        "comments": "Preprint submitted for publication to Parallel Computing",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.PF",
        "license": null,
        "abstract": "  In this paper, a comprehensive performance review of a MPI-based high-order\nspectral and mortar element method C++ toolbox is presented. The focus is put\non the performance evaluation of several aspects with a particular emphasis on\nthe parallel efficiency. The performance evaluation is analyzed and compared to\npredictions given by a heuristic model, the so-called Gamma model. A\ntailor-made CFD computation benchmark case is introduced and used to carry out\nthis review, stressing the particular interest for commodity clusters.\nConclusions are drawn from this extensive series of analyses and modeling\nleading to specific recommendations concerning such toolbox development and\nparallel implementation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Sep 2007 08:52:32 GMT"
            }
        ],
        "update_date": "2007-09-10",
        "authors_parsed": [
            [
                "Bouffanais",
                "Roland",
                ""
            ],
            [
                "Keller",
                "Vincent",
                ""
            ],
            [
                "Gruber",
                "Ralf",
                ""
            ],
            [
                "Deville",
                "Michel O.",
                ""
            ]
        ]
    },
    {
        "id": "0709.1056",
        "submitter": "Stephane Norte",
        "authors": "Stephane Norte",
        "title": "A Sudoku Game for People with Motor Impairments",
        "comments": "7 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.CY",
        "license": null,
        "abstract": "  Computer games are motivating and beneficial in learning different\neducational skills. Most people use their fingers, hands, and arms when using a\ncomputer game. However, for people with motor disabilities this task can be a\nbarrier. We present a new Sudoku game for people whose motion is impaired,\ncalled Sudoku 4ALL. With this special interface a person can control the game\nwith the voice or with a single switch. Our research aims to cautiously search\nfor issues that might be appropriate for computational support and to build\nenabling technologies that increase individuals' functional independence in a\ngame environment.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Sep 2007 11:59:22 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 13 Sep 2007 13:26:17 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 17 Sep 2007 21:08:35 GMT"
            }
        ],
        "update_date": "2007-09-18",
        "authors_parsed": [
            [
                "Norte",
                "Stephane",
                ""
            ]
        ]
    },
    {
        "id": "0709.1080",
        "submitter": "Cas Cremers",
        "authors": "Cas Cremers",
        "title": "On the Protocol Composition Logic PCL",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.LO",
        "license": null,
        "abstract": "  A recent development in formal security protocol analysis is the Protocol\nComposition Logic (PCL). We identify a number of problems with this logic as\nwell as with extensions of the logic, as defined in\n[DDMP05,HSD+05,He05,Dat05,Der06,DDMR07]. The identified problems imply strong\nrestrictions on the scope of PCL, and imply that some currently claimed PCL\nproofs cannot be proven within the logic, or make use of unsound axioms. Where\npossible, we propose solutions for these problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Sep 2007 13:32:06 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 9 Sep 2007 12:14:41 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 20 Sep 2007 21:51:00 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 1 Oct 2007 13:28:21 GMT"
            },
            {
                "version": "v5",
                "created": "Fri, 2 Nov 2007 11:10:19 GMT"
            },
            {
                "version": "v6",
                "created": "Wed, 19 Dec 2007 15:58:11 GMT"
            },
            {
                "version": "v7",
                "created": "Fri, 22 Feb 2008 16:12:11 GMT"
            }
        ],
        "update_date": "2008-02-22",
        "authors_parsed": [
            [
                "Cremers",
                "Cas",
                ""
            ]
        ]
    },
    {
        "id": "0709.1099",
        "submitter": "Cherif Smaili",
        "authors": "Cherif Smaili (INRIA Lorraine - LORIA), Maan El Badaoui El Najjar\n  (INRIA Lorraine - LORIA), Fran\\c{c}ois Charpillet (INRIA Lorraine - LORIA)",
        "title": "Multi-Sensor Fusion Method using Dynamic Bayesian Network for Precise\n  Vehicle Localization and Road Matching",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.RO",
        "license": null,
        "abstract": "  This paper presents a multi-sensor fusion strategy for a novel road-matching\nmethod designed to support real-time navigational features within advanced\ndriving-assistance systems. Managing multihypotheses is a useful strategy for\nthe road-matching problem. The multi-sensor fusion and multi-modal estimation\nare realized using Dynamical Bayesian Network. Experimental results, using data\nfrom Antilock Braking System (ABS) sensors, a differential Global Positioning\nSystem (GPS) receiver and an accurate digital roadmap, illustrate the\nperformances of this approach, especially in ambiguous situations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Sep 2007 15:03:37 GMT"
            }
        ],
        "update_date": "2007-09-10",
        "authors_parsed": [
            [
                "Smaili",
                "Cherif",
                "",
                "INRIA Lorraine - LORIA"
            ],
            [
                "Najjar",
                "Maan El Badaoui El",
                "",
                "INRIA Lorraine - LORIA"
            ],
            [
                "Charpillet",
                "Fran\u00e7ois",
                "",
                "INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0709.1166",
        "submitter": "Daniel Lemire",
        "authors": "Daniel Lemire, Martin Brooks, Yuhong Yan",
        "title": "An Optimal Linear Time Algorithm for Quasi-Monotonic Segmentation",
        "comments": "This is the extended version of our ICDM'05 paper (arXiv:cs/0702142)",
        "journal-ref": "Daniel Lemire, Martin Brooks and Yuhong Yan, An Optimal Linear\n  Time Algorithm for Quasi-Monotonic Segmentation. International Journal of\n  Computer Mathematics 86 (7), 2009.",
        "doi": "10.1080/00207160701694153",
        "report-no": "NRC-00156",
        "categories": "cs.DB",
        "license": null,
        "abstract": "  Monotonicity is a simple yet significant qualitative characteristic. We\nconsider the problem of segmenting a sequence in up to K segments. We want\nsegments to be as monotonic as possible and to alternate signs. We propose a\nquality metric for this problem using the l_inf norm, and we present an optimal\nlinear time algorithm based on novel formalism. Moreover, given a\nprecomputation in time O(n log n) consisting of a labeling of all extrema, we\ncompute any optimal segmentation in constant time. We compare experimentally\nits performance to two piecewise linear segmentation heuristics (top-down and\nbottom-up). We show that our algorithm is faster and more accurate.\nApplications include pattern recognition and qualitative modeling.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Sep 2007 21:18:57 GMT"
            }
        ],
        "update_date": "2009-09-01",
        "authors_parsed": [
            [
                "Lemire",
                "Daniel",
                ""
            ],
            [
                "Brooks",
                "Martin",
                ""
            ],
            [
                "Yan",
                "Yuhong",
                ""
            ]
        ]
    },
    {
        "id": "0709.1167",
        "submitter": "Marko Antonio Rodriguez",
        "authors": "Marko A. Rodriguez, Jennifer H. Watkins, Johan Bollen, Carlos\n  Gershenson",
        "title": "Using RDF to Model the Structure and Process of Systems",
        "comments": "International Conference on Complex Systems, Boston MA, October 2007",
        "journal-ref": "InterJournal of Complex Systems, 2131, ISSN: 1081-0625, February\n  2008",
        "doi": null,
        "report-no": "LAUR-07-5720",
        "categories": "cs.AI",
        "license": null,
        "abstract": "  Many systems can be described in terms of networks of discrete elements and\ntheir various relationships to one another. A semantic network, or\nmulti-relational network, is a directed labeled graph consisting of a\nheterogeneous set of entities connected by a heterogeneous set of\nrelationships. Semantic networks serve as a promising general-purpose modeling\nsubstrate for complex systems. Various standardized formats and tools are now\navailable to support practical, large-scale semantic network models. First, the\nResource Description Framework (RDF) offers a standardized semantic network\ndata model that can be further formalized by ontology modeling languages such\nas RDF Schema (RDFS) and the Web Ontology Language (OWL). Second, the recent\nintroduction of highly performant triple-stores (i.e. semantic network\ndatabases) allows semantic network models on the order of $10^9$ edges to be\nefficiently stored and manipulated. RDF and its related technologies are\ncurrently used extensively in the domains of computer science, digital library\nscience, and the biological sciences. This article will provide an introduction\nto RDF/RDFS/OWL and an examination of its suitability to model discrete element\ncomplex systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 8 Sep 2007 01:18:18 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 15 Oct 2007 16:00:19 GMT"
            }
        ],
        "update_date": "2008-11-03",
        "authors_parsed": [
            [
                "Rodriguez",
                "Marko A.",
                ""
            ],
            [
                "Watkins",
                "Jennifer H.",
                ""
            ],
            [
                "Bollen",
                "Johan",
                ""
            ],
            [
                "Gershenson",
                "Carlos",
                ""
            ]
        ]
    },
    {
        "id": "0709.1190",
        "submitter": "Mohsen Bayati",
        "authors": "Mohsen Bayati, Christian Borgs, Jennifer Chayes, Riccardo Zecchina",
        "title": "Belief-Propagation for Weighted b-Matchings on Arbitrary Graphs and its\n  Relation to Linear Programs with Integer Solutions",
        "comments": "28 pages, 2 figures. Submitted to SIAM journal on Discrete\n  Mathematics on March 19, 2009; accepted for publication (in revised form)\n  August 30, 2010; published electronically July 1, 2011",
        "journal-ref": "SIAM J. Discrete Math. 2011, Vol 25, Issue 2, pp. 989-1011",
        "doi": "10.1137/090753115",
        "report-no": null,
        "categories": "cs.IT cs.AI math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the general problem of finding the minimum weight $\\bm$-matching\non arbitrary graphs. We prove that, whenever the linear programming (LP)\nrelaxation of the problem has no fractional solutions, then the belief\npropagation (BP) algorithm converges to the correct solution. We also show that\nwhen the LP relaxation has a fractional solution then the BP algorithm can be\nused to solve the LP relaxation. Our proof is based on the notion of graph\ncovers and extends the analysis of (Bayati-Shah-Sharma 2005 and Huang-Jebara\n2007}.\n  These results are notable in the following regards: (1) It is one of a very\nsmall number of proofs showing correctness of BP without any constraint on the\ngraph structure. (2) Variants of the proof work for both synchronous and\nasynchronous BP; it is the first proof of convergence and correctness of an\nasynchronous BP algorithm for a combinatorial optimization problem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 8 Sep 2007 08:21:34 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 9 Feb 2008 02:56:38 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 4 Aug 2011 21:43:21 GMT"
            }
        ],
        "update_date": "2015-03-13",
        "authors_parsed": [
            [
                "Bayati",
                "Mohsen",
                ""
            ],
            [
                "Borgs",
                "Christian",
                ""
            ],
            [
                "Chayes",
                "Jennifer",
                ""
            ],
            [
                "Zecchina",
                "Riccardo",
                ""
            ]
        ]
    },
    {
        "id": "0709.1227",
        "submitter": "Yanghua Xiao",
        "authors": "Yanghua Xiao, Wentao Wu, Wei Wang and Zhengying He",
        "title": "Efficient Algorithms for Node Disjoint Subgraph Homeomorphism\n  Determination",
        "comments": "15 pages, 11 figures, submitted to DASFAA 2008",
        "journal-ref": "In Proceeding of 13th International Conference on Database Systems\n  for Advanced Applications, 2008",
        "doi": "10.1007/978-3-540-78568-2",
        "report-no": null,
        "categories": "cs.DS cs.DB",
        "license": null,
        "abstract": "  Recently, great efforts have been dedicated to researches on the management\nof large scale graph based data such as WWW, social networks, biological\nnetworks. In the study of graph based data management, node disjoint subgraph\nhomeomorphism relation between graphs is more suitable than (sub)graph\nisomorphism in many cases, especially in those cases that node skipping and\nnode mismatching are allowed. However, no efficient node disjoint subgraph\nhomeomorphism determination (ndSHD) algorithms have been available. In this\npaper, we propose two computationally efficient ndSHD algorithms based on state\nspaces searching with backtracking, which employ many heuristics to prune the\nsearch spaces. Experimental results on synthetic data sets show that the\nproposed algorithms are efficient, require relative little time in most of the\ntesting cases, can scale to large or dense graphs, and can accommodate to more\ncomplex fuzzy matching cases.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 8 Sep 2007 18:14:47 GMT"
            }
        ],
        "update_date": "2008-10-09",
        "authors_parsed": [
            [
                "Xiao",
                "Yanghua",
                ""
            ],
            [
                "Wu",
                "Wentao",
                ""
            ],
            [
                "Wang",
                "Wei",
                ""
            ],
            [
                "He",
                "Zhengying",
                ""
            ]
        ]
    },
    {
        "id": "0709.1272",
        "submitter": "Alfredo Buttari Dr",
        "authors": "Alfredo Buttari, Julien Langou, Jakub Kurzak, Jack Dongarra",
        "title": "A Class of Parallel Tiled Linear Algebra Algorithms for Multicore\n  Architectures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "Lapack working Note 191",
        "categories": "cs.MS cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As multicore systems continue to gain ground in the High Performance\nComputing world, linear algebra algorithms have to be reformulated or new\nalgorithms have to be developed in order to take advantage of the architectural\nfeatures on these new processors. Fine grain parallelism becomes a major\nrequirement and introduces the necessity of loose synchronization in the\nparallel execution of an operation. This paper presents an algorithm for the\nCholesky, LU and QR factorization where the operations can be represented as a\nsequence of small tasks that operate on square blocks of data. These tasks can\nbe dynamically scheduled for execution based on the dependencies among them and\non the availability of computational resources. This may result in an out of\norder execution of the tasks which will completely hide the presence of\nintrinsically sequential tasks in the factorization. Performance comparisons\nare presented with the LAPACK algorithms where parallelism can only be\nexploited at the level of the BLAS operations and vendor implementations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 9 Sep 2007 16:32:46 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 11 Sep 2007 07:14:07 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 12 Jun 2008 18:09:13 GMT"
            }
        ],
        "update_date": "2008-06-12",
        "authors_parsed": [
            [
                "Buttari",
                "Alfredo",
                ""
            ],
            [
                "Langou",
                "Julien",
                ""
            ],
            [
                "Kurzak",
                "Jakub",
                ""
            ],
            [
                "Dongarra",
                "Jack",
                ""
            ]
        ]
    },
    {
        "id": "0709.1272",
        "submitter": "Alfredo Buttari Dr",
        "authors": "Alfredo Buttari, Julien Langou, Jakub Kurzak, Jack Dongarra",
        "title": "A Class of Parallel Tiled Linear Algebra Algorithms for Multicore\n  Architectures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "Lapack working Note 191",
        "categories": "cs.MS cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As multicore systems continue to gain ground in the High Performance\nComputing world, linear algebra algorithms have to be reformulated or new\nalgorithms have to be developed in order to take advantage of the architectural\nfeatures on these new processors. Fine grain parallelism becomes a major\nrequirement and introduces the necessity of loose synchronization in the\nparallel execution of an operation. This paper presents an algorithm for the\nCholesky, LU and QR factorization where the operations can be represented as a\nsequence of small tasks that operate on square blocks of data. These tasks can\nbe dynamically scheduled for execution based on the dependencies among them and\non the availability of computational resources. This may result in an out of\norder execution of the tasks which will completely hide the presence of\nintrinsically sequential tasks in the factorization. Performance comparisons\nare presented with the LAPACK algorithms where parallelism can only be\nexploited at the level of the BLAS operations and vendor implementations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 9 Sep 2007 16:32:46 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 11 Sep 2007 07:14:07 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 12 Jun 2008 18:09:13 GMT"
            }
        ],
        "update_date": "2008-06-12",
        "authors_parsed": [
            [
                "Buttari",
                "Alfredo",
                ""
            ],
            [
                "Langou",
                "Julien",
                ""
            ],
            [
                "Kurzak",
                "Jakub",
                ""
            ],
            [
                "Dongarra",
                "Jack",
                ""
            ]
        ]
    },
    {
        "id": "0709.1401",
        "submitter": "Arnaud Spiwack",
        "authors": "Thierry Coquand and Arnaud Spiwack",
        "title": "A proof of strong normalisation using domain theory",
        "comments": "16 pages",
        "journal-ref": "Logical Methods in Computer Science, Volume 3, Issue 4 (December\n  4, 2007) lmcs:1099",
        "doi": "10.2168/LMCS-3(4:12)2007",
        "report-no": null,
        "categories": "cs.LO cs.PL",
        "license": null,
        "abstract": "  Ulrich Berger presented a powerful proof of strong normalisation using\ndomains, in particular it simplifies significantly Tait's proof of strong\nnormalisation of Spector's bar recursion. The main contribution of this paper\nis to show that, using ideas from intersection types and Martin-Lof's domain\ninterpretation of type theory one can in turn simplify further U. Berger's\nargument. We build a domain model for an untyped programming language where U.\nBerger has an interpretation only for typed terms or alternatively has an\ninterpretation for untyped terms but need an extra condition to deduce strong\nnormalisation. As a main application, we show that Martin-L\\\"{o}f dependent\ntype theory extended with a program for Spector double negation shift.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 10 Sep 2007 14:08:26 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 4 Dec 2007 14:43:36 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Coquand",
                "Thierry",
                ""
            ],
            [
                "Spiwack",
                "Arnaud",
                ""
            ]
        ]
    },
    {
        "id": "0709.1449",
        "submitter": "Yue Li",
        "authors": "Yue Li and Yu Liu",
        "title": "Supervised secure entanglement sharing for faithful quantum\n  teleportation via tripartite W states",
        "comments": "7 pages, 4 figures, revised the detection mode and security analysis,\n  added one successful eavesdropping rate plot in Fig. 3",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "quant-ph cs.CR",
        "license": null,
        "abstract": "  We present a supervised secure entanglement sharing protocol via tripartite W\nstates for faithful quantum teleportation. By guaranteeing a secure\nentanglement distribution in the charge of a third believed supervisor, quantum\ninformation of an unknown state of a 2-level particle can be faithfully\nteleported from the sender to the remote receiver via the Bell states distilled\nfrom the tripartite W states. We emphasize that reliable teleportation after\nour protocol between two communication parties depends on the agreement of the\nsupervisor to cooperate via taking the W states as both the quantum channel and\neavesdropping detector. The security against typical individual eavesdropping\nattacks is proved and its experimental feasibility is briefly illustrated.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 10 Sep 2007 17:01:14 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 18 Nov 2007 14:18:48 GMT"
            }
        ],
        "update_date": "2007-11-19",
        "authors_parsed": [
            [
                "Li",
                "Yue",
                ""
            ],
            [
                "Liu",
                "Yu",
                ""
            ]
        ]
    },
    {
        "id": "0709.1516",
        "submitter": "Marcus Hutter",
        "authors": "Marcus Hutter",
        "title": "On Universal Prediction and Bayesian Confirmation",
        "comments": "24 pages",
        "journal-ref": "Theoretical Computer Science, 384 (2007) pages 33-48",
        "doi": null,
        "report-no": null,
        "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH",
        "license": null,
        "abstract": "  The Bayesian framework is a well-studied and successful framework for\ninductive reasoning, which includes hypothesis testing and confirmation,\nparameter estimation, sequence prediction, classification, and regression. But\nstandard statistical guidelines for choosing the model class and prior are not\nalways available or fail, in particular in complex situations. Solomonoff\ncompleted the Bayesian framework by providing a rigorous, unique, formal, and\nuniversal choice for the model class and the prior. We discuss in breadth how\nand in which sense universal (non-i.i.d.) sequence prediction solves various\n(philosophical) problems of traditional Bayesian sequence prediction. We show\nthat Solomonoff's model possesses many desirable properties: Strong total and\nweak instantaneous bounds, and in contrast to most classical continuous prior\ndensities has no zero p(oste)rior problem, i.e. can confirm universal\nhypotheses, is reparametrization and regrouping invariant, and avoids the\nold-evidence and updating problem. It even performs well (actually better) in\nnon-computable environments.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Sep 2007 01:39:20 GMT"
            }
        ],
        "update_date": "2008-06-26",
        "authors_parsed": [
            [
                "Hutter",
                "Marcus",
                ""
            ]
        ]
    },
    {
        "id": "0709.1667",
        "submitter": "Federico Ricci-Tersenghi",
        "authors": "Andrea Montanari, Federico Ricci-Tersenghi and Guilhem Semerjian",
        "title": "Solving Constraint Satisfaction Problems through Belief\n  Propagation-guided decimation",
        "comments": "10 pages, 4 figures. A longer version can be found as arXiv:0904.3395\n  [cond-mat.dis-nn]",
        "journal-ref": "Proceedings of the 45th Annual Allerton Conference on\n  Communication, Control, and Computing (Monticello, IL, USA), 352-359 (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cond-mat.dis-nn cond-mat.stat-mech cs.CC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Message passing algorithms have proved surprisingly successful in solving\nhard constraint satisfaction problems on sparse random graphs. In such\napplications, variables are fixed sequentially to satisfy the constraints.\nMessage passing is run after each step. Its outcome provides an heuristic to\nmake choices at next step. This approach has been referred to as `decimation,'\nwith reference to analogous procedures in statistical physics.\n  The behavior of decimation procedures is poorly understood. Here we consider\na simple randomized decimation algorithm based on belief propagation (BP), and\nanalyze its behavior on random k-satisfiability formulae. In particular, we\npropose a tree model for its analysis and we conjecture that it provides\nasymptotically exact predictions in the limit of large instances. This\nconjecture is confirmed by numerical simulations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Sep 2007 15:48:56 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 1 Oct 2007 15:01:01 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 4 Jun 2019 11:43:45 GMT"
            }
        ],
        "update_date": "2019-06-05",
        "authors_parsed": [
            [
                "Montanari",
                "Andrea",
                ""
            ],
            [
                "Ricci-Tersenghi",
                "Federico",
                ""
            ],
            [
                "Semerjian",
                "Guilhem",
                ""
            ]
        ]
    },
    {
        "id": "0709.1699",
        "submitter": "Paul Fodor",
        "authors": "Paul Fodor",
        "title": "Efficient Tabling Mechanisms for Transaction Logic Programs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.AI",
        "license": null,
        "abstract": "  In this paper we present efficient evaluation algorithms for the Horn\nTransaction Logic (a generalization of the regular Horn logic programs with\nstate updates). We present two complementary methods for optimizing the\nimplementation of Transaction Logic. The first method is based on tabling and\nwe modified the proof theory to table calls and answers on states (practically,\nequivalent to dynamic programming). The call-answer table is indexed on the\ncall and a signature of the state in which the call was made. The answer\ncolumns contain the answer unification and a signature of the state after the\ncall was executed. The states are signed efficiently using a technique based on\ntries and counting. The second method is based on incremental evaluation and it\napplies when the data oracle contains derived relations. The deletions and\ninsertions (executed in the transaction oracle) change the state of the\ndatabase. Using the heuristic of inertia (only a part of the state changes in\nresponse to elementary updates), most of the time it is cheaper to compute only\nthe changes in the state than to recompute the entire state from scratch. The\ntwo methods are complementary by the fact that the first method optimizes the\nevaluation when a call is repeated in the same state, and the second method\noptimizes the evaluation of a new state when a call-state pair is not found by\nthe tabling mechanism (i.e. the first method). The proof theory of Transaction\nLogic with the application of tabling and incremental evaluation is sound and\ncomplete with respect to its model theory.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Sep 2007 19:00:02 GMT"
            }
        ],
        "update_date": "2007-09-12",
        "authors_parsed": [
            [
                "Fodor",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0709.1701",
        "submitter": "Jean Dezert",
        "authors": "Xinde Li (ICRL), Xinhan Huang (ICRL), Florentin Smarandache (UNM),\n  Jean Dezert (ONERA)",
        "title": "Enrichment of Qualitative Beliefs for Reasoning under Uncertainty",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  This paper deals with enriched qualitative belief functions for reasoning\nunder uncertainty and for combining information expressed in natural language\nthrough linguistic labels. In this work, two possible enrichments (quantitative\nand/or qualitative) of linguistic labels are considered and operators\n(addition, multiplication, division, etc) for dealing with them are proposed\nand explained. We denote them $qe$-operators, $qe$ standing for\n\"qualitative-enriched\" operators. These operators can be seen as a direct\nextension of the classical qualitative operators ($q$-operators) proposed\nrecently in the Dezert-Smarandache Theory of plausible and paradoxist reasoning\n(DSmT). $q$-operators are also justified in details in this paper. The\nquantitative enrichment of linguistic label is a numerical supporting degree in\n$[0,\\infty)$, while the qualitative enrichment takes its values in a finite\nordered set of linguistic values. Quantitative enrichment is less precise than\nqualitative enrichment, but it is expected more close with what human experts\ncan easily provide when expressing linguistic labels with supporting degrees.\nTwo simple examples are given to show how the fusion of qualitative-enriched\nbelief assignments can be done.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Sep 2007 19:12:25 GMT"
            }
        ],
        "update_date": "2007-09-12",
        "authors_parsed": [
            [
                "Li",
                "Xinde",
                "",
                "ICRL"
            ],
            [
                "Huang",
                "Xinhan",
                "",
                "ICRL"
            ],
            [
                "Smarandache",
                "Florentin",
                "",
                "UNM"
            ],
            [
                "Dezert",
                "Jean",
                "",
                "ONERA"
            ]
        ]
    },
    {
        "id": "0709.1771",
        "submitter": "Heng Lian",
        "authors": "Heng Lian",
        "title": "Variational local structure estimation for image super-resolution",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  Super-resolution is an important but difficult problem in image/video\nprocessing. If a video sequence or some training set other than the given\nlow-resolution image is available, this kind of extra information can greatly\naid in the reconstruction of the high-resolution image. The problem is\nsubstantially more difficult with only a single low-resolution image on hand.\nThe image reconstruction methods designed primarily for denoising is\ninsufficient for super-resolution problem in the sense that it tends to\noversmooth images with essentially no noise. We propose a new adaptive linear\ninterpolation method based on variational method and inspired by local linear\nembedding (LLE). The experimental result shows that our method avoids the\nproblem of oversmoothing and preserves image structures well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 12 Sep 2007 08:41:36 GMT"
            }
        ],
        "update_date": "2007-09-13",
        "authors_parsed": [
            [
                "Lian",
                "Heng",
                ""
            ]
        ]
    },
    {
        "id": "0709.1920",
        "submitter": "Aurelie Bugeau",
        "authors": "Aurelie Bugeau (IRISA), Patrick P\\'erez (IRISA)",
        "title": "Bandwidth selection for kernel estimation in mixed multi-dimensional\n  spaces",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  Kernel estimation techniques, such as mean shift, suffer from one major\ndrawback: the kernel bandwidth selection. The bandwidth can be fixed for all\nthe data set or can vary at each points. Automatic bandwidth selection becomes\na real challenge in case of multidimensional heterogeneous features. This paper\npresents a solution to this problem. It is an extension of \\cite{Comaniciu03a}\nwhich was based on the fundamental property of normal distributions regarding\nthe bias of the normalized density gradient. The selection is done iteratively\nfor each type of features, by looking for the stability of local bandwidth\nestimates across a predefined range of bandwidths. A pseudo balloon mean shift\nfiltering and partitioning are introduced. The validity of the method is\ndemonstrated in the context of color image segmentation based on a\n5-dimensional space.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 12 Sep 2007 16:02:25 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 14 Sep 2007 08:35:21 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Bugeau",
                "Aurelie",
                "",
                "IRISA"
            ],
            [
                "P\u00e9rez",
                "Patrick",
                "",
                "IRISA"
            ]
        ]
    },
    {
        "id": "0709.2016",
        "submitter": "Konstantin Avrachenkov",
        "authors": "Konstantin Avrachenkov, Nelly Litvak, Kim Son Pham",
        "title": "Distribution of PageRank Mass Among Principle Components of the Web",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.DS",
        "license": null,
        "abstract": "  We study the PageRank mass of principal components in a bow-tie Web Graph, as\na function of the damping factor c. Using a singular perturbation approach, we\nshow that the PageRank share of IN and SCC components remains high even for\nvery large values of the damping factor, in spite of the fact that it drops to\nzero when c goes to one. However, a detailed study of the OUT component reveals\nthe presence ``dead-ends'' (small groups of pages linking only to each other)\nthat receive an unfairly high ranking when c is close to one. We argue that\nthis problem can be mitigated by choosing c as small as 1/2.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Sep 2007 08:29:53 GMT"
            }
        ],
        "update_date": "2007-09-14",
        "authors_parsed": [
            [
                "Avrachenkov",
                "Konstantin",
                ""
            ],
            [
                "Litvak",
                "Nelly",
                ""
            ],
            [
                "Pham",
                "Kim Son",
                ""
            ]
        ]
    },
    {
        "id": "0709.2065",
        "submitter": "Andrei Khrennikov",
        "authors": "Andrei Khrennikov",
        "title": "Toward Psycho-robots",
        "comments": null,
        "journal-ref": "Paladyn Volume 1, Number 2, 99-108, 2010",
        "doi": "10.2478/s13230-010-0014-0",
        "report-no": null,
        "categories": "cs.AI",
        "license": null,
        "abstract": "  We try to perform geometrization of psychology by representing mental states,\n<<ideas>>, by points of a metric space, <<mental space>>. Evolution of ideas is\ndescribed by dynamical systems in metric mental space. We apply the mental\nspace approach for modeling of flows of unconscious and conscious information\nin the human brain. In a series of models, Models 1-4, we consider cognitive\nsystems with increasing complexity of psychological behavior determined by\nstructure of flows of ideas. Since our models are in fact models of the\nAI-type, one immediately recognizes that they can be used for creation of\nAI-systems, which we call psycho-robots, exhibiting important elements of human\npsyche. Creation of such psycho-robots may be useful improvement of domestic\nrobots. At the moment domestic robots are merely simple working devices (e.g.\nvacuum cleaners or lawn mowers) . However, in future one can expect demand in\nsystems which be able not only perform simple work tasks, but would have\nelements of human self-developing psyche. Such AI-psyche could play an\nimportant role both in relations between psycho-robots and their owners as well\nas between psycho-robots. Since the presence of a huge numbers of\npsycho-complexes is an essential characteristic of human psychology, it would\nbe interesting to model them in the AI-framework.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Sep 2007 13:06:34 GMT"
            }
        ],
        "update_date": "2010-11-30",
        "authors_parsed": [
            [
                "Khrennikov",
                "Andrei",
                ""
            ]
        ]
    },
    {
        "id": "0709.2074",
        "submitter": "Vladimir Gudkov",
        "authors": "V. Gudkov and V. Montealegre",
        "title": "Generalized entropies and open random and scale-free networks",
        "comments": "talk at CTNEXT07 (July 2007)",
        "journal-ref": null,
        "doi": "10.1063/1.2828754",
        "report-no": null,
        "categories": "cond-mat.dis-nn cs.NI physics.soc-ph",
        "license": null,
        "abstract": "  We propose the concept of open network as an arbitrary selection of nodes of\na large unknown network. Using the hypothesis that information of the whole\nnetwork structure can be extrapolated from an arbitrary set of its nodes, we\nuse Renyi mutual entropies in different q-orders to establish the minimum\ncritical size of a random set of nodes that represents reliably the information\nof the main network structure. We also identify the clusters of nodes\nresponsible for the structure of their containing network.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Sep 2007 13:37:58 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Gudkov",
                "V.",
                ""
            ],
            [
                "Montealegre",
                "V.",
                ""
            ]
        ]
    },
    {
        "id": "0709.2225",
        "submitter": "Ananthanarayanan Chockalingam",
        "authors": "T. Srikanth, K. Vishnu Vardhan, A. Chockalingam, and L. B. Milstein",
        "title": "Improved Linear Parallel Interference Cancellers",
        "comments": "Accepted in IEEE Trans. on Wireless Communications",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.SC cs.SD cs.SE math.IT",
        "license": null,
        "abstract": "  In this paper, taking the view that a linear parallel interference canceller\n(LPIC) can be seen as a linear matrix filter, we propose new linear matrix\nfilters that can result in improved bit error performance compared to other\nLPICs in the literature. The motivation for the proposed filters arises from\nthe possibility of avoiding the generation of certain interference and noise\nterms in a given stage that would have been present in a conventional LPIC\n(CLPIC). In the proposed filters, we achieve such avoidance of the generation\nof interference and noise terms in a given stage by simply making the diagonal\nelements of a certain matrix in that stage equal to zero. Hence, the proposed\nfilters do not require additional complexity compared to the CLPIC, and they\ncan allow achieving a certain error performance using fewer LPIC stages. We\nalso extend the proposed matrix filter solutions to a multicarrier DS-CDMA\nsystem, where we consider two types of receivers. In one receiver (referred to\nas Type-I receiver), LPIC is performed on each subcarrier first, followed by\nmulticarrier combining (MCC). In the other receiver (called Type-II receiver),\nMCC is performed first, followed by LPIC. We show that in both Type-I and\nType-II receivers, the proposed matrix filters outperform other matrix filters.\nAlso, Type-II receiver performs better than Type-I receiver because of enhanced\naccuracy of the interference estimates achieved due to frequency diversity\noffered by MCC.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 14 Sep 2007 07:56:21 GMT"
            }
        ],
        "update_date": "2007-09-17",
        "authors_parsed": [
            [
                "Srikanth",
                "T.",
                ""
            ],
            [
                "Vardhan",
                "K. Vishnu",
                ""
            ],
            [
                "Chockalingam",
                "A.",
                ""
            ],
            [
                "Milstein",
                "L. B.",
                ""
            ]
        ]
    },
    {
        "id": "0709.2225",
        "submitter": "Ananthanarayanan Chockalingam",
        "authors": "T. Srikanth, K. Vishnu Vardhan, A. Chockalingam, and L. B. Milstein",
        "title": "Improved Linear Parallel Interference Cancellers",
        "comments": "Accepted in IEEE Trans. on Wireless Communications",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.SC cs.SD cs.SE math.IT",
        "license": null,
        "abstract": "  In this paper, taking the view that a linear parallel interference canceller\n(LPIC) can be seen as a linear matrix filter, we propose new linear matrix\nfilters that can result in improved bit error performance compared to other\nLPICs in the literature. The motivation for the proposed filters arises from\nthe possibility of avoiding the generation of certain interference and noise\nterms in a given stage that would have been present in a conventional LPIC\n(CLPIC). In the proposed filters, we achieve such avoidance of the generation\nof interference and noise terms in a given stage by simply making the diagonal\nelements of a certain matrix in that stage equal to zero. Hence, the proposed\nfilters do not require additional complexity compared to the CLPIC, and they\ncan allow achieving a certain error performance using fewer LPIC stages. We\nalso extend the proposed matrix filter solutions to a multicarrier DS-CDMA\nsystem, where we consider two types of receivers. In one receiver (referred to\nas Type-I receiver), LPIC is performed on each subcarrier first, followed by\nmulticarrier combining (MCC). In the other receiver (called Type-II receiver),\nMCC is performed first, followed by LPIC. We show that in both Type-I and\nType-II receivers, the proposed matrix filters outperform other matrix filters.\nAlso, Type-II receiver performs better than Type-I receiver because of enhanced\naccuracy of the interference estimates achieved due to frequency diversity\noffered by MCC.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 14 Sep 2007 07:56:21 GMT"
            }
        ],
        "update_date": "2007-09-17",
        "authors_parsed": [
            [
                "Srikanth",
                "T.",
                ""
            ],
            [
                "Vardhan",
                "K. Vishnu",
                ""
            ],
            [
                "Chockalingam",
                "A.",
                ""
            ],
            [
                "Milstein",
                "L. B.",
                ""
            ]
        ]
    },
    {
        "id": "0709.2252",
        "submitter": "Mathias Boc",
        "authors": "Mathias Boc, Anne Fladenmuller and Marcelo dias de Amorim",
        "title": "Otiy: Loactors tracking nodes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  We propose Otiy, a node-centric location service that limits the impact of\nlocation updates generate by mobile nodes in IEEE802.11-based wireless mesh\nnetworks. Existing location services use node identifiers to determine the\nlocator (aka anchor) that is responsible for keeping track of a node's\nlocation. Such a strategy can be inefficient because: (i) identifiers give no\nclue on the node's mobility and (ii) locators can be far from the\nsource/destination shortest path, which increases both location delays and\nbandwidth consumption. To solve these issues, Otiy introduces a new strategy\nthat identifies nodes to play the role of locators based on the likelihood of a\ndestination to be close to these nodes- i.e., locators are identified depending\non the mobility pattern of nodes. Otiy relies on the cyclic mobility patterns\nof nodes and creates a slotted agenda composed of a set of predicted locations,\ndefined according to the past and present patterns of mobility. Correspondent\nnodes fetch this agenda only once and use it as a reference for identifying\nwhich locators are responsible for the node at different points in time. Over a\nperiod of about one year, the weekly proportion of nodes having at least 50% of\nexact location predictions is in average about 75%. This proportion increases\nby 10% when nodes also consider their closeness to the locator from only what\nthey know about the network.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 14 Sep 2007 10:21:45 GMT"
            }
        ],
        "update_date": "2007-09-17",
        "authors_parsed": [
            [
                "Boc",
                "Mathias",
                ""
            ],
            [
                "Fladenmuller",
                "Anne",
                ""
            ],
            [
                "de Amorim",
                "Marcelo dias",
                ""
            ]
        ]
    },
    {
        "id": "0709.2410",
        "submitter": "Gesualdo Scutari",
        "authors": "Gesualdo Scutari, Sergio Barbarossa, Loreto Pescosolido",
        "title": "Distributed Decision Through Self-Synchronizing Sensor Networks in the\n  Presence of Propagation Delays and Asymmetric Channels",
        "comments": "To be published on IEEE Transactions on Signal Processing",
        "journal-ref": null,
        "doi": "10.1109/TSP.2007.909377",
        "report-no": null,
        "categories": "cs.MA cs.DC",
        "license": null,
        "abstract": "  In this paper we propose and analyze a distributed algorithm for achieving\nglobally optimal decisions, either estimation or detection, through a\nself-synchronization mechanism among linearly coupled integrators initialized\nwith local measurements. We model the interaction among the nodes as a directed\ngraph with weights (possibly) dependent on the radio channels and we pose\nspecial attention to the effect of the propagation delay occurring in the\nexchange of data among sensors, as a function of the network geometry. We\nderive necessary and sufficient conditions for the proposed system to reach a\nconsensus on globally optimal decision statistics. One of the major results\nproved in this work is that a consensus is reached with exponential convergence\nspeed for any bounded delay condition if and only if the directed graph is\nquasi-strongly connected. We provide a closed form expression for the global\nconsensus, showing that the effect of delays is, in general, the introduction\nof a bias in the final decision. Finally, we exploit our closed form expression\nto devise a double-step consensus mechanism able to provide an unbiased\nestimate with minimum extra complexity, without the need to know or estimate\nthe channel parameters.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 15 Sep 2007 08:40:18 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 19 Sep 2007 15:29:37 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Scutari",
                "Gesualdo",
                ""
            ],
            [
                "Barbarossa",
                "Sergio",
                ""
            ],
            [
                "Pescosolido",
                "Loreto",
                ""
            ]
        ]
    },
    {
        "id": "0709.2446",
        "submitter": "Fangwen Fu",
        "authors": "Fangwen Fu, Mihaela van der Schaar",
        "title": "Learning for Dynamic Bidding in Cognitive Radio Resources",
        "comments": "29pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.GT",
        "license": null,
        "abstract": "  In this paper, we model the various wireless users in a cognitive radio\nnetwork as a collection of selfish, autonomous agents that strategically\ninteract in order to acquire the dynamically available spectrum opportunities.\nOur main focus is on developing solutions for wireless users to successfully\ncompete with each other for the limited and time-varying spectrum\nopportunities, given the experienced dynamics in the wireless network. We\ncategorize these dynamics into two types: one is the disturbance due to the\nenvironment (e.g. wireless channel conditions, source traffic characteristics,\netc.) and the other is the impact caused by competing users. To analyze the\ninteractions among users given the environment disturbance, we propose a\ngeneral stochastic framework for modeling how the competition among users for\nspectrum opportunities evolves over time. At each stage of the dynamic resource\nallocation, a central spectrum moderator auctions the available resources and\nthe users strategically bid for the required resources. The joint bid actions\naffect the resource allocation and hence, the rewards and future strategies of\nall users. Based on the observed resource allocation and corresponding rewards\nfrom previous allocations, we propose a best response learning algorithm that\ncan be deployed by wireless users to improve their bidding policy at each\nstage. The simulation results show that by deploying the proposed best response\nlearning algorithm, the wireless users can significantly improve their own\nperformance in terms of both the packet loss rate and the incurred cost for the\nused resources.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 15 Sep 2007 20:48:57 GMT"
            }
        ],
        "update_date": "2007-09-18",
        "authors_parsed": [
            [
                "Fu",
                "Fangwen",
                ""
            ],
            [
                "van der Schaar",
                "Mihaela",
                ""
            ]
        ]
    },
    {
        "id": "0709.2506",
        "submitter": "Tshilidzi Marwala",
        "authors": "Vukosi N. Marivate, Fulufhelo V. Nelwamodo, Tshilidzi Marwala",
        "title": "Autoencoder, Principal Component Analysis and Support Vector Regression\n  for Data Imputation",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.DB",
        "license": null,
        "abstract": "  Data collection often results in records that have missing values or\nvariables. This investigation compares 3 different data imputation models and\nidentifies their merits by using accuracy measures. Autoencoder Neural\nNetworks, Principal components and Support Vector regression are used for\nprediction and combined with a genetic algorithm to then impute missing\nvariables. The use of PCA improves the overall performance of the autoencoder\nnetwork while the use of support vector regression shows promising potential\nfor future investigation. Accuracies of up to 97.4 % on imputation of some of\nthe variables were achieved.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 16 Sep 2007 18:15:01 GMT"
            }
        ],
        "update_date": "2007-09-18",
        "authors_parsed": [
            [
                "Marivate",
                "Vukosi N.",
                ""
            ],
            [
                "Nelwamodo",
                "Fulufhelo V.",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0709.2618",
        "submitter": "Andreea Picu",
        "authors": "Andreea Picu (CITI, INRIA Rh\\^one-Alpes), Antoine Fraboulet (CITI,\n  INRIA Rh\\^one-Alpes), Eric Fleury (CITI, INRIA Rh\\^one-Alpes)",
        "title": "On Frequency Optimisation for Power Saving in WSNs: Finding Optimum\n  Hardware Timers Frequencies",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "RR-6290",
        "categories": "cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Wireless Sensor Networks research and demand are now in full expansion, since\npeople came to understand these are the key to a large number of issues in\nindustry, commerce, home automation, healthcare, agriculture and environment,\nmonitoring, public safety etc. One of the most challenging research problems in\nsensor networks research is power awareness and power-saving techniques. In\nthis master's thesis, we have studied one particular power-saving technique,\ni.e. frequency scaling. In particular, we analysed the close relationship\nbetween clock frequencies in a microcontroller and several types of constraints\nimposed on these frequencies, e.g. by other components of the microcontroller,\nby protocol specifications, by external factors etc. Among these constraints,\nwe were especially interested in the ones imposed by the timer service and by\nthe serial ports' transmission rates. Our efforts resulted in a microcontroller\nconfiguration management tool which aims at assisting application programmers\nin choosing microcontroller configurations, in function of the particular needs\nand constraints of their application.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Sep 2007 06:25:50 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 27 Feb 2012 14:50:50 GMT"
            }
        ],
        "update_date": "2012-02-28",
        "authors_parsed": [
            [
                "Picu",
                "Andreea",
                "",
                "CITI, INRIA Rh\u00f4ne-Alpes"
            ],
            [
                "Fraboulet",
                "Antoine",
                "",
                "CITI,\n  INRIA Rh\u00f4ne-Alpes"
            ],
            [
                "Fleury",
                "Eric",
                "",
                "CITI, INRIA Rh\u00f4ne-Alpes"
            ]
        ]
    },
    {
        "id": "0709.2635",
        "submitter": "Greg Kohring",
        "authors": "G.A. Kohring and L. Lo Iacono",
        "title": "Non-Blocking Signature of very large SOAP Messages",
        "comments": "13 pages, 5 figures",
        "journal-ref": "in H.-G. Hegering, A. Lehmann, H.J. Ohlbach and C. Scheideler,\n  eds., INFORMATIK 2008, Beherrschbare Systeme - dank Informatik (Koellen Druck\n  & Verlag GmbH, Bonn, 2008).",
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Data transfer and staging services are common components in Grid-based, or\nmore generally, in service-oriented applications. Security mechanisms play a\ncentral role in such services, especially when they are deployed in sensitive\napplication fields like e-health. The adoption of WS-Security and related\nstandards to SOAP-based transfer services is, however, problematic as a\nstraightforward adoption of SOAP with MTOM introduces considerable\ninefficiencies in the signature generation process when large data sets are\ninvolved. This paper proposes a non-blocking, signature generation approach\nenabling a stream-like processing with considerable performance enhancements.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Sep 2007 13:50:42 GMT"
            }
        ],
        "update_date": "2009-04-01",
        "authors_parsed": [
            [
                "Kohring",
                "G. A.",
                ""
            ],
            [
                "Iacono",
                "L. Lo",
                ""
            ]
        ]
    },
    {
        "id": "0709.2935",
        "submitter": "Eberhard H.-A. Gerbracht",
        "authors": "Eberhard H.-A. Gerbracht",
        "title": "An Extension to an Algebraic Method for Linear Time-Invariant System and\n  Network Theory: The full AC-Calculus",
        "comments": "V1: documentclass IEEEtran, 6 pages, 1 figure. Re-release of the\n  printed version, with some minor typographical errors corrected",
        "journal-ref": "Proceedings of the Fifth International Workshop on Symbolic\n  Methods and Applications in Circuit Design, SMACD '98, Kaiserslautern,\n  October 8-9, 1998; The SMACD Committee (Ed.); pp. 134-139; ISSN 1029-9505",
        "doi": null,
        "report-no": null,
        "categories": "math.CA cs.SC",
        "license": null,
        "abstract": "  Being inspired by phasor analysis in linear circuit theory, and its algebraic\ncounterpart - the AC-(operational)-calculus for sinusoids developed by W.\nMarten and W. Mathis - we define a complex structure on several spaces of\nreal-valued elementary functions. This is used to algebraize inhomogeneous\nlinear ordinary differential equations with inhomogenities stemming from these\nspaces. Thus we deduce an effective method to calculate particular solutions of\nthese ODEs in a purely algebraic way.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Sep 2007 00:17:40 GMT"
            }
        ],
        "update_date": "2007-09-20",
        "authors_parsed": [
            [
                "Gerbracht",
                "Eberhard H. -A.",
                ""
            ]
        ]
    },
    {
        "id": "0709.3013",
        "submitter": "Patrick Heas",
        "authors": "Patrick H\\'eas (IRISA), Mihai Datcu (DLR, Oberpfaffenhofen)",
        "title": "Supervised learning on graphs of spatio-temporal similarity in satellite\n  image sequences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  High resolution satellite image sequences are multidimensional signals\ncomposed of spatio-temporal patterns associated to numerous and various\nphenomena. Bayesian methods have been previously proposed in (Heas and Datcu,\n2005) to code the information contained in satellite image sequences in a graph\nrepresentation using Bayesian methods. Based on such a representation, this\npaper further presents a supervised learning methodology of semantics\nassociated to spatio-temporal patterns occurring in satellite image sequences.\nIt enables the recognition and the probabilistic retrieval of similar events.\nIndeed, graphs are attached to statistical models for spatio-temporal\nprocesses, which at their turn describe physical changes in the observed scene.\nTherefore, we adjust a parametric model evaluating similarity types between\ngraph patterns in order to represent user-specific semantics attached to\nspatio-temporal phenomena. The learning step is performed by the incremental\ndefinition of similarity types via user-provided spatio-temporal pattern\nexamples attached to positive or/and negative semantics. From these examples,\nprobabilities are inferred using a Bayesian network and a Dirichlet model. This\nenables to links user interest to a specific similarity model between graph\npatterns. According to the current state of learning, semantic posterior\nprobabilities are updated for all possible graph patterns so that similar\nspatio-temporal phenomena can be recognized and retrieved from the image\nsequence. Few experiments performed on a multi-spectral SPOT image sequence\nillustrate the proposed spatio-temporal recognition method.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Sep 2007 13:18:18 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 20 Sep 2007 07:59:26 GMT"
            }
        ],
        "update_date": "2007-09-20",
        "authors_parsed": [
            [
                "H\u00e9as",
                "Patrick",
                "",
                "IRISA"
            ],
            [
                "Datcu",
                "Mihai",
                "",
                "DLR, Oberpfaffenhofen"
            ]
        ]
    },
    {
        "id": "0709.3034",
        "submitter": "Anastasia Analyti",
        "authors": "Carlo Meghini, Yannis Tzitzikas, Anastasia Analyti",
        "title": "Query Evaluation in P2P Systems of Taxonomy-based Sources: Algorithms,\n  Complexity, and Optimizations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.DC cs.DS cs.LO",
        "license": null,
        "abstract": "  In this study, we address the problem of answering queries over a\npeer-to-peer system of taxonomy-based sources. A taxonomy states subsumption\nrelationships between negation-free DNF formulas on terms and negation-free\nconjunctions of terms. To the end of laying the foundations of our study, we\nfirst consider the centralized case, deriving the complexity of the decision\nproblem and of query evaluation. We conclude by presenting an algorithm that is\nefficient in data complexity and is based on hypergraphs. More expressive forms\nof taxonomies are also investigated, which however lead to intractability. We\nthen move to the distributed case, and introduce a logical model of a network\nof taxonomy-based sources. On such network, a distributed version of the\ncentralized algorithm is then presented, based on a message passing paradigm,\nand its correctness is proved. We finally discuss optimization issues, and\nrelate our work to the literature.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Sep 2007 15:10:05 GMT"
            }
        ],
        "update_date": "2007-09-20",
        "authors_parsed": [
            [
                "Meghini",
                "Carlo",
                ""
            ],
            [
                "Tzitzikas",
                "Yannis",
                ""
            ],
            [
                "Analyti",
                "Anastasia",
                ""
            ]
        ]
    },
    {
        "id": "0709.3034",
        "submitter": "Anastasia Analyti",
        "authors": "Carlo Meghini, Yannis Tzitzikas, Anastasia Analyti",
        "title": "Query Evaluation in P2P Systems of Taxonomy-based Sources: Algorithms,\n  Complexity, and Optimizations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.DC cs.DS cs.LO",
        "license": null,
        "abstract": "  In this study, we address the problem of answering queries over a\npeer-to-peer system of taxonomy-based sources. A taxonomy states subsumption\nrelationships between negation-free DNF formulas on terms and negation-free\nconjunctions of terms. To the end of laying the foundations of our study, we\nfirst consider the centralized case, deriving the complexity of the decision\nproblem and of query evaluation. We conclude by presenting an algorithm that is\nefficient in data complexity and is based on hypergraphs. More expressive forms\nof taxonomies are also investigated, which however lead to intractability. We\nthen move to the distributed case, and introduce a logical model of a network\nof taxonomy-based sources. On such network, a distributed version of the\ncentralized algorithm is then presented, based on a message passing paradigm,\nand its correctness is proved. We finally discuss optimization issues, and\nrelate our work to the literature.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Sep 2007 15:10:05 GMT"
            }
        ],
        "update_date": "2007-09-20",
        "authors_parsed": [
            [
                "Meghini",
                "Carlo",
                ""
            ],
            [
                "Tzitzikas",
                "Yannis",
                ""
            ],
            [
                "Analyti",
                "Anastasia",
                ""
            ]
        ]
    },
    {
        "id": "0709.3094",
        "submitter": "Sergey Dorogovtsev",
        "authors": "S. N. Dorogovtsev, P. L. Krapivsky, J. F. F. Mendes",
        "title": "Transition from small to large world in growing networks",
        "comments": "5 pages",
        "journal-ref": "EPL 81, 30004 (2008)",
        "doi": "10.1209/0295-5075/81/30004",
        "report-no": null,
        "categories": "cond-mat.stat-mech cs.NI math-ph math.MP physics.soc-ph",
        "license": null,
        "abstract": "  We examine the global organization of growing networks in which a new vertex\nis attached to already existing ones with a probability depending on their age.\nWe find that the network is infinite- or finite-dimensional depending on\nwhether the attachment probability decays slower or faster than $(age)^{-1}$.\nThe network becomes one-dimensional when the attachment probability decays\nfaster than $(age)^{-2}$. We describe structural characteristics of these\nphases and transitions between them.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Sep 2007 19:45:18 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 27 Sep 2007 06:57:37 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 23 Nov 2007 16:58:01 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Dorogovtsev",
                "S. N.",
                ""
            ],
            [
                "Krapivsky",
                "P. L.",
                ""
            ],
            [
                "Mendes",
                "J. F. F.",
                ""
            ]
        ]
    },
    {
        "id": "0709.3334",
        "submitter": "Yong Wang",
        "authors": "Yong Wang",
        "title": "Mistake Analyses on Proof about Perfect Secrecy of One-time-pad",
        "comments": "This paper has been withdrawn",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  This paper has been withdrawn\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Sep 2007 02:31:45 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 18 May 2011 01:19:14 GMT"
            }
        ],
        "update_date": "2011-05-19",
        "authors_parsed": [
            [
                "Wang",
                "Yong",
                ""
            ]
        ]
    },
    {
        "id": "0709.3420",
        "submitter": "Marc Barthelemy",
        "authors": "Eric D. Kolaczyk, David B. Chua, Marc Barthelemy",
        "title": "Co-Betweenness: A Pairwise Notion of Centrality",
        "comments": "9 pages, 9 figures",
        "journal-ref": "Social Networks 31 (2009), pp. 190-203",
        "doi": null,
        "report-no": null,
        "categories": "cs.NI physics.soc-ph",
        "license": null,
        "abstract": "  Betweenness centrality is a metric that seeks to quantify a sense of the\nimportance of a vertex in a network graph in terms of its \"control\" on the\ndistribution of information along geodesic paths throughout that network. This\nquantity however does not capture how different vertices participate together\nin such control. In order to allow for the uncovering of finer details in this\nregard, we introduce here an extension of betweenness centrality to pairs of\nvertices, which we term co-betweenness, that provides the basis for quantifying\nvarious analogous pairwise notions of importance and control. More\nspecifically, we motivate and define a precise notion of co-betweenness, we\npresent an efficient algorithm for its computation, extending the algorithm of\nBrandes in a natural manner, and we illustrate the utilization of this\nco-betweenness on a handful of different communication networks. From these\nreal-world examples, we show that the co-betweenness allows one to identify\ncertain vertices which are not the most central vertices but which,\nnevertheless, act as important actors in the relaying and dispatching of\ninformation in the network.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Sep 2007 12:20:05 GMT"
            }
        ],
        "update_date": "2009-08-28",
        "authors_parsed": [
            [
                "Kolaczyk",
                "Eric D.",
                ""
            ],
            [
                "Chua",
                "David B.",
                ""
            ],
            [
                "Barthelemy",
                "Marc",
                ""
            ]
        ]
    },
    {
        "id": "0709.3427",
        "submitter": "Fabrice Rossi",
        "authors": "Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis), Amaury\n  Lendasse (CIS), Damien Fran\\c{c}ois (CESAME), Vincent Wertz (CESAME), Michel\n  Verleysen (DICE - MLG)",
        "title": "Mutual information for the selection of relevant variables in\n  spectrometric nonlinear modelling",
        "comments": null,
        "journal-ref": "Chemometrics and Intelligent Laboratory Systems / I Mathematical\n  Background Chemometrics Intell Lab Syst 80, 2 (2006) 215-226",
        "doi": "10.1016/j.chemolab.2005.06.010",
        "report-no": null,
        "categories": "cs.LG cs.NE stat.AP",
        "license": null,
        "abstract": "  Data from spectrophotometers form vectors of a large number of exploitable\nvariables. Building quantitative models using these variables most often\nrequires using a smaller set of variables than the initial one. Indeed, a too\nlarge number of input variables to a model results in a too large number of\nparameters, leading to overfitting and poor generalization abilities. In this\npaper, we suggest the use of the mutual information measure to select variables\nfrom the initial set. The mutual information measures the information content\nin input variables with respect to the model output, without making any\nassumption on the model that will be used; it is thus suitable for nonlinear\nmodelling. In addition, it leads to the selection of variables among the\ninitial set, and not to linear or nonlinear combinations of them. Without\ndecreasing the model performances compared to other variable projection\nmethods, it allows therefore a greater interpretability of the results.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Sep 2007 12:49:47 GMT"
            }
        ],
        "update_date": "2007-09-26",
        "authors_parsed": [
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis"
            ],
            [
                "Lendasse",
                "Amaury",
                "",
                "CIS"
            ],
            [
                "Fran\u00e7ois",
                "Damien",
                "",
                "CESAME"
            ],
            [
                "Wertz",
                "Vincent",
                "",
                "CESAME"
            ],
            [
                "Verleysen",
                "Michel",
                "",
                "DICE - MLG"
            ]
        ]
    },
    {
        "id": "0709.3427",
        "submitter": "Fabrice Rossi",
        "authors": "Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis), Amaury\n  Lendasse (CIS), Damien Fran\\c{c}ois (CESAME), Vincent Wertz (CESAME), Michel\n  Verleysen (DICE - MLG)",
        "title": "Mutual information for the selection of relevant variables in\n  spectrometric nonlinear modelling",
        "comments": null,
        "journal-ref": "Chemometrics and Intelligent Laboratory Systems / I Mathematical\n  Background Chemometrics Intell Lab Syst 80, 2 (2006) 215-226",
        "doi": "10.1016/j.chemolab.2005.06.010",
        "report-no": null,
        "categories": "cs.LG cs.NE stat.AP",
        "license": null,
        "abstract": "  Data from spectrophotometers form vectors of a large number of exploitable\nvariables. Building quantitative models using these variables most often\nrequires using a smaller set of variables than the initial one. Indeed, a too\nlarge number of input variables to a model results in a too large number of\nparameters, leading to overfitting and poor generalization abilities. In this\npaper, we suggest the use of the mutual information measure to select variables\nfrom the initial set. The mutual information measures the information content\nin input variables with respect to the model output, without making any\nassumption on the model that will be used; it is thus suitable for nonlinear\nmodelling. In addition, it leads to the selection of variables among the\ninitial set, and not to linear or nonlinear combinations of them. Without\ndecreasing the model performances compared to other variable projection\nmethods, it allows therefore a greater interpretability of the results.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Sep 2007 12:49:47 GMT"
            }
        ],
        "update_date": "2007-09-26",
        "authors_parsed": [
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis"
            ],
            [
                "Lendasse",
                "Amaury",
                "",
                "CIS"
            ],
            [
                "Fran\u00e7ois",
                "Damien",
                "",
                "CESAME"
            ],
            [
                "Wertz",
                "Vincent",
                "",
                "CESAME"
            ],
            [
                "Verleysen",
                "Michel",
                "",
                "DICE - MLG"
            ]
        ]
    },
    {
        "id": "0709.3461",
        "submitter": "Fabrice Rossi",
        "authors": "Brieuc Conan-Guez (LITA), Fabrice Rossi (INRIA Rocquencourt / INRIA\n  Sophia Antipolis), A\\\"icha El Golli (INRIA Rocquencourt / INRIA Sophia\n  Antipolis)",
        "title": "Fast Algorithm and Implementation of Dissimilarity Self-Organizing Maps",
        "comments": null,
        "journal-ref": "Neural Networks 19, 6-7 (2006) 855-863",
        "doi": "10.1016/j.neunet.2006.05.002",
        "report-no": null,
        "categories": "cs.NE cs.LG",
        "license": null,
        "abstract": "  In many real world applications, data cannot be accurately represented by\nvectors. In those situations, one possible solution is to rely on dissimilarity\nmeasures that enable sensible comparison between observations. Kohonen's\nSelf-Organizing Map (SOM) has been adapted to data described only through their\ndissimilarity matrix. This algorithm provides both non linear projection and\nclustering of non vector data. Unfortunately, the algorithm suffers from a high\ncost that makes it quite difficult to use with voluminous data sets. In this\npaper, we propose a new algorithm that provides an important reduction of the\ntheoretical cost of the dissimilarity SOM without changing its outcome (the\nresults are exactly the same as the ones obtained with the original algorithm).\nMoreover, we introduce implementation methods that result in very short running\ntimes. Improvements deduced from the theoretical cost model are validated on\nsimulated and real world data (a word list clustering problem). We also\ndemonstrate that the proposed implementation methods reduce by a factor up to 3\nthe running time of the fast algorithm over a standard implementation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Sep 2007 15:20:07 GMT"
            }
        ],
        "update_date": "2007-09-24",
        "authors_parsed": [
            [
                "Conan-Guez",
                "Brieuc",
                "",
                "LITA"
            ],
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt / INRIA\n  Sophia Antipolis"
            ],
            [
                "Golli",
                "A\u00efcha El",
                "",
                "INRIA Rocquencourt / INRIA Sophia\n  Antipolis"
            ]
        ]
    },
    {
        "id": "0709.3461",
        "submitter": "Fabrice Rossi",
        "authors": "Brieuc Conan-Guez (LITA), Fabrice Rossi (INRIA Rocquencourt / INRIA\n  Sophia Antipolis), A\\\"icha El Golli (INRIA Rocquencourt / INRIA Sophia\n  Antipolis)",
        "title": "Fast Algorithm and Implementation of Dissimilarity Self-Organizing Maps",
        "comments": null,
        "journal-ref": "Neural Networks 19, 6-7 (2006) 855-863",
        "doi": "10.1016/j.neunet.2006.05.002",
        "report-no": null,
        "categories": "cs.NE cs.LG",
        "license": null,
        "abstract": "  In many real world applications, data cannot be accurately represented by\nvectors. In those situations, one possible solution is to rely on dissimilarity\nmeasures that enable sensible comparison between observations. Kohonen's\nSelf-Organizing Map (SOM) has been adapted to data described only through their\ndissimilarity matrix. This algorithm provides both non linear projection and\nclustering of non vector data. Unfortunately, the algorithm suffers from a high\ncost that makes it quite difficult to use with voluminous data sets. In this\npaper, we propose a new algorithm that provides an important reduction of the\ntheoretical cost of the dissimilarity SOM without changing its outcome (the\nresults are exactly the same as the ones obtained with the original algorithm).\nMoreover, we introduce implementation methods that result in very short running\ntimes. Improvements deduced from the theoretical cost model are validated on\nsimulated and real world data (a word list clustering problem). We also\ndemonstrate that the proposed implementation methods reduce by a factor up to 3\nthe running time of the fast algorithm over a standard implementation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Sep 2007 15:20:07 GMT"
            }
        ],
        "update_date": "2007-09-24",
        "authors_parsed": [
            [
                "Conan-Guez",
                "Brieuc",
                "",
                "LITA"
            ],
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt / INRIA\n  Sophia Antipolis"
            ],
            [
                "Golli",
                "A\u00efcha El",
                "",
                "INRIA Rocquencourt / INRIA Sophia\n  Antipolis"
            ]
        ]
    },
    {
        "id": "0709.3553",
        "submitter": "Sergey Andreyev",
        "authors": "Sergey Andreyev",
        "title": "Design of moveable and resizable graphics",
        "comments": "21 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.GR cs.HC",
        "license": null,
        "abstract": "  We are communicating with computers on two different levels. On upper level\nwe have a very flexible system of windows: we can move them, resize, overlap or\nput side by side. At any moment we decide what would be the best view and\nreorganize the whole view easily. Then we start any application, go to the\ninner level, and everything changes. Here we are stripped of all the\nflexibility and can work only inside the scenario, developed by the designer of\nthe program. Interface will allow us to change some tiny details, but in\ngeneral everything is fixed: graphics is neither moveable, nor resizable, and\nthe same with controls. Author designed an extremely powerful mechanism of\nturning graphical objects and controls into moveable and resizable. This can\nnot only significantly improve the existing applications, but this will bring\nthe applications to another level. (To estimate the possible difference, try to\nimagine the Windows system without its flexibility and compare it with the\ncurrent one.) This article explains in details the construction and use of\nmoveable and resizable graphical objects.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 22 Sep 2007 00:21:08 GMT"
            }
        ],
        "update_date": "2007-09-25",
        "authors_parsed": [
            [
                "Andreyev",
                "Sergey",
                ""
            ]
        ]
    },
    {
        "id": "0709.3586",
        "submitter": "Fabrice Rossi",
        "authors": "A\\\"icha El Golli (INRIA Rocquencourt / INRIA Sophia Antipolis),\n  Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis), Brieuc\n  Conan-Guez (LITA), Yves Lechevallier (INRIA Rocquencourt / INRIA Sophia\n  Antipolis)",
        "title": "Une adaptation des cartes auto-organisatrices pour des donn\\'ees\n  d\\'ecrites par un tableau de dissimilarit\\'es",
        "comments": null,
        "journal-ref": "Revue de Statistique Appliqu\\'ee LIV, 3 (2006) 33-64",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.LG",
        "license": null,
        "abstract": "  Many data analysis methods cannot be applied to data that are not represented\nby a fixed number of real values, whereas most of real world observations are\nnot readily available in such a format. Vector based data analysis methods have\ntherefore to be adapted in order to be used with non standard complex data. A\nflexible and general solution for this adaptation is to use a (dis)similarity\nmeasure. Indeed, thanks to expert knowledge on the studied data, it is\ngenerally possible to define a measure that can be used to make pairwise\ncomparison between observations. General data analysis methods are then\nobtained by adapting existing methods to (dis)similarity matrices. In this\narticle, we propose an adaptation of Kohonen's Self Organizing Map (SOM) to\n(dis)similarity data. The proposed algorithm is an adapted version of the\nvector based batch SOM. The method is validated on real world data: we provide\nan analysis of the usage patterns of the web site of the Institut National de\nRecherche en Informatique et Automatique, constructed thanks to web log mining\nmethod.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 22 Sep 2007 15:53:54 GMT"
            }
        ],
        "update_date": "2007-09-25",
        "authors_parsed": [
            [
                "Golli",
                "A\u00efcha El",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis"
            ],
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis"
            ],
            [
                "Conan-Guez",
                "Brieuc",
                "",
                "LITA"
            ],
            [
                "Lechevallier",
                "Yves",
                "",
                "INRIA Rocquencourt / INRIA Sophia\n  Antipolis"
            ]
        ]
    },
    {
        "id": "0709.3586",
        "submitter": "Fabrice Rossi",
        "authors": "A\\\"icha El Golli (INRIA Rocquencourt / INRIA Sophia Antipolis),\n  Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis), Brieuc\n  Conan-Guez (LITA), Yves Lechevallier (INRIA Rocquencourt / INRIA Sophia\n  Antipolis)",
        "title": "Une adaptation des cartes auto-organisatrices pour des donn\\'ees\n  d\\'ecrites par un tableau de dissimilarit\\'es",
        "comments": null,
        "journal-ref": "Revue de Statistique Appliqu\\'ee LIV, 3 (2006) 33-64",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.LG",
        "license": null,
        "abstract": "  Many data analysis methods cannot be applied to data that are not represented\nby a fixed number of real values, whereas most of real world observations are\nnot readily available in such a format. Vector based data analysis methods have\ntherefore to be adapted in order to be used with non standard complex data. A\nflexible and general solution for this adaptation is to use a (dis)similarity\nmeasure. Indeed, thanks to expert knowledge on the studied data, it is\ngenerally possible to define a measure that can be used to make pairwise\ncomparison between observations. General data analysis methods are then\nobtained by adapting existing methods to (dis)similarity matrices. In this\narticle, we propose an adaptation of Kohonen's Self Organizing Map (SOM) to\n(dis)similarity data. The proposed algorithm is an adapted version of the\nvector based batch SOM. The method is validated on real world data: we provide\nan analysis of the usage patterns of the web site of the Institut National de\nRecherche en Informatique et Automatique, constructed thanks to web log mining\nmethod.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 22 Sep 2007 15:53:54 GMT"
            }
        ],
        "update_date": "2007-09-25",
        "authors_parsed": [
            [
                "Golli",
                "A\u00efcha El",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis"
            ],
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis"
            ],
            [
                "Conan-Guez",
                "Brieuc",
                "",
                "LITA"
            ],
            [
                "Lechevallier",
                "Yves",
                "",
                "INRIA Rocquencourt / INRIA Sophia\n  Antipolis"
            ]
        ]
    },
    {
        "id": "0709.3587",
        "submitter": "Fabrice Rossi",
        "authors": "A\\\"icha El Golli (INRIA Rocquencourt / INRIA Sophia Antipolis), Brieuc\n  Conan-Guez (INRIA Rocquencourt / INRIA Sophia Antipolis), Fabrice Rossi\n  (INRIA Rocquencourt / INRIA Sophia Antipolis)",
        "title": "Self-organizing maps and symbolic data",
        "comments": null,
        "journal-ref": "Journal of Symbolic Data Analysis 2, 1 (2004)",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.LG",
        "license": null,
        "abstract": "  In data analysis new forms of complex data have to be considered like for\nexample (symbolic data, functional data, web data, trees, SQL query and\nmultimedia data, ...). In this context classical data analysis for knowledge\ndiscovery based on calculating the center of gravity can not be used because\ninput are not $\\mathbb{R}^p$ vectors. In this paper, we present an application\non real world symbolic data using the self-organizing map. To this end, we\npropose an extension of the self-organizing map that can handle symbolic data.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 22 Sep 2007 15:54:37 GMT"
            }
        ],
        "update_date": "2007-09-25",
        "authors_parsed": [
            [
                "Golli",
                "A\u00efcha El",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis"
            ],
            [
                "Conan-Guez",
                "Brieuc",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis"
            ],
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis"
            ]
        ]
    },
    {
        "id": "0709.3587",
        "submitter": "Fabrice Rossi",
        "authors": "A\\\"icha El Golli (INRIA Rocquencourt / INRIA Sophia Antipolis), Brieuc\n  Conan-Guez (INRIA Rocquencourt / INRIA Sophia Antipolis), Fabrice Rossi\n  (INRIA Rocquencourt / INRIA Sophia Antipolis)",
        "title": "Self-organizing maps and symbolic data",
        "comments": null,
        "journal-ref": "Journal of Symbolic Data Analysis 2, 1 (2004)",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.LG",
        "license": null,
        "abstract": "  In data analysis new forms of complex data have to be considered like for\nexample (symbolic data, functional data, web data, trees, SQL query and\nmultimedia data, ...). In this context classical data analysis for knowledge\ndiscovery based on calculating the center of gravity can not be used because\ninput are not $\\mathbb{R}^p$ vectors. In this paper, we present an application\non real world symbolic data using the self-organizing map. To this end, we\npropose an extension of the self-organizing map that can handle symbolic data.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 22 Sep 2007 15:54:37 GMT"
            }
        ],
        "update_date": "2007-09-25",
        "authors_parsed": [
            [
                "Golli",
                "A\u00efcha El",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis"
            ],
            [
                "Conan-Guez",
                "Brieuc",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis"
            ],
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis"
            ]
        ]
    },
    {
        "id": "0709.3639",
        "submitter": "Fabrice Rossi",
        "authors": "Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis), Damien\n  Fran\\c{c}ois (CESAME), Vincent Wertz (CESAME), Marc Meurens (BNUT), Michel\n  Verleysen (DICE - MLG)",
        "title": "Fast Selection of Spectral Variables with B-Spline Compression",
        "comments": null,
        "journal-ref": "Chemometrics and Intelligent Laboratory Systems / I Mathematical\n  Background Chemometrics Intell Lab Syst 86, 2 (2007) 208-218",
        "doi": "10.1016/j.chemolab.2006.06.007",
        "report-no": null,
        "categories": "cs.LG stat.AP",
        "license": null,
        "abstract": "  The large number of spectral variables in most data sets encountered in\nspectral chemometrics often renders the prediction of a dependent variable\nuneasy. The number of variables hopefully can be reduced, by using either\nprojection techniques or selection methods; the latter allow for the\ninterpretation of the selected variables. Since the optimal approach of testing\nall possible subsets of variables with the prediction model is intractable, an\nincremental selection approach using a nonparametric statistics is a good\noption, as it avoids the computationally intensive use of the model itself. It\nhas two drawbacks however: the number of groups of variables to test is still\nhuge, and colinearities can make the results unstable. To overcome these\nlimitations, this paper presents a method to select groups of spectral\nvariables. It consists in a forward-backward procedure applied to the\ncoefficients of a B-Spline representation of the spectra. The criterion used in\nthe forward-backward procedure is the mutual information, allowing to find\nnonlinear dependencies between variables, on the contrary of the generally used\ncorrelation. The spline representation is used to get interpretability of the\nresults, as groups of consecutive spectral variables will be selected. The\nexperiments conducted on NIR spectra from fescue grass and diesel fuels show\nthat the method provides clearly identified groups of selected variables,\nmaking interpretation easy, while keeping a low computational load. The\nprediction performances obtained using the selected coefficients are higher\nthan those obtained by the same method applied directly to the original\nvariables and similar to those obtained using traditional models, although\nusing significantly less spectral variables.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 23 Sep 2007 14:08:51 GMT"
            }
        ],
        "update_date": "2007-09-26",
        "authors_parsed": [
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis"
            ],
            [
                "Fran\u00e7ois",
                "Damien",
                "",
                "CESAME"
            ],
            [
                "Wertz",
                "Vincent",
                "",
                "CESAME"
            ],
            [
                "Meurens",
                "Marc",
                "",
                "BNUT"
            ],
            [
                "Verleysen",
                "Michel",
                "",
                "DICE - MLG"
            ]
        ]
    },
    {
        "id": "0709.3640",
        "submitter": "Fabrice Rossi",
        "authors": "Damien Fran\\c{c}ois (CESAME), Fabrice Rossi (INRIA Rocquencourt /\n  INRIA Sophia Antipolis), Vincent Wertz (CESAME), Michel Verleysen (DICE -\n  MLG)",
        "title": "Resampling methods for parameter-free and robust feature selection with\n  mutual information",
        "comments": null,
        "journal-ref": "Neurocomputing 70, 7-9 (2007) 1276-1288",
        "doi": "10.1016/j.neucom.2006.11.019",
        "report-no": null,
        "categories": "cs.LG stat.AP",
        "license": null,
        "abstract": "  Combining the mutual information criterion with a forward feature selection\nstrategy offers a good trade-off between optimality of the selected feature\nsubset and computation time. However, it requires to set the parameter(s) of\nthe mutual information estimator and to determine when to halt the forward\nprocedure. These two choices are difficult to make because, as the\ndimensionality of the subset increases, the estimation of the mutual\ninformation becomes less and less reliable. This paper proposes to use\nresampling methods, a K-fold cross-validation and the permutation test, to\naddress both issues. The resampling methods bring information about the\nvariance of the estimator, information which can then be used to automatically\nset the parameter and to calculate a threshold to stop the forward procedure.\nThe procedure is illustrated on a synthetic dataset as well as on real-world\nexamples.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 23 Sep 2007 14:09:28 GMT"
            }
        ],
        "update_date": "2007-09-26",
        "authors_parsed": [
            [
                "Fran\u00e7ois",
                "Damien",
                "",
                "CESAME"
            ],
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt /\n  INRIA Sophia Antipolis"
            ],
            [
                "Wertz",
                "Vincent",
                "",
                "CESAME"
            ],
            [
                "Verleysen",
                "Michel",
                "",
                "DICE -\n  MLG"
            ]
        ]
    },
    {
        "id": "0709.3641",
        "submitter": "Fabrice Rossi",
        "authors": "Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE),\n  Nicolas Delannay (DICE - MLG), Brieuc Conan-Guez (INRIA Rocquencourt / INRIA\n  Sophia Antipolis, CEREMADE), Michel Verleysen (DICE - MLG)",
        "title": "Representation of Functional Data in Neural Networks",
        "comments": "Also available online from:\n  http://www.sciencedirect.com/science/journal/09252312",
        "journal-ref": "Neurocomputing 64 (2005) 183--210",
        "doi": "10.1016/j.neucom.2004.11.012",
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  Functional Data Analysis (FDA) is an extension of traditional data analysis\nto functional data, for example spectra, temporal series, spatio-temporal\nimages, gesture recognition data, etc. Functional data are rarely known in\npractice; usually a regular or irregular sampling is known. For this reason,\nsome processing is needed in order to benefit from the smooth character of\nfunctional data in the analysis methods. This paper shows how to extend the\nRadial-Basis Function Networks (RBFN) and Multi-Layer Perceptron (MLP) models\nto functional data inputs, in particular when the latter are known through\nlists of input-output pairs. Various possibilities for functional processing\nare discussed, including the projection on smooth bases, Functional Principal\nComponent Analysis, functional centering and reduction, and the use of\ndifferential operators. It is shown how to incorporate these functional\nprocessing into the RBFN and MLP models. The functional approach is illustrated\non a benchmark of spectrometric data analysis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 23 Sep 2007 14:10:08 GMT"
            }
        ],
        "update_date": "2007-09-25",
        "authors_parsed": [
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE"
            ],
            [
                "Delannay",
                "Nicolas",
                "",
                "DICE - MLG"
            ],
            [
                "Conan-Guez",
                "Brieuc",
                "",
                "INRIA Rocquencourt / INRIA\n  Sophia Antipolis, CEREMADE"
            ],
            [
                "Verleysen",
                "Michel",
                "",
                "DICE - MLG"
            ]
        ]
    },
    {
        "id": "0709.3642",
        "submitter": "Fabrice Rossi",
        "authors": "Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE),\n  Brieuc Conan-Guez (INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE)",
        "title": "Functional Multi-Layer Perceptron: a Nonlinear Tool for Functional Data\n  Analysis",
        "comments": "http://www.sciencedirect.com/science/journal/08936080",
        "journal-ref": "Neural Networks 18, 1 (2005) 45--60",
        "doi": "10.1016/j.neunet.2004.07.001",
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  In this paper, we study a natural extension of Multi-Layer Perceptrons (MLP)\nto functional inputs. We show that fundamental results for classical MLP can be\nextended to functional MLP. We obtain universal approximation results that show\nthe expressive power of functional MLP is comparable to that of numerical MLP.\nWe obtain consistency results which imply that the estimation of optimal\nparameters for functional MLP is statistically well defined. We finally show on\nsimulated and real world data that the proposed model performs in a very\nsatisfactory way.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 23 Sep 2007 14:10:48 GMT"
            }
        ],
        "update_date": "2007-09-25",
        "authors_parsed": [
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE"
            ],
            [
                "Conan-Guez",
                "Brieuc",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE"
            ]
        ]
    },
    {
        "id": "0709.3689",
        "submitter": "Ming-xue Liao",
        "authors": "Liao Ming-Xue, He Xiao-Xin, Fan Zhi-Hua",
        "title": "Static Deadlock Detection in MPI Synchronization Communication",
        "comments": "accepted by HPC Asia 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  It is very common to use dynamic methods to detect deadlocks in MPI programs\nfor the reason that static methods have some restrictions. To guarantee high\nreliability of some important MPI-based application software, a model of MPI\nsynchronization communication is abstracted and a type of static method is\ndevised to examine deadlocks in such modes. The model has three forms with\ndifferent complexity: sequential model, single-loop model and nested-loop\nmodel. Sequential model is a base for all models. Single-loop model must be\ntreated with a special type of equation group and nested-loop model extends the\nmethods for the other two models. A standard Java-based software framework\noriginated from these methods is constructed for determining whether MPI\nprograms are free from synchronization communication deadlocks. Our practice\nshows the software framework is better than those tools using dynamic methods\nbecause it can dig out all synchronization communication deadlocks before an\nMPI-based program goes into running.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 24 Sep 2007 05:33:29 GMT"
            }
        ],
        "update_date": "2007-09-25",
        "authors_parsed": [
            [
                "Ming-Xue",
                "Liao",
                ""
            ],
            [
                "Xiao-Xin",
                "He",
                ""
            ],
            [
                "Zhi-Hua",
                "Fan",
                ""
            ]
        ]
    },
    {
        "id": "0709.3692",
        "submitter": "Ming-xue Liao",
        "authors": "Ming-xue Liao, Zhi-hua Fan",
        "title": "Deadlock Detection in Basic Models of MPI Synchronization Communication\n  Programs",
        "comments": "accepted by Acta Electronica Sinica (in Chinese)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  A model of MPI synchronization communication programs is presented and its\nthree basic simplified models are also defined. A series of theorems and\nmethods for deciding whether deadlocks will occur among the three models are\ngiven and proved strictly. These theories and methods for simple models'\ndeadlock detection are the necessary base for real MPI program deadlock\ndetection. The methods are based on a static analysis through programs and with\nruntime detection in necessary cases and they are able to determine before\ncompiling whether it will be deadlocked for two of the three basic models. For\nanother model, some deadlock cases can be found before compiling and others at\nruntime. Our theorems can be used to prove the correctness of currently popular\nMPI program deadlock detection algorithms. Our methods may decrease codes that\nthose algorithms need to change to MPI source or profiling interface and may\ndetects deadlocks ahead of program execution, thus the overheads can be reduced\ngreatly.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 24 Sep 2007 06:02:29 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 28 Jan 2008 08:07:23 GMT"
            }
        ],
        "update_date": "2008-01-28",
        "authors_parsed": [
            [
                "Liao",
                "Ming-xue",
                ""
            ],
            [
                "Fan",
                "Zhi-hua",
                ""
            ]
        ]
    },
    {
        "id": "0709.3693",
        "submitter": "Ming-xue Liao",
        "authors": "Liao Ming-Xue, He Xiao-Xin, Fan Zhi-Hua",
        "title": "Algorithm of Static Deadlock Detection in MPI Synchronization\n  Communication Sequential Model",
        "comments": "This paper has been accepted by Computer Engineering (in Chinese)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Detecting deadlocks in MPI synchronization communication programs is very\ndifficult and need building program models. All complex models are based on\nsequential models. The sequential model is mapped into a set of character\nstrings and its deadlock detection problem is translated into an equivalent\nmulti-queue string matching problem. An algorithm is devised and implemented to\nstatically detect deadlocks in sequential models of MPI synchronization\ncommunication programs. The time and space complexity of the algorithm is O(n)\nwhere n is the amount of message in model. The algorithm is better than usual\ncircle-detection methods and can adapt well to dynamic message stream.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 24 Sep 2007 06:06:34 GMT"
            }
        ],
        "update_date": "2007-09-25",
        "authors_parsed": [
            [
                "Ming-Xue",
                "Liao",
                ""
            ],
            [
                "Xiao-Xin",
                "He",
                ""
            ],
            [
                "Zhi-Hua",
                "Fan",
                ""
            ]
        ]
    },
    {
        "id": "0709.3826",
        "submitter": "Gerald Krafft",
        "authors": "Gerald Krafft",
        "title": "Research Paper on Transaction-Oriented Simulation In Ad Hoc Grids",
        "comments": "6 pages, condensed research paper of my thesis with the same title",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  This paper analyses the requirements of performing parallel\ntransaction-oriented simulations with a special focus on the space-parallel\napproach and discrete event simulation synchronisation algorithms that are\nsuitable for transaction-oriented simulation and the target environment of Ad\nHoc Grids. To demonstrate the findings a Java-based parallel\ntransaction-oriented simulator for the simulation language GPSS/H is\nimplemented on the basis of the most promising Shock Resistant Time Warp\nsynchronisation algorithm and using the Grid framework ProActive. The\nvalidation of this parallel simulator shows that the Shock Resistant Time Warp\nalgorithm can successfully reduce the number of rolled back Transaction moves\nbut it also reveals circumstances in which the Shock Resistant Time Warp\nalgorithm can be outperformed by the normal Time Warp algorithm. The conclusion\nof this paper suggests possible improvements to the Shock Resistant Time Warp\nalgorithm to avoid such problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 24 Sep 2007 19:25:27 GMT"
            }
        ],
        "update_date": "2007-09-25",
        "authors_parsed": [
            [
                "Krafft",
                "Gerald",
                ""
            ]
        ]
    },
    {
        "id": "0709.3921",
        "submitter": "Alexandros Dimakis",
        "authors": "Alexandros G. Dimakis, Anand D. Sarwate, Martin J. Wainwright",
        "title": "Geographic Gossip: Efficient Averaging for Sensor Networks",
        "comments": "To appear, IEEE Transactions on Signal Processing",
        "journal-ref": null,
        "doi": "10.1109/TSP.2007.908946",
        "report-no": null,
        "categories": "cs.IT cs.NI math.IT math.PR",
        "license": null,
        "abstract": "  Gossip algorithms for distributed computation are attractive due to their\nsimplicity, distributed nature, and robustness in noisy and uncertain\nenvironments. However, using standard gossip algorithms can lead to a\nsignificant waste in energy by repeatedly recirculating redundant information.\nFor realistic sensor network model topologies like grids and random geometric\ngraphs, the inefficiency of gossip schemes is related to the slow mixing times\nof random walks on the communication graph. We propose and analyze an\nalternative gossiping scheme that exploits geographic information. By utilizing\ngeographic routing combined with a simple resampling method, we demonstrate\nsubstantial gains over previously proposed gossip protocols. For regular graphs\nsuch as the ring or grid, our algorithm improves standard gossip by factors of\n$n$ and $\\sqrt{n}$ respectively. For the more challenging case of random\ngeometric graphs, our algorithm computes the true average to accuracy\n$\\epsilon$ using $O(\\frac{n^{1.5}}{\\sqrt{\\log n}} \\log \\epsilon^{-1})$ radio\ntransmissions, which yields a $\\sqrt{\\frac{n}{\\log n}}$ factor improvement over\nstandard gossip algorithms. We illustrate these theoretical results with\nexperimental comparisons between our algorithm and standard methods as applied\nto various classes of random fields.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Sep 2007 11:15:28 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Dimakis",
                "Alexandros G.",
                ""
            ],
            [
                "Sarwate",
                "Anand D.",
                ""
            ],
            [
                "Wainwright",
                "Martin J.",
                ""
            ]
        ]
    },
    {
        "id": "0709.3965",
        "submitter": "Tshilidzi Marwala",
        "authors": "Greg Hulley and Tshilidzi Marwala",
        "title": "Evolving Classifiers: Methods for Incremental Learning",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI cs.NE",
        "license": null,
        "abstract": "  The ability of a classifier to take on new information and classes by\nevolving the classifier without it having to be fully retrained is known as\nincremental learning. Incremental learning has been successfully applied to\nmany classification problems, where the data is changing and is not all\navailable at once. In this paper there is a comparison between Learn++, which\nis one of the most recent incremental learning algorithms, and the new proposed\nmethod of Incremental Learning Using Genetic Algorithm (ILUGA). Learn++ has\nshown good incremental learning capabilities on benchmark datasets on which the\nnew ILUGA method has been tested. ILUGA has also shown good incremental\nlearning ability using only a few classifiers and does not suffer from\ncatastrophic forgetting. The results obtained for ILUGA on the Optical\nCharacter Recognition (OCR) and Wine datasets are good, with an overall\naccuracy of 93% and 94% respectively showing a 4% improvement over Learn++.MT\nfor the difficult multi-class OCR dataset.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Sep 2007 14:28:32 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 26 Sep 2007 10:37:00 GMT"
            }
        ],
        "update_date": "2007-09-26",
        "authors_parsed": [
            [
                "Hulley",
                "Greg",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0709.3965",
        "submitter": "Tshilidzi Marwala",
        "authors": "Greg Hulley and Tshilidzi Marwala",
        "title": "Evolving Classifiers: Methods for Incremental Learning",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI cs.NE",
        "license": null,
        "abstract": "  The ability of a classifier to take on new information and classes by\nevolving the classifier without it having to be fully retrained is known as\nincremental learning. Incremental learning has been successfully applied to\nmany classification problems, where the data is changing and is not all\navailable at once. In this paper there is a comparison between Learn++, which\nis one of the most recent incremental learning algorithms, and the new proposed\nmethod of Incremental Learning Using Genetic Algorithm (ILUGA). Learn++ has\nshown good incremental learning capabilities on benchmark datasets on which the\nnew ILUGA method has been tested. ILUGA has also shown good incremental\nlearning ability using only a few classifiers and does not suffer from\ncatastrophic forgetting. The results obtained for ILUGA on the Optical\nCharacter Recognition (OCR) and Wine datasets are good, with an overall\naccuracy of 93% and 94% respectively showing a 4% improvement over Learn++.MT\nfor the difficult multi-class OCR dataset.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Sep 2007 14:28:32 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 26 Sep 2007 10:37:00 GMT"
            }
        ],
        "update_date": "2007-09-26",
        "authors_parsed": [
            [
                "Hulley",
                "Greg",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ]
        ]
    },
    {
        "id": "0709.3967",
        "submitter": "Tshilidzi Marwala",
        "authors": "Gidudu Anthony, Hulley Greg and Marwala Tshilidzi",
        "title": "Classification of Images Using Support Vector Machines",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI",
        "license": null,
        "abstract": "  Support Vector Machines (SVMs) are a relatively new supervised classification\ntechnique to the land cover mapping community. They have their roots in\nStatistical Learning Theory and have gained prominence because they are robust,\naccurate and are effective even when using a small training sample. By their\nnature SVMs are essentially binary classifiers, however, they can be adopted to\nhandle the multiple classification tasks common in remote sensing studies. The\ntwo approaches commonly used are the One-Against-One (1A1) and One-Against-All\n(1AA) techniques. In this paper, these approaches are evaluated in as far as\ntheir impact and implication for land cover mapping. The main finding from this\nresearch is that whereas the 1AA technique is more predisposed to yielding\nunclassified and mixed pixels, the resulting classification accuracy is not\nsignificantly different from 1A1 approach. It is the authors conclusions that\nultimately the choice of technique adopted boils down to personal preference\nand the uniqueness of the dataset at hand.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Sep 2007 14:37:40 GMT"
            }
        ],
        "update_date": "2007-09-26",
        "authors_parsed": [
            [
                "Anthony",
                "Gidudu",
                ""
            ],
            [
                "Greg",
                "Hulley",
                ""
            ],
            [
                "Tshilidzi",
                "Marwala",
                ""
            ]
        ]
    },
    {
        "id": "0709.4048",
        "submitter": "P. Oscar Boykin",
        "authors": "P. Oscar Boykin, Jesse S. A. Bridgewater, Joseph S. Kong, Kamen M.\n  Lozev, Behnam A. Rezaei, Vwani P. Roychowdhury",
        "title": "A Symphony Conducted by Brunet",
        "comments": "13 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.NI",
        "license": null,
        "abstract": "  We introduce BruNet, a general P2P software framework which we use to produce\nthe first implementation of Symphony, a 1-D Kleinberg small-world architecture.\nOur framework is designed to easily implement and measure different P2P\nprotocols over different transport layers such as TCP or UDP. This paper\ndiscusses our implementation of the Symphony network, which allows each node to\nkeep $k \\le \\log N$ shortcut connections and to route to any other node with a\nshort average delay of $O(\\frac{1}{k}\\log^2 N)$. %This provides a continuous\ntrade-off between node degree and routing latency. We present experimental\nresults taken from several PlanetLab deployments of size up to 1060 nodes.\nThese succes sful deployments represent some of the largest PlanetLab\ndeployments of P2P overlays found in the literature, and show our\nimplementation's robustness to massive node dynamics in a WAN environment.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Sep 2007 22:09:17 GMT"
            }
        ],
        "update_date": "2007-09-27",
        "authors_parsed": [
            [
                "Boykin",
                "P. Oscar",
                ""
            ],
            [
                "Bridgewater",
                "Jesse S. A.",
                ""
            ],
            [
                "Kong",
                "Joseph S.",
                ""
            ],
            [
                "Lozev",
                "Kamen M.",
                ""
            ],
            [
                "Rezaei",
                "Behnam A.",
                ""
            ],
            [
                "Roychowdhury",
                "Vwani P.",
                ""
            ]
        ]
    },
    {
        "id": "0709.4048",
        "submitter": "P. Oscar Boykin",
        "authors": "P. Oscar Boykin, Jesse S. A. Bridgewater, Joseph S. Kong, Kamen M.\n  Lozev, Behnam A. Rezaei, Vwani P. Roychowdhury",
        "title": "A Symphony Conducted by Brunet",
        "comments": "13 pages, 11 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.NI",
        "license": null,
        "abstract": "  We introduce BruNet, a general P2P software framework which we use to produce\nthe first implementation of Symphony, a 1-D Kleinberg small-world architecture.\nOur framework is designed to easily implement and measure different P2P\nprotocols over different transport layers such as TCP or UDP. This paper\ndiscusses our implementation of the Symphony network, which allows each node to\nkeep $k \\le \\log N$ shortcut connections and to route to any other node with a\nshort average delay of $O(\\frac{1}{k}\\log^2 N)$. %This provides a continuous\ntrade-off between node degree and routing latency. We present experimental\nresults taken from several PlanetLab deployments of size up to 1060 nodes.\nThese succes sful deployments represent some of the largest PlanetLab\ndeployments of P2P overlays found in the literature, and show our\nimplementation's robustness to massive node dynamics in a WAN environment.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Sep 2007 22:09:17 GMT"
            }
        ],
        "update_date": "2007-09-27",
        "authors_parsed": [
            [
                "Boykin",
                "P. Oscar",
                ""
            ],
            [
                "Bridgewater",
                "Jesse S. A.",
                ""
            ],
            [
                "Kong",
                "Joseph S.",
                ""
            ],
            [
                "Lozev",
                "Kamen M.",
                ""
            ],
            [
                "Rezaei",
                "Behnam A.",
                ""
            ],
            [
                "Roychowdhury",
                "Vwani P.",
                ""
            ]
        ]
    },
    {
        "id": "0709.4303",
        "submitter": "Yong Wang",
        "authors": "Yong Wang",
        "title": "Security Analyses of One-time System",
        "comments": "This paper has been withdrawn",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  This paper has been withdrawn\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 27 Sep 2007 03:11:58 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 17 May 2011 15:19:40 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 18 May 2011 01:19:32 GMT"
            }
        ],
        "update_date": "2011-05-19",
        "authors_parsed": [
            [
                "Wang",
                "Yong",
                ""
            ]
        ]
    },
    {
        "id": "0709.4420",
        "submitter": "Yong Wang",
        "authors": "Yong Wang",
        "title": "Confirmation of Shannon's Mistake about Perfect Secrecy of One-time-pad",
        "comments": "This paper has been withdrawn",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  This paper has been withdrawn\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 27 Sep 2007 15:05:52 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 18 May 2011 01:18:50 GMT"
            }
        ],
        "update_date": "2011-05-19",
        "authors_parsed": [
            [
                "Wang",
                "Yong",
                ""
            ]
        ]
    },
    {
        "id": "0709.4464",
        "submitter": "Jesus-Emeterio Navarro-Barrientos",
        "authors": "J.-Emeterio Navarro",
        "title": "Adaptive Investment Strategies For Periodic Environments",
        "comments": "Paper submitted to Advances in Complex Systems (November, 2007) 22\n  pages, 9 figures",
        "journal-ref": "Advances in Complex Systems Vol. 11, No. 5 (2008) 761-787",
        "doi": "10.1142/S0219525908001933",
        "report-no": null,
        "categories": "cs.CE cs.NE",
        "license": null,
        "abstract": "  In this paper, we present an adaptive investment strategy for environments\nwith periodic returns on investment. In our approach, we consider an investment\nmodel where the agent decides at every time step the proportion of wealth to\ninvest in a risky asset, keeping the rest of the budget in a risk-free asset.\nEvery investment is evaluated in the market via a stylized return on investment\nfunction (RoI), which is modeled by a stochastic process with unknown\nperiodicities and levels of noise. For comparison reasons, we present two\nreference strategies which represent the case of agents with zero-knowledge and\ncomplete-knowledge of the dynamics of the returns. We consider also an\ninvestment strategy based on technical analysis to forecast the next return by\nfitting a trend line to previous received returns. To account for the\nperformance of the different strategies, we perform some computer experiments\nto calculate the average budget that can be obtained with them over a certain\nnumber of time steps. To assure for fair comparisons, we first tune the\nparameters of each strategy. Afterwards, we compare the performance of these\nstrategies for RoIs with different periodicities and levels of noise.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 27 Sep 2007 19:04:00 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 28 Nov 2007 16:13:07 GMT"
            }
        ],
        "update_date": "2008-12-01",
        "authors_parsed": [
            [
                "Navarro",
                "J. -Emeterio",
                ""
            ]
        ]
    },
    {
        "id": "0709.4464",
        "submitter": "Jesus-Emeterio Navarro-Barrientos",
        "authors": "J.-Emeterio Navarro",
        "title": "Adaptive Investment Strategies For Periodic Environments",
        "comments": "Paper submitted to Advances in Complex Systems (November, 2007) 22\n  pages, 9 figures",
        "journal-ref": "Advances in Complex Systems Vol. 11, No. 5 (2008) 761-787",
        "doi": "10.1142/S0219525908001933",
        "report-no": null,
        "categories": "cs.CE cs.NE",
        "license": null,
        "abstract": "  In this paper, we present an adaptive investment strategy for environments\nwith periodic returns on investment. In our approach, we consider an investment\nmodel where the agent decides at every time step the proportion of wealth to\ninvest in a risky asset, keeping the rest of the budget in a risk-free asset.\nEvery investment is evaluated in the market via a stylized return on investment\nfunction (RoI), which is modeled by a stochastic process with unknown\nperiodicities and levels of noise. For comparison reasons, we present two\nreference strategies which represent the case of agents with zero-knowledge and\ncomplete-knowledge of the dynamics of the returns. We consider also an\ninvestment strategy based on technical analysis to forecast the next return by\nfitting a trend line to previous received returns. To account for the\nperformance of the different strategies, we perform some computer experiments\nto calculate the average budget that can be obtained with them over a certain\nnumber of time steps. To assure for fair comparisons, we first tune the\nparameters of each strategy. Afterwards, we compare the performance of these\nstrategies for RoIs with different periodicities and levels of noise.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 27 Sep 2007 19:04:00 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 28 Nov 2007 16:13:07 GMT"
            }
        ],
        "update_date": "2008-12-01",
        "authors_parsed": [
            [
                "Navarro",
                "J. -Emeterio",
                ""
            ]
        ]
    },
    {
        "id": "0709.4552",
        "submitter": "Derek Groen",
        "authors": "Derek Groen, Simon Portegies Zwart, Steve McMillan and Jun Makino",
        "title": "Distributed N-body Simulation on the Grid Using Dedicated Hardware",
        "comments": "(in press) New Astronomy, 24 pages, 5 figures",
        "journal-ref": "NewAstron.13:348-358,2008",
        "doi": "10.1016/j.newast.2007.11.004",
        "report-no": null,
        "categories": "astro-ph cs.DC",
        "license": null,
        "abstract": "  We present performance measurements of direct gravitational N -body\nsimulation on the grid, with and without specialized (GRAPE-6) hardware. Our\ninter-continental virtual organization consists of three sites, one in Tokyo,\none in Philadelphia and one in Amsterdam. We run simulations with up to 196608\nparticles for a variety of topologies. In many cases, high performance\nsimulations over the entire planet are dominated by network bandwidth rather\nthan latency. With this global grid of GRAPEs our calculation time remains\ndominated by communication over the entire range of N, which was limited due to\nthe use of three sites. Increasing the number of particles will result in a\nmore efficient execution. Based on these timings we construct and calibrate a\nmodel to predict the performance of our simulation on any grid infrastructure\nwith or without GRAPE. We apply this model to predict the simulation\nperformance on the Netherlands DAS-3 wide area computer. Equipping the DAS-3\nwith GRAPE-6Af hardware would achieve break-even between calculation and\ncommunication at a few million particles, resulting in a compute time of just\nover ten hours for 1 N -body time unit. Key words: high-performance computing,\ngrid, N-body simulation, performance modelling\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 28 Sep 2007 08:48:31 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 5 Nov 2007 09:51:26 GMT"
            }
        ],
        "update_date": "2008-11-26",
        "authors_parsed": [
            [
                "Groen",
                "Derek",
                ""
            ],
            [
                "Zwart",
                "Simon Portegies",
                ""
            ],
            [
                "McMillan",
                "Steve",
                ""
            ],
            [
                "Makino",
                "Jun",
                ""
            ]
        ]
    },
    {
        "id": "0709.4558",
        "submitter": "Jeremy Lee",
        "authors": "Jeremy Lee",
        "title": "Practical Multiwriter Lock-Free Queues for \"Hard Real-Time\" Systems\n  without CAS",
        "comments": "9 Pages, Preprint",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.DC",
        "license": null,
        "abstract": "  FIFO queues with a single reader and writer can be insufficient for \"hard\nreal-time\" systems where interrupt handlers require wait-free guarantees when\nwriting to message queues. We present an algorithm which elegantly and\npractically solves this problem on small processors that are often found in\nembedded systems. The algorithm does not require special CPU instructions (such\nas atomic CAS), and therefore is more robust than many existing methods that\nsuffer the ABA problem associated with swing pointers. The algorithm gives\n\"first-in, almost first-out\" guarantees under pathological interrupt\nconditions, which manifests as arbitrary \"shoving\" among nearly-simultaneous\narrivals at the end of the queue.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 28 Sep 2007 09:04:27 GMT"
            }
        ],
        "update_date": "2007-10-01",
        "authors_parsed": [
            [
                "Lee",
                "Jeremy",
                ""
            ]
        ]
    },
    {
        "id": "0709.4558",
        "submitter": "Jeremy Lee",
        "authors": "Jeremy Lee",
        "title": "Practical Multiwriter Lock-Free Queues for \"Hard Real-Time\" Systems\n  without CAS",
        "comments": "9 Pages, Preprint",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.DC",
        "license": null,
        "abstract": "  FIFO queues with a single reader and writer can be insufficient for \"hard\nreal-time\" systems where interrupt handlers require wait-free guarantees when\nwriting to message queues. We present an algorithm which elegantly and\npractically solves this problem on small processors that are often found in\nembedded systems. The algorithm does not require special CPU instructions (such\nas atomic CAS), and therefore is more robust than many existing methods that\nsuffer the ABA problem associated with swing pointers. The algorithm gives\n\"first-in, almost first-out\" guarantees under pathological interrupt\nconditions, which manifests as arbitrary \"shoving\" among nearly-simultaneous\narrivals at the end of the queue.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 28 Sep 2007 09:04:27 GMT"
            }
        ],
        "update_date": "2007-10-01",
        "authors_parsed": [
            [
                "Lee",
                "Jeremy",
                ""
            ]
        ]
    },
    {
        "id": "0709.4655",
        "submitter": "Jan Van den Bussche",
        "authors": "Jan Van den Bussche",
        "title": "Mining for trees in a graph is NP-complete",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.AI",
        "license": null,
        "abstract": "  Mining for trees in a graph is shown to be NP-complete.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 28 Sep 2007 17:08:39 GMT"
            }
        ],
        "update_date": "2007-10-01",
        "authors_parsed": [
            [
                "Bussche",
                "Jan Van den",
                ""
            ]
        ]
    },
    {
        "id": "0710.0020",
        "submitter": "Moslem Noori",
        "authors": "Moslem Noori, Masoud Ardakani",
        "title": "A Probability Model for Lifetime of Wireless Sensor Networks",
        "comments": "9 Pages, Submitted to INFOCOM 2008",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  Considering a wireless sensor network whose nodes are distributed randomly\nover a given area, a probability model for the network lifetime is provided.\nUsing this model and assuming that packet generation follows a Poisson\ndistribution, an analytical expression for the complementary cumulative density\nfunction (ccdf) of the lifetime is obtained. Using this ccdf, one can\naccurately find the probability that the network achieves a given lifetime. It\nis also shown that when the number of sensors, $N$, is large, with an error\nexponentially decaying with $N$, one can predict whether or not a certain\nlifetime can be achieved. The results of this work are obtained for both\nmulti-hop and single-hop wireless sensor networks and are verified with\ncomputer simulation. The approaches of this paper are shown to be applicable to\nother packet generation models and the effect of the area shape is also\ninvestigated.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 28 Sep 2007 22:12:42 GMT"
            }
        ],
        "update_date": "2007-10-02",
        "authors_parsed": [
            [
                "Noori",
                "Moslem",
                ""
            ],
            [
                "Ardakani",
                "Masoud",
                ""
            ]
        ]
    },
    {
        "id": "0710.0021",
        "submitter": "Maryna Nesterenko dr.",
        "authors": "Maryna Nesterenko, Jiri Patera, Dmytro Zhavrotskyj",
        "title": "New families of cryptographic systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR math.GM",
        "license": null,
        "abstract": "  A symmetric encryption method based on properties of quasicrystals is\nproposed. The advantages of the cipher are strict aperiodicity and everywhere\ndiscontinuous property as well as the speed of computation, simplicity of\nimplementation and a straightforward possibility of extending the method to\nencryption of higher dimensional data.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 28 Sep 2007 22:15:28 GMT"
            }
        ],
        "update_date": "2007-10-02",
        "authors_parsed": [
            [
                "Nesterenko",
                "Maryna",
                ""
            ],
            [
                "Patera",
                "Jiri",
                ""
            ],
            [
                "Zhavrotskyj",
                "Dmytro",
                ""
            ]
        ]
    },
    {
        "id": "0710.0043",
        "submitter": "Julian McAuley",
        "authors": "Julian J. McAuley, Tiberio S. Caetano and Marconi S. Barbosa",
        "title": "Graph rigidity, Cyclic Belief Propagation and Point Pattern Matching",
        "comments": "9 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  A recent paper \\cite{CaeCaeSchBar06} proposed a provably optimal, polynomial\ntime method for performing near-isometric point pattern matching by means of\nexact probabilistic inference in a chordal graphical model. Their fundamental\nresult is that the chordal graph in question is shown to be globally rigid,\nimplying that exact inference provides the same matching solution as exact\ninference in a complete graphical model. This implies that the algorithm is\noptimal when there is no noise in the point patterns. In this paper, we present\na new graph which is also globally rigid but has an advantage over the graph\nproposed in \\cite{CaeCaeSchBar06}: its maximal clique size is smaller,\nrendering inference significantly more efficient. However, our graph is not\nchordal and thus standard Junction Tree algorithms cannot be directly applied.\nNevertheless, we show that loopy belief propagation in such a graph converges\nto the optimal solution. This allows us to retain the optimality guarantee in\nthe noiseless case, while substantially reducing both memory requirements and\nprocessing time. Our experimental results show that the accuracy of the\nproposed solution is indistinguishable from that of \\cite{CaeCaeSchBar06} when\nthere is noise in the point patterns.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 29 Sep 2007 06:19:09 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 3 Oct 2007 06:54:12 GMT"
            }
        ],
        "update_date": "2007-10-03",
        "authors_parsed": [
            [
                "McAuley",
                "Julian J.",
                ""
            ],
            [
                "Caetano",
                "Tiberio S.",
                ""
            ],
            [
                "Barbosa",
                "Marconi S.",
                ""
            ]
        ]
    },
    {
        "id": "0710.0213",
        "submitter": "Marc Schoenauer",
        "authors": "Fei Jiang (INRIA Futurs, INRIA Futurs), Hugues Berry (INRIA Futurs),\n  Marc Schoenauer (INRIA Futurs)",
        "title": "Optimising the topology of complex neural networks",
        "comments": null,
        "journal-ref": "Dans ECCS'07 (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  In this paper, we study instances of complex neural networks, i.e. neural\nnetwo rks with complex topologies. We use Self-Organizing Map neural networks\nwhose n eighbourhood relationships are defined by a complex network, to\nclassify handwr itten digits. We show that topology has a small impact on\nperformance and robus tness to neuron failures, at least at long learning\ntimes. Performance may howe ver be increased (by almost 10%) by artificial\nevolution of the network topo logy. In our experimental conditions, the evolved\nnetworks are more random than their parents, but display a more heterogeneous\ndegree distribution.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 1 Oct 2007 06:51:42 GMT"
            }
        ],
        "update_date": "2007-10-02",
        "authors_parsed": [
            [
                "Jiang",
                "Fei",
                "",
                "INRIA Futurs, INRIA Futurs"
            ],
            [
                "Berry",
                "Hugues",
                "",
                "INRIA Futurs"
            ],
            [
                "Schoenauer",
                "Marc",
                "",
                "INRIA Futurs"
            ]
        ]
    },
    {
        "id": "0710.0243",
        "submitter": "Julian McAuley",
        "authors": "Julian John McAuley, Tiberio S. Caetano",
        "title": "High-Order Nonparametric Belief-Propagation for Fast Image Inpainting",
        "comments": "8 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  In this paper, we use belief-propagation techniques to develop fast\nalgorithms for image inpainting. Unlike traditional gradient-based approaches,\nwhich may require many iterations to converge, our techniques achieve\ncompetitive results after only a few iterations. On the other hand, while\nbelief-propagation techniques are often unable to deal with high-order models\ndue to the explosion in the size of messages, we avoid this problem by\napproximating our high-order prior model using a Gaussian mixture. By using\nsuch an approximation, we are able to inpaint images quickly while at the same\ntime retaining good visual results.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 1 Oct 2007 09:18:36 GMT"
            }
        ],
        "update_date": "2007-10-02",
        "authors_parsed": [
            [
                "McAuley",
                "Julian John",
                ""
            ],
            [
                "Caetano",
                "Tiberio S.",
                ""
            ]
        ]
    },
    {
        "id": "0710.0244",
        "submitter": "Philip Baback Alipour",
        "authors": "Philip B. Alipour",
        "title": "Theoretical Engineering and Satellite Comlink of a PTVD-SHAM System",
        "comments": "50 pages, 10 figures (3 multi-figures), 2 tables. v.1: 1 postulate\n  entailing hypothetical ideas, design and model on future technological\n  advances of PTVD-SHAM. The results of the previous paper [arXiv:0707.1151v6],\n  are extended in order to prove some introductory conjectures in theoretical\n  engineering advanced to architectural analysis",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.AR",
        "license": null,
        "abstract": "  This paper focuses on super helical memory system's design, 'Engineering,\nArchitectural and Satellite Communications' as a theoretical approach of an\ninvention-model to 'store time-data'. The current release entails three\nconcepts: 1- an in-depth theoretical physics engineering of the chip including\nits, 2- architectural concept based on VLSI methods, and 3- the time-data\nversus data-time algorithm. The 'Parallel Time Varying & Data Super-helical\nAccess Memory' (PTVD-SHAM), possesses a waterfall effect in its architecture\ndealing with the process of voltage output-switch into diverse logic and\nquantum states described as 'Boolean logic & image-logic', respectively.\nQuantum dot computational methods are explained by utilizing coiled carbon\nnanotubes (CCNTs) and CNT field effect transistors (CNFETs) in the chip's\narchitecture. Quantum confinement, categorized quantum well substrate, and\nB-field flux involvements are discussed in theory. Multi-access of coherent\nsequences of 'qubit addressing' in any magnitude, gained as pre-defined, here\ne.g., the 'big O notation' asymptotically confined into singularity while\npossessing a magnitude of 'infinity' for the orientation of array displacement.\nGaussian curvature of k<0 versus k'>(k<0) is debated in aim of specifying the\n2D electron gas characteristics, data storage system for defining short and\nlong time cycles for different CCNT diameters where space-time continuum is\nfolded by chance for the particle. Precise pre/post data timing for, e.g.,\nseismic waves before earthquake mantle-reach event occurrence, including time\nvarying self-clocking devices in diverse geographic locations for radar systems\nis illustrated in the Subsections of the paper. The theoretical fabrication\nprocess, electromigration between chip's components is discussed as well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 1 Oct 2007 09:35:30 GMT"
            }
        ],
        "update_date": "2009-12-05",
        "authors_parsed": [
            [
                "Alipour",
                "Philip B.",
                ""
            ]
        ]
    },
    {
        "id": "0710.0262",
        "submitter": "Sebastian Roch",
        "authors": "Elchanan Mossel and Sebastien Roch",
        "title": "Incomplete Lineage Sorting: Consistent Phylogeny Estimation From\n  Multiple Loci",
        "comments": "Added a section on more general distance-based methods",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-bio.PE cs.CE cs.DS math.PR math.ST stat.TH",
        "license": null,
        "abstract": "  We introduce a simple algorithm for reconstructing phylogenies from multiple\ngene trees in the presence of incomplete lineage sorting, that is, when the\ntopology of the gene trees may differ from that of the species tree. We show\nthat our technique is statistically consistent under standard stochastic\nassumptions, that is, it returns the correct tree given sufficiently many\nunlinked loci. We also show that it can tolerate moderate estimation errors.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 1 Oct 2007 11:11:43 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 2 Nov 2007 03:41:04 GMT"
            }
        ],
        "update_date": "2011-09-30",
        "authors_parsed": [
            [
                "Mossel",
                "Elchanan",
                ""
            ],
            [
                "Roch",
                "Sebastien",
                ""
            ]
        ]
    },
    {
        "id": "0710.0270",
        "submitter": "Supriya Krishnamurthy",
        "authors": "Supriya Krishnamurthy, Sameh El-Ansary, Erik Aurell and Seif Haridi",
        "title": "An Analytical Study of a Structured Overlay in the presence of Dynamic\n  Membership",
        "comments": "12 pages, 14 figures, to appear in IEEE/ACM Transactions on\n  Networking",
        "journal-ref": null,
        "doi": "10.1109/TNET.2007.905590",
        "report-no": null,
        "categories": "cs.NI cond-mat.stat-mech cs.DC",
        "license": null,
        "abstract": "  In this paper we present an analytical study of dynamic membership (aka\nchurn) in structured peer-to-peer networks. We use a fluid model approach to\ndescribe steady-state or transient phenomena, and apply it to the Chord system.\nFor any rate of churn and stabilization rates, and any system size, we\naccurately account for the functional form of the probability of network\ndisconnection as well as the fraction of failed or incorrect successor and\nfinger pointers. We show how we can use these quantities to predict both the\nperformance and consistency of lookups under churn. All theoretical predictions\nmatch simulation results. The analysis includes both features that are generic\nto structured overlays deploying a ring as well as Chord-specific details, and\nopens the door to a systematic comparative analysis of, at least, ring-based\nstructured overlay systems under churn.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 1 Oct 2007 12:17:43 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Krishnamurthy",
                "Supriya",
                ""
            ],
            [
                "El-Ansary",
                "Sameh",
                ""
            ],
            [
                "Aurell",
                "Erik",
                ""
            ],
            [
                "Haridi",
                "Seif",
                ""
            ]
        ]
    },
    {
        "id": "0710.0270",
        "submitter": "Supriya Krishnamurthy",
        "authors": "Supriya Krishnamurthy, Sameh El-Ansary, Erik Aurell and Seif Haridi",
        "title": "An Analytical Study of a Structured Overlay in the presence of Dynamic\n  Membership",
        "comments": "12 pages, 14 figures, to appear in IEEE/ACM Transactions on\n  Networking",
        "journal-ref": null,
        "doi": "10.1109/TNET.2007.905590",
        "report-no": null,
        "categories": "cs.NI cond-mat.stat-mech cs.DC",
        "license": null,
        "abstract": "  In this paper we present an analytical study of dynamic membership (aka\nchurn) in structured peer-to-peer networks. We use a fluid model approach to\ndescribe steady-state or transient phenomena, and apply it to the Chord system.\nFor any rate of churn and stabilization rates, and any system size, we\naccurately account for the functional form of the probability of network\ndisconnection as well as the fraction of failed or incorrect successor and\nfinger pointers. We show how we can use these quantities to predict both the\nperformance and consistency of lookups under churn. All theoretical predictions\nmatch simulation results. The analysis includes both features that are generic\nto structured overlays deploying a ring as well as Chord-specific details, and\nopens the door to a systematic comparative analysis of, at least, ring-based\nstructured overlay systems under churn.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 1 Oct 2007 12:17:43 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Krishnamurthy",
                "Supriya",
                ""
            ],
            [
                "El-Ansary",
                "Sameh",
                ""
            ],
            [
                "Aurell",
                "Erik",
                ""
            ],
            [
                "Haridi",
                "Seif",
                ""
            ]
        ]
    },
    {
        "id": "0710.0386",
        "submitter": "Supriya Krishnamurthy",
        "authors": "Supriya Krishnamurthy, Sameh El-Ansary, Erik Aurell and Seif Haridi",
        "title": "Comparing Maintenance Strategies for Overlays",
        "comments": "10 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": "Tech. Report TR-2007-01, Swedish Institute of Computer Science",
        "categories": "cs.NI cond-mat.stat-mech cs.DC",
        "license": null,
        "abstract": "  In this paper, we present an analytical tool for understanding the\nperformance of structured overlay networks under churn based on the\nmaster-equation approach of physics. We motivate and derive an equation for the\naverage number of hops taken by lookups during churn, for the Chord network. We\nanalyse this equation in detail to understand the behaviour with and without\nchurn. We then use this understanding to predict how lookups will scale for\nvarying peer population as well as varying the sizes of the routing tables. We\nthen consider a change in the maintenance algorithm of the overlay, from\nperiodic stabilisation to a reactive one which corrects fingers only when a\nchange is detected. We generalise our earlier analysis to underdstand how the\nreactive strategy compares with the periodic one.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 1 Oct 2007 20:45:04 GMT"
            }
        ],
        "update_date": "2007-10-03",
        "authors_parsed": [
            [
                "Krishnamurthy",
                "Supriya",
                ""
            ],
            [
                "El-Ansary",
                "Sameh",
                ""
            ],
            [
                "Aurell",
                "Erik",
                ""
            ],
            [
                "Haridi",
                "Seif",
                ""
            ]
        ]
    },
    {
        "id": "0710.0386",
        "submitter": "Supriya Krishnamurthy",
        "authors": "Supriya Krishnamurthy, Sameh El-Ansary, Erik Aurell and Seif Haridi",
        "title": "Comparing Maintenance Strategies for Overlays",
        "comments": "10 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": "Tech. Report TR-2007-01, Swedish Institute of Computer Science",
        "categories": "cs.NI cond-mat.stat-mech cs.DC",
        "license": null,
        "abstract": "  In this paper, we present an analytical tool for understanding the\nperformance of structured overlay networks under churn based on the\nmaster-equation approach of physics. We motivate and derive an equation for the\naverage number of hops taken by lookups during churn, for the Chord network. We\nanalyse this equation in detail to understand the behaviour with and without\nchurn. We then use this understanding to predict how lookups will scale for\nvarying peer population as well as varying the sizes of the routing tables. We\nthen consider a change in the maintenance algorithm of the overlay, from\nperiodic stabilisation to a reactive one which corrects fingers only when a\nchange is detected. We generalise our earlier analysis to underdstand how the\nreactive strategy compares with the periodic one.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 1 Oct 2007 20:45:04 GMT"
            }
        ],
        "update_date": "2007-10-03",
        "authors_parsed": [
            [
                "Krishnamurthy",
                "Supriya",
                ""
            ],
            [
                "El-Ansary",
                "Sameh",
                ""
            ],
            [
                "Aurell",
                "Erik",
                ""
            ],
            [
                "Haridi",
                "Seif",
                ""
            ]
        ]
    },
    {
        "id": "0710.0410",
        "submitter": "Philip Baback Alipour",
        "authors": "Philip B. Alipour",
        "title": "The Theory of Unified Relativity for a Biovielectroluminescence\n  Phenomenon via Fly's Visual and Imaging System",
        "comments": "51 pages, 4 figures (2 multi-figures), 4 tables, 3 Appendices, 1\n  Animation clip. This is a personalized report, extension to project license\n  No. TXU001347562. A very concise report is to be published in other journals\n  encompassing the relevant categories on computing and physical sciences",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.CV",
        "license": null,
        "abstract": "  The elucidation upon fly's neuronal patterns as a link to computer graphics\nand memory cards I/O's, is investigated for the phenomenon by propounding a\nunified theory of Einstein's two known relativities. It is conclusive that\nflies could contribute a certain amount of neuromatrices indicating an imagery\nfunction of a visual-computational system into computer graphics and storage\nsystems. The visual system involves the time aspect, whereas flies possess\nfaster pulses compared to humans' visual ability due to the E-field state on an\nactive fly's eye surface. This behaviour can be tested on a dissected fly\nspecimen at its ommatidia. Electro-optical contacts and electrodes are wired\nthrough the flesh forming organic emitter layer to stimulate light emission,\nthereby to a computer circuit. The next step is applying a threshold voltage\nwith secondary voltages to the circuit denoting an array of essential\nelectrodes for bit switch. As a result, circuit's dormant pulses versus active\npulses at the specimen's area are recorded. The outcome matrix possesses a\nconstruction of RGB and time radicals expressing the time problem in\nconsumption, allocating time into computational algorithms, enhancing the\ntechnology far beyond. The obtained formulation generates consumed distance\ncons(x), denoting circuital travel between data source/sink for pixel data and\nbendable wavelengths. Once 'image logic' is in place, incorporating this point\nof graphical acceleration permits one to enhance graphics and optimize\nimmensely central processing, data transmissions between memory and computer\nvisual system. The phenomenon can be mainly used in 360-deg. display/viewing,\n3D scanning techniques, military and medicine, a robust and cheap substitution\nfor e.g. pre-motion pattern analysis, real-time rendering and LCDs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 1 Oct 2007 23:55:50 GMT"
            }
        ],
        "update_date": "2009-12-05",
        "authors_parsed": [
            [
                "Alipour",
                "Philip B.",
                ""
            ]
        ]
    },
    {
        "id": "0710.0410",
        "submitter": "Philip Baback Alipour",
        "authors": "Philip B. Alipour",
        "title": "The Theory of Unified Relativity for a Biovielectroluminescence\n  Phenomenon via Fly's Visual and Imaging System",
        "comments": "51 pages, 4 figures (2 multi-figures), 4 tables, 3 Appendices, 1\n  Animation clip. This is a personalized report, extension to project license\n  No. TXU001347562. A very concise report is to be published in other journals\n  encompassing the relevant categories on computing and physical sciences",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.CV",
        "license": null,
        "abstract": "  The elucidation upon fly's neuronal patterns as a link to computer graphics\nand memory cards I/O's, is investigated for the phenomenon by propounding a\nunified theory of Einstein's two known relativities. It is conclusive that\nflies could contribute a certain amount of neuromatrices indicating an imagery\nfunction of a visual-computational system into computer graphics and storage\nsystems. The visual system involves the time aspect, whereas flies possess\nfaster pulses compared to humans' visual ability due to the E-field state on an\nactive fly's eye surface. This behaviour can be tested on a dissected fly\nspecimen at its ommatidia. Electro-optical contacts and electrodes are wired\nthrough the flesh forming organic emitter layer to stimulate light emission,\nthereby to a computer circuit. The next step is applying a threshold voltage\nwith secondary voltages to the circuit denoting an array of essential\nelectrodes for bit switch. As a result, circuit's dormant pulses versus active\npulses at the specimen's area are recorded. The outcome matrix possesses a\nconstruction of RGB and time radicals expressing the time problem in\nconsumption, allocating time into computational algorithms, enhancing the\ntechnology far beyond. The obtained formulation generates consumed distance\ncons(x), denoting circuital travel between data source/sink for pixel data and\nbendable wavelengths. Once 'image logic' is in place, incorporating this point\nof graphical acceleration permits one to enhance graphics and optimize\nimmensely central processing, data transmissions between memory and computer\nvisual system. The phenomenon can be mainly used in 360-deg. display/viewing,\n3D scanning techniques, military and medicine, a robust and cheap substitution\nfor e.g. pre-motion pattern analysis, real-time rendering and LCDs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 1 Oct 2007 23:55:50 GMT"
            }
        ],
        "update_date": "2009-12-05",
        "authors_parsed": [
            [
                "Alipour",
                "Philip B.",
                ""
            ]
        ]
    },
    {
        "id": "0710.0485",
        "submitter": "Vladimir Vovk",
        "authors": "Vladimir Vovk and Fedor Zhdanov",
        "title": "Prediction with expert advice for the Brier game",
        "comments": "34 pages, 22 figures, 2 tables. The conference version (8 pages) is\n  published in the ICML 2008 Proceedings",
        "journal-ref": "Journal of Machine Learning Research 10 (2009), 2413 - 2440",
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show that the Brier game of prediction is mixable and find the optimal\nlearning rate and substitution function for it. The resulting prediction\nalgorithm is applied to predict results of football and tennis matches. The\ntheoretical performance guarantee turns out to be rather tight on these data\nsets, especially in the case of the more extensive tennis data.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 2 Oct 2007 10:08:41 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 27 Jun 2008 18:45:01 GMT"
            }
        ],
        "update_date": "2009-11-02",
        "authors_parsed": [
            [
                "Vovk",
                "Vladimir",
                ""
            ],
            [
                "Zhdanov",
                "Fedor",
                ""
            ]
        ]
    },
    {
        "id": "0710.0510",
        "submitter": "Jean-Guillaume Dumas",
        "authors": "Jean-Guillaume Dumas (LJK)",
        "title": "Q-adic Transform revisited",
        "comments": "International Symposium on Symbolic and Algebraic Computation 2008,\n  Hagenberg : Autriche (2008)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC",
        "license": null,
        "abstract": "  We present an algorithm to perform a simultaneous modular reduction of\nseveral residues. This algorithm is applied fast modular polynomial\nmultiplication. The idea is to convert the $X$-adic representation of modular\npolynomials, with $X$ an indeterminate, to a $q$-adic representation where $q$\nis an integer larger than the field characteristic. With some control on the\ndifferent involved sizes it is then possible to perform some of the $q$-adic\narithmetic directly with machine integers or floating points. Depending also on\nthe number of performed numerical operations one can then convert back to the\n$q$-adic or $X$-adic representation and eventually mod out high residues. In\nthis note we present a new version of both conversions: more tabulations and a\nway to reduce the number of divisions involved in the process are presented.\nThe polynomial multiplication is then applied to arithmetic in small finite\nfield extensions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 2 Oct 2007 12:02:07 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 26 Oct 2007 18:29:40 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 21 Nov 2007 14:56:34 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 4 Apr 2008 09:23:09 GMT"
            },
            {
                "version": "v5",
                "created": "Mon, 23 Jun 2008 08:19:23 GMT"
            }
        ],
        "update_date": "2008-06-23",
        "authors_parsed": [
            [
                "Dumas",
                "Jean-Guillaume",
                "",
                "LJK"
            ]
        ]
    },
    {
        "id": "0710.0528",
        "submitter": "Francesca Scozzari",
        "authors": "Gianluca Amato and Francesca Scozzari",
        "title": "On the interaction between sharing and linearity",
        "comments": null,
        "journal-ref": "Theory and Practice of Logic Programming, volume 10, issue 01, pp.\n  49-112, 2010",
        "doi": "10.1017/S1471068409990160",
        "report-no": null,
        "categories": "cs.PL cs.LO",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the analysis of logic programs, abstract domains for detecting sharing and\nlinearity information are widely used. Devising abstract unification algorithms\nfor such domains has proved to be rather hard. At the moment, the available\nalgorithms are correct but not optimal, i.e., they cannot fully exploit the\ninformation conveyed by the abstract domains. In this paper, we define a new\n(infinite) domain ShLin-w which can be thought of as a general framework from\nwhich other domains can be easily derived by abstraction. ShLin-w makes the\ninteraction between sharing and linearity explicit. We provide a constructive\ncharacterization of the optimal abstract unification operator on ShLin-w and we\nlift it to two well-known abstractions of ShLin-w. Namely, to the classical\nSharing X Lin abstract domain and to the more precise ShLin-2 abstract domain\nby Andy King. In the case of single binding substitutions, we obtain optimal\nabstract unification algorithms for such domains.\n  To appear in Theory and Practice of Logic Programming (TPLP).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 2 Oct 2007 13:29:28 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 28 Jul 2009 09:35:55 GMT"
            }
        ],
        "update_date": "2012-10-09",
        "authors_parsed": [
            [
                "Amato",
                "Gianluca",
                ""
            ],
            [
                "Scozzari",
                "Francesca",
                ""
            ]
        ]
    },
    {
        "id": "0710.0531",
        "submitter": "Massimiliano Laddomada Ph.D.",
        "authors": "Fred Daneshgaran, Massimiliano Laddomada, Marina Mondin",
        "title": "The Problem of Localization in Networks of Randomly Deployed Nodes:\n  Asymptotic and Finite Analysis, and Thresholds",
        "comments": "Submitted to IEEE transactions on information theory (Submission date\n  October 1, 2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DM cs.IT cs.NI math.IT",
        "license": null,
        "abstract": "  We derive the probability that a randomly chosen NL-node over $S$ gets\nlocalized as a function of a variety of parameters. Then, we derive the\nprobability that the whole network of NL-nodes over $S$ gets localized. In\nconnection with the asymptotic thresholds, we show the presence of asymptotic\nthresholds on the network localization probability in two different scenarios.\nThe first refers to dense networks, which arise when the domain $S$ is bounded\nand the densities of the two kinds of nodes tend to grow unboundedly. The\nsecond kind of thresholds manifest themselves when the considered domain\nincreases but the number of nodes grow in such a way that the L-node density\nremains constant throughout the investigated domain. In this scenario, what\nmatters is the minimum value of the maximum transmission range averaged over\nthe fading process, denoted as $d_{max}$, above which the network of NL-nodes\nalmost surely gets asymptotically localized.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 2 Oct 2007 13:36:20 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Daneshgaran",
                "Fred",
                ""
            ],
            [
                "Laddomada",
                "Massimiliano",
                ""
            ],
            [
                "Mondin",
                "Marina",
                ""
            ]
        ]
    },
    {
        "id": "0710.0550",
        "submitter": "Vladimir Gudkov",
        "authors": "V. Gudkov and V. Montealegre",
        "title": "Community Detection in Complex Networks by Dynamical Simplex Evolution",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1103/PhysRevE.78.016113",
        "report-no": null,
        "categories": "cond-mat.dis-nn cs.NI physics.soc-ph",
        "license": null,
        "abstract": "  We benchmark the dynamical simplex evolution (DSE) method with several of the\ncurrently available algorithms to detect communities in complex networks by\ncomparing the fraction of correctly identified nodes for different levels of\n``fuzziness'' of random networks composed of well defined communities. The\npotential benefits of the DSE method to detect hierarchical sub structures in\ncomplex networks are discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 2 Oct 2007 14:55:35 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Gudkov",
                "V.",
                ""
            ],
            [
                "Montealegre",
                "V.",
                ""
            ]
        ]
    },
    {
        "id": "0710.0658",
        "submitter": "Yi Lu",
        "authors": "Yi Lu, Andrea Montanari and Balaji Prabhakar",
        "title": "Detailed Network Measurements Using Sparse Graph Counters: The Theory",
        "comments": "8 pages. Allerton conference",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.IT math.IT",
        "license": null,
        "abstract": "  Measuring network flow sizes is important for tasks like accounting/billing,\nnetwork forensics and security. Per-flow accounting is considered hard because\nit requires that many counters be updated at a very high speed; however, the\nlarge fast memories needed for storing the counters are prohibitively\nexpensive. Therefore, current approaches aim to obtain approximate flow counts;\nthat is, to detect large elephant flows and then measure their sizes.\n  Recently the authors and their collaborators have developed [1] a novel\nmethod for per-flow traffic measurement that is fast, highly memory efficient\nand accurate. At the core of this method is a novel counter architecture called\n\"counter braids.'' In this paper, we analyze the performance of the counter\nbraid architecture under a Maximum Likelihood (ML) flow size estimation\nalgorithm and show that it is optimal; that is, the number of bits needed to\nstore the size of a flow matches the entropy lower bound. While the ML\nalgorithm is optimal, it is too complex to implement. In [1] we have developed\nan easy-to-implement and efficient message passing algorithm for estimating\nflow sizes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 2 Oct 2007 22:03:28 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 7 Oct 2007 05:42:39 GMT"
            }
        ],
        "update_date": "2007-10-07",
        "authors_parsed": [
            [
                "Lu",
                "Yi",
                ""
            ],
            [
                "Montanari",
                "Andrea",
                ""
            ],
            [
                "Prabhakar",
                "Balaji",
                ""
            ]
        ]
    },
    {
        "id": "0710.0672",
        "submitter": "Valmir Barbosa",
        "authors": "Fabio R. J. Vieira, Valmir C. Barbosa",
        "title": "Optimization of supply diversity for the self-assembly of simple objects\n  in two and three dimensions",
        "comments": "Minor typos corrected",
        "journal-ref": "Natural Computing 10 (2011), 551-581",
        "doi": "10.1007/s11047-010-9209-x",
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  The field of algorithmic self-assembly is concerned with the design and\nanalysis of self-assembly systems from a computational perspective, that is,\nfrom the perspective of mathematical problems whose study may give insight into\nthe natural processes through which elementary objects self-assemble into more\ncomplex ones. One of the main problems of algorithmic self-assembly is the\nminimum tile set problem (MTSP), which asks for a collection of types of\nelementary objects (called tiles) to be found for the self-assembly of an\nobject having a pre-established shape. Such a collection is to be as concise as\npossible, thus minimizing supply diversity, while satisfying a set of stringent\nconstraints having to do with the termination and other properties of the\nself-assembly process from its tile types. We present a study of what we think\nis the first practical approach to MTSP. Our study starts with the introduction\nof an evolutionary heuristic to tackle MTSP and includes results from extensive\nexperimentation with the heuristic on the self-assembly of simple objects in\ntwo and three dimensions. The heuristic we introduce combines classic elements\nfrom the field of evolutionary computation with a problem-specific variant of\nPareto dominance into a multi-objective approach to MTSP.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 3 Oct 2007 18:29:12 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 4 Oct 2007 13:21:09 GMT"
            }
        ],
        "update_date": "2011-03-14",
        "authors_parsed": [
            [
                "Vieira",
                "Fabio R. J.",
                ""
            ],
            [
                "Barbosa",
                "Valmir C.",
                ""
            ]
        ]
    },
    {
        "id": "0710.0736",
        "submitter": "Alessandro Tomasi",
        "authors": "David A Kay (Oxford University Computational Laboratory), Alessandro\n  Tomasi (University of Sussex)",
        "title": "Colour image segmentation by the vector-valued Allen-Cahn phase-field\n  model: a multigrid solution",
        "comments": "17 pages, 9 figures",
        "journal-ref": "IEEE Trans. Im. Proc. 18.10 (2009)",
        "doi": "10.1109/TIP.2009.2026678",
        "report-no": null,
        "categories": "cs.CV cs.NA",
        "license": null,
        "abstract": "  We propose a new method for the numerical solution of a PDE-driven model for\ncolour image segmentation and give numerical examples of the results. The\nmethod combines the vector-valued Allen-Cahn phase field equation with initial\ndata fitting terms. This method is known to be closely related to the\nMumford-Shah problem and the level set segmentation by Chan and Vese. Our\nnumerical solution is performed using a multigrid splitting of a finite element\nspace, thereby producing an efficient and robust method for the segmentation of\nlarge images.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 3 Oct 2007 08:51:44 GMT"
            }
        ],
        "update_date": "2018-06-08",
        "authors_parsed": [
            [
                "Kay",
                "David A",
                "",
                "Oxford University Computational Laboratory"
            ],
            [
                "Tomasi",
                "Alessandro",
                "",
                "University of Sussex"
            ]
        ]
    },
    {
        "id": "0710.0789",
        "submitter": "Ying Jun Zhang Ph.D.",
        "authors": "Ying Jun Zhang, Peng Xuan Zheng, Soung Chang Liew",
        "title": "Wireless Local Area Networks with Multiple-Packet Reception Capability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.NI",
        "license": null,
        "abstract": "  Thanks to its simplicity and cost efficiency, wireless local area network\n(WLAN) enjoys unique advantages in providing high-speed and low-cost wireless\nservices in hot spots and indoor environments. Traditional WLAN\nmedium-access-control (MAC) protocols assume that only one station can transmit\nat a time: simultaneous transmissions of more than one station causes the\ndestruction of all packets involved. By exploiting recent advances in PHY-layer\nmultiuser detection (MUD) techniques, it is possible for a receiver to receive\nmultiple packets simultaneously. This paper argues that such multipacket\nreception (MPR) capability can greatly enhance the capacity of future WLANs. In\naddition, it provides the MAC-layer and PHY-layer designs needed to achieve the\nimproved capacity. First, to demonstrate MUD/MPR as a powerful\ncapacity-enhancement technique, we prove a \"super-linearity\" result, which\nstates that the system throughput per unit cost increases as the MPR capability\nincreases. Second, we show that the commonly deployed binary exponential\nbackoff (BEB) algorithm in today's WLAN MAC may not be optimal in an MPR\nsystem, and that the optimal backoff factor increases with the MPR capability:\nthe number of packets that can be received simultaneously. Third, based on the\nabove insights, we design a joint MAC-PHY layer protocol for an IEEE\n802.11-like WLAN that incorporates advanced PHY-layer blind detection and MUD\ntechniques to implement MPR\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 3 Oct 2007 13:46:01 GMT"
            }
        ],
        "update_date": "2007-10-04",
        "authors_parsed": [
            [
                "Zhang",
                "Ying Jun",
                ""
            ],
            [
                "Zheng",
                "Peng Xuan",
                ""
            ],
            [
                "Liew",
                "Soung Chang",
                ""
            ]
        ]
    },
    {
        "id": "0710.0789",
        "submitter": "Ying Jun Zhang Ph.D.",
        "authors": "Ying Jun Zhang, Peng Xuan Zheng, Soung Chang Liew",
        "title": "Wireless Local Area Networks with Multiple-Packet Reception Capability",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.NI",
        "license": null,
        "abstract": "  Thanks to its simplicity and cost efficiency, wireless local area network\n(WLAN) enjoys unique advantages in providing high-speed and low-cost wireless\nservices in hot spots and indoor environments. Traditional WLAN\nmedium-access-control (MAC) protocols assume that only one station can transmit\nat a time: simultaneous transmissions of more than one station causes the\ndestruction of all packets involved. By exploiting recent advances in PHY-layer\nmultiuser detection (MUD) techniques, it is possible for a receiver to receive\nmultiple packets simultaneously. This paper argues that such multipacket\nreception (MPR) capability can greatly enhance the capacity of future WLANs. In\naddition, it provides the MAC-layer and PHY-layer designs needed to achieve the\nimproved capacity. First, to demonstrate MUD/MPR as a powerful\ncapacity-enhancement technique, we prove a \"super-linearity\" result, which\nstates that the system throughput per unit cost increases as the MPR capability\nincreases. Second, we show that the commonly deployed binary exponential\nbackoff (BEB) algorithm in today's WLAN MAC may not be optimal in an MPR\nsystem, and that the optimal backoff factor increases with the MPR capability:\nthe number of packets that can be received simultaneously. Third, based on the\nabove insights, we design a joint MAC-PHY layer protocol for an IEEE\n802.11-like WLAN that incorporates advanced PHY-layer blind detection and MUD\ntechniques to implement MPR\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 3 Oct 2007 13:46:01 GMT"
            }
        ],
        "update_date": "2007-10-04",
        "authors_parsed": [
            [
                "Zhang",
                "Ying Jun",
                ""
            ],
            [
                "Zheng",
                "Peng Xuan",
                ""
            ],
            [
                "Liew",
                "Soung Chang",
                ""
            ]
        ]
    },
    {
        "id": "0710.0824",
        "submitter": "Norman Danner",
        "authors": "Norman Danner and James S. Royer",
        "title": "Two algorithms in search of a type system",
        "comments": "30 pages. Final version to appear in Theory of Computing Systems",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.PL",
        "license": null,
        "abstract": "  The authors' ATR programming formalism is a version of call-by-value PCF\nunder a complexity-theoretically motivated type system. ATR programs run in\ntype-2 polynomial-time and all standard type-2 basic feasible functionals are\nATR-definable (ATR types are confined to levels 0, 1, and 2). A limitation of\nthe original version of ATR is that the only directly expressible recursions\nare tail-recursions. Here we extend ATR so that a broad range of affine\nrecursions are directly expressible. In particular, the revised ATR can fairly\nnaturally express the classic insertion- and selection-sort algorithms, thus\novercoming a sticking point of most prior implicit-complexity-based formalisms.\nThe paper's main work is in refining the original time-complexity semantics for\nATR to show that these new recursion schemes do not lead out of the realm of\nfeasibility.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 3 Oct 2007 16:04:59 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 18 Apr 2008 19:17:30 GMT"
            }
        ],
        "update_date": "2008-04-18",
        "authors_parsed": [
            [
                "Danner",
                "Norman",
                ""
            ],
            [
                "Royer",
                "James S.",
                ""
            ]
        ]
    },
    {
        "id": "0710.0842",
        "submitter": "Alexis Clay",
        "authors": "Alexis Clay (LIPSI)",
        "title": "Syst\\`emes interactifs sensibles aux \\'emotions : architecture\n  logicielle",
        "comments": "4 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  We propose a software architecture for interactive systems which allows\nintegrating the user's emotion. Emotion can be involved in interaction at\nseveral levels. In our application case - ballet dance - emotions is\nexplicitely manipulated by the interactive system to produce emotion-wise\noutput. Our architecture model to develop emotion-wise applications is based on\nthe PAC-Amodeus model. We add a branch to this model, divided into three\ncomponents: Data capture, analysis and cue extraction, and finally\ninterpretation of those cues. We show the different data flows between this\narchitecture's components depending on the entry point of the emotion branch\nwithin the system. We then illustrate our model by describing our application\ncase: capturing a ballet dancer's movement to extract the emotions he expresses\nand use these emotions to generate graphical content that is displayed on\nstage.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 3 Oct 2007 17:23:59 GMT"
            }
        ],
        "update_date": "2007-10-04",
        "authors_parsed": [
            [
                "Clay",
                "Alexis",
                "",
                "LIPSI"
            ]
        ]
    },
    {
        "id": "0710.0847",
        "submitter": "Alexis Clay",
        "authors": "Alexis Clay (LIPSI), Nadine Couture (LIPSI), Laurence Nigay (CLIPS -\n  IMAG)",
        "title": "Emotion capture based on body postures and movements",
        "comments": "22 pages",
        "journal-ref": "Proceedings of the International Conference on Computing and\n  e-systems 2007 (TIGERA'07), Hammamet : Tunisie (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  In this paper we present a preliminary study for designing interactive\nsystems that are sensible to human emotions based on the body movements. To do\nso, we first review the literature on the various approaches for defining and\ncharacterizing human emotions. After justifying the adopted characterization\nspace for emotions, we then focus on the movement characteristics that must be\ncaptured by the system for being able to recognize the human emotions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 3 Oct 2007 17:36:07 GMT"
            }
        ],
        "update_date": "2007-10-04",
        "authors_parsed": [
            [
                "Clay",
                "Alexis",
                "",
                "LIPSI"
            ],
            [
                "Couture",
                "Nadine",
                "",
                "LIPSI"
            ],
            [
                "Nigay",
                "Laurence",
                "",
                "CLIPS -\n  IMAG"
            ]
        ]
    },
    {
        "id": "0710.0859",
        "submitter": "Publications Loria",
        "authors": "Suzanne Kieffer (INRIA Rocquencourt / INRIA Lorraine - LORIA),\n  No\\\"elle Carbonell (INRIA Rocquencourt / INRIA Lorraine - LORIA)",
        "title": "Assistance orale \\`a la recherche visuelle - \\'etude exp\\'erimentale de\n  l'apport d'indications spatiales \\`a la d\\'etection de cibles",
        "comments": "http://www.hcirn.com/res/period/rihm.php",
        "journal-ref": "Revue d'Interaction Homme-Machine 7, 1 (2006) 30 p",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  This paper describes an experimental study that aims at assessing the actual\ncontribution of voice system messages to visual search efficiency and comfort.\nMessages which include spatial information on the target location are meant to\nsupport search for familiar targets in collections of photographs (30 per\ndisplay). 24 participants carried out 240 visual search tasks in two conditions\ndiffering from each other in initial target presentation only. The isolated\ntarget was presented either simultaneously with an oral message (multimodal\npresentation, MP), or without any message (visual presentation, VP). Averaged\ntarget selection times were thrice longer and errors almost twice more frequent\nin the VP condition than in the MP condition. In addition, the contribution of\nspatial messages to visual search rapidity and accuracy was influenced by\ndisplay layout and task difficulty. Most results are statistically significant.\nBesides, subjective judgments indicate that oral messages were well accepted.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 3 Oct 2007 18:30:07 GMT"
            }
        ],
        "update_date": "2007-10-04",
        "authors_parsed": [
            [
                "Kieffer",
                "Suzanne",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ],
            [
                "Carbonell",
                "No\u00eblle",
                "",
                "INRIA Rocquencourt / INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0710.0865",
        "submitter": "Lifeng Lai",
        "authors": "Lifeng Lai, Hesham El Gamal and H. Vincent Poor",
        "title": "Secrecy Capacity of the Wiretap Channel with Noisy Feedback",
        "comments": "To appear in the Proceedings of the 45th Annual Allerton Conference\n  on Communication, Control and Computing, Monticello, IL, September 26 - 28,\n  2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.CR math.IT",
        "license": null,
        "abstract": "  In this work, the role of noisy feedback in enhancing the secrecy capacity of\nthe wiretap channel is investigated. A model is considered in which the\nfeed-forward and feedback signals share the same noisy channel. More\nspecifically, a discrete memoryless modulo-additive channel with a full-duplex\ndestination node is considered first, and it is shown that a judicious use of\nfeedback increases the perfect secrecy capacity to the capacity of the\nsource-destination channel in the absence of the wiretapper. In the\nachievability scheme, the feedback signal corresponds to a private key, known\nonly to the destination. Then a half-duplex system is considered, for which a\nnovel feedback technique that always achieves a positive perfect secrecy rate\n(even when the source-wiretapper channel is less noisy than the\nsource-destination channel) is proposed. These results hinge on the\nmodulo-additive property of the channel, which is exploited by the destination\nto perform encryption over the channel without revealing its key to the source.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 3 Oct 2007 19:21:08 GMT"
            }
        ],
        "update_date": "2007-10-04",
        "authors_parsed": [
            [
                "Lai",
                "Lifeng",
                ""
            ],
            [
                "Gamal",
                "Hesham El",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0710.1190",
        "submitter": "Nitin Salodkar",
        "authors": "Nitin Salodkar, Abhay Karandikar and Vivek S. Borkar",
        "title": "Power Efficient Scheduling under Delay Constraints over Multi-user\n  Wireless Channels",
        "comments": "14 pages, 14 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.MA",
        "license": null,
        "abstract": "  In this paper, we consider the problem of power efficient uplink scheduling\nin a Time Division Multiple Access (TDMA) system over a fading wireless\nchannel. The objective is to minimize the power expenditure of each user\nsubject to satisfying individual user delay. We make the practical assumption\nthat the system statistics are unknown, i.e., the probability distributions of\nthe user arrivals and channel states are unknown. The problem has the structure\nof a Constrained Markov Decision Problem (CMDP). Determining an optimal policy\nunder for the CMDP faces the problems of state space explosion and unknown\nsystem statistics. To tackle the problem of state space explosion, we suggest\ndetermining the transmission rate of a particular user in each slot based on\nits channel condition and buffer occupancy only. The rate allocation algorithm\nfor a particular user is a learning algorithm that learns about the buffer\noccupancy and channel states of that user during system execution and thus\naddresses the issue of unknown system statistics. Once the rate of each user is\ndetermined, the proposed algorithm schedules the user with the best rate. Our\nsimulations within an IEEE 802.16 system demonstrate that the algorithm is\nindeed able to satisfy the user specified delay constraints. We compare the\nperformance of our algorithm with the well known M-LWDF algorithm. Moreover, we\ndemonstrate that the power expended by the users under our algorithm is quite\nlow.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 5 Oct 2007 12:51:46 GMT"
            }
        ],
        "update_date": "2007-10-08",
        "authors_parsed": [
            [
                "Salodkar",
                "Nitin",
                ""
            ],
            [
                "Karandikar",
                "Abhay",
                ""
            ],
            [
                "Borkar",
                "Vivek S.",
                ""
            ]
        ]
    },
    {
        "id": "0710.1203",
        "submitter": "Dimitri Petritis",
        "authors": "Thomas Sierocinski (IRMAR), Anthony Le B\\'echec, Nathalie Th\\'eret,\n  Dimitri Petritis (IRMAR)",
        "title": "Semantic distillation: a method for clustering objects by their\n  contextual specificity",
        "comments": "Accepted for publication in Studies in Computational Intelligence,\n  Springer-Verlag",
        "journal-ref": null,
        "doi": null,
        "report-no": "2007-58",
        "categories": "math.PR cs.DB math.ST q-bio.QM stat.ML stat.TH",
        "license": null,
        "abstract": "  Techniques for data-mining, latent semantic analysis, contextual search of\ndatabases, etc. have long ago been developed by computer scientists working on\ninformation retrieval (IR). Experimental scientists, from all disciplines,\nhaving to analyse large collections of raw experimental data (astronomical,\nphysical, biological, etc.) have developed powerful methods for their\nstatistical analysis and for clustering, categorising, and classifying objects.\nFinally, physicists have developed a theory of quantum measurement, unifying\nthe logical, algebraic, and probabilistic aspects of queries into a single\nformalism. The purpose of this paper is twofold: first to show that when\nformulated at an abstract level, problems from IR, from statistical data\nanalysis, and from physical measurement theories are very similar and hence can\nprofitably be cross-fertilised, and, secondly, to propose a novel method of\nfuzzy hierarchical clustering, termed \\textit{semantic distillation} --\nstrongly inspired from the theory of quantum measurement --, we developed to\nanalyse raw data coming from various types of experiments on DNA arrays. We\nillustrate the method by analysing DNA arrays experiments and clustering the\ngenes of the array according to their specificity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 5 Oct 2007 12:30:43 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 6 Oct 2007 09:57:46 GMT"
            }
        ],
        "update_date": "2007-10-09",
        "authors_parsed": [
            [
                "Sierocinski",
                "Thomas",
                "",
                "IRMAR"
            ],
            [
                "B\u00e9chec",
                "Anthony Le",
                "",
                "IRMAR"
            ],
            [
                "Th\u00e9ret",
                "Nathalie",
                "",
                "IRMAR"
            ],
            [
                "Petritis",
                "Dimitri",
                "",
                "IRMAR"
            ]
        ]
    },
    {
        "id": "0710.1385",
        "submitter": "Lifeng Lai",
        "authors": "Lifeng Lai, Hesham El Gamal, Hai Jiang and H. Vincent Poor",
        "title": "Cognitive Medium Access: Exploration, Exploitation and Competition",
        "comments": "Submitted to IEEE/ACM Trans. on Networking, 14 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.NI math.IT",
        "license": null,
        "abstract": "  This paper establishes the equivalence between cognitive medium access and\nthe competitive multi-armed bandit problem. First, the scenario in which a\nsingle cognitive user wishes to opportunistically exploit the availability of\nempty frequency bands in the spectrum with multiple bands is considered. In\nthis scenario, the availability probability of each channel is unknown to the\ncognitive user a priori. Hence efficient medium access strategies must strike a\nbalance between exploring the availability of other free channels and\nexploiting the opportunities identified thus far. By adopting a Bayesian\napproach for this classical bandit problem, the optimal medium access strategy\nis derived and its underlying recursive structure is illustrated via examples.\nTo avoid the prohibitive computational complexity of the optimal strategy, a\nlow complexity asymptotically optimal strategy is developed. The proposed\nstrategy does not require any prior statistical knowledge about the traffic\npattern on the different channels. Next, the multi-cognitive user scenario is\nconsidered and low complexity medium access protocols, which strike the optimal\nbalance between exploration and exploitation in such competitive environments,\nare developed. Finally, this formalism is extended to the case in which each\ncognitive user is capable of sensing and using multiple channels\nsimultaneously.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 6 Oct 2007 18:24:33 GMT"
            }
        ],
        "update_date": "2007-10-09",
        "authors_parsed": [
            [
                "Lai",
                "Lifeng",
                ""
            ],
            [
                "Gamal",
                "Hesham El",
                ""
            ],
            [
                "Jiang",
                "Hai",
                ""
            ],
            [
                "Poor",
                "H. Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0710.1404",
        "submitter": "M Sabu THAMPI",
        "authors": "Sabu M. Thampi, Ashwin a K",
        "title": "Performance Comparison of Persistence Frameworks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.IR",
        "license": null,
        "abstract": "  One of the essential and most complex components in the software development\nprocess is the database. The complexity increases when the \"orientation\" of the\ninteracting components differs. A persistence framework moves the program data\nin its most natural form to and from a permanent data store, the database. Thus\na persistence framework manages the database and the mapping between the\ndatabase and the objects. This paper compares the performance of two\npersistence frameworks ? Hibernate and iBatis?s SQLMaps using a banking\ndatabase. The performance of both of these tools in single and multi-user\nenvironments are evaluated.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 7 Oct 2007 08:22:53 GMT"
            }
        ],
        "update_date": "2007-10-09",
        "authors_parsed": [
            [
                "Thampi",
                "Sabu M.",
                ""
            ],
            [
                "K",
                "Ashwin a",
                ""
            ]
        ]
    },
    {
        "id": "0710.1436",
        "submitter": "Wojciech Wislicki",
        "authors": "Ryszard Gokieli, Krzysztof Nawrocki, Adam Padee, Dorota Stojda, Karol\n  Wawrzyniak, Wojciech Wislicki",
        "title": "Polish grid infrastructure for science and research",
        "comments": "Proceeedings of IEEE Eurocon 2007, Warsaw, Poland, 9-12 Sep. 2007,\n  p.446",
        "journal-ref": "2007, ISBN 1-4244-0813-X",
        "doi": "10.1109/EURCON.2007.4400477",
        "report-no": null,
        "categories": "cs.DC hep-ex",
        "license": null,
        "abstract": "  Structure, functionality, parameters and organization of the computing Grid\nin Poland is described, mainly from the perspective of high-energy particle\nphysics community, currently its largest consumer and developer. It represents\ndistributed Tier-2 in the worldwide Grid infrastructure. It also provides\nservices and resources for data-intensive applications in other sciences.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 7 Oct 2007 17:45:59 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Gokieli",
                "Ryszard",
                ""
            ],
            [
                "Nawrocki",
                "Krzysztof",
                ""
            ],
            [
                "Padee",
                "Adam",
                ""
            ],
            [
                "Stojda",
                "Dorota",
                ""
            ],
            [
                "Wawrzyniak",
                "Karol",
                ""
            ],
            [
                "Wislicki",
                "Wojciech",
                ""
            ]
        ]
    },
    {
        "id": "0710.1455",
        "submitter": "Mark Burgin",
        "authors": "Mark Burgin",
        "title": "Superrecursive Features of Interactive Computation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.PF",
        "license": null,
        "abstract": "  Functioning and interaction of distributed devices and concurrent algorithms\nare analyzed in the context of the theory of algorithms. Our main concern here\nis how and under what conditions algorithmic interactive devices can be more\npowerful than the recursive models of computation, such as Turing machines.\nRealization of such a higher computing power makes these systems\nsuperrecursive. We find here five sources for superrecursiveness in\ninteraction. In addition, we prove that when all of these sources are excluded,\nthe algorithmic interactive system in question is able to perform only\nrecursive computations. These results provide computer scientists with\nnecessary and sufficient conditions for achieving superrecursiveness by\nalgorithmic interactive devices.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 8 Oct 2007 01:56:20 GMT"
            }
        ],
        "update_date": "2007-10-09",
        "authors_parsed": [
            [
                "Burgin",
                "Mark",
                ""
            ]
        ]
    },
    {
        "id": "0710.1455",
        "submitter": "Mark Burgin",
        "authors": "Mark Burgin",
        "title": "Superrecursive Features of Interactive Computation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.PF",
        "license": null,
        "abstract": "  Functioning and interaction of distributed devices and concurrent algorithms\nare analyzed in the context of the theory of algorithms. Our main concern here\nis how and under what conditions algorithmic interactive devices can be more\npowerful than the recursive models of computation, such as Turing machines.\nRealization of such a higher computing power makes these systems\nsuperrecursive. We find here five sources for superrecursiveness in\ninteraction. In addition, we prove that when all of these sources are excluded,\nthe algorithmic interactive system in question is able to perform only\nrecursive computations. These results provide computer scientists with\nnecessary and sufficient conditions for achieving superrecursiveness by\nalgorithmic interactive devices.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 8 Oct 2007 01:56:20 GMT"
            }
        ],
        "update_date": "2007-10-09",
        "authors_parsed": [
            [
                "Burgin",
                "Mark",
                ""
            ]
        ]
    },
    {
        "id": "0710.1482",
        "submitter": "Amey Karkare",
        "authors": "Amey Karkare, Amitabha Sanyal, Uday Khedker",
        "title": "Heap Reference Analysis for Functional Programs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  Current garbage collectors leave a lot of garbage uncollected because they\nconservatively approximate liveness by reachability from program variables. In\nthis paper, we describe a sequence of static analyses that takes as input a\nprogram written in a first-order, eager functional programming language, and\nfinds at each program point the references to objects that are guaranteed not\nto be used in the future. Such references are made null by a transformation\npass. If this makes the object unreachable, it can be collected by the garbage\ncollector. This causes more garbage to be collected, resulting in fewer\ncollections. Additionally, for those garbage collectors which scavenge live\nobjects, it makes each collection faster.\n  The interesting aspects of our method are both in the identification of the\nanalyses required to solve the problem and the way they are carried out. We\nidentify three different analyses -- liveness, sharing and accessibility. In\nliveness and sharing analyses, the function definitions are analyzed\nindependently of the calling context. This is achieved by using a variable to\nrepresent the unknown context of the function being analyzed and setting up\nconstraints expressing the effect of the function with respect to the variable.\nThe solution of the constraints is a summary of the function that is\nparameterized with respect to a calling context and is used to analyze function\ncalls. As a result we achieve context sensitivity at call sites without\nanalyzing the function multiple number of times.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 8 Oct 2007 08:43:58 GMT"
            }
        ],
        "update_date": "2007-10-09",
        "authors_parsed": [
            [
                "Karkare",
                "Amey",
                ""
            ],
            [
                "Sanyal",
                "Amitabha",
                ""
            ],
            [
                "Khedker",
                "Uday",
                ""
            ]
        ]
    },
    {
        "id": "0710.1482",
        "submitter": "Amey Karkare",
        "authors": "Amey Karkare, Amitabha Sanyal, Uday Khedker",
        "title": "Heap Reference Analysis for Functional Programs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  Current garbage collectors leave a lot of garbage uncollected because they\nconservatively approximate liveness by reachability from program variables. In\nthis paper, we describe a sequence of static analyses that takes as input a\nprogram written in a first-order, eager functional programming language, and\nfinds at each program point the references to objects that are guaranteed not\nto be used in the future. Such references are made null by a transformation\npass. If this makes the object unreachable, it can be collected by the garbage\ncollector. This causes more garbage to be collected, resulting in fewer\ncollections. Additionally, for those garbage collectors which scavenge live\nobjects, it makes each collection faster.\n  The interesting aspects of our method are both in the identification of the\nanalyses required to solve the problem and the way they are carried out. We\nidentify three different analyses -- liveness, sharing and accessibility. In\nliveness and sharing analyses, the function definitions are analyzed\nindependently of the calling context. This is achieved by using a variable to\nrepresent the unknown context of the function being analyzed and setting up\nconstraints expressing the effect of the function with respect to the variable.\nThe solution of the constraints is a summary of the function that is\nparameterized with respect to a calling context and is used to analyze function\ncalls. As a result we achieve context sensitivity at call sites without\nanalyzing the function multiple number of times.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 8 Oct 2007 08:43:58 GMT"
            }
        ],
        "update_date": "2007-10-09",
        "authors_parsed": [
            [
                "Karkare",
                "Amey",
                ""
            ],
            [
                "Sanyal",
                "Amitabha",
                ""
            ],
            [
                "Khedker",
                "Uday",
                ""
            ]
        ]
    },
    {
        "id": "0710.1484",
        "submitter": "Denis Kutuzov V",
        "authors": "Denis Kutuzov",
        "title": "The structure and modeling results of the parallel spatial switching\n  system",
        "comments": "3 pages, 2 figure",
        "journal-ref": "IEEE International Siberian Conference on Control and\n  Communications (SIBCON-2007). Proceedings. Tomsk, April 20-21, 2007. (pp.\n  86-88). IEEE Catalog Number: 07EX1367",
        "doi": "10.1109/SIBCON.2007.371303",
        "report-no": null,
        "categories": "cs.NI cs.DC",
        "license": null,
        "abstract": "  Problems of the switching parallel system designing provided spatial\nswitching of packets from random time are discussed. Results of modeling of\nswitching system as systems of mass service are resulted.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 8 Oct 2007 08:39:50 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Kutuzov",
                "Denis",
                ""
            ]
        ]
    },
    {
        "id": "0710.1484",
        "submitter": "Denis Kutuzov V",
        "authors": "Denis Kutuzov",
        "title": "The structure and modeling results of the parallel spatial switching\n  system",
        "comments": "3 pages, 2 figure",
        "journal-ref": "IEEE International Siberian Conference on Control and\n  Communications (SIBCON-2007). Proceedings. Tomsk, April 20-21, 2007. (pp.\n  86-88). IEEE Catalog Number: 07EX1367",
        "doi": "10.1109/SIBCON.2007.371303",
        "report-no": null,
        "categories": "cs.NI cs.DC",
        "license": null,
        "abstract": "  Problems of the switching parallel system designing provided spatial\nswitching of packets from random time are discussed. Results of modeling of\nswitching system as systems of mass service are resulted.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 8 Oct 2007 08:39:50 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Kutuzov",
                "Denis",
                ""
            ]
        ]
    },
    {
        "id": "0710.1499",
        "submitter": "Jukka Suomela",
        "authors": "Patrik Flor\\'een, Petteri Kaski, Topi Musto, Jukka Suomela",
        "title": "Approximating max-min linear programs with local algorithms",
        "comments": "16 pages, 2 figures",
        "journal-ref": null,
        "doi": "10.1109/IPDPS.2008.4536235",
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  A local algorithm is a distributed algorithm where each node must operate\nsolely based on the information that was available at system startup within a\nconstant-size neighbourhood of the node. We study the applicability of local\nalgorithms to max-min LPs where the objective is to maximise $\\min_k \\sum_v\nc_{kv} x_v$ subject to $\\sum_v a_{iv} x_v \\le 1$ for each $i$ and $x_v \\ge 0$\nfor each $v$. Here $c_{kv} \\ge 0$, $a_{iv} \\ge 0$, and the support sets $V_i =\n\\{v : a_{iv} > 0 \\}$, $V_k = \\{v : c_{kv}>0 \\}$, $I_v = \\{i : a_{iv} > 0 \\}$\nand $K_v = \\{k : c_{kv} > 0 \\}$ have bounded size. In the distributed setting,\neach agent $v$ is responsible for choosing the value of $x_v$, and the\ncommunication network is a hypergraph $\\mathcal{H}$ where the sets $V_k$ and\n$V_i$ constitute the hyperedges. We present inapproximability results for a\nwide range of structural assumptions; for example, even if $|V_i|$ and $|V_k|$\nare bounded by some constants larger than 2, there is no local approximation\nscheme. To contrast the negative results, we present a local approximation\nalgorithm which achieves good approximation ratios if we can bound the relative\ngrowth of the vertex neighbourhoods in $\\mathcal{H}$.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 8 Oct 2007 09:46:47 GMT"
            }
        ],
        "update_date": "2008-09-09",
        "authors_parsed": [
            [
                "Flor\u00e9en",
                "Patrik",
                ""
            ],
            [
                "Kaski",
                "Petteri",
                ""
            ],
            [
                "Musto",
                "Topi",
                ""
            ],
            [
                "Suomela",
                "Jukka",
                ""
            ]
        ]
    },
    {
        "id": "0710.1772",
        "submitter": "Flore Barcellini",
        "authors": "Flore Barcellini (INRIA Rocquencourt), Fran\\c{c}oise D\\'etienne (INRIA\n  Rocquencourt), Jean-Marie Burkhardt (INRIA Rocquencourt, LEI)",
        "title": "Cross-Participants : fostering design-use mediation in an Open Source\n  Software community",
        "comments": null,
        "journal-ref": "Dans European Conference on Cognitive Ergonomics (2007) 57-64",
        "doi": null,
        "report-no": null,
        "categories": "cs.CY cs.HC cs.SE",
        "license": null,
        "abstract": "  Motivation - This research aims at investigating emerging roles and forms of\nparticipation fostering design-use mediation during the Open Source Software\ndesign process Research approach - We compare online interactions for a\nsuccessful \"pushed-by-users\" design process with unsuccessful previous\nproposals. The methodology developed, articulate structural analyses of the\ndiscussions (organization of discussions, participation) to actions to the code\nand documentation made by participants to the project. We focus on the\nuseroriented and the developer-oriented mailing-lists of the Python project.\nFindings/Design - We find that key-participants, the cross-participants, foster\nthe design process and act as boundary spanners between the users and the\ndevelopers' communities. Research limitations/Implications - These findings can\nbe reinforced developing software to automate the structural analysis of\ndiscussions and actions to the code and documentation. Further analyses,\nsupported by these tools, will be necessary to generalise our results.\nOriginality/Value - The analysis of participation among the three interaction\nspaces of OSS design (discussion, documentation and implementation) is the main\noriginality of this work compared to other OSS research that mainly analyse one\nor two spaces. Take away message - Beside the idealistic picture that users may\nintervene freely in the process, OSS design is boost and framed by some\nkey-participants and specific rules and there can be barriers to users'\nparticipation\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 9 Oct 2007 14:44:44 GMT"
            }
        ],
        "update_date": "2007-10-10",
        "authors_parsed": [
            [
                "Barcellini",
                "Flore",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "D\u00e9tienne",
                "Fran\u00e7oise",
                "",
                "INRIA\n  Rocquencourt"
            ],
            [
                "Burkhardt",
                "Jean-Marie",
                "",
                "INRIA Rocquencourt, LEI"
            ]
        ]
    },
    {
        "id": "0710.1772",
        "submitter": "Flore Barcellini",
        "authors": "Flore Barcellini (INRIA Rocquencourt), Fran\\c{c}oise D\\'etienne (INRIA\n  Rocquencourt), Jean-Marie Burkhardt (INRIA Rocquencourt, LEI)",
        "title": "Cross-Participants : fostering design-use mediation in an Open Source\n  Software community",
        "comments": null,
        "journal-ref": "Dans European Conference on Cognitive Ergonomics (2007) 57-64",
        "doi": null,
        "report-no": null,
        "categories": "cs.CY cs.HC cs.SE",
        "license": null,
        "abstract": "  Motivation - This research aims at investigating emerging roles and forms of\nparticipation fostering design-use mediation during the Open Source Software\ndesign process Research approach - We compare online interactions for a\nsuccessful \"pushed-by-users\" design process with unsuccessful previous\nproposals. The methodology developed, articulate structural analyses of the\ndiscussions (organization of discussions, participation) to actions to the code\nand documentation made by participants to the project. We focus on the\nuseroriented and the developer-oriented mailing-lists of the Python project.\nFindings/Design - We find that key-participants, the cross-participants, foster\nthe design process and act as boundary spanners between the users and the\ndevelopers' communities. Research limitations/Implications - These findings can\nbe reinforced developing software to automate the structural analysis of\ndiscussions and actions to the code and documentation. Further analyses,\nsupported by these tools, will be necessary to generalise our results.\nOriginality/Value - The analysis of participation among the three interaction\nspaces of OSS design (discussion, documentation and implementation) is the main\noriginality of this work compared to other OSS research that mainly analyse one\nor two spaces. Take away message - Beside the idealistic picture that users may\nintervene freely in the process, OSS design is boost and framed by some\nkey-participants and specific rules and there can be barriers to users'\nparticipation\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 9 Oct 2007 14:44:44 GMT"
            }
        ],
        "update_date": "2007-10-10",
        "authors_parsed": [
            [
                "Barcellini",
                "Flore",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "D\u00e9tienne",
                "Fran\u00e7oise",
                "",
                "INRIA\n  Rocquencourt"
            ],
            [
                "Burkhardt",
                "Jean-Marie",
                "",
                "INRIA Rocquencourt, LEI"
            ]
        ]
    },
    {
        "id": "0710.1784",
        "submitter": "Marc Shapiro",
        "authors": "Marc Shapiro (LIP6, INRIA Rocquencourt), Nuno Pregui\\c{c}a (INRIA\n  Rocquencourt)",
        "title": "Designing a commutative replicated data type",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Commuting operations greatly simplify consistency in distributed systems.\nThis paper focuses on designing for commutativity, a topic neglected\npreviously. We show that the replicas of \\emph{any} data type for which\nconcurrent operations commute converges to a correct value, under some simple\nand standard assumptions. We also show that such a data type supports\ntransactions with very low cost. We identify a number of approaches and\ntechniques to ensure commutativity. We re-use some existing ideas\n(non-destructive updates coupled with invariant identification), but propose a\nmuch more efficient implementation. Furthermore, we propose a new technique,\nbackground consensus. We illustrate these ideas with a shared edit buffer data\ntype.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 9 Oct 2007 14:38:50 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Shapiro",
                "Marc",
                "",
                "LIP6, INRIA Rocquencourt"
            ],
            [
                "Pregui\u00e7a",
                "Nuno",
                "",
                "INRIA\n  Rocquencourt"
            ]
        ]
    },
    {
        "id": "0710.1870",
        "submitter": "Mireille Boutin",
        "authors": "Mireille Boutin and Gregor Kemper",
        "title": "Lossless Representation of Graphs using Distributions",
        "comments": "19 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.CO cs.CV",
        "license": null,
        "abstract": "  We consider complete graphs with edge weights and/or node weights taking\nvalues in some set. In the first part of this paper, we show that a large\nnumber of graphs are completely determined, up to isomorphism, by the\ndistribution of their sub-triangles. In the second part, we propose graph\nrepresentations in terms of one-dimensional distributions (e.g., distribution\nof the node weights, sum of adjacent weights, etc.). For the case when the\nweights of the graph are real-valued vectors, we show that all graphs, except\nfor a set of measure zero, are uniquely determined, up to isomorphism, from\nthese distributions. The motivating application for this paper is the problem\nof browsing through large sets of graphs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 9 Oct 2007 20:01:59 GMT"
            }
        ],
        "update_date": "2007-10-11",
        "authors_parsed": [
            [
                "Boutin",
                "Mireille",
                ""
            ],
            [
                "Kemper",
                "Gregor",
                ""
            ]
        ]
    },
    {
        "id": "0710.1920",
        "submitter": "Fr\\'ed\\'erique Oggier",
        "authors": "Fr\\'ed\\'erique Oggier and Babak Hassibi",
        "title": "The Secrecy Capacity of the MIMO Wiretap Channel",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.CR math.IT",
        "license": null,
        "abstract": "  We consider the MIMO wiretap channel, that is a MIMO broadcast channel where\nthe transmitter sends some confidential information to one user which is a\nlegitimate receiver, while the other user is an eavesdropper. Perfect secrecy\nis achieved when the the transmitter and the legitimate receiver can\ncommunicate at some positive rate, while insuring that the eavesdropper gets\nzero bits of information. In this paper, we compute the perfect secrecy\ncapacity of the multiple antenna MIMO broadcast channel, where the number of\nantennas is arbitrary for both the transmitter and the two receivers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 10 Oct 2007 03:52:34 GMT"
            }
        ],
        "update_date": "2007-10-11",
        "authors_parsed": [
            [
                "Oggier",
                "Fr\u00e9d\u00e9rique",
                ""
            ],
            [
                "Hassibi",
                "Babak",
                ""
            ]
        ]
    },
    {
        "id": "0710.1924",
        "submitter": "Mohsen Ravanbakhsh",
        "authors": "Mohsen Ravanbakhsh, Yasin Abbasi-Yadkori, Maghsoud Abbaspour, Hamid\n  Sarbazi-Azad",
        "title": "A Heuristic Routing Mechanism Using a New Addressing Scheme",
        "comments": "8 pages, because of lack of space journal reference just contains the\n  reference to the proceeding",
        "journal-ref": "Proceedings of First International Conference on Bio Inspired\n  models of Networks, Information and Computing Systems (BIONETICS), Cavalese,\n  Italy, December 2006",
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.AI",
        "license": null,
        "abstract": "  Current methods of routing are based on network information in the form of\nrouting tables, in which routing protocols determine how to update the tables\naccording to the network changes. Despite the variability of data in routing\ntables, node addresses are constant. In this paper, we first introduce the new\nconcept of variable addresses, which results in a novel framework to cope with\nrouting problems using heuristic solutions. Then we propose a heuristic routing\nmechanism based on the application of genes for determination of network\naddresses in a variable address network and describe how this method flexibly\nsolves different problems and induces new ideas in providing integral solutions\nfor variety of problems. The case of ad-hoc networks is where simulation\nresults are more supportive and original solutions have been proposed for\nissues like mobility.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 10 Oct 2007 04:29:24 GMT"
            }
        ],
        "update_date": "2007-10-11",
        "authors_parsed": [
            [
                "Ravanbakhsh",
                "Mohsen",
                ""
            ],
            [
                "Abbasi-Yadkori",
                "Yasin",
                ""
            ],
            [
                "Abbaspour",
                "Maghsoud",
                ""
            ],
            [
                "Sarbazi-Azad",
                "Hamid",
                ""
            ]
        ]
    },
    {
        "id": "0710.2037",
        "submitter": "Wu Jiang Mr.",
        "authors": "Wu Jiang, Fei Ding and Qiao-liang Xiang",
        "title": "An Affinity Propagation Based method for Vector Quantization Codebook\n  Design",
        "comments": "In this version we make some explaination about the network-support\n  similarity",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  In this paper, we firstly modify a parameter in affinity propagation (AP) to\nimprove its convergence ability, and then, we apply it to vector quantization\n(VQ) codebook design problem. In order to improve the quality of the resulted\ncodebook, we combine the improved AP (IAP) with the conventional LBG algorithm\nto generate an effective algorithm call IAP-LBG. According to the experimental\nresults, the proposed method not only enhances the convergence abilities but\nalso is capable of providing higher-quality codebooks than conventional LBG\nmethod.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 10 Oct 2007 15:12:20 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 11 Oct 2007 11:32:25 GMT"
            }
        ],
        "update_date": "2007-10-11",
        "authors_parsed": [
            [
                "Jiang",
                "Wu",
                ""
            ],
            [
                "Ding",
                "Fei",
                ""
            ],
            [
                "Xiang",
                "Qiao-liang",
                ""
            ]
        ]
    },
    {
        "id": "0710.2083",
        "submitter": "Oliver Schulte",
        "authors": "Oliver Schulte, Flavia Moser, Martin Ester and Zhiyong Lu",
        "title": "Association Rules in the Relational Calculus",
        "comments": "16 pages, 13 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": "SFU School of Computing Science, TR 2007-23",
        "categories": "cs.DB cs.LG cs.LO",
        "license": null,
        "abstract": "  One of the most utilized data mining tasks is the search for association\nrules. Association rules represent significant relationships between items in\ntransactions. We extend the concept of association rule to represent a much\nbroader class of associations, which we refer to as \\emph{entity-relationship\nrules.} Semantically, entity-relationship rules express associations between\nproperties of related objects. Syntactically, these rules are based on a broad\nsubclass of safe domain relational calculus queries. We propose a new\ndefinition of support and confidence for entity-relationship rules and for the\nfrequency of entity-relationship queries. We prove that the definition of\nfrequency satisfies standard probability axioms and the Apriori property.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 10 Oct 2007 18:00:44 GMT"
            }
        ],
        "update_date": "2007-10-11",
        "authors_parsed": [
            [
                "Schulte",
                "Oliver",
                ""
            ],
            [
                "Moser",
                "Flavia",
                ""
            ],
            [
                "Ester",
                "Martin",
                ""
            ],
            [
                "Lu",
                "Zhiyong",
                ""
            ]
        ]
    },
    {
        "id": "0710.2083",
        "submitter": "Oliver Schulte",
        "authors": "Oliver Schulte, Flavia Moser, Martin Ester and Zhiyong Lu",
        "title": "Association Rules in the Relational Calculus",
        "comments": "16 pages, 13 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": "SFU School of Computing Science, TR 2007-23",
        "categories": "cs.DB cs.LG cs.LO",
        "license": null,
        "abstract": "  One of the most utilized data mining tasks is the search for association\nrules. Association rules represent significant relationships between items in\ntransactions. We extend the concept of association rule to represent a much\nbroader class of associations, which we refer to as \\emph{entity-relationship\nrules.} Semantically, entity-relationship rules express associations between\nproperties of related objects. Syntactically, these rules are based on a broad\nsubclass of safe domain relational calculus queries. We propose a new\ndefinition of support and confidence for entity-relationship rules and for the\nfrequency of entity-relationship queries. We prove that the definition of\nfrequency satisfies standard probability axioms and the Apriori property.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 10 Oct 2007 18:00:44 GMT"
            }
        ],
        "update_date": "2007-10-11",
        "authors_parsed": [
            [
                "Schulte",
                "Oliver",
                ""
            ],
            [
                "Moser",
                "Flavia",
                ""
            ],
            [
                "Ester",
                "Martin",
                ""
            ],
            [
                "Lu",
                "Zhiyong",
                ""
            ]
        ]
    },
    {
        "id": "0710.2092",
        "submitter": "Marian Boguna",
        "authors": "M. Angeles Serrano, Dmitri Krioukov, and Marian Boguna",
        "title": "Self-similarity of complex networks and hidden metric spaces",
        "comments": null,
        "journal-ref": "Physical Review Letters 100, 078701 (2008)",
        "doi": "10.1103/PhysRevLett.100.078701",
        "report-no": null,
        "categories": "cond-mat.dis-nn cs.NI physics.soc-ph",
        "license": null,
        "abstract": "  We demonstrate that the self-similarity of some scale-free networks with\nrespect to a simple degree-thresholding renormalization scheme finds a natural\ninterpretation in the assumption that network nodes exist in hidden metric\nspaces. Clustering, i.e., cycles of length three, plays a crucial role in this\nframework as a topological reflection of the triangle inequality in the hidden\ngeometry. We prove that a class of hidden variable models with underlying\nmetric spaces are able to accurately reproduce the self-similarity properties\nthat we measured in the real networks. Our findings indicate that hidden\ngeometries underlying these real networks are a plausible explanation for their\nobserved topologies and, in particular, for their self-similarity with respect\nto the degree-based renormalization.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 10 Oct 2007 18:45:26 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 20 Feb 2008 20:53:35 GMT"
            }
        ],
        "update_date": "2008-12-03",
        "authors_parsed": [
            [
                "Serrano",
                "M. Angeles",
                ""
            ],
            [
                "Krioukov",
                "Dmitri",
                ""
            ],
            [
                "Boguna",
                "Marian",
                ""
            ]
        ]
    },
    {
        "id": "0710.2156",
        "submitter": "Daniel Lemire",
        "authors": "Kamel Aouiche, Daniel Lemire and Robert Godin",
        "title": "Collaborative OLAP with Tag Clouds: Web 2.0 OLAP Formalism and\n  Experimental Evaluation",
        "comments": "Software at https://github.com/lemire/OLAPTagCloud",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": "http://creativecommons.org/licenses/by/4.0/",
        "abstract": "  Increasingly, business projects are ephemeral. New Business Intelligence\ntools must support ad-lib data sources and quick perusal. Meanwhile, tag clouds\nare a popular community-driven visualization technique. Hence, we investigate\ntag-cloud views with support for OLAP operations such as roll-ups, slices,\ndices, clustering, and drill-downs. As a case study, we implemented an\napplication where users can upload data and immediately navigate through its ad\nhoc dimensions. To support social networking, views can be easily shared and\nembedded in other Web sites. Algorithmically, our tag-cloud views are\napproximate range top-k queries over spontaneous data cubes. We present\nexperimental evidence that iceberg cuboids provide adequate online\napproximations. We benchmark several browser-oblivious tag-cloud layout\noptimizations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 11 Oct 2007 19:48:10 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 14 Jan 2008 21:23:11 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 15 Mar 2016 21:52:12 GMT"
            }
        ],
        "update_date": "2016-03-17",
        "authors_parsed": [
            [
                "Aouiche",
                "Kamel",
                ""
            ],
            [
                "Lemire",
                "Daniel",
                ""
            ],
            [
                "Godin",
                "Robert",
                ""
            ]
        ]
    },
    {
        "id": "0710.2227",
        "submitter": "M Sabu THAMPI",
        "authors": "Sabu M. Thampi, K. Chandra Sekaran",
        "title": "A System for Predicting Subcellular Localization of Yeast Genome Using\n  Neural Network",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  The subcellular location of a protein can provide valuable information about\nits function. With the rapid increase of sequenced genomic data, the need for\nan automated and accurate tool to predict subcellular localization becomes\nincreasingly important. Many efforts have been made to predict protein\nsubcellular localization. This paper aims to merge the artificial neural\nnetworks and bioinformatics to predict the location of protein in yeast genome.\nWe introduce a new subcellular prediction method based on a backpropagation\nneural network. The results show that the prediction within an error limit of 5\nto 10 percentage can be achieved with the system.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 11 Oct 2007 12:09:33 GMT"
            }
        ],
        "update_date": "2007-10-12",
        "authors_parsed": [
            [
                "Thampi",
                "Sabu M.",
                ""
            ],
            [
                "Sekaran",
                "K. Chandra",
                ""
            ]
        ]
    },
    {
        "id": "0710.2231",
        "submitter": "Daniel Keysers",
        "authors": "Daniel Keysers",
        "title": "Comparison and Combination of State-of-the-art Techniques for\n  Handwritten Character Recognition: Topping the MNIST Benchmark",
        "comments": "13 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  Although the recognition of isolated handwritten digits has been a research\ntopic for many years, it continues to be of interest for the research community\nand for commercial applications. We show that despite the maturity of the\nfield, different approaches still deliver results that vary enough to allow\nimprovements by using their combination. We do so by choosing four\nwell-motivated state-of-the-art recognition systems for which results on the\nstandard MNIST benchmark are available. When comparing the errors made, we\nobserve that the errors made differ between all four systems, suggesting the\nuse of classifier combination. We then determine the error rate of a\nhypothetical system that combines the output of the four systems. The result\nobtained in this manner is an error rate of 0.35% on the MNIST data, the best\nresult published so far. We furthermore discuss the statistical significance of\nthe combined result and of the results of the individual classifiers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 11 Oct 2007 12:22:27 GMT"
            }
        ],
        "update_date": "2007-10-12",
        "authors_parsed": [
            [
                "Keysers",
                "Daniel",
                ""
            ]
        ]
    },
    {
        "id": "0710.2284",
        "submitter": "Andreas Witzel",
        "authors": "Andreas Witzel",
        "title": "Symmetric and Synchronous Communication in Peer-to-Peer Networks",
        "comments": "polished, modernized references; incorporated referee feedback from\n  MPC'08",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.GT",
        "license": null,
        "abstract": "  Motivated by distributed implementations of game-theoretical algorithms, we\nstudy symmetric process systems and the problem of attaining common knowledge\nbetween processes. We formalize our setting by defining a notion of\npeer-to-peer networks(*) and appropriate symmetry concepts in the context of\nCommunicating Sequential Processes (CSP), due to the common knowledge creating\neffects of its synchronous communication primitives. We then prove that CSP\nwith input and output guards makes common knowledge in symmetric peer-to-peer\nnetworks possible, but not the restricted version which disallows output\nstatements in guards and is commonly implemented.\n  (*) Please note that we are not dealing with fashionable incarnations such as\nfile-sharing networks, but merely use this name for a mathematical notion of a\nnetwork consisting of directly connected peers \"treated on an equal footing\",\ni.e. not having a client-server structure or otherwise pre-determined roles.)\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 11 Oct 2007 16:14:38 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 17 Oct 2007 10:41:05 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 21 Jan 2008 12:51:56 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 14 Apr 2008 15:11:16 GMT"
            }
        ],
        "update_date": "2008-04-14",
        "authors_parsed": [
            [
                "Witzel",
                "Andreas",
                ""
            ]
        ]
    },
    {
        "id": "0710.2358",
        "submitter": "Catherine Recanati",
        "authors": "C. Recanati",
        "title": "Success and failure of programming environments - report on the design\n  and use of a graphic abstract syntax tree editor",
        "comments": "This is an old paper (1990) of 29 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": "Esprit project no. 891 (STAPLE), Technical Report no 90/1, Paris,\n  Jan 1990",
        "categories": "cs.PL cs.HC",
        "license": null,
        "abstract": "  The STAPLE project investigated (at the end of the eighties), a persistent\narchitecture for functional programming. Work has been done in two directions:\nthe development of a programming environment for a functional language within a\npersistent system and an experiment on transferring the expertise of functional\nprototyping into industry. This paper is a report on the first activity. The\nfirst section gives a general description of Absynte - the abstract syntax tree\neditor developed within the Project. Following sections make an attempt at\nmeasuring the effectiveness of such an editor and discuss the problems raised\nby structured syntax editing - specially environments based on abstract syntax\ntrees.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 11 Oct 2007 23:40:47 GMT"
            }
        ],
        "update_date": "2007-10-15",
        "authors_parsed": [
            [
                "Recanati",
                "C.",
                ""
            ]
        ]
    },
    {
        "id": "0710.2358",
        "submitter": "Catherine Recanati",
        "authors": "C. Recanati",
        "title": "Success and failure of programming environments - report on the design\n  and use of a graphic abstract syntax tree editor",
        "comments": "This is an old paper (1990) of 29 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": "Esprit project no. 891 (STAPLE), Technical Report no 90/1, Paris,\n  Jan 1990",
        "categories": "cs.PL cs.HC",
        "license": null,
        "abstract": "  The STAPLE project investigated (at the end of the eighties), a persistent\narchitecture for functional programming. Work has been done in two directions:\nthe development of a programming environment for a functional language within a\npersistent system and an experiment on transferring the expertise of functional\nprototyping into industry. This paper is a report on the first activity. The\nfirst section gives a general description of Absynte - the abstract syntax tree\neditor developed within the Project. Following sections make an attempt at\nmeasuring the effectiveness of such an editor and discuss the problems raised\nby structured syntax editing - specially environments based on abstract syntax\ntrees.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 11 Oct 2007 23:40:47 GMT"
            }
        ],
        "update_date": "2007-10-15",
        "authors_parsed": [
            [
                "Recanati",
                "C.",
                ""
            ]
        ]
    },
    {
        "id": "0710.2446",
        "submitter": "Catherine Recanati",
        "authors": "Catherine Recanati (LIPN), Nicoleta Rogovschi (LIPN), Youn\\`es Bennani\n  (LIPN)",
        "title": "The structure of verbal sequences analyzed with unsupervised learning\n  techniques",
        "comments": null,
        "journal-ref": "Dans Proceedings - The 3rd Language & Technology Conference: Human\n  Language Technologies as a Challenge for Computer Science and Linguistics,\n  Poznan : Pologne (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.CL cs.AI cs.LG",
        "license": null,
        "abstract": "  Data mining allows the exploration of sequences of phenomena, whereas one\nusually tends to focus on isolated phenomena or on the relation between two\nphenomena. It offers invaluable tools for theoretical analyses and exploration\nof the structure of sentences, texts, dialogues, and speech. We report here the\nresults of an attempt at using it for inspecting sequences of verbs from French\naccounts of road accidents. This analysis comes from an original approach of\nunsupervised training allowing the discovery of the structure of sequential\ndata. The entries of the analyzer were only made of the verbs appearing in the\nsentences. It provided a classification of the links between two successive\nverbs into four distinct clusters, allowing thus text segmentation. We give\nhere an interpretation of these clusters by applying a statistical analysis to\nindependent semantic annotations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 12 Oct 2007 12:44:11 GMT"
            }
        ],
        "update_date": "2007-10-15",
        "authors_parsed": [
            [
                "Recanati",
                "Catherine",
                "",
                "LIPN"
            ],
            [
                "Rogovschi",
                "Nicoleta",
                "",
                "LIPN"
            ],
            [
                "Bennani",
                "Youn\u00e8s",
                "",
                "LIPN"
            ]
        ]
    },
    {
        "id": "0710.2604",
        "submitter": "Raymond Chi-Wing Wong",
        "authors": "Raymond Chi-Wing Wong, Ada Wai-chee Fu, Jian Pei, Yip Sing Ho, Tai\n  Wong, Yubao Liu",
        "title": "Efficient Skyline Querying with Variable User Preferences on Nominal\n  Attributes",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  Current skyline evaluation techniques assume a fixed ordering on the\nattributes. However, dynamic preferences on nominal attributes are more\nrealistic in known applications. In order to generate online response for any\nsuch preference issued by a user, we propose two methods of different\ncharacteristics. The first one is a semi-materialization method and the second\nis an adaptive SFS method. Finally, we conduct experiments to show the\nefficiency of our proposed algorithms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 13 Oct 2007 11:47:14 GMT"
            }
        ],
        "update_date": "2007-10-16",
        "authors_parsed": [
            [
                "Wong",
                "Raymond Chi-Wing",
                ""
            ],
            [
                "Fu",
                "Ada Wai-chee",
                ""
            ],
            [
                "Pei",
                "Jian",
                ""
            ],
            [
                "Ho",
                "Yip Sing",
                ""
            ],
            [
                "Wong",
                "Tai",
                ""
            ],
            [
                "Liu",
                "Yubao",
                ""
            ]
        ]
    },
    {
        "id": "0710.2705",
        "submitter": "Shih-Chun  Lin",
        "authors": "Shih-Chun Lin, Mohammad Shahmohammadi, Hesham El Gamal",
        "title": "Fingerprinting with Minimum Distance Decoding",
        "comments": "26 pages, 6 figures, submitted to IEEE Transactions on Information\n  Forensics and Security",
        "journal-ref": "IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 4,\n  NO. 1, MARCH 2009 pp.59-69",
        "doi": "10.1109/TIFS.2008.2012201",
        "report-no": null,
        "categories": "cs.IT cs.CR math.IT",
        "license": null,
        "abstract": "  This work adopts an information theoretic framework for the design of\ncollusion-resistant coding/decoding schemes for digital fingerprinting. More\nspecifically, the minimum distance decision rule is used to identify 1 out of t\npirates. Achievable rates, under this detection rule, are characterized in two\ndistinct scenarios. First, we consider the averaging attack where a random\ncoding argument is used to show that the rate 1/2 is achievable with t=2\npirates. Our study is then extended to the general case of arbitrary $t$\nhighlighting the underlying complexity-performance tradeoff. Overall, these\nresults establish the significant performance gains offered by minimum distance\ndecoding as compared to other approaches based on orthogonal codes and\ncorrelation detectors. In the second scenario, we characterize the achievable\nrates, with minimum distance decoding, under any collusion attack that\nsatisfies the marking assumption. For t=2 pirates, we show that the rate\n$1-H(0.25)\\approx 0.188$ is achievable using an ensemble of random linear\ncodes. For $t\\geq 3$, the existence of a non-resolvable collusion attack, with\nminimum distance decoding, for any non-zero rate is established. Inspired by\nour theoretical analysis, we then construct coding/decoding schemes for\nfingerprinting based on the celebrated Belief-Propagation framework. Using an\nexplicit repeat-accumulate code, we obtain a vanishingly small probability of\nmisidentification at rate 1/3 under averaging attack with t=2. For collusion\nattacks which satisfy the marking assumption, we use a more sophisticated\naccumulate repeat accumulate code to obtain a vanishingly small\nmisidentification probability at rate 1/9 with t=2. These results represent a\nmarked improvement over the best available designs in the literature.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 15 Oct 2007 01:20:01 GMT"
            }
        ],
        "update_date": "2009-03-02",
        "authors_parsed": [
            [
                "Lin",
                "Shih-Chun",
                ""
            ],
            [
                "Shahmohammadi",
                "Mohammad",
                ""
            ],
            [
                "Gamal",
                "Hesham El",
                ""
            ]
        ]
    },
    {
        "id": "0710.2716",
        "submitter": "Li Rong",
        "authors": "Rong Li, Zhisheng Duan and Guanrong Chen",
        "title": "Cost and Effects of Pinning Control for Network Synchronization",
        "comments": "12 pages, 18 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  In this paper, the problem of pinning control for synchronization of complex\ndynamical networks is discussed. A cost function of the controlled network is\ndefined by the feedback gain and the coupling strength of the network. An\ninteresting result is that lower cost is achieved by the control scheme of\npinning nodes with smaller degrees. Some rigorous mathematical analysis is\npresented for achieving lower cost in the synchronization of different\nstar-shaped networks. Numerical simulations on some non-regular complex\nnetworks generated by the Barabasi-Albert model and various star-shaped\nnetworks are shown for verification and illustration.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 15 Oct 2007 05:45:49 GMT"
            }
        ],
        "update_date": "2007-10-16",
        "authors_parsed": [
            [
                "Li",
                "Rong",
                ""
            ],
            [
                "Duan",
                "Zhisheng",
                ""
            ],
            [
                "Chen",
                "Guanrong",
                ""
            ]
        ]
    },
    {
        "id": "0710.2736",
        "submitter": "Chao Liu",
        "authors": "Chao Liu, Zhisheng Duan and Guanrong Chen and Lin Huang",
        "title": "L2 norm performance index of synchronization and optimal control\n  synthesis of complex networks",
        "comments": "15 peges, 11 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI",
        "license": null,
        "abstract": "  In this paper, the synchronizability problem of dynamical networks is\naddressed, where better synchronizability means that the network synchronizes\nfaster with lower-overshoot. The L2 norm of the error vector e is taken as a\nperformance index to measure this kind of synchronizability. For the\nequilibrium synchronization case, it is shown that there is a close\nrelationship between the L2 norm of the error vector e and the H2 norm of the\ntransfer function G of the linearized network about the equilibrium point.\nConsequently, the effect of the network coupling topology on the H2 norm of the\ntransfer function G is analyzed. Finally, an optimal controller is designed,\naccording to the so-called LQR problem in modern control theory, which can\ndrive the whole network to its equilibrium point and meanwhile minimize the L2\nnorm of the output of the linearized network.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 15 Oct 2007 08:16:47 GMT"
            }
        ],
        "update_date": "2007-10-16",
        "authors_parsed": [
            [
                "Liu",
                "Chao",
                ""
            ],
            [
                "Duan",
                "Zhisheng",
                ""
            ],
            [
                "Chen",
                "Guanrong",
                ""
            ],
            [
                "Huang",
                "Lin",
                ""
            ]
        ]
    },
    {
        "id": "0710.2782",
        "submitter": "Leonardo Emmendorfer",
        "authors": "Leonardo Emmendorfer and Aurora Pozo",
        "title": "Effective linkage learning using low-order statistics and clustering",
        "comments": "Submitted to IEEE Transactions on Evolutionary Computation",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  The adoption of probabilistic models for the best individuals found so far is\na powerful approach for evolutionary computation. Increasingly more complex\nmodels have been used by estimation of distribution algorithms (EDAs), which\noften result better effectiveness on finding the global optima for hard\noptimization problems. Supervised and unsupervised learning of Bayesian\nnetworks are very effective options, since those models are able to capture\ninteractions of high order among the variables of a problem. Diversity\npreservation, through niching techniques, has also shown to be very important\nto allow the identification of the problem structure as much as for keeping\nseveral global optima. Recently, clustering was evaluated as an effective\nniching technique for EDAs, but the performance of simpler low-order EDAs was\nnot shown to be much improved by clustering, except for some simple multimodal\nproblems. This work proposes and evaluates a combination operator guided by a\nmeasure from information theory which allows a clustered low-order EDA to\neffectively solve a comprehensive range of benchmark optimization problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 15 Oct 2007 15:28:47 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 16 Oct 2007 13:27:03 GMT"
            }
        ],
        "update_date": "2007-10-16",
        "authors_parsed": [
            [
                "Emmendorfer",
                "Leonardo",
                ""
            ],
            [
                "Pozo",
                "Aurora",
                ""
            ]
        ]
    },
    {
        "id": "0710.2848",
        "submitter": "Francis Bach",
        "authors": "Francis Bach (WILLOW Project - Inria/Ens)",
        "title": "Consistency of trace norm minimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  Regularization by the sum of singular values, also referred to as the trace\nnorm, is a popular technique for estimating low rank rectangular matrices. In\nthis paper, we extend some of the consistency results of the Lasso to provide\nnecessary and sufficient conditions for rank consistency of trace norm\nminimization with the square loss. We also provide an adaptive version that is\nrank consistent even when the necessary condition for the non adaptive version\nis not fulfilled.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 15 Oct 2007 15:38:33 GMT"
            }
        ],
        "update_date": "2007-10-16",
        "authors_parsed": [
            [
                "Bach",
                "Francis",
                "",
                "WILLOW Project - Inria/Ens"
            ]
        ]
    },
    {
        "id": "0710.2887",
        "submitter": "Olivier Zendra",
        "authors": "Roland Ducournau (LIRMM), Etienne Gagnon, Chandra Krintz (RACE LAB),\n  Philippe Mulet, Jan Vitek (S3L), Olivier Zendra (INRIA Lorraine - LORIA)",
        "title": "Implementation, Compilation, Optimization of Object-Oriented Languages,\n  Programs and Systems - Report on the Workshop ICOOOLPS'2006 at ECOOP'06",
        "comments": "The original publication is available at http://www.springerlink.com",
        "journal-ref": "Object-Oriented Technology. ECOOP 2006 Workshop Reader - ECOOP\n  2006 Workshops, Nantes, France, July 3-7, 2006, Final Reports Springer Berlin\n  / Heidelberg (Ed.) (2007) 1-14",
        "doi": "10.1007/978-3-540-71774-4_1",
        "report-no": null,
        "categories": "cs.PF cs.PL cs.SE",
        "license": null,
        "abstract": "  ICOOOLPS'2006 was the first edition of ECOOP-ICOOOLPS workshop. It intended\nto bring researchers and practitioners both from academia and industry\ntogether, with a spirit of openness, to try and identify and begin to address\nthe numerous and very varied issues of optimization. This succeeded, as can be\nseen from the papers, the attendance and the liveliness of the discussions that\ntook place during and after the workshop, not to mention a few new cooperations\nor postdoctoral contracts. The 22 talented people from different groups who\nparticipated were unanimous to appreciate this first edition and recommend that\nICOOOLPS be continued next year. A community is thus beginning to form, and\nshould be reinforced by a second edition next year, with all the improvements\nthis first edition made emerge.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 15 Oct 2007 17:53:49 GMT"
            }
        ],
        "update_date": "2007-10-16",
        "authors_parsed": [
            [
                "Ducournau",
                "Roland",
                "",
                "LIRMM"
            ],
            [
                "Gagnon",
                "Etienne",
                "",
                "RACE LAB"
            ],
            [
                "Krintz",
                "Chandra",
                "",
                "RACE LAB"
            ],
            [
                "Mulet",
                "Philippe",
                "",
                "S3L"
            ],
            [
                "Vitek",
                "Jan",
                "",
                "S3L"
            ],
            [
                "Zendra",
                "Olivier",
                "",
                "INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0710.2887",
        "submitter": "Olivier Zendra",
        "authors": "Roland Ducournau (LIRMM), Etienne Gagnon, Chandra Krintz (RACE LAB),\n  Philippe Mulet, Jan Vitek (S3L), Olivier Zendra (INRIA Lorraine - LORIA)",
        "title": "Implementation, Compilation, Optimization of Object-Oriented Languages,\n  Programs and Systems - Report on the Workshop ICOOOLPS'2006 at ECOOP'06",
        "comments": "The original publication is available at http://www.springerlink.com",
        "journal-ref": "Object-Oriented Technology. ECOOP 2006 Workshop Reader - ECOOP\n  2006 Workshops, Nantes, France, July 3-7, 2006, Final Reports Springer Berlin\n  / Heidelberg (Ed.) (2007) 1-14",
        "doi": "10.1007/978-3-540-71774-4_1",
        "report-no": null,
        "categories": "cs.PF cs.PL cs.SE",
        "license": null,
        "abstract": "  ICOOOLPS'2006 was the first edition of ECOOP-ICOOOLPS workshop. It intended\nto bring researchers and practitioners both from academia and industry\ntogether, with a spirit of openness, to try and identify and begin to address\nthe numerous and very varied issues of optimization. This succeeded, as can be\nseen from the papers, the attendance and the liveliness of the discussions that\ntook place during and after the workshop, not to mention a few new cooperations\nor postdoctoral contracts. The 22 talented people from different groups who\nparticipated were unanimous to appreciate this first edition and recommend that\nICOOOLPS be continued next year. A community is thus beginning to form, and\nshould be reinforced by a second edition next year, with all the improvements\nthis first edition made emerge.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 15 Oct 2007 17:53:49 GMT"
            }
        ],
        "update_date": "2007-10-16",
        "authors_parsed": [
            [
                "Ducournau",
                "Roland",
                "",
                "LIRMM"
            ],
            [
                "Gagnon",
                "Etienne",
                "",
                "RACE LAB"
            ],
            [
                "Krintz",
                "Chandra",
                "",
                "RACE LAB"
            ],
            [
                "Mulet",
                "Philippe",
                "",
                "S3L"
            ],
            [
                "Vitek",
                "Jan",
                "",
                "S3L"
            ],
            [
                "Zendra",
                "Olivier",
                "",
                "INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0710.2887",
        "submitter": "Olivier Zendra",
        "authors": "Roland Ducournau (LIRMM), Etienne Gagnon, Chandra Krintz (RACE LAB),\n  Philippe Mulet, Jan Vitek (S3L), Olivier Zendra (INRIA Lorraine - LORIA)",
        "title": "Implementation, Compilation, Optimization of Object-Oriented Languages,\n  Programs and Systems - Report on the Workshop ICOOOLPS'2006 at ECOOP'06",
        "comments": "The original publication is available at http://www.springerlink.com",
        "journal-ref": "Object-Oriented Technology. ECOOP 2006 Workshop Reader - ECOOP\n  2006 Workshops, Nantes, France, July 3-7, 2006, Final Reports Springer Berlin\n  / Heidelberg (Ed.) (2007) 1-14",
        "doi": "10.1007/978-3-540-71774-4_1",
        "report-no": null,
        "categories": "cs.PF cs.PL cs.SE",
        "license": null,
        "abstract": "  ICOOOLPS'2006 was the first edition of ECOOP-ICOOOLPS workshop. It intended\nto bring researchers and practitioners both from academia and industry\ntogether, with a spirit of openness, to try and identify and begin to address\nthe numerous and very varied issues of optimization. This succeeded, as can be\nseen from the papers, the attendance and the liveliness of the discussions that\ntook place during and after the workshop, not to mention a few new cooperations\nor postdoctoral contracts. The 22 talented people from different groups who\nparticipated were unanimous to appreciate this first edition and recommend that\nICOOOLPS be continued next year. A community is thus beginning to form, and\nshould be reinforced by a second edition next year, with all the improvements\nthis first edition made emerge.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 15 Oct 2007 17:53:49 GMT"
            }
        ],
        "update_date": "2007-10-16",
        "authors_parsed": [
            [
                "Ducournau",
                "Roland",
                "",
                "LIRMM"
            ],
            [
                "Gagnon",
                "Etienne",
                "",
                "RACE LAB"
            ],
            [
                "Krintz",
                "Chandra",
                "",
                "RACE LAB"
            ],
            [
                "Mulet",
                "Philippe",
                "",
                "S3L"
            ],
            [
                "Vitek",
                "Jan",
                "",
                "S3L"
            ],
            [
                "Zendra",
                "Olivier",
                "",
                "INRIA Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0710.2889",
        "submitter": "Nir Ailon",
        "authors": "Nir Ailon and Mehryar Mohri",
        "title": "An efficient reduction of ranking to classification",
        "comments": "Revised paper: Improved results: Upper bounds for regret (constant\n  down to 1 for bipartite case) and also lower bound on deterministic\n  algorithms for bipartite case. Total number of pages 22",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.IR",
        "license": null,
        "abstract": "  This paper describes an efficient reduction of the learning problem of\nranking to binary classification. The reduction guarantees an average pairwise\nmisranking regret of at most that of the binary classifier regret, improving a\nrecent result of Balcan et al which only guarantees a factor of 2. Moreover,\nour reduction applies to a broader class of ranking loss functions, admits a\nsimpler proof, and the expected running time complexity of our algorithm in\nterms of number of calls to a classifier or preference function is improved\nfrom $\\Omega(n^2)$ to $O(n \\log n)$. In addition, when the top $k$ ranked\nelements only are required ($k \\ll n$), as in many applications in information\nextraction or search engines, the time complexity of our algorithm can be\nfurther reduced to $O(k \\log k + n)$. Our reduction and algorithm are thus\npractical for realistic applications where the number of points to rank exceeds\nseveral thousands. Much of our results also extend beyond the bipartite case\npreviously studied.\n  Our rediction is a randomized one. To complement our result, we also derive\nlower bounds on any deterministic reduction from binary (preference)\nclassification to ranking, implying that our use of a randomized reduction is\nessentially necessary for the guarantees we provide.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 15 Oct 2007 18:25:15 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 7 Dec 2007 00:02:44 GMT"
            }
        ],
        "update_date": "2007-12-07",
        "authors_parsed": [
            [
                "Ailon",
                "Nir",
                ""
            ],
            [
                "Mohri",
                "Mehryar",
                ""
            ]
        ]
    },
    {
        "id": "0710.2970",
        "submitter": "An-Ping Li",
        "authors": "An-Ping Li",
        "title": "A generic attack to ciphers",
        "comments": "4 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  In this paper, we present a generic attack for ciphers, which is in essence a\ncollision attack on the secret keys of ciphers .\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 16 Oct 2007 07:45:58 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 17 Oct 2007 10:34:37 GMT"
            }
        ],
        "update_date": "2009-03-21",
        "authors_parsed": [
            [
                "Li",
                "An-Ping",
                ""
            ]
        ]
    },
    {
        "id": "0710.3185",
        "submitter": "Harki Tanaka",
        "authors": "Harki Tanaka, Neli Regina Siqueira Ortega, Mauricio Stanzione Galizia,\n  Joao Batista Borges Sobrinho, and Marcelo Britto Passos Amato",
        "title": "Fuzzy Modeling of Electrical Impedance Tomography Image of the Lungs",
        "comments": "10 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.CV",
        "license": null,
        "abstract": "  Electrical Impedance Tomography (EIT) is a functional imaging method that is\nbeing developed for bedside use in critical care medicine. Aiming at improving\nthe chest anatomical resolution of EIT images we developed a fuzzy model based\non EIT high temporal resolution and the functional information contained in the\npulmonary perfusion and ventilation signals. EIT data from an experimental\nanimal model were collected during normal ventilation and apnea while an\ninjection of hypertonic saline was used as a reference . The fuzzy model was\nelaborated in three parts: a modeling of the heart, a pulmonary map from\nventilation images and, a pulmonary map from perfusion images. Image\nsegmentation was performed using a threshold method and a ventilation/perfusion\nmap was generated. EIT images treated by the fuzzy model were compared with the\nhypertonic saline injection method and CT-scan images, presenting good results\nin both qualitative (the image obtained by the model was very similar to that\nof the CT-scan) and quantitative (the ROC curve provided an area equal to 0.93)\npoint of view. Undoubtedly, these results represent an important step in the\nEIT images area, since they open the possibility of developing EIT-based\nbedside clinical methods, which are not available nowadays. These achievements\ncould serve as the base to develop EIT diagnosis system for some\nlife-threatening diseases commonly found in critical care medicine.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 16 Oct 2007 22:13:11 GMT"
            }
        ],
        "update_date": "2007-10-18",
        "authors_parsed": [
            [
                "Tanaka",
                "Harki",
                ""
            ],
            [
                "Ortega",
                "Neli Regina Siqueira",
                ""
            ],
            [
                "Galizia",
                "Mauricio Stanzione",
                ""
            ],
            [
                "Sobrinho",
                "Joao Batista Borges",
                ""
            ],
            [
                "Amato",
                "Marcelo Britto Passos",
                ""
            ]
        ]
    },
    {
        "id": "0710.3283",
        "submitter": "Meixia Tao",
        "authors": "Meixia Tao",
        "title": "Effects of Non-Identical Rayleigh Fading on Differential Unitary\n  Space-Time Modulation",
        "comments": "This paper has been withdrawn",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.IT math.IT",
        "license": null,
        "abstract": "  This paper has been withdrawn by the author.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 17 Oct 2007 12:32:29 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 20 Oct 2007 08:21:37 GMT"
            }
        ],
        "update_date": "2007-10-20",
        "authors_parsed": [
            [
                "Tao",
                "Meixia",
                ""
            ]
        ]
    },
    {
        "id": "0710.3305",
        "submitter": "Laurent Vigneron",
        "authors": "Francis Klay (FT R&D), Judson Santiago (DIMAP - UFRN), Laurent\n  Vigneron (INRIA Lorraine - LORIA / LIFC)",
        "title": "Automatic Methods for Analyzing Non-Repudiation Protocols with an Active\n  Intruder",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.CR",
        "license": null,
        "abstract": "  Non-repudiation protocols have an important role in many areas where secured\ntransactions with proofs of participation are necessary. Formal methods are\nclever and without error, therefore using them for verifying such protocols is\ncrucial. In this purpose, we show how to partially represent non-repudiation as\na combination of authentications on the Fair Zhou-Gollmann protocol. After\ndiscussing its limits, we define a new method based on the handling of the\nknowledge of protocol participants. This method is very general and is of\nnatural use, as it consists in adding simple annotations, like for\nauthentication problems. The method is very easy to implement in tools able to\nhandle participants knowledge. We have implemented it in the AVISPA Tool and\nanalyzed the optimistic Cederquist-Corin- Dashti protocol, discovering two\nunknown attacks. This extension of the AVISPA Tool for handling non-repudiation\nopens a highway to the specification of many other properties, without any more\nchange in the tool itself.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 17 Oct 2007 14:16:25 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 22 Oct 2007 06:40:14 GMT"
            }
        ],
        "update_date": "2007-10-22",
        "authors_parsed": [
            [
                "Klay",
                "Francis",
                "",
                "FT R&D"
            ],
            [
                "Santiago",
                "Judson",
                "",
                "DIMAP - UFRN"
            ],
            [
                "Vigneron",
                "Laurent",
                "",
                "INRIA Lorraine - LORIA / LIFC"
            ]
        ]
    },
    {
        "id": "0710.3561",
        "submitter": "Arturo Berrones",
        "authors": "Arturo Berrones",
        "title": "Stationary probability density of stochastic search processes in global\n  optimization",
        "comments": null,
        "journal-ref": "J. Stat. Mech. (2008) P01013",
        "doi": "10.1088/1742-5468/2008/01/P01013",
        "report-no": null,
        "categories": "cs.AI cond-mat.stat-mech cs.NE",
        "license": null,
        "abstract": "  A method for the construction of approximate analytical expressions for the\nstationary marginal densities of general stochastic search processes is\nproposed. By the marginal densities, regions of the search space that with high\nprobability contain the global optima can be readily defined. The density\nestimation procedure involves a controlled number of linear operations, with a\ncomputational cost per iteration that grows linearly with problem size.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 18 Oct 2007 18:04:42 GMT"
            }
        ],
        "update_date": "2008-01-30",
        "authors_parsed": [
            [
                "Berrones",
                "Arturo",
                ""
            ]
        ]
    },
    {
        "id": "0710.3621",
        "submitter": "Withawat Withayachumnankul",
        "authors": "Withawat Withayachumnankul, Bernd M. Fischer, Samuel P. Mickan, Derek\n  Abbott",
        "title": "Numerical removal of water-vapor effects from THz-TDS measurements",
        "comments": null,
        "journal-ref": "Proceedings of the Royal Society A: Mathematical, Physical &\n  Engineering Sciences, vol. 464, no. 2097, pp 2435-2456, 2008",
        "doi": "10.1098/rspa.2007.0294",
        "report-no": null,
        "categories": "cs.CE physics.comp-ph",
        "license": null,
        "abstract": "  One source of disturbance in a pulsed T-ray signal is attributed to ambient\nwater vapor. Water molecules in the gas phase selectively absorb T-rays at\ndiscrete frequencies corresponding to their molecular rotational transitions.\nThis results in prominent resonances spread over the T-ray spectrum, and in the\ntime domain the T-ray signal is observed as fluctuations after the main pulse.\nThese effects are generally undesired, since they may mask critical\nspectroscopic data. So, ambient water vapor is commonly removed from the T-ray\npath by using a closed chamber during the measurement. Yet, in some\napplications a closed chamber is not applicable. This situation, therefore,\nmotivates the need for another method to reduce these unwanted artifacts. This\npaper presents a study on a computational means to address the problem.\nInitially, a complex frequency response of water vapor is modeled from a\nspectroscopic catalog. Using a deconvolution technique, together with fine\ntuning of the strength of each resonance, parts of the water-vapor response are\nremoved from a measured T-ray signal, with minimal signal distortion.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 19 Oct 2007 02:17:35 GMT"
            }
        ],
        "update_date": "2008-07-16",
        "authors_parsed": [
            [
                "Withayachumnankul",
                "Withawat",
                ""
            ],
            [
                "Fischer",
                "Bernd M.",
                ""
            ],
            [
                "Mickan",
                "Samuel P.",
                ""
            ],
            [
                "Abbott",
                "Derek",
                ""
            ]
        ]
    },
    {
        "id": "0710.3779",
        "submitter": "Sumanth Gangasani",
        "authors": "Sumanth Kumar Reddy Gangasani",
        "title": "Testing D-Sequences for their Randomness",
        "comments": "8 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  This paper examines the randomness of d-sequences, which are decimal\nsequences to an arbitrary base. Our motivation is to check their suitability\nfor application to cryptography, spread-spectrum systems and use as\npseudorandom sequence.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 19 Oct 2007 20:18:42 GMT"
            }
        ],
        "update_date": "2007-10-23",
        "authors_parsed": [
            [
                "Gangasani",
                "Sumanth Kumar Reddy",
                ""
            ]
        ]
    },
    {
        "id": "0710.3824",
        "submitter": "Sebastien Tixeuil",
        "authors": "Sylvie Dela\\\"et (LRI), Partha Sarathi Mandal (INRIA Futurs), Mariusz\n  Rokicki (LRI), S\\'ebastien Tixeuil (INRIA Futurs, LIP6)",
        "title": "Deterministic Secure Positioning in Wireless Sensor Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.DC cs.DS cs.NI",
        "license": null,
        "abstract": "  Properly locating sensor nodes is an important building block for a large\nsubset of wireless sensor networks (WSN) applications. As a result, the\nperformance of the WSN degrades significantly when misbehaving nodes report\nfalse location and distance information in order to fake their actual location.\nIn this paper we propose a general distributed deterministic protocol for\naccurate identification of faking sensors in a WSN. Our scheme does \\emph{not}\nrely on a subset of \\emph{trusted} nodes that are not allowed to misbehave and\nare known to every node in the network. Thus, any subset of nodes is allowed to\ntry faking its position. As in previous approaches, our protocol is based on\ndistance evaluation techniques developed for WSN. On the positive side, we show\nthat when the received signal strength (RSS) technique is used, our protocol\nhandles at most $\\lfloor \\frac{n}{2} \\rfloor-2$ faking sensors. Also, when the\ntime of flight (ToF) technique is used, our protocol manages at most $\\lfloor\n\\frac{n}{2} \\rfloor - 3$ misbehaving sensors. On the negative side, we prove\nthat no deterministic protocol can identify faking sensors if their number is\n$\\lceil \\frac{n}{2}\\rceil -1$. Thus our scheme is almost optimal with respect\nto the number of faking sensors. We discuss application of our technique in the\ntrusted sensor model. More precisely our results can be used to minimize the\nnumber of trusted sensors that are needed to defeat faking ones.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 22 Oct 2007 07:29:13 GMT"
            }
        ],
        "update_date": "2007-10-23",
        "authors_parsed": [
            [
                "Dela\u00ebt",
                "Sylvie",
                "",
                "LRI"
            ],
            [
                "Mandal",
                "Partha Sarathi",
                "",
                "INRIA Futurs"
            ],
            [
                "Rokicki",
                "Mariusz",
                "",
                "LRI"
            ],
            [
                "Tixeuil",
                "S\u00e9bastien",
                "",
                "INRIA Futurs, LIP6"
            ]
        ]
    },
    {
        "id": "0710.3824",
        "submitter": "Sebastien Tixeuil",
        "authors": "Sylvie Dela\\\"et (LRI), Partha Sarathi Mandal (INRIA Futurs), Mariusz\n  Rokicki (LRI), S\\'ebastien Tixeuil (INRIA Futurs, LIP6)",
        "title": "Deterministic Secure Positioning in Wireless Sensor Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.DC cs.DS cs.NI",
        "license": null,
        "abstract": "  Properly locating sensor nodes is an important building block for a large\nsubset of wireless sensor networks (WSN) applications. As a result, the\nperformance of the WSN degrades significantly when misbehaving nodes report\nfalse location and distance information in order to fake their actual location.\nIn this paper we propose a general distributed deterministic protocol for\naccurate identification of faking sensors in a WSN. Our scheme does \\emph{not}\nrely on a subset of \\emph{trusted} nodes that are not allowed to misbehave and\nare known to every node in the network. Thus, any subset of nodes is allowed to\ntry faking its position. As in previous approaches, our protocol is based on\ndistance evaluation techniques developed for WSN. On the positive side, we show\nthat when the received signal strength (RSS) technique is used, our protocol\nhandles at most $\\lfloor \\frac{n}{2} \\rfloor-2$ faking sensors. Also, when the\ntime of flight (ToF) technique is used, our protocol manages at most $\\lfloor\n\\frac{n}{2} \\rfloor - 3$ misbehaving sensors. On the negative side, we prove\nthat no deterministic protocol can identify faking sensors if their number is\n$\\lceil \\frac{n}{2}\\rceil -1$. Thus our scheme is almost optimal with respect\nto the number of faking sensors. We discuss application of our technique in the\ntrusted sensor model. More precisely our results can be used to minimize the\nnumber of trusted sensors that are needed to defeat faking ones.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 22 Oct 2007 07:29:13 GMT"
            }
        ],
        "update_date": "2007-10-23",
        "authors_parsed": [
            [
                "Dela\u00ebt",
                "Sylvie",
                "",
                "LRI"
            ],
            [
                "Mandal",
                "Partha Sarathi",
                "",
                "INRIA Futurs"
            ],
            [
                "Rokicki",
                "Mariusz",
                "",
                "LRI"
            ],
            [
                "Tixeuil",
                "S\u00e9bastien",
                "",
                "INRIA Futurs, LIP6"
            ]
        ]
    },
    {
        "id": "0710.3916",
        "submitter": "Bernard Cousin",
        "authors": "Wojtek Bigos (IRISA), St\\'ephane Gosselin (IRISA), Bernard Cousin\n  (IRISA), Morgane Le Foll (IRISA), Hisao Nakajima (IRISA)",
        "title": "Optimized Design of Survivable MPLS over Optical Transport Networks.\n  Optical Switching and Networking",
        "comments": null,
        "journal-ref": "Optical Switching and Networking 3, 3-4 (2006) 202-218",
        "doi": "10.1016/j.osn.2006.08.001",
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": null,
        "abstract": "  In this paper we study different options for the survivability implementation\nin MPLS over Optical Transport Networks in terms of network resource usage and\nconfiguration cost. We investigate two approaches to the survivability\ndeployment: single layer and multilayer survivability and present various\nmethods for spare capacity allocation (SCA) to reroute disrupted traffic. The\ncomparative analysis shows the influence of the traffic granularity on the\nsurvivability cost: for high bandwidth LSPs, close to the optical channel\ncapacity, the multilayer survivability outperforms the single layer one,\nwhereas for low bandwidth LSPs the single layer survivability is more\ncost-efficient. For the multilayer survivability we demonstrate that by mapping\nefficiently the spare capacity of the MPLS layer onto the resources of the\noptical layer one can achieve up to 22% savings in the total configuration cost\nand up to 37% in the optical layer cost. Further savings (up to 9 %) in the\nwavelength use can be obtained with the integrated approach to network\nconfiguration over the sequential one, however, at the increase in the\noptimization problem complexity. These results are based on a cost model with\nactual technology pricing and were obtained for networks targeted to a\nnationwide coverage.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 21 Oct 2007 09:40:38 GMT"
            }
        ],
        "update_date": "2007-10-23",
        "authors_parsed": [
            [
                "Bigos",
                "Wojtek",
                "",
                "IRISA"
            ],
            [
                "Gosselin",
                "St\u00e9phane",
                "",
                "IRISA"
            ],
            [
                "Cousin",
                "Bernard",
                "",
                "IRISA"
            ],
            [
                "Foll",
                "Morgane Le",
                "",
                "IRISA"
            ],
            [
                "Nakajima",
                "Hisao",
                "",
                "IRISA"
            ]
        ]
    },
    {
        "id": "0710.3955",
        "submitter": "Massimiliano Laddomada Ph.D.",
        "authors": "F. Daneshgaran, Massimiliano Laddomada, F. Mesiti, M. Mondin",
        "title": "On the Behavior of the Distributed Coordination Function of IEEE 802.11\n  with Multirate Capability under General Transmission Conditions",
        "comments": "Submitted to IEEE Transactions on Wireless Communications, October\n  21, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": null,
        "abstract": "  The aim of this paper is threefold. First, it presents a multi-dimensional\nMarkovian state transition model characterizing the behavior of the IEEE 802.11\nprotocol at the Medium Access Control layer which accounts for packet\ntransmission failures due to channel errors modeling both saturated and\nnon-saturated traffic conditions. Second, it provides a throughput analysis of\nthe IEEE 802.11 protocol at the data link layer in both saturated and\nnon-saturated traffic conditions taking into account the impact of both the\nphysical propagation channel and multirate transmission in Rayleigh fading\nenvironment. The general traffic model assumed is M/M/1/K. Finally, it shows\nthat the behavior of the throughput in non-saturated traffic conditions is a\nlinear combination of two system parameters; the payload size and the packet\nrates, $\\lambda^{(s)}$, of each contending station. The validity interval of\nthe proposed model is also derived.\n  Simulation results closely match the theoretical derivations, confirming the\neffectiveness of the proposed models.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 21 Oct 2007 22:59:27 GMT"
            }
        ],
        "update_date": "2007-10-23",
        "authors_parsed": [
            [
                "Daneshgaran",
                "F.",
                ""
            ],
            [
                "Laddomada",
                "Massimiliano",
                ""
            ],
            [
                "Mesiti",
                "F.",
                ""
            ],
            [
                "Mondin",
                "M.",
                ""
            ]
        ]
    },
    {
        "id": "0710.3979",
        "submitter": "William Yurcik",
        "authors": "William Yurcik, Clay Woolam, Greg Hellings, Latifur Khan, Bhavani\n  Thuraisingham",
        "title": "Toward Trusted Sharing of Network Packet Traces Using Anonymization:\n  Single-Field Privacy/Analysis Tradeoffs",
        "comments": "8 pages,1 figure, 4 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.NI",
        "license": null,
        "abstract": "  Network data needs to be shared for distributed security analysis.\nAnonymization of network data for sharing sets up a fundamental tradeoff\nbetween privacy protection versus security analysis capability. This\nprivacy/analysis tradeoff has been acknowledged by many researchers but this is\nthe first paper to provide empirical measurements to characterize the\nprivacy/analysis tradeoff for an enterprise dataset. Specifically we perform\nanonymization options on single-fields within network packet traces and then\nmake measurements using intrusion detection system alarms as a proxy for\nsecurity analysis capability. Our results show: (1) two fields have a zero sum\ntradeoff (more privacy lessens security analysis and vice versa) and (2) eight\nfields have a more complex tradeoff (that is not zero sum) in which both\nprivacy and analysis can both be simultaneously accomplished.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 22 Oct 2007 19:18:11 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 26 Oct 2007 14:55:08 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Yurcik",
                "William",
                ""
            ],
            [
                "Woolam",
                "Clay",
                ""
            ],
            [
                "Hellings",
                "Greg",
                ""
            ],
            [
                "Khan",
                "Latifur",
                ""
            ],
            [
                "Thuraisingham",
                "Bhavani",
                ""
            ]
        ]
    },
    {
        "id": "0710.4180",
        "submitter": "Akisato Kimura",
        "authors": "Akisato Kimura, Kunio Kashino, Takayuki Kurozumi, Hiroshi Murase",
        "title": "A quick search method for audio signals based on a piecewise linear\n  representation of feature trajectories",
        "comments": "20 pages, to appear in IEEE Transactions on Audio, Speech and\n  Language Processing",
        "journal-ref": "IEEE Transactions on Audio, Speech and Language Processing,\n  Vol.16, No.2, pp.396-407, February 2008.",
        "doi": "10.1109/TASL.2007.912362",
        "report-no": null,
        "categories": "cs.MM cs.DB",
        "license": null,
        "abstract": "  This paper presents a new method for a quick similarity-based search through\nlong unlabeled audio streams to detect and locate audio clips provided by\nusers. The method involves feature-dimension reduction based on a piecewise\nlinear representation of a sequential feature trajectory extracted from a long\naudio stream. Two techniques enable us to obtain a piecewise linear\nrepresentation: the dynamic segmentation of feature trajectories and the\nsegment-based Karhunen-L\\'{o}eve (KL) transform. The proposed search method\nguarantees the same search results as the search method without the proposed\nfeature-dimension reduction method in principle. Experiment results indicate\nsignificant improvements in search speed. For example the proposed method\nreduced the total search time to approximately 1/12 that of previous methods\nand detected queries in approximately 0.3 seconds from a 200-hour audio\ndatabase.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 23 Oct 2007 03:06:53 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Kimura",
                "Akisato",
                ""
            ],
            [
                "Kashino",
                "Kunio",
                ""
            ],
            [
                "Kurozumi",
                "Takayuki",
                ""
            ],
            [
                "Murase",
                "Hiroshi",
                ""
            ]
        ]
    },
    {
        "id": "0710.4182",
        "submitter": "Roman Ilin",
        "authors": "Roman Ilin, Robert Kozma, Paul J. Werbos",
        "title": "Beyond Feedforward Models Trained by Backpropagation: a Practical\n  Training Tool for a More Efficient Universal Approximator",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  Cellular Simultaneous Recurrent Neural Network (SRN) has been shown to be a\nfunction approximator more powerful than the MLP. This means that the\ncomplexity of MLP would be prohibitively large for some problems while SRN\ncould realize the desired mapping with acceptable computational constraints.\nThe speed of training of complex recurrent networks is crucial to their\nsuccessful application. Present work improves the previous results by training\nthe network with extended Kalman filter (EKF). We implemented a generic\nCellular SRN and applied it for solving two challenging problems: 2D maze\nnavigation and a subset of the connectedness problem. The speed of convergence\nhas been improved by several orders of magnitude in comparison with the earlier\nresults in the case of maze navigation, and superior generalization has been\ndemonstrated in the case of connectedness. The implications of this\nimprovements are discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 23 Oct 2007 03:43:57 GMT"
            }
        ],
        "update_date": "2007-10-24",
        "authors_parsed": [
            [
                "Ilin",
                "Roman",
                ""
            ],
            [
                "Kozma",
                "Robert",
                ""
            ],
            [
                "Werbos",
                "Paul J.",
                ""
            ]
        ]
    },
    {
        "id": "0710.4261",
        "submitter": "Bernard Cousin",
        "authors": "Wojtek Bigos (FT R&D), Bernard Cousin (IRISA), St\\'ephane Gosselin (FT\n  R&D), Morgane Le Foll (FT R&D), Hisao Nakajima (FT R&D)",
        "title": "Survivable MPLS Over Optical Transport Networks: Cost and Resource Usage\n  Analysis",
        "comments": null,
        "journal-ref": "IEEE Journal on Selected Areas in Communications 25, 5 (2007)\n  949-962",
        "doi": "10.1109/JSAC.2007.070608",
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": null,
        "abstract": "  In this paper we study different options for the survivability implementation\nin MPLS over Optical Transport Networks (OTN) in terms of network resource\nusage and configuration cost. We investigate two approaches to the\nsurvivability deployment: single layer and multilayer survivability and present\nvarious methods for spare capacity allocation (SCA) to reroute disrupted\ntraffic. The comparative analysis shows the influence of the offered traffic\ngranularity and the physical network structure on the survivability cost: for\nhigh bandwidth LSPs, close to the optical channel capacity, the multilayer\nsurvivability outperforms the single layer one, whereas for low bandwidth LSPs\nthe single layer survivability is more cost-efficient. On the other hand,\nsparse networks of low connectivity parameter use more wavelengths for optical\npath routing and increase the configuration cost, as compared with dense\nnetworks. We demonstrate that by mapping efficiently the spare capacity of the\nMPLS layer onto the resources of the optical layer one can achieve up to 22%\nsavings in the total configuration cost and up to 37% in the optical layer\ncost. Further savings (up to 9 %) in the wavelength use can be obtained with\nthe integrated approach to network configuration over the sequential one,\nhowever, at the increase in the optimization problem complexity. These results\nare based on a cost model with different cost variations, and were obtained for\nnetworks targeted to a nationwide coverage.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 23 Oct 2007 11:25:44 GMT"
            }
        ],
        "update_date": "2007-10-24",
        "authors_parsed": [
            [
                "Bigos",
                "Wojtek",
                "",
                "FT R&D"
            ],
            [
                "Cousin",
                "Bernard",
                "",
                "IRISA"
            ],
            [
                "Gosselin",
                "St\u00e9phane",
                "",
                "FT\n  R&D"
            ],
            [
                "Foll",
                "Morgane Le",
                "",
                "FT R&D"
            ],
            [
                "Nakajima",
                "Hisao",
                "",
                "FT R&D"
            ]
        ]
    },
    {
        "id": "0710.4318",
        "submitter": "Evelyne Hubert",
        "authors": "Evelyne Hubert",
        "title": "Differential invariants of a Lie group action: syzygies on a generating\n  set",
        "comments": "Journal of Symbolic Computation (2008)",
        "journal-ref": null,
        "doi": "10.1016/j.jsc.2008.08.003",
        "report-no": null,
        "categories": "cs.SC math.DG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a group action, known by its infinitesimal generators, we exhibit a\ncomplete set of syzygies on a generating set of differential invariants. For\nthat we elaborate on the reinterpretation of Cartan's moving frame by Fels and\nOlver (1999). This provides constructive tools for exploring algebras of\ndifferential invariants.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 23 Oct 2007 19:20:10 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 6 Dec 2007 15:49:21 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 4 Sep 2008 15:06:25 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 3 Nov 2008 10:42:30 GMT"
            }
        ],
        "update_date": "2008-11-03",
        "authors_parsed": [
            [
                "Hubert",
                "Evelyne",
                ""
            ]
        ]
    },
    {
        "id": "0710.4486",
        "submitter": "Michel Fliess",
        "authors": "Michel Fliess (INRIA Futurs), C\\'edric Join (INRIA Futurs, CRAN),\n  Hebertt Sira-Ramirez",
        "title": "Non-linear estimation is easy",
        "comments": null,
        "journal-ref": "Int. J. Modelling Identification and Control 4, 1 (2008) 12-27",
        "doi": "10.1504/IJMIC.2008.020996",
        "report-no": null,
        "categories": "cs.CE cs.NA cs.PF math.AC math.NA math.OC",
        "license": null,
        "abstract": "  Non-linear state estimation and some related topics, like parametric\nestimation, fault diagnosis, and perturbation attenuation, are tackled here via\na new methodology in numerical differentiation. The corresponding basic system\ntheoretic definitions and properties are presented within the framework of\ndifferential algebra, which permits to handle system variables and their\nderivatives of any order. Several academic examples and their computer\nsimulations, with on-line estimations, are illustrating our viewpoint.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 24 Oct 2007 14:48:39 GMT"
            }
        ],
        "update_date": "2008-11-06",
        "authors_parsed": [
            [
                "Fliess",
                "Michel",
                "",
                "INRIA Futurs"
            ],
            [
                "Join",
                "C\u00e9dric",
                "",
                "INRIA Futurs, CRAN"
            ],
            [
                "Sira-Ramirez",
                "Hebertt",
                ""
            ]
        ]
    },
    {
        "id": "0710.4486",
        "submitter": "Michel Fliess",
        "authors": "Michel Fliess (INRIA Futurs), C\\'edric Join (INRIA Futurs, CRAN),\n  Hebertt Sira-Ramirez",
        "title": "Non-linear estimation is easy",
        "comments": null,
        "journal-ref": "Int. J. Modelling Identification and Control 4, 1 (2008) 12-27",
        "doi": "10.1504/IJMIC.2008.020996",
        "report-no": null,
        "categories": "cs.CE cs.NA cs.PF math.AC math.NA math.OC",
        "license": null,
        "abstract": "  Non-linear state estimation and some related topics, like parametric\nestimation, fault diagnosis, and perturbation attenuation, are tackled here via\na new methodology in numerical differentiation. The corresponding basic system\ntheoretic definitions and properties are presented within the framework of\ndifferential algebra, which permits to handle system variables and their\nderivatives of any order. Several academic examples and their computer\nsimulations, with on-line estimations, are illustrating our viewpoint.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 24 Oct 2007 14:48:39 GMT"
            }
        ],
        "update_date": "2008-11-06",
        "authors_parsed": [
            [
                "Fliess",
                "Michel",
                "",
                "INRIA Futurs"
            ],
            [
                "Join",
                "C\u00e9dric",
                "",
                "INRIA Futurs, CRAN"
            ],
            [
                "Sira-Ramirez",
                "Hebertt",
                ""
            ]
        ]
    },
    {
        "id": "0710.4508",
        "submitter": "Gregorio Malajovich",
        "authors": "Felipe Cucker, Teresa Krick, Gregorio Malajovich and Mario Wschebor",
        "title": "A Numerical Algorithm for Zero Counting. I: Complexity and Accuracy",
        "comments": "We made minor but necessary improvements in the presentation",
        "journal-ref": "Journal of Complexity 24 Issues 5-6, pp 582-605 (Oct-Dec 2008)",
        "doi": "10.1016/j.jco.2008.03.001",
        "report-no": null,
        "categories": "cs.CC cs.NA cs.SC math.NA",
        "license": null,
        "abstract": "  We describe an algorithm to count the number of distinct real zeros of a\npolynomial (square) system f. The algorithm performs O(n D kappa(f)) iterations\nwhere n is the number of polynomials (as well as the dimension of the ambient\nspace), D is a bound on the polynomials' degree, and kappa(f) is a condition\nnumber for the system. Each iteration uses an exponential number of operations.\nThe algorithm uses finite-precision arithmetic and a polynomial bound for the\nprecision required to ensure the returned output is correct is exhibited. This\nbound is a major feature of our algorithm since it is in contrast with the\nexponential precision required by the existing (symbolic) algorithms for\ncounting real zeros. The algorithm parallelizes well in the sense that each\niteration can be computed in parallel polynomial time with an exponential\nnumber of processors.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 24 Oct 2007 16:33:07 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 19 Mar 2008 20:28:15 GMT"
            }
        ],
        "update_date": "2010-07-12",
        "authors_parsed": [
            [
                "Cucker",
                "Felipe",
                ""
            ],
            [
                "Krick",
                "Teresa",
                ""
            ],
            [
                "Malajovich",
                "Gregorio",
                ""
            ],
            [
                "Wschebor",
                "Mario",
                ""
            ]
        ]
    },
    {
        "id": "0710.4633",
        "submitter": "EDA Publishing Association",
        "authors": "Bharat Sukhwani, Uday Padmanabhan, Janet M. Wang",
        "title": "Nano-Sim: A Step Wise Equivalent Conductance based Statistical Simulator\n  for Nanotechnology Circuit Design",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": "10.1109/DATE.2005.221",
        "report-no": null,
        "categories": "cs.PF",
        "license": null,
        "abstract": "  New nanotechnology based devices are replacing CMOS devices to overcome CMOS\ntechnology's scaling limitations. However, many such devices exhibit\nnon-monotonic I-V characteristics and uncertain properties which lead to the\nnegative differential resistance (NDR) problem and the chaotic performance.\nThis paper proposes a new circuit simulation approach that can effectively\nsimulate nanotechnology devices with uncertain input sources and negative\ndifferential resistance (NDR) problem. The experimental results show a 20-30\ntimes speedup comparing with existing simulators.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 08:07:46 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Sukhwani",
                "Bharat",
                ""
            ],
            [
                "Padmanabhan",
                "Uday",
                ""
            ],
            [
                "Wang",
                "Janet M.",
                ""
            ]
        ]
    },
    {
        "id": "0710.4635",
        "submitter": "EDA Publishing Association",
        "authors": "Tadashi Takeuchi",
        "title": "OS Debugging Method Using a Lightweight Virtual Machine Monitor",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": null,
        "abstract": "  Demands for implementing original OSs that can achieve high I/O performance\non PC/AT compatible hardware have recently been increasing, but conventional OS\ndebugging environments have not been able to simultaneously assure their\nstability, be easily customized to new OSs and new I/O devices, and assure\nefficient execution of I/O operations. We therefore developed a novel OS\ndebugging method using a lightweight virtual machine. We evaluated this\ndebugging method experimentally and confirmed that it can transfer data about\n5.4 times as fast as the conventional virtual machine monitor.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 08:09:07 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Takeuchi",
                "Tadashi",
                ""
            ]
        ]
    },
    {
        "id": "0710.4640",
        "submitter": "EDA Publishing Association",
        "authors": "Ilya Issenin, Nikil Dutt",
        "title": "FORAY-GEN: Automatic Generation of Affine Functions for Memory\n  Optimizations",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  In today's embedded applications a significant portion of energy is spent in\nthe memory subsystem. Several approaches have been proposed to minimize this\nenergy, including the use of scratch pad memories, with many based on static\nanalysis of a program. However, often it is not possible to perform static\nanalysis and optimization of a program's memory access behavior unless the\nprogram is specifically written for this purpose. In this paper we introduce\nthe FORAY model of a program that permits aggressive analysis of the\napplication's memory behavior that further enables such optimizations since it\nconsists of 'for' loops and array accesses which are easily analyzable. We\npresent FORAY-GEN: an automated profile-based approach for extraction of the\nFORAY model from the original program. We also demonstrate how FORAY-GEN\nenhances applicability of other memory subsystem optimization approaches,\nresulting in an average of two times increase in the number of memory\nreferences that can be analyzed by existing static approaches.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 08:11:20 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Issenin",
                "Ilya",
                ""
            ],
            [
                "Dutt",
                "Nikil",
                ""
            ]
        ]
    },
    {
        "id": "0710.4641",
        "submitter": "EDA Publishing Association",
        "authors": "Tim Schattkowsky",
        "title": "UML 2.0 - Overview and Perspectives in SoC Design",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  The design productivity gap requires more efficient design methods. Software\nsystems have faced the same challenge and seem to have mastered it with the\nintroduction of more abstract design methods. The UML has become the standard\nfor software systems modeling and thus the foundation of new design methods.\nAlthough the UML is defined as a general purpose modeling language, its\napplication to hardware and hardware/software codesign is very limited. In\norder to successfully apply the UML at these fields, it is essential to\nunderstand its capabilities and to map it to a new domain.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 08:11:39 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Schattkowsky",
                "Tim",
                ""
            ]
        ]
    },
    {
        "id": "0710.4643",
        "submitter": "EDA Publishing Association",
        "authors": "Mehrdad Reshadi, Nikil Dutt",
        "title": "Generic Pipelined Processor Modeling and High Performance Cycle-Accurate\n  Simulator Generation",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": "10.1109/DATE.2005.166",
        "report-no": null,
        "categories": "cs.AR cs.PF",
        "license": null,
        "abstract": "  Detailed modeling of processors and high performance cycle-accurate\nsimulators are essential for today's hardware and software design. These\nproblems are challenging enough by themselves and have seen many previous\nresearch efforts. Addressing both simultaneously is even more challenging, with\nmany existing approaches focusing on one over another. In this paper, we\npropose the Reduced Colored Petri Net (RCPN) model that has two advantages:\nfirst, it offers a very simple and intuitive way of modeling pipelined\nprocessors; second, it can generate high performance cycle-accurate simulators.\nRCPN benefits from all the useful features of Colored Petri Nets without\nsuffering from their exponential growth in complexity. RCPN processor models\nare very intuitive since they are a mirror image of the processor pipeline\nblock diagram. Furthermore, in our experiments on the generated cycle-accurate\nsimulators for XScale and StrongArm processor models, we achieved an order of\nmagnitude (~15 times) speedup over the popular SimpleScalar ARM simulator.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 08:14:40 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Reshadi",
                "Mehrdad",
                ""
            ],
            [
                "Dutt",
                "Nikil",
                ""
            ]
        ]
    },
    {
        "id": "0710.4682",
        "submitter": "EDA Publishing Association",
        "authors": "Ian Oliver",
        "title": "Applying UML and MDA to Real Systems Design",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  Traditionally system design has been made from a black box/functionality only\nperspective which forces the developer to concentrate on how the functionality\ncan be decomposed and recomposed into so called components. While this\ntechnique is well established and well known it does suffer fromsome drawbacks;\nnamely that the systems produced can often be forced into certain, incompatible\narchitectures, difficult to maintain or reuse and the code itself difficult to\ndebug. Now that ideas such as the OMG's Model Based Architecture (MDA) or Model\nBased Engineering (MBE) and the ubiquitous modelling language UML are being\nused (allegedly) and desired we face a number of challenges to existing\ntechniques.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 09:07:10 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Oliver",
                "Ian",
                ""
            ]
        ]
    },
    {
        "id": "0710.4683",
        "submitter": "EDA Publishing Association",
        "authors": "Stephen A. Edwards",
        "title": "The Challenges of Hardware Synthesis from C-Like Languages",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  MANY TECHNIQUES for synthesizing digital hardware from C-like languages have\nbeen proposed, but none have emerged as successful as Verilog or VHDL for\nregister-transfer-level design. This paper looks at two of the fundamental\nchallenges: concurrency and timing control.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 09:07:39 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Edwards",
                "Stephen A.",
                ""
            ]
        ]
    },
    {
        "id": "0710.4700",
        "submitter": "EDA Publishing Association",
        "authors": "Greg Stitt, Frank Vahid",
        "title": "A Decompilation Approach to Partitioning Software for\n  Microprocessor/FPGA Platforms",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  In this paper, we present a software compilation approach for\nmicroprocessor/FPGA platforms that partitions a software binary onto custom\nhardware implemented in the FPGA. Our approach imposes less restrictions on\nsoftware tool flow than previous compiler approaches, allowing software\ndesigners to use any software language and compiler. Our approach uses a\nback-end partitioning tool that utilizes decompilation techniques to recover\nimportant high-level information, resulting in performance comparable to\nhigh-level compiler-based approaches.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 09:22:50 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Stitt",
                "Greg",
                ""
            ],
            [
                "Vahid",
                "Frank",
                ""
            ]
        ]
    },
    {
        "id": "0710.4701",
        "submitter": "EDA Publishing Association",
        "authors": "Jae-Gon Lee, Moo-Kyoung Chung, Ki-Yong Ahn, Sang-Heon Lee, Chong-Min\n  Kyung",
        "title": "A Prediction Packetizing Scheme for Reducing Channel Traffic in\n  Transaction-Level Hardware/Software Co-Emulation",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PF",
        "license": null,
        "abstract": "  This paper presents a scheme for efficient channel usage between simulator\nand accelerator where the accelerator models some RTL sub-blocks in the\naccelerator-based hardware/software co-simulation while the simulator runs\ntransaction-level model of the remaining part of the whole chip being verified.\nWith conventional simulation accelerator, evaluations of simulator and\naccelerator alternate at every valid simulation time, which results in poor\nsimulation performance due to startup overhead of simulator-accelerator channel\naccess. The startup overhead can be reduced by merging multiple transactions on\nthe channel into a single burst traffic. We propose a predictive packetizing\nscheme for reducing channel traffic by merging as many transactions into a\nburst traffic as possible based on 'prediction and rollback.' Under ideal\ncondition with 100% prediction accuracy, the proposed method shows a\nperformance gain of 1500% compared to the conventional one.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 09:27:03 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Lee",
                "Jae-Gon",
                ""
            ],
            [
                "Chung",
                "Moo-Kyoung",
                ""
            ],
            [
                "Ahn",
                "Ki-Yong",
                ""
            ],
            [
                "Lee",
                "Sang-Heon",
                ""
            ],
            [
                "Kyung",
                "Chong-Min",
                ""
            ]
        ]
    },
    {
        "id": "0710.4702",
        "submitter": "EDA Publishing Association",
        "authors": "Nastaran Baradaran, Pedro C. Diniz",
        "title": "A Register Allocation Algorithm in the Presence of Scalar Replacement\n  for Fine-Grain Configurable Architectures",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  The aggressive application of scalar replacement to array references\nsubstantially reduces the number of memory operations at the expense of a\npossibly very large number of registers. In this paper we describe a register\nallocation algorithm that assigns registers to scalar replaced array references\nalong the critical paths of a computation, in many cases exploiting the\nopportunity for concurrent memory accesses. Experimental results, for a set of\nimage/signal processing code kernels, reveal that the proposed algorithm leads\nto a substantial reduction of the number of execution cycles for the\ncorresponding hardware implementation on a contemporary\nField-Programmable-Gate-Array (FPGA) when compared to other greedy allocation\nalgorithms, in some cases, using even fewer number of registers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 09:27:20 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Baradaran",
                "Nastaran",
                ""
            ],
            [
                "Diniz",
                "Pedro C.",
                ""
            ]
        ]
    },
    {
        "id": "0710.4723",
        "submitter": "EDA Publishing Association",
        "authors": "C. Soens, G. Van Der Plas, P. Wambacq, S. Donnay",
        "title": "Simulation Methodology for Analysis of Substrate Noise Impact on Analog\n  / RF Circuits Including Interconnect Resistance",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PF",
        "license": null,
        "abstract": "  This paper reports a novel simulation methodology for analysis and prediction\nof substrate noise impact on analog / RF circuits taking into account the role\nof the parasitic resistance of the on-chip interconnect in the impact\nmechanism. This methodology allows investigation of the role of the separate\ndevices (also parasitic devices) in the analog / RF circuit in the overall\nimpact. This way is revealed which devices have to be taken care of (shielding,\ntopology change) to protect the circuit against substrate noise. The developed\nmethodology is used to analyze impact of substrate noise on a 3 GHz LC-tank\nVoltage Controlled Oscillator (VCO) designed in a high-ohmic 0.18 $\\mu$m 1PM6\nCMOS technology. For this VCO (in the investigated frequency range from DC to\n15 MHz) impact is mainly caused by resistive coupling of noise from the\nsubstrate to the non-ideal on-chip ground interconnect, resulting in analog\nground bounce and frequency modulation. Hence, the presented test-case reveals\nthe important role of the on-chip interconnect in the phenomenon of substrate\nnoise impact.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 09:37:18 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Soens",
                "C.",
                ""
            ],
            [
                "Van Der Plas",
                "G.",
                ""
            ],
            [
                "Wambacq",
                "P.",
                ""
            ],
            [
                "Donnay",
                "S.",
                ""
            ]
        ]
    },
    {
        "id": "0710.4725",
        "submitter": "EDA Publishing Association",
        "authors": "Carlos Eduardo Savioli, Claudio C. Czendrodi, Jose Vicente Calvano,\n  Antonio Carneiro De Mesquita Filho",
        "title": "Fault-Trajectory Approach for Fault Diagnosis on Analog Circuits",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  This issue discusses the fault-trajectory approach suitability for fault\ndiagnosis on analog networks. Recent works have shown promising results\nconcerning a method based on this concept for ATPG for diagnosing faults on\nanalog networks. Such method relies on evolutionary techniques, where a generic\nalgorithm (GA) is coded to generate a set of optimum frequencies capable to\ndisclose faults.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 09:37:48 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Savioli",
                "Carlos Eduardo",
                ""
            ],
            [
                "Czendrodi",
                "Claudio C.",
                ""
            ],
            [
                "Calvano",
                "Jose Vicente",
                ""
            ],
            [
                "Filho",
                "Antonio Carneiro De Mesquita",
                ""
            ]
        ]
    },
    {
        "id": "0710.4734",
        "submitter": "EDA Publishing Association",
        "authors": "Eric Liau, Doris Schmitt-Landsiedel",
        "title": "Computational Intelligence Characterization Method of Semiconductor\n  Device",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.NE",
        "license": null,
        "abstract": "  Characterization of semiconductor devices is used to gather as much data\nabout the device as possible to determine weaknesses in design or trends in the\nmanufacturing process. In this paper, we propose a novel multiple trip point\ncharacterization concept to overcome the constraint of single trip point\nconcept in device characterization phase. In addition, we use computational\nintelligence techniques (e.g. neural network, fuzzy and genetic algorithm) to\nfurther manipulate these sets of multiple trip point values and tests based on\nsemiconductor test equipments, Our experimental results demonstrate an\nexcellent design parameter variation analysis in device characterization phase,\nas well as detection of a set of worst case tests that can provoke the worst\ncase variation, while traditional approach was not capable of detecting them.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 09:41:43 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Liau",
                "Eric",
                ""
            ],
            [
                "Schmitt-Landsiedel",
                "Doris",
                ""
            ]
        ]
    },
    {
        "id": "0710.4746",
        "submitter": "EDA Publishing Association",
        "authors": "M. Abdelsalam Hassan, Keishi Sakanushi, Yoshinori Takeuchi, Masaharu\n  Imai",
        "title": "RTK-Spec TRON: A Simulation Model of an ITRON Based RTOS Kernel in\n  SystemC",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": null,
        "abstract": "  This paper presents the methodology and the modeling constructs we have\ndeveloped to capture the real time aspects of RTOS simulation models in a\nSystem Level Design Language (SLDL) like SystemC. We describe these constructs\nand show how they are used to build a simulation model of an RTOS kernel\ntargeting the $\\mu$-ITRON OS specification standard.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 09:47:35 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Hassan",
                "M. Abdelsalam",
                ""
            ],
            [
                "Sakanushi",
                "Keishi",
                ""
            ],
            [
                "Takeuchi",
                "Yoshinori",
                ""
            ],
            [
                "Imai",
                "Masaharu",
                ""
            ]
        ]
    },
    {
        "id": "0710.4755",
        "submitter": "EDA Publishing Association",
        "authors": "Fernando Rincon, Francisco Moya, Jesus Barba, Juan Carlos Lopez",
        "title": "Model Reuse through Hardware Design Patterns",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  Increasing reuse opportunities is a well-known problem for software designers\nas well as for hardware designers. Nonetheless, current software and hardware\nengineering practices have embraced different approaches to this problem.\nSoftware designs are usually modelled after a set of proven solutions to\nrecurrent problems called design patterns. This approach differs from the\ncomponent-based reuse usually found in hardware designs: design patterns do not\nspecify unnecessary implementation details. Several authors have already\nproposed translating structural design patterns concepts to hardware design. In\nthis paper we extend the discussion to behavioural design patterns.\nSpecifically, we describe how the hardware version of the Iterator can be used\nto enhance model reuse.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 09:53:16 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Rincon",
                "Fernando",
                ""
            ],
            [
                "Moya",
                "Francisco",
                ""
            ],
            [
                "Barba",
                "Jesus",
                ""
            ],
            [
                "Lopez",
                "Juan Carlos",
                ""
            ]
        ]
    },
    {
        "id": "0710.4756",
        "submitter": "EDA Publishing Association",
        "authors": "Kris Tiri, Ingrid Verbauwhede",
        "title": "Design Method for Constant Power Consumption of Differential Logic\n  Circuits",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  Side channel attacks are a major security concern for smart cards and other\nembedded devices. They analyze the variations on the power consumption to find\nthe secret key of the encryption algorithm implemented within the security IC.\nTo address this issue, logic gates that have a constant power dissipation\nindependent of the input signals, are used in security ICs. This paper presents\na design methodology to create fully connected differential pull down networks.\nFully connected differential pull down networks are transistor networks that\nfor any complementary input combination connect all the internal nodes of the\nnetwork to one of the external nodes of the network. They are memoryless and\nfor that reason have a constant load capacitance and power consumption. This\ntype of networks is used in specialized logic gates to guarantee a constant\ncontribution of the internal nodes into the total power consumption of the\nlogic gate.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 09:53:18 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Tiri",
                "Kris",
                ""
            ],
            [
                "Verbauwhede",
                "Ingrid",
                ""
            ]
        ]
    },
    {
        "id": "0710.4780",
        "submitter": "Jesus M. Almendros-Jimenez Dr.",
        "authors": "J. M. Almendros-Jim\\'enez and A. Becerra-Ter\\'on and F. J.\n  Enciso-Ba\\~nos",
        "title": "Querying XML Documents in Logic Programming",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.DB",
        "license": null,
        "abstract": "  Extensible Markup Language (XML) is a simple, very flexible text format\nderived from SGML. Originally designed to meet the challenges of large-scale\nelectronic publishing, XML is also playing an increasingly important role in\nthe exchange of a wide variety of data on the Web and elsewhere. XPath language\nis the result of an effort to provide address parts of an XML document. In\nsupport of this primary purpose, it becomes in a query language against an XML\ndocument. In this paper we present a proposal for the implementation of the\nXPath language in logic programming. With this aim we will describe the\nrepresentation of XML documents by means of a logic program. Rules and facts\ncan be used for representing the document schema and the XML document itself.\nIn particular, we will present how to index XML documents in logic programs:\nrules are supposed to be stored in main memory, however facts are stored in\nsecondary memory by using two kind of indexes: one for each XML tag, and other\nfor each group of terminal items. In addition, we will study how to query by\nmeans of the XPath language against a logic program representing an XML\ndocument. It evolves the specialization of the logic program with regard to the\nXPath expression. Finally, we will also explain how to combine the indexing and\nthe top-down evaluation of the logic program. To appear in Theory and Practice\nof Logic Programming (TPLP)\"\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 10:45:08 GMT"
            }
        ],
        "update_date": "2007-10-26",
        "authors_parsed": [
            [
                "Almendros-Jim\u00e9nez",
                "J. M.",
                ""
            ],
            [
                "Becerra-Ter\u00f3n",
                "A.",
                ""
            ],
            [
                "Enciso-Ba\u00f1os",
                "F. J.",
                ""
            ]
        ]
    },
    {
        "id": "0710.4780",
        "submitter": "Jesus M. Almendros-Jimenez Dr.",
        "authors": "J. M. Almendros-Jim\\'enez and A. Becerra-Ter\\'on and F. J.\n  Enciso-Ba\\~nos",
        "title": "Querying XML Documents in Logic Programming",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.DB",
        "license": null,
        "abstract": "  Extensible Markup Language (XML) is a simple, very flexible text format\nderived from SGML. Originally designed to meet the challenges of large-scale\nelectronic publishing, XML is also playing an increasingly important role in\nthe exchange of a wide variety of data on the Web and elsewhere. XPath language\nis the result of an effort to provide address parts of an XML document. In\nsupport of this primary purpose, it becomes in a query language against an XML\ndocument. In this paper we present a proposal for the implementation of the\nXPath language in logic programming. With this aim we will describe the\nrepresentation of XML documents by means of a logic program. Rules and facts\ncan be used for representing the document schema and the XML document itself.\nIn particular, we will present how to index XML documents in logic programs:\nrules are supposed to be stored in main memory, however facts are stored in\nsecondary memory by using two kind of indexes: one for each XML tag, and other\nfor each group of terminal items. In addition, we will study how to query by\nmeans of the XPath language against a logic program representing an XML\ndocument. It evolves the specialization of the logic program with regard to the\nXPath expression. Finally, we will also explain how to combine the indexing and\nthe top-down evaluation of the logic program. To appear in Theory and Practice\nof Logic Programming (TPLP)\"\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 10:45:08 GMT"
            }
        ],
        "update_date": "2007-10-26",
        "authors_parsed": [
            [
                "Almendros-Jim\u00e9nez",
                "J. M.",
                ""
            ],
            [
                "Becerra-Ter\u00f3n",
                "A.",
                ""
            ],
            [
                "Enciso-Ba\u00f1os",
                "F. J.",
                ""
            ]
        ]
    },
    {
        "id": "0710.4793",
        "submitter": "EDA Publishing Association",
        "authors": "He Hai, Zhong Yi-Fang, Cai Chi-Lan",
        "title": "Unified Modeling of Complex Real-Time Control Systems",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  Complex real-time control system is a software dense and algorithms dense\nsystem, which needs modern software engineering techniques to design. UML is an\nobject-oriented industrial standard modeling language, used more and more in\nreal-time domain. This paper first analyses the advantages and problems of\nusing UML for real-time control systems design. Then, it proposes an extension\nof UML-RT to support time-continuous subsystems modeling. So we can unify\nmodeling of complex real-time control systems on UML-RT platform, from\nrequirement analysis, model design, simulation, until generation code.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 11:50:54 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Hai",
                "He",
                ""
            ],
            [
                "Yi-Fang",
                "Zhong",
                ""
            ],
            [
                "Chi-Lan",
                "Cai",
                ""
            ]
        ]
    },
    {
        "id": "0710.4800",
        "submitter": "EDA Publishing Association",
        "authors": "Hala A. Farouk, Magdy Saeb",
        "title": "An Improved FPGA Implementation of the Modified Hybrid Hiding Encryption\n  Algorithm (MHHEA) for Data Communication Security",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe | Designers'Forum -\n  DATE'05, Munich : Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  The hybrid hiding encryption algorithm, as its name implies, embraces\nconcepts from both steganography and cryptography. In this exertion, an\nimproved micro-architecture Field Programmable Gate Array (FPGA) implementation\nof this algorithm is presented. This design overcomes the observed limitations\nof a previously-designed micro-architecture. These observed limitations are: no\nexploitation of the possibility of parallel bit replacement, and the fact that\nthe input plaintext was encrypted serially, which caused a dependency between\nthe throughput and the nature of the used secret key. This dependency can be\nviewed by some as vulnerability in the security of the implemented\nmicro-architecture. The proposed modified micro-architecture is constructed\nusing five basic modules. These modules are; the message cache, the message\nalignment module, the key cache, the comparator, and at last the encryption\nmodule. In this work, we provide comprehensive simulation and implementation\nresults. These are: the timing diagrams, the post-implementation timing and\nrouting reports, and finally the floor plan. Moreover, a detailed comparison\nwith other FPGA implementations is made available and discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 11:55:19 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Farouk",
                "Hala A.",
                ""
            ],
            [
                "Saeb",
                "Magdy",
                ""
            ]
        ]
    },
    {
        "id": "0710.4803",
        "submitter": "EDA Publishing Association",
        "authors": "R. Elbaz, L. Torres (LIRMM), G. Sassatelli (LIRMM), P. Guillemin, C.\n  Anguille, M. Bardouillet, C. Buatois, J. B. Rigaud",
        "title": "Hardware Engines for Bus Encryption: A Survey of Existing Techniques",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe | Designers'Forum -\n  DATE'05, Munich : Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  The widening spectrum of applications and services provided by portable and\nembedded devices bring a new dimension of concerns in security. Most of those\nembedded systems (pay-TV, PDAs, mobile phones, etc...) make use of external\nmemory. As a result, the main problem is that data and instructions are\nconstantly exchanged between memory (RAM) and CPU in clear form on the bus.\nThis memory may contain confidential data like commercial software or private\ncontents, which either the end-user or the content provider is willing to\nprotect. The goal of this paper is to clearly describe the problem of\nprocessor-memory bus communications in this regard and the existing techniques\napplied to secure the communication channel through encryption - Performance\noverheads implied by those solutions will be extensively discussed in this\npaper.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 11:57:22 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Elbaz",
                "R.",
                "",
                "LIRMM"
            ],
            [
                "Torres",
                "L.",
                "",
                "LIRMM"
            ],
            [
                "Sassatelli",
                "G.",
                "",
                "LIRMM"
            ],
            [
                "Guillemin",
                "P.",
                ""
            ],
            [
                "Anguille",
                "C.",
                ""
            ],
            [
                "Bardouillet",
                "M.",
                ""
            ],
            [
                "Buatois",
                "C.",
                ""
            ],
            [
                "Rigaud",
                "J. B.",
                ""
            ]
        ]
    },
    {
        "id": "0710.4807",
        "submitter": "EDA Publishing Association",
        "authors": "G. Chen, M. Kandemir, M. Karakoy",
        "title": "A Constraint Network Based Approach to Memory Layout Optimization",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe - DATE'05, Munich :\n  Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  While loop restructuring based code optimization for array intensive\napplications has been successful in the past, it has several problems such as\nthe requirement of checking dependences (legality issues) and transformation of\nall of the array references within the loop body indiscriminately (while some\nof the references can benefit from the transformation, others may not). As a\nresult, data transformations, i.e., transformations that modify memory layout\nof array data instead of loop structure have been proposed. One of the problems\nassociated with data transformations is the difficulty of selecting a memory\nlayout for an array that is acceptable to the entire program (not just to a\nsingle loop). In this paper, we formulate the problem of determining the memory\nlayouts of arrays as a constraint network, and explore several methods of\nsolution in a systematic way. Our experiments provide strong support in favor\nof employing constraint processing, and point out future research directions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 11:59:01 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Chen",
                "G.",
                ""
            ],
            [
                "Kandemir",
                "M.",
                ""
            ],
            [
                "Karakoy",
                "M.",
                ""
            ]
        ]
    },
    {
        "id": "0710.4810",
        "submitter": "EDA Publishing Association",
        "authors": "Zoya Dyka, Peter Langendoerfer",
        "title": "Area Efficient Hardware Implementation of Elliptic Curve Cryptography by\n  Iteratively Applying Karatsuba's Method",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe | Designers'Forum -\n  DATE'05, Munich : Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  Securing communication channels is especially needed in wireless\nenvironments. But applying cipher mechanisms in software is limited by the\ncalculation and energy resources of the mobile devices. If hardware is applied\nto realize cryptographic operations cost becomes an issue. In this paper we\ndescribe an approach which tackles all these three points. We implemented a\nhardware accelerator for polynomial multiplication in extended Galois fields\n(GF) applying Karatsuba's method iteratively. With this approach the area\nconsumption is reduced to 2.1 mm^2 in comparison to. 6.2 mm^2 for the standard\napplication of Karatsuba's method i.e. for recursive application. Our approach\nalso reduces the energy consumption to 60 per cent of the original approach.\nThe price we have to pay for these achievement is the increased execution time.\nIn our implementation a polynomial multiplication takes 3 clock cycles whereas\nthe recurisve Karatsuba approach needs only one clock cycle. But considering\narea, energy and calculation speed we are convinced that the benefits of our\napproach outweigh its drawback.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 12:00:04 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Dyka",
                "Zoya",
                ""
            ],
            [
                "Langendoerfer",
                "Peter",
                ""
            ]
        ]
    },
    {
        "id": "0710.4817",
        "submitter": "EDA Publishing Association",
        "authors": "Daniel Thull, Roberto Sannino",
        "title": "Performance Considerations for an Embedded Implementation of OMA DRM 2",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe | Designers'Forum -\n  DATE'05, Munich : Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  As digital content services gain importance in the mobile world, Digital\nRights Management (DRM) applications will become a key component of mobile\nterminals. This paper examines the effect dedicated hardware macros for\nspecific cryptographic functions have on the performance of a mobile terminal\nthat supports version 2 of the open standard for Digital Rights Management\ndefined by the Open Mobile Alliance (OMA). Following a general description of\nthe standard, the paper contains a detailed analysis of the cryptographic\noperations that have to be carried out before protected content can be\naccessed. The combination of this analysis with data on execution times for\nspecific algorithms realized in hardware and software has made it possible to\nbuild a model which has allowed us to assert that hardware acceleration for\nspecific cryptographic algorithms can significantly reduce the impact DRM has\non a mobile terminal's processing performance and battery life.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 12:02:24 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Thull",
                "Daniel",
                ""
            ],
            [
                "Sannino",
                "Roberto",
                ""
            ]
        ]
    },
    {
        "id": "0710.4828",
        "submitter": "Hossein Hajiabolhassan",
        "authors": "Hossein Hajiabolhassan and Abbas Cheraghi",
        "title": "Bounds for Visual Cryptography Schemes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we investigate the best pixel expansion of the various models\nof visual cryptography schemes. In this regard, we consider visual cryptography\nschemes introduced by Tzeng and Hu [13]. In such a model, only minimal\nqualified sets can recover the secret image and that the recovered secret image\ncan be darker or lighter than the background. Blundo et al. [4] introduced a\nlower bound for the best pixel expansion of this scheme in terms of minimal\nqualified sets. We present another lower bound for the best pixel expansion of\nthe scheme. As a corollary, we introduce a lower bound, based on an induced\nmatching of hypergraph of qualified sets, for the best pixel expansion of the\naforementioned model and the traditional model of visual cryptography realized\nby basis matrices. Finally, we study access structures based on graphs and we\npresent an upper bound for the smallest pixel expansion in terms of strong\nchromatic index.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 12:17:15 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 20 Oct 2008 01:49:21 GMT"
            },
            {
                "version": "v3",
                "created": "Sat, 6 Jun 2009 07:08:14 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 7 Sep 2009 03:55:32 GMT"
            },
            {
                "version": "v5",
                "created": "Thu, 3 Dec 2009 11:57:02 GMT"
            }
        ],
        "update_date": "2009-12-03",
        "authors_parsed": [
            [
                "Hajiabolhassan",
                "Hossein",
                ""
            ],
            [
                "Cheraghi",
                "Abbas",
                ""
            ]
        ]
    },
    {
        "id": "0710.4829",
        "submitter": "EDA Publishing Association",
        "authors": "Dirk Ziegenbein, Peter Braun, Ulrich Freund, Andreas Bauer, Jan\n  Romberg, Bernhard Schatz",
        "title": "AutoMoDe - Model-Based Development of Automotive Software",
        "comments": "Submitted on behalf of EDAA (http://www.edaa.com/)",
        "journal-ref": "Dans Design, Automation and Test in Europe | Designers'Forum -\n  DATE'05, Munich : Allemagne (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  This paper describes first results from the AutoMoDe (Automotive Model-Based\nDevelopment) project. The overall goal of the project is to develop an\nintegrated methodology for model-based development of automotive control\nsoftware, based on problem-specific design notations with an explicit formal\nfoundation. Based on the existing AutoFOCUS framework, a tool prototype is\nbeing developed in order to illustrate and validate the key elements of our\napproach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Oct 2007 12:08:52 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Ziegenbein",
                "Dirk",
                ""
            ],
            [
                "Braun",
                "Peter",
                ""
            ],
            [
                "Freund",
                "Ulrich",
                ""
            ],
            [
                "Bauer",
                "Andreas",
                ""
            ],
            [
                "Romberg",
                "Jan",
                ""
            ],
            [
                "Schatz",
                "Bernhard",
                ""
            ]
        ]
    },
    {
        "id": "0710.4999",
        "submitter": "Willemien Visser",
        "authors": "Willemien Visser (INRIA Rocquencourt)",
        "title": "L'analyse de l'expertise du point de vue de l'ergonomie cognitive",
        "comments": null,
        "journal-ref": "Dans Les expertises sensorielles : Nature et acquisition (2006)\n  1-12",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  This paper presents a review of methods for collecting and analysing data on\ncomplex activities. Starting with methods developed for design, we examine the\npossibility to transpose them to other complex activities, especially\nactivities referring to sensorial expertise. R\\'esum\\'e Ce texte pr\\'esente une\nrevue de m\\'ethodes pour recueillir et analyser des donn\\'ees sur des\nactvit\\'es complexes. A partir de m\\'ethodes d\\'evelopp\\'ees pour des\nactvit\\'es de conception, nous examinons la possibilit\\'e de les transposer \\`a\nd'autres actvit\\'es complexes, notamment des actvit\\'es faisant \\`a appel \\`a\ndes expertises sensorielles.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 26 Oct 2007 06:32:22 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 8 Nov 2007 08:56:00 GMT"
            }
        ],
        "update_date": "2007-11-08",
        "authors_parsed": [
            [
                "Visser",
                "Willemien",
                "",
                "INRIA Rocquencourt"
            ]
        ]
    },
    {
        "id": "0710.5002",
        "submitter": "Boris Skoric",
        "authors": "B. Skoric",
        "title": "The entropy of keys derived from laser speckle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.CV",
        "license": null,
        "abstract": "  Laser speckle has been proposed in a number of papers as a high-entropy\nsource of unpredictable bits for use in security applications. Bit strings\nderived from speckle can be used for a variety of security purposes such as\nidentification, authentication, anti-counterfeiting, secure key storage, random\nnumber generation and tamper protection. The choice of laser speckle as a\nsource of random keys is quite natural, given the chaotic properties of\nspeckle. However, this same chaotic behaviour also causes reproducibility\nproblems. Cryptographic protocols require either zero noise or very low noise\nin their inputs; hence the issue of error rates is critical to applications of\nlaser speckle in cryptography. Most of the literature uses an error reduction\nmethod based on Gabor filtering. Though the method is successful, it has not\nbeen thoroughly analysed.\n  In this paper we present a statistical analysis of Gabor-filtered speckle\npatterns. We introduce a model in which perturbations are described as random\nphase changes in the source plane. Using this model we compute the second and\nfourth order statistics of Gabor coefficients. We determine the mutual\ninformation between perturbed and unperturbed Gabor coefficients and the bit\nerror rate in the derived bit string. The mutual information provides an\nabsolute upper bound on the number of secure bits that can be reproducibly\nextracted from noisy measurements.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 26 Oct 2007 06:56:23 GMT"
            }
        ],
        "update_date": "2007-10-29",
        "authors_parsed": [
            [
                "Skoric",
                "B.",
                ""
            ]
        ]
    },
    {
        "id": "0710.5002",
        "submitter": "Boris Skoric",
        "authors": "B. Skoric",
        "title": "The entropy of keys derived from laser speckle",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.CV",
        "license": null,
        "abstract": "  Laser speckle has been proposed in a number of papers as a high-entropy\nsource of unpredictable bits for use in security applications. Bit strings\nderived from speckle can be used for a variety of security purposes such as\nidentification, authentication, anti-counterfeiting, secure key storage, random\nnumber generation and tamper protection. The choice of laser speckle as a\nsource of random keys is quite natural, given the chaotic properties of\nspeckle. However, this same chaotic behaviour also causes reproducibility\nproblems. Cryptographic protocols require either zero noise or very low noise\nin their inputs; hence the issue of error rates is critical to applications of\nlaser speckle in cryptography. Most of the literature uses an error reduction\nmethod based on Gabor filtering. Though the method is successful, it has not\nbeen thoroughly analysed.\n  In this paper we present a statistical analysis of Gabor-filtered speckle\npatterns. We introduce a model in which perturbations are described as random\nphase changes in the source plane. Using this model we compute the second and\nfourth order statistics of Gabor coefficients. We determine the mutual\ninformation between perturbed and unperturbed Gabor coefficients and the bit\nerror rate in the derived bit string. The mutual information provides an\nabsolute upper bound on the number of secure bits that can be reproducibly\nextracted from noisy measurements.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 26 Oct 2007 06:56:23 GMT"
            }
        ],
        "update_date": "2007-10-29",
        "authors_parsed": [
            [
                "Skoric",
                "B.",
                ""
            ]
        ]
    },
    {
        "id": "0710.5006",
        "submitter": "Paul Gardner-Stephen",
        "authors": "Paul Gardner-Stephen",
        "title": "CANE: The Content Addressed Network Environment",
        "comments": "18 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.CR cs.DC",
        "license": null,
        "abstract": "  The fragmented nature and asymmetry of local and remote file access and\nnetwork access, combined with the current lack of robust authenticity and\nprivacy, hamstrings the current internet. The collection of disjoint and often\nad-hoc technologies currently in use are at least partially responsible for the\nmagnitude and potency of the plagues besetting the information economy, of\nwhich spam and email borne virii are canonical examples. The proposed\nreplacement for the internet, Internet Protocol Version 6 (IPv6), does little\nto tackle these underlying issues, instead concentrating on addressing the\ntechnical issues of a decade ago.\n  This paper introduces CANE, a Content Addressed Network Environment, and\ncompares it against current internet and related technologies. Specifically,\nCANE presents a simple computing environment in which location is abstracted\naway in favour of identity, and trust is explicitly defined. Identity is\ncryptographically verified and yet remains pervasively open in nature. It is\nargued that this approach is capable of being generalised such that file\nstorage and network access can be unified and subsequently combined with human\ninterfaces to result in a Unified Theory of Access, which addresses many of the\nsignificant problems besetting the internet community of the early 21st\ncentury.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 26 Oct 2007 07:20:13 GMT"
            }
        ],
        "update_date": "2007-10-29",
        "authors_parsed": [
            [
                "Gardner-Stephen",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0710.5006",
        "submitter": "Paul Gardner-Stephen",
        "authors": "Paul Gardner-Stephen",
        "title": "CANE: The Content Addressed Network Environment",
        "comments": "18 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.CR cs.DC",
        "license": null,
        "abstract": "  The fragmented nature and asymmetry of local and remote file access and\nnetwork access, combined with the current lack of robust authenticity and\nprivacy, hamstrings the current internet. The collection of disjoint and often\nad-hoc technologies currently in use are at least partially responsible for the\nmagnitude and potency of the plagues besetting the information economy, of\nwhich spam and email borne virii are canonical examples. The proposed\nreplacement for the internet, Internet Protocol Version 6 (IPv6), does little\nto tackle these underlying issues, instead concentrating on addressing the\ntechnical issues of a decade ago.\n  This paper introduces CANE, a Content Addressed Network Environment, and\ncompares it against current internet and related technologies. Specifically,\nCANE presents a simple computing environment in which location is abstracted\naway in favour of identity, and trust is explicitly defined. Identity is\ncryptographically verified and yet remains pervasively open in nature. It is\nargued that this approach is capable of being generalised such that file\nstorage and network access can be unified and subsequently combined with human\ninterfaces to result in a Unified Theory of Access, which addresses many of the\nsignificant problems besetting the internet community of the early 21st\ncentury.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 26 Oct 2007 07:20:13 GMT"
            }
        ],
        "update_date": "2007-10-29",
        "authors_parsed": [
            [
                "Gardner-Stephen",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0710.5116",
        "submitter": "Taneli Mielik\\\"ainen",
        "authors": "Matti K\\\"a\\\"ari\\\"ainen, Niels Landwehr, Sampsa Lappalainen and Taneli\n  Mielik\\\"ainen",
        "title": "Combining haplotypers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "C-2007-57",
        "categories": "cs.LG cs.CE q-bio.QM",
        "license": null,
        "abstract": "  Statistically resolving the underlying haplotype pair for a genotype\nmeasurement is an important intermediate step in gene mapping studies, and has\nreceived much attention recently. Consequently, a variety of methods for this\nproblem have been developed. Different methods employ different statistical\nmodels, and thus implicitly encode different assumptions about the nature of\nthe underlying haplotype structure. Depending on the population sample in\nquestion, their relative performance can vary greatly, and it is unclear which\nmethod to choose for a particular sample. Instead of choosing a single method,\nwe explore combining predictions returned by different methods in a principled\nway, and thereby circumvent the problem of method selection.\n  We propose several techniques for combining haplotype reconstructions and\nanalyze their computational properties. In an experimental study on real-world\nhaplotype data we show that such techniques can provide more accurate and\nrobust reconstructions, and are useful for outlier detection. Typically, the\ncombined prediction is at least as accurate as or even more accurate than the\nbest individual method, effectively circumventing the method selection problem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 26 Oct 2007 15:13:21 GMT"
            }
        ],
        "update_date": "2007-10-29",
        "authors_parsed": [
            [
                "K\u00e4\u00e4ri\u00e4inen",
                "Matti",
                ""
            ],
            [
                "Landwehr",
                "Niels",
                ""
            ],
            [
                "Lappalainen",
                "Sampsa",
                ""
            ],
            [
                "Mielik\u00e4inen",
                "Taneli",
                ""
            ]
        ]
    },
    {
        "id": "0710.5116",
        "submitter": "Taneli Mielik\\\"ainen",
        "authors": "Matti K\\\"a\\\"ari\\\"ainen, Niels Landwehr, Sampsa Lappalainen and Taneli\n  Mielik\\\"ainen",
        "title": "Combining haplotypers",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "C-2007-57",
        "categories": "cs.LG cs.CE q-bio.QM",
        "license": null,
        "abstract": "  Statistically resolving the underlying haplotype pair for a genotype\nmeasurement is an important intermediate step in gene mapping studies, and has\nreceived much attention recently. Consequently, a variety of methods for this\nproblem have been developed. Different methods employ different statistical\nmodels, and thus implicitly encode different assumptions about the nature of\nthe underlying haplotype structure. Depending on the population sample in\nquestion, their relative performance can vary greatly, and it is unclear which\nmethod to choose for a particular sample. Instead of choosing a single method,\nwe explore combining predictions returned by different methods in a principled\nway, and thereby circumvent the problem of method selection.\n  We propose several techniques for combining haplotype reconstructions and\nanalyze their computational properties. In an experimental study on real-world\nhaplotype data we show that such techniques can provide more accurate and\nrobust reconstructions, and are useful for outlier detection. Typically, the\ncombined prediction is at least as accurate as or even more accurate than the\nbest individual method, effectively circumventing the method selection problem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 26 Oct 2007 15:13:21 GMT"
            }
        ],
        "update_date": "2007-10-29",
        "authors_parsed": [
            [
                "K\u00e4\u00e4ri\u00e4inen",
                "Matti",
                ""
            ],
            [
                "Landwehr",
                "Niels",
                ""
            ],
            [
                "Lappalainen",
                "Sampsa",
                ""
            ],
            [
                "Mielik\u00e4inen",
                "Taneli",
                ""
            ]
        ]
    },
    {
        "id": "0710.5327",
        "submitter": "Paul Gardner-Stephen",
        "authors": "Paul Gardner-Stephen",
        "title": "Escalating The War On SPAM Through Practical POW Exchange",
        "comments": "To be presented at the IEEE Conference On Networking, Adelaide,\n  Australia, November 19-21, 2007",
        "journal-ref": null,
        "doi": "10.1109/ICON.2007.4444132",
        "report-no": null,
        "categories": "cs.NI cs.CR",
        "license": null,
        "abstract": "  Proof-of-work (POW) schemes have been proposed in the past. One prominent\nsystem is HASHCASH (Back, 2002) which uses cryptographic puzzles . However,\nwork by Laurie and Clayton (2004) has shown that for a uniform proof-of-work\nscheme on email to have an impact on SPAM, it would also be onerous enough to\nimpact on senders of \"legitimate\" email. I suggest that a non-uniform\nproof-of-work scheme on email may be a solution to this problem, and describe a\nframework that has the potential to limit SPAM, without unduly penalising\nlegitimate senders, and is constructed using only current SPAM filter\ntechnology, and a small change to the SMTP (Simple Mail Transfer Protocol).\nSpecifically, I argue that it is possible to make sending SPAM 1,000 times more\nexpensive than sending \"legitimate\" email (so called HAM). Also, unlike the\nsystem proposed by Debin Liu and Jean Camp (2006), it does not require the\ncomplications of maintaining a reputation system.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 29 Oct 2007 02:01:09 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Gardner-Stephen",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0710.5333",
        "submitter": "Haibin Wang",
        "authors": "Haibin Wang, Rajshekhar Sunderraman, Florentin Smarandache, Andre\n  Rogatko",
        "title": "Neutrosophic Relational Data Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  In this paper, we present a generalization of the relational data model based\non interval neutrosophic set. Our data model is capable of manipulating\nincomplete as well as inconsistent information. Fuzzy relation or\nintuitionistic fuzzy relation can only handle incomplete information.\nAssociated with each relation are two membership functions one is called\ntruth-membership function T which keeps track of the extent to which we believe\nthe tuple is in the relation, another is called falsity-membership function F\nwhich keeps track of the extent to which we believe that it is not in the\nrelation. A neutrosophic relation is inconsistent if there exists one tuple a\nsuch that T(a) + F(a) > 1 . In order to handle inconsistent situation, we\npropose an operator called \"split\" to transform inconsistent neutrosophic\nrelations into pseudo-consistent neutrosophic relations and do the\nset-theoretic and relation-theoretic operations on them and finally use another\noperator called \"combine\" to transform the result back to neutrosophic\nrelation. For this data model, we define algebraic operators that are\ngeneralizations of the usual operators such as intersection, union, selection,\njoin on fuzzy relations. Our data model can underlie any database and\nknowledge-base management system that deals with incomplete and inconsistent\ninformation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 29 Oct 2007 19:46:58 GMT"
            }
        ],
        "update_date": "2007-10-30",
        "authors_parsed": [
            [
                "Wang",
                "Haibin",
                ""
            ],
            [
                "Sunderraman",
                "Rajshekhar",
                ""
            ],
            [
                "Smarandache",
                "Florentin",
                ""
            ],
            [
                "Rogatko",
                "Andre",
                ""
            ]
        ]
    },
    {
        "id": "0710.5348",
        "submitter": "Virginie Legrand Contes",
        "authors": "Cristian Ruz (INRIA Sophia Antipolis), Fran\\c{c}oise Baude (INRIA\n  Sophia Antipolis), Virginie Legrand Contes (INRIA Sophia Antipolis)",
        "title": "Towards Grid Monitoring and deployment in Jade, using ProActive",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  This document describes our current effort to gridify Jade, a java-based\nenvironment for the autonomic management of clustered J2EE application servers,\ndeveloped in the INRIA SARDES research team. Towards this objective, we use the\njava ProActive grid technology. We first present some of the challenges to turn\nsuch an autonomic management system initially dedicated to distributed\napplications running on clusters of machines, into one that can provide\nself-management capabilities to large-scale systems, i.e. deployed on grid\ninfrastructures. This leads us to a brief state of the art on grid monitoring\nsystems. Then, we recall the architecture of Jade, and consequently propose to\nreorganize it in a potentially more scalable way. Practical experiments pertain\nto the use of the grid deployment feature offered by ProActive to easily\nconduct the deployment of the Jade system or its revised version on any sort of\ngrid.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 29 Oct 2007 10:44:35 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Ruz",
                "Cristian",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Baude",
                "Fran\u00e7oise",
                "",
                "INRIA\n  Sophia Antipolis"
            ],
            [
                "Contes",
                "Virginie Legrand",
                "",
                "INRIA Sophia Antipolis"
            ]
        ]
    },
    {
        "id": "0710.5425",
        "submitter": "{\\L}ukasz Chmielewski Mr.",
        "authors": "{\\L}ukasz Chmielewski and Jaap-Henk Hoepman",
        "title": "Fuzzy Private Matching (Extended Abstract)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  In the private matching problem, a client and a server each hold a set of $n$\ninput elements. The client wants to privately compute the intersection of these\ntwo sets: he learns which elements he has in common with the server (and\nnothing more), while the server gains no information at all. In certain\napplications it would be useful to have a private matching protocol that\nreports a match even if two elements are only similar instead of equal. Such a\nprivate matching protocol is called \\emph{fuzzy}, and is useful, for instance,\nwhen elements may be inaccurate or corrupted by errors.\n  We consider the fuzzy private matching problem, in a semi-honest environment.\nElements are similar if they match on $t$ out of $T$ attributes. First we show\nthat the original solution proposed by Freedman et al. is incorrect.\nSubsequently we present two fuzzy private matching protocols. The first,\nsimple, protocol has bit message complexity $O(n \\binom{T}{t} (T\n\\log{|D|}+k))$. The second, improved, protocol has a much better bit message\ncomplexity of $O(n T (\\log{|D|}+k))$, but here the client incurs a O(n) factor\ntime complexity. Additionally, we present protocols based on the computation of\nthe Hamming distance and on oblivious transfer, that have different, sometimes\nmore efficient, performance characteristics.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 29 Oct 2007 13:35:36 GMT"
            }
        ],
        "update_date": "2007-10-30",
        "authors_parsed": [
            [
                "Chmielewski",
                "\u0141ukasz",
                ""
            ],
            [
                "Hoepman",
                "Jaap-Henk",
                ""
            ]
        ]
    },
    {
        "id": "0710.5455",
        "submitter": "Shujun Li Dr.",
        "authors": "Shujun Li, Gonzalo Alvarez, Zhong Li and Wolfgang A. Halang",
        "title": "Analog Chaos-based Secure Communications and Cryptanalysis: A Brief\n  Survey",
        "comments": "6 pages, 3 figures, ieeeconf.cls",
        "journal-ref": "3rd International IEEE Scientific Conference on Physics and\n  Control (PhysCon 2007), http://lib.physcon.ru/?item=1368",
        "doi": null,
        "report-no": null,
        "categories": "nlin.CD cs.CR",
        "license": null,
        "abstract": "  A large number of analog chaos-based secure communication systems have been\nproposed since the early 1990s exploiting the technique of chaos\nsynchronization. A brief survey of these chaos-based cryptosystems and of\nrelated cryptanalytic results is given. Some recently proposed countermeasures\nagainst known attacks are also introduced.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 29 Oct 2007 15:22:26 GMT"
            }
        ],
        "update_date": "2010-06-23",
        "authors_parsed": [
            [
                "Li",
                "Shujun",
                ""
            ],
            [
                "Alvarez",
                "Gonzalo",
                ""
            ],
            [
                "Li",
                "Zhong",
                ""
            ],
            [
                "Halang",
                "Wolfgang A.",
                ""
            ]
        ]
    },
    {
        "id": "0710.5465",
        "submitter": "Shujun Li Dr.",
        "authors": "David Arroyo, Chengqing Li, Shujun Li, Gonzalo Alvarez and Wolfgang A.\n  Halang",
        "title": "Cryptanalysis of an image encryption scheme based on a new total\n  shuffling algorithm",
        "comments": "8 pages, 2 figures, 1 table",
        "journal-ref": "Chaos, Solitons & Fractals, vol. 41, no. 5, pp. 2613-2616, 2009",
        "doi": "10.1016/j.chaos.2008.09.051",
        "report-no": null,
        "categories": "nlin.CD cs.CR cs.MM",
        "license": null,
        "abstract": "  Chaotic systems have been broadly exploited through the last two decades to\nbuild encryption methods. Recently, two new image encryption schemes have been\nproposed, where the encryption process involves a permutation operation and an\nXOR-like transformation of the shuffled pixels, which are controlled by three\nchaotic systems. This paper discusses some defects of the schemes and how to\nbreak them with a chosen-plaintext attack.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 29 Oct 2007 16:00:56 GMT"
            }
        ],
        "update_date": "2010-06-23",
        "authors_parsed": [
            [
                "Arroyo",
                "David",
                ""
            ],
            [
                "Li",
                "Chengqing",
                ""
            ],
            [
                "Li",
                "Shujun",
                ""
            ],
            [
                "Alvarez",
                "Gonzalo",
                ""
            ],
            [
                "Halang",
                "Wolfgang A.",
                ""
            ]
        ]
    },
    {
        "id": "0710.5471",
        "submitter": "Shujun Li Dr.",
        "authors": "David Arroyo, Chengqing Li, Shujun Li and Gonzalo Alvarez",
        "title": "Cryptanalysis of a computer cryptography scheme based on a filter bank",
        "comments": "6 pages, 1 figure",
        "journal-ref": "Chaos, Solitons & Fractals, vol. 41, no. 1, pp. 410-413, 2009",
        "doi": "10.1016/j.chaos.2008.01.020",
        "report-no": null,
        "categories": "nlin.CD cs.CR",
        "license": null,
        "abstract": "  This paper analyzes the security of a recently-proposed signal encryption\nscheme based on a filter bank. A very critical weakness of this new signal\nencryption procedure is exploited in order to successfully recover the\nassociated secret key.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 29 Oct 2007 16:18:59 GMT"
            }
        ],
        "update_date": "2010-06-23",
        "authors_parsed": [
            [
                "Arroyo",
                "David",
                ""
            ],
            [
                "Li",
                "Chengqing",
                ""
            ],
            [
                "Li",
                "Shujun",
                ""
            ],
            [
                "Alvarez",
                "Gonzalo",
                ""
            ]
        ]
    },
    {
        "id": "0710.5512",
        "submitter": "Santiago Moreno",
        "authors": "U. Horst, S. Moreno",
        "title": "Risk Minimization and Optimal Derivative Design in a Principal Agent\n  Game",
        "comments": "28 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": null,
        "abstract": "  We consider the problem of Adverse Selection and optimal derivative design\nwithin a Principal-Agent framework. The principal's income is exposed to\nnon-hedgeable risk factors arising, for instance, from weather or climate\nphenomena. She evaluates her risk using a coherent and law invariant risk\nmeasure and tries minimize her exposure by selling derivative securities on her\nincome to individual agents. The agents have mean-variance preferences with\nheterogeneous risk aversion coefficients. An agent's degree of risk aversion is\nprivate information and hidden to the principal who only knows the overall\ndistribution. We show that the principal's risk minimization problem has a\nsolution and illustrate the effects of risk transfer on her income by means of\ntwo specific examples. Our model extends earlier work of Barrieu and El Karoui\n(2005) and Carlier, Ekeland and Touzi (2007).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 29 Oct 2007 20:00:15 GMT"
            }
        ],
        "update_date": "2007-10-31",
        "authors_parsed": [
            [
                "Horst",
                "U.",
                ""
            ],
            [
                "Moreno",
                "S.",
                ""
            ]
        ]
    },
    {
        "id": "0710.5547",
        "submitter": "Miguel Angel Miron C.E.",
        "authors": "M. Miron Bernal, H. Coyote Estrada, J. Figueroa Nazuno",
        "title": "Code Similarity on High Level Programs",
        "comments": "Proceedings of the 18th Autumn Meeting on Communications, Computers,\n  Electronics and Industrial Exposition. (IEEE - ROCC07). Acapulco, Guerrero,\n  Mexico. 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.DS",
        "license": null,
        "abstract": "  This paper presents a new approach for code similarity on High Level\nprograms. Our technique is based on Fast Dynamic Time Warping, that builds a\nwarp path or points relation with local restrictions. The source code is\nrepresented into Time Series using the operators inside programming languages\nthat makes possible the comparison. This makes possible subsequence detection\nthat represent similar code instructions. In contrast with other code\nsimilarity algorithms, we do not make features extraction. The experiments show\nthat two source codes are similar when their respective Time Series are\nsimilar.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 29 Oct 2007 22:39:21 GMT"
            }
        ],
        "update_date": "2007-10-31",
        "authors_parsed": [
            [
                "Bernal",
                "M. Miron",
                ""
            ],
            [
                "Estrada",
                "H. Coyote",
                ""
            ],
            [
                "Nazuno",
                "J. Figueroa",
                ""
            ]
        ]
    },
    {
        "id": "0710.5674",
        "submitter": "Mounira Kourjieh",
        "authors": "Yannick Chevalier (IRIT), Mounira Kourjieh (IRIT)",
        "title": "Key Substitution in the Symbolic Analysis of Cryptographic Protocols\n  (extended version)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  Key substitution vulnerable signature schemes are signature schemes that\npermit an intruder, given a public verification key and a signed message, to\ncompute a pair of signature and verification keys such that the message appears\nto be signed with the new signature key. A digital signature scheme is said to\nbe vulnerable to destructive exclusive ownership property (DEO) If it is\ncomputationaly feasible for an intruder, given a public verification key and a\npair of message and its valid signature relatively to the given public key, to\ncompute a pair of signature and verification keys and a new message such that\nthe given signature appears to be valid for the new message relatively to the\nnew verification key. In this paper, we prove decidability of the insecurity\nproblem of cryptographic protocols where the signature schemes employed in the\nconcrete realisation have this two properties.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 30 Oct 2007 15:13:47 GMT"
            }
        ],
        "update_date": "2007-10-31",
        "authors_parsed": [
            [
                "Chevalier",
                "Yannick",
                "",
                "IRIT"
            ],
            [
                "Kourjieh",
                "Mounira",
                "",
                "IRIT"
            ]
        ]
    },
    {
        "id": "0710.5697",
        "submitter": "Kristina Lerman",
        "authors": "Kristina Lerman",
        "title": "Social Browsing & Information Filtering in Social Media",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CY cs.HC",
        "license": null,
        "abstract": "  Social networks are a prominent feature of many social media sites, a new\ngeneration of Web sites that allow users to create and share content. Sites\nsuch as Digg, Flickr, and Del.icio.us allow users to designate others as\n\"friends\" or \"contacts\" and provide a single-click interface to track friends'\nactivity. How are these social networks used? Unlike pure social networking\nsites (e.g., LinkedIn and Facebook), which allow users to articulate their\nonline professional and personal relationships, social media sites are not, for\nthe most part, aimed at helping users create or foster online relationships.\nInstead, we claim that social media users create social networks to express\ntheir tastes and interests, and use them to filter the vast stream of new\nsubmissions to find interesting content. Social networks, in fact, facilitate\nnew ways of interacting with information: what we call social browsing. Through\nan extensive analysis of data from Digg and Flickr, we show that social\nbrowsing is one of the primary usage modalities on these social media sites.\nThis finding has implications for how social media sites rate and personalize\ncontent.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 30 Oct 2007 16:38:54 GMT"
            }
        ],
        "update_date": "2007-10-31",
        "authors_parsed": [
            [
                "Lerman",
                "Kristina",
                ""
            ]
        ]
    },
    {
        "id": "0710.5895",
        "submitter": "Wim Vanhoof",
        "authors": "Francois Gobert, Baudouin Le Charlier",
        "title": "Source-to-source optimizing transformations of Prolog programs based on\n  abstract interpretation",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.LO cs.SE",
        "license": null,
        "abstract": "  Making a Prolog program more efficient by transforming its source code,\nwithout changing its operational semantics, is not an obvious task. It requires\nthe user to have a clear understanding of how the Prolog compiler works, and in\nparticular, of the effects of impure features like the cut. The way a Prolog\ncode is written - e.g., the order of clauses, the order of literals in a\nclause, the use of cuts or negations - influences its efficiency. Furthermore,\ndifferent optimization techniques may be redundant or conflicting when they are\napplied together, depending on the way a procedure is called - e.g., inserting\ncuts and enabling indexing. We present an optimiser, based on abstract\ninterpretation, that automatically performs safe code transformations of Prolog\nprocedures in the context of some class of input calls. The method is more\neffective if procedures are annotated with additional information about modes,\ntypes, sharing, number of solutions and the like. Thus the approach is similar\nto Mercury. It applies to any Prolog program, however.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 31 Oct 2007 15:59:50 GMT"
            }
        ],
        "update_date": "2007-11-01",
        "authors_parsed": [
            [
                "Gobert",
                "Francois",
                ""
            ],
            [
                "Charlier",
                "Baudouin Le",
                ""
            ]
        ]
    },
    {
        "id": "0710.5895",
        "submitter": "Wim Vanhoof",
        "authors": "Francois Gobert, Baudouin Le Charlier",
        "title": "Source-to-source optimizing transformations of Prolog programs based on\n  abstract interpretation",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.LO cs.SE",
        "license": null,
        "abstract": "  Making a Prolog program more efficient by transforming its source code,\nwithout changing its operational semantics, is not an obvious task. It requires\nthe user to have a clear understanding of how the Prolog compiler works, and in\nparticular, of the effects of impure features like the cut. The way a Prolog\ncode is written - e.g., the order of clauses, the order of literals in a\nclause, the use of cuts or negations - influences its efficiency. Furthermore,\ndifferent optimization techniques may be redundant or conflicting when they are\napplied together, depending on the way a procedure is called - e.g., inserting\ncuts and enabling indexing. We present an optimiser, based on abstract\ninterpretation, that automatically performs safe code transformations of Prolog\nprocedures in the context of some class of input calls. The method is more\neffective if procedures are annotated with additional information about modes,\ntypes, sharing, number of solutions and the like. Thus the approach is similar\nto Mercury. It applies to any Prolog program, however.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 31 Oct 2007 15:59:50 GMT"
            }
        ],
        "update_date": "2007-11-01",
        "authors_parsed": [
            [
                "Gobert",
                "Francois",
                ""
            ],
            [
                "Charlier",
                "Baudouin Le",
                ""
            ]
        ]
    },
    {
        "id": "0711.0048",
        "submitter": "Lee Naish",
        "authors": "Lee Naish",
        "title": "Declarative Diagnosis of Floundering",
        "comments": "12 pages, 0 figures, uses llncs.sty In Proceedings of the 17th\n  Workshop on Logic-based methods in Programming Environments, Eds. Patricia\n  Hill and Wim Vanhoof, pp 48-60, September 2007, Porto, Portugal",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  Many logic programming languages have delay primitives which allow\ncoroutining. This introduces a class of bug symptoms -- computations can\nflounder when they are intended to succeed or finitely fail. For concurrent\nlogic programs this is normally called deadlock. Similarly, constraint logic\nprograms can fail to invoke certain constraint solvers because variables are\ninsufficiently instantiated or constrained. Diagnosing such faults has received\nrelatively little attention to date. Since delay primitives affect the\nprocedural but not the declarative view of programs, it may be expected that\ndebugging would have to consider the often complex details of interleaved\nexecution. However, recent work on semantics has suggested an alternative\napproach. In this paper we show how the declarative debugging paradigm can be\nused to diagnose unexpected floundering, insulating the user from the\ncomplexities of the execution.\n  Keywords: logic programming, coroutining, delay, debugging, floundering,\ndeadlock, constraints\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 1 Nov 2007 01:40:50 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Naish",
                "Lee",
                ""
            ]
        ]
    },
    {
        "id": "0711.0048",
        "submitter": "Lee Naish",
        "authors": "Lee Naish",
        "title": "Declarative Diagnosis of Floundering",
        "comments": "12 pages, 0 figures, uses llncs.sty In Proceedings of the 17th\n  Workshop on Logic-based methods in Programming Environments, Eds. Patricia\n  Hill and Wim Vanhoof, pp 48-60, September 2007, Porto, Portugal",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  Many logic programming languages have delay primitives which allow\ncoroutining. This introduces a class of bug symptoms -- computations can\nflounder when they are intended to succeed or finitely fail. For concurrent\nlogic programs this is normally called deadlock. Similarly, constraint logic\nprograms can fail to invoke certain constraint solvers because variables are\ninsufficiently instantiated or constrained. Diagnosing such faults has received\nrelatively little attention to date. Since delay primitives affect the\nprocedural but not the declarative view of programs, it may be expected that\ndebugging would have to consider the often complex details of interleaved\nexecution. However, recent work on semantics has suggested an alternative\napproach. In this paper we show how the declarative debugging paradigm can be\nused to diagnose unexpected floundering, insulating the user from the\ncomplexities of the execution.\n  Keywords: logic programming, coroutining, delay, debugging, floundering,\ndeadlock, constraints\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 1 Nov 2007 01:40:50 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Naish",
                "Lee",
                ""
            ]
        ]
    },
    {
        "id": "0711.0128",
        "submitter": "Manoj Kumar",
        "authors": "Manoj Kumar",
        "title": "Security Analysis of a Remote User Authentication Scheme with Smart\n  Cards",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  Yoon et al. proposed a new efficient remote user authentication scheme using\nsmart cards to solve the security problems of W. C. Ku and S. M. Chen scheme.\nThis paper reviews Yoon et al. scheme and then proves that the password change\nphase of Yoon et al. scheme is still insecure. This paper also proves that the\nYoon et al. is still vulnerable to parallel session attack.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 1 Nov 2007 14:24:41 GMT"
            }
        ],
        "update_date": "2007-11-02",
        "authors_parsed": [
            [
                "Kumar",
                "Manoj",
                ""
            ]
        ]
    },
    {
        "id": "0711.0189",
        "submitter": "Ulrike von Luxburg",
        "authors": "Ulrike von Luxburg",
        "title": "A Tutorial on Spectral Clustering",
        "comments": null,
        "journal-ref": "Statistics and Computing 17(4), 2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.LG",
        "license": null,
        "abstract": "  In recent years, spectral clustering has become one of the most popular\nmodern clustering algorithms. It is simple to implement, can be solved\nefficiently by standard linear algebra software, and very often outperforms\ntraditional clustering algorithms such as the k-means algorithm. On the first\nglance spectral clustering appears slightly mysterious, and it is not obvious\nto see why it works at all and what it really does. The goal of this tutorial\nis to give some intuition on those questions. We describe different graph\nLaplacians and their basic properties, present the most common spectral\nclustering algorithms, and derive those algorithms from scratch by several\ndifferent approaches. Advantages and disadvantages of the different spectral\nclustering algorithms are discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 1 Nov 2007 19:04:43 GMT"
            }
        ],
        "update_date": "2007-11-02",
        "authors_parsed": [
            [
                "von Luxburg",
                "Ulrike",
                ""
            ]
        ]
    },
    {
        "id": "0711.0314",
        "submitter": "Aleksandar Lazarevic",
        "authors": "Aleksandar Lazarevic, Lionel Sacks",
        "title": "Resource and Application Models for Advanced Grid Schedulers",
        "comments": null,
        "journal-ref": "London Communications Symposium 2003",
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  As Grid computing is becoming an inevitable future, managing, scheduling and\nmonitoring dynamic, heterogeneous resources will present new challenges.\nSolutions will have to be agile and adaptive, support self-organization and\nautonomous management, while maintaining optimal resource utilisation.\nPresented in this paper are basic principles and architectural concepts for\nefficient resource allocation in heterogeneous Grid environment.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 Nov 2007 14:03:46 GMT"
            }
        ],
        "update_date": "2007-11-05",
        "authors_parsed": [
            [
                "Lazarevic",
                "Aleksandar",
                ""
            ],
            [
                "Sacks",
                "Lionel",
                ""
            ]
        ]
    },
    {
        "id": "0711.0315",
        "submitter": "Aleksandar Lazarevic",
        "authors": "Aleksandar Lazarevic, Lionel Sacks",
        "title": "Measuring and Monitoring Grid Resource Utilisation",
        "comments": null,
        "journal-ref": "London Communications Symposium 2004",
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Effective resource utilisation monitoring and highly granular yet adaptive\nmeasurements are prerequisites for a more efficient Grid scheduler. We present\na suite of measurement applications able to monitor per-process resource\nutilisation, and a customisable tool for emulating observed utilisation models.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 Nov 2007 14:12:27 GMT"
            }
        ],
        "update_date": "2007-11-05",
        "authors_parsed": [
            [
                "Lazarevic",
                "Aleksandar",
                ""
            ],
            [
                "Sacks",
                "Lionel",
                ""
            ]
        ]
    },
    {
        "id": "0711.0316",
        "submitter": "Aleksandar Lazarevic",
        "authors": "Aleksandar Lazarevic, Lionel Sacks",
        "title": "A Study of Grid Applications: Scheduling Perspective",
        "comments": null,
        "journal-ref": "London Communications Symposium 2005",
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  As the Grid evolves from a high performance cluster middleware to a\nmultipurpose utility computing framework, a good understanding of Grid\napplications, their statistics and utilisation patterns is required. This study\nlooks at job execution times and resource utilisations in a Grid environment,\nand their significance in cluster and network dimensioning, local level\nscheduling and resource management.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 Nov 2007 14:15:45 GMT"
            }
        ],
        "update_date": "2007-11-05",
        "authors_parsed": [
            [
                "Lazarevic",
                "Aleksandar",
                ""
            ],
            [
                "Sacks",
                "Lionel",
                ""
            ]
        ]
    },
    {
        "id": "0711.0325",
        "submitter": "Aleksandar Lazarevic",
        "authors": "Ioannis Liabotis, Ognjen Prnjat, Tope Olukemi, Adrian Li Mow Ching,\n  Aleksandar Lazarevic, Lionel Sacks, Mike Fisher, Paul McKee",
        "title": "Self-Organising management of Grid environments",
        "comments": null,
        "journal-ref": "International Symposium on Telecommunications 2003",
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  This paper presents basic concepts, architectural principles and algorithms\nfor efficient resource and security management in cluster computing\nenvironments and the Grid. The work presented in this paper is funded by\nBTExacT and the EPSRC project SO-GRM (GR/S21939).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 Nov 2007 15:26:48 GMT"
            }
        ],
        "update_date": "2007-11-05",
        "authors_parsed": [
            [
                "Liabotis",
                "Ioannis",
                ""
            ],
            [
                "Prnjat",
                "Ognjen",
                ""
            ],
            [
                "Olukemi",
                "Tope",
                ""
            ],
            [
                "Ching",
                "Adrian Li Mow",
                ""
            ],
            [
                "Lazarevic",
                "Aleksandar",
                ""
            ],
            [
                "Sacks",
                "Lionel",
                ""
            ],
            [
                "Fisher",
                "Mike",
                ""
            ],
            [
                "McKee",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0711.0326",
        "submitter": "Aleksandar Lazarevic",
        "authors": "Aleksandar Lazarevic, Lionel Sacks, Ognjen Prnjat",
        "title": "Enabling Adaptive Grid Scheduling and Resource Management",
        "comments": null,
        "journal-ref": "International Symposium on Integrated Network Management 2005",
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Wider adoption of the Grid concept has led to an increasing amount of\nfederated computational, storage and visualisation resources being available to\nscientists and researchers. Distributed and heterogeneous nature of these\nresources renders most of the legacy cluster monitoring and management\napproaches inappropriate, and poses new challenges in workflow scheduling on\nsuch systems. Effective resource utilisation monitoring and highly granular yet\nadaptive measurements are prerequisites for a more efficient Grid scheduler. We\npresent a suite of measurement applications able to monitor per-process\nresource utilisation, and a customisable tool for emulating observed\nutilisation models. We also outline our future work on a predictive and\nprobabilistic Grid scheduler. The research is undertaken as part of UK\ne-Science EPSRC sponsored project SO-GRM (Self-Organising Grid Resource\nManagement) in cooperation with BT.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 Nov 2007 15:30:53 GMT"
            }
        ],
        "update_date": "2007-11-05",
        "authors_parsed": [
            [
                "Lazarevic",
                "Aleksandar",
                ""
            ],
            [
                "Sacks",
                "Lionel",
                ""
            ],
            [
                "Prnjat",
                "Ognjen",
                ""
            ]
        ]
    },
    {
        "id": "0711.0327",
        "submitter": "Aleksandar Lazarevic",
        "authors": "Aleksandar Lazarevic, Lionel Sacks, Ognjen Prnjat",
        "title": "Managing Uncertainty: A Case for Probabilistic Grid Scheduling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  The Grid technology is evolving into a global, service-orientated\narchitecture, a universal platform for delivering future high demand\ncomputational services. Strong adoption of the Grid and the utility computing\nconcept is leading to an increasing number of Grid installations running a wide\nrange of applications of different size and complexity. In this paper we\naddress the problem of elivering deadline/economy based scheduling in a\nheterogeneous application environment using statistical properties of job\nhistorical executions and its associated meta-data. This approach is motivated\nby a study of six-month computational load generated by Grid applications in a\nmulti-purpose Grid cluster serving a community of twenty e-Science projects.\nThe observed job statistics, resource utilisation and user behaviour is\ndiscussed in the context of management approaches and models most suitable for\nsupporting a probabilistic and autonomous scheduling architecture.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 Nov 2007 15:36:36 GMT"
            }
        ],
        "update_date": "2007-11-05",
        "authors_parsed": [
            [
                "Lazarevic",
                "Aleksandar",
                ""
            ],
            [
                "Sacks",
                "Lionel",
                ""
            ],
            [
                "Prnjat",
                "Ognjen",
                ""
            ]
        ]
    },
    {
        "id": "0711.0344",
        "submitter": "Wim Vanhoof",
        "authors": "Guillem Marpons-Ucero, Julio Mari\\~no, \\'Angel Herranz, Lars-{\\AA}ke\n  Fredlund, Manuel Carro, Juan Jos\\'e Moreno-Navarro",
        "title": "Automatic Coding Rule Conformance Checking Using Logic Programs",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  Some approaches to increasing program reliability involve a disciplined use\nof programming languages so as to minimise the hazards introduced by\nerror-prone features. This is realised by writing code that is constrained to a\nsubset of the a priori admissible programs, and that, moreover, may use only a\nsubset of the language. These subsets are determined by a collection of\nso-called coding rules.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 Nov 2007 16:53:34 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Marpons-Ucero",
                "Guillem",
                ""
            ],
            [
                "Mari\u00f1o",
                "Julio",
                ""
            ],
            [
                "Herranz",
                "\u00c1ngel",
                ""
            ],
            [
                "Fredlund",
                "Lars-\u00c5ke",
                ""
            ],
            [
                "Carro",
                "Manuel",
                ""
            ],
            [
                "Moreno-Navarro",
                "Juan Jos\u00e9",
                ""
            ]
        ]
    },
    {
        "id": "0711.0344",
        "submitter": "Wim Vanhoof",
        "authors": "Guillem Marpons-Ucero, Julio Mari\\~no, \\'Angel Herranz, Lars-{\\AA}ke\n  Fredlund, Manuel Carro, Juan Jos\\'e Moreno-Navarro",
        "title": "Automatic Coding Rule Conformance Checking Using Logic Programs",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  Some approaches to increasing program reliability involve a disciplined use\nof programming languages so as to minimise the hazards introduced by\nerror-prone features. This is realised by writing code that is constrained to a\nsubset of the a priori admissible programs, and that, moreover, may use only a\nsubset of the language. These subsets are determined by a collection of\nso-called coding rules.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 Nov 2007 16:53:34 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Marpons-Ucero",
                "Guillem",
                ""
            ],
            [
                "Mari\u00f1o",
                "Julio",
                ""
            ],
            [
                "Herranz",
                "\u00c1ngel",
                ""
            ],
            [
                "Fredlund",
                "Lars-\u00c5ke",
                ""
            ],
            [
                "Carro",
                "Manuel",
                ""
            ],
            [
                "Moreno-Navarro",
                "Juan Jos\u00e9",
                ""
            ]
        ]
    },
    {
        "id": "0711.0345",
        "submitter": "Wim Vanhoof",
        "authors": "Roberto Bagnara, Patricia Hill, Enea Zaffanella",
        "title": "A Prolog-based Environment for Reasoning about Programming Languages\n  (Extended abstract)",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  ECLAIR is a Prolog-based prototype system aiming to provide a functionally\ncomplete environment for the study, development and evaluation of programming\nlanguage analysis and implementation tools. In this paper, we sketch the\noverall structure of the system, outlining the main methodologies and\ntechnologies underlying its components. We also discuss the appropriateness of\nProlog as the implementation language for the system: besides highlighting its\nstrengths, we also point out a few potential weaknesses, hinting at possible\nsolutions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 Nov 2007 16:40:10 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Bagnara",
                "Roberto",
                ""
            ],
            [
                "Hill",
                "Patricia",
                ""
            ],
            [
                "Zaffanella",
                "Enea",
                ""
            ]
        ]
    },
    {
        "id": "0711.0345",
        "submitter": "Wim Vanhoof",
        "authors": "Roberto Bagnara, Patricia Hill, Enea Zaffanella",
        "title": "A Prolog-based Environment for Reasoning about Programming Languages\n  (Extended abstract)",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  ECLAIR is a Prolog-based prototype system aiming to provide a functionally\ncomplete environment for the study, development and evaluation of programming\nlanguage analysis and implementation tools. In this paper, we sketch the\noverall structure of the system, outlining the main methodologies and\ntechnologies underlying its components. We also discuss the appropriateness of\nProlog as the implementation language for the system: besides highlighting its\nstrengths, we also point out a few potential weaknesses, hinting at possible\nsolutions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 Nov 2007 16:40:10 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Bagnara",
                "Roberto",
                ""
            ],
            [
                "Hill",
                "Patricia",
                ""
            ],
            [
                "Zaffanella",
                "Enea",
                ""
            ]
        ]
    },
    {
        "id": "0711.0348",
        "submitter": "Wim Vanhoof",
        "authors": "Bernd Bra{\\ss}el, Michael Hanus and Marion Muller",
        "title": "Compiling ER Specifications into Declarative Programs",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  This paper proposes an environment to support high-level database programming\nin a declarative programming language. In order to ensure safe database\nupdates, all access and update operations related to the database are generated\nfrom high-level descriptions in the entity- relationship (ER) model. We propose\na representation of ER diagrams in the declarative language Curry so that they\ncan be constructed by various tools and then translated into this\nrepresentation. Furthermore, we have implemented a compiler from this\nrepresentation into a Curry program that provides access and update operations\nbased on a high-level API for database programming.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 Nov 2007 16:49:30 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Bra\u00dfel",
                "Bernd",
                ""
            ],
            [
                "Hanus",
                "Michael",
                ""
            ],
            [
                "Muller",
                "Marion",
                ""
            ]
        ]
    },
    {
        "id": "0711.0348",
        "submitter": "Wim Vanhoof",
        "authors": "Bernd Bra{\\ss}el, Michael Hanus and Marion Muller",
        "title": "Compiling ER Specifications into Declarative Programs",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  This paper proposes an environment to support high-level database programming\nin a declarative programming language. In order to ensure safe database\nupdates, all access and update operations related to the database are generated\nfrom high-level descriptions in the entity- relationship (ER) model. We propose\na representation of ER diagrams in the declarative language Curry so that they\ncan be constructed by various tools and then translated into this\nrepresentation. Furthermore, we have implemented a compiler from this\nrepresentation into a Curry program that provides access and update operations\nbased on a high-level API for database programming.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 Nov 2007 16:49:30 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Bra\u00dfel",
                "Bernd",
                ""
            ],
            [
                "Hanus",
                "Michael",
                ""
            ],
            [
                "Muller",
                "Marion",
                ""
            ]
        ]
    },
    {
        "id": "0711.0528",
        "submitter": "L.T. Handoko",
        "authors": "Z. Akbar and L.T. Handoko",
        "title": "Web-based Interface in Public Cluster",
        "comments": "9 pages, Proceeding of the 9th International Conference on\n  Information Integration and Web-based Applications and Services 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": "FISIKALIPI-07016",
        "categories": "cs.DC cs.CY",
        "license": null,
        "abstract": "  A web-based interface dedicated for cluster computer which is publicly\naccessible for free is introduced. The interface plays an important role to\nenable secure public access, while providing user-friendly computational\nenvironment for end-users and easy maintainance for administrators as well. The\nwhole architecture which integrates both aspects of hardware and software is\nbriefly explained. It is argued that the public cluster is globally a unique\napproach, and could be a new kind of e-learning system especially for parallel\nprogramming communities.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 4 Nov 2007 16:39:47 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Akbar",
                "Z.",
                ""
            ],
            [
                "Handoko",
                "L. T.",
                ""
            ]
        ]
    },
    {
        "id": "0711.0538",
        "submitter": "Grenville Croll",
        "authors": "Thomas A. Grossman",
        "title": "Spreadsheet Engineering: A Research Framework",
        "comments": "12 Pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2002 23-34 ISBN 1 86166\n  182 7",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  Spreadsheet engineering adapts the lessons of software engineering to\nspreadsheets, providing eight principles as a framework for organizing\nspreadsheet programming recommendations. Spreadsheets raise issues inadequately\naddressed by software engineering. Spreadsheets are a powerful modeling\nlanguage, allowing strategic rapid model change, and enabling exploratory\nmodeling. Spreadsheets users learn slowly with experience because they focus on\nthe problem domain not programming. The heterogeneity of spreadsheet users\nrequires a taxonomy to guide recommendations. Deployment of best practices is\ndifficult and merits research.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 4 Nov 2007 19:24:57 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Grossman",
                "Thomas A.",
                ""
            ]
        ]
    },
    {
        "id": "0711.0607",
        "submitter": "Bart Van Rompaey",
        "authors": "Bart Van Rompaey and Serge Demeyer",
        "title": "Exploring the Composition of Unit Test Suites",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "UA TR2007-01",
        "categories": "cs.SE",
        "license": null,
        "abstract": "  In agile software development, test code can considerably contribute to the\noverall source code size. Being a valuable asset both in terms of verification\nand documentation, the composition of a test suite needs to be well understood\nin order to identify opportunities as well as weaknesses for further evolution.\nIn this paper, we argue that the visualization of structural characteristics is\na viable means to support the exploration of test suites. Thanks to general\nagreement on a limited set of key test design principles, such visualizations\nare relatively easy to interpret. In particular, we present visualizations that\nsupport testers in (i) locating test cases; (ii) examining the relation between\ntest code and production code; and (iii) studying the composition of and\ndependencies within test cases. By means of two case studies, we demonstrate\nhow visual patterns help to identify key test suite characteristics. This\napproach forms the first step in assisting a developer to build up\nunderstanding about test suites beyond code reading.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 5 Nov 2007 11:05:42 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Van Rompaey",
                "Bart",
                ""
            ],
            [
                "Demeyer",
                "Serge",
                ""
            ]
        ]
    },
    {
        "id": "0711.0618",
        "submitter": "Wim Vanhoof",
        "authors": "Jan Wielemaker, Anjo Anjewierden",
        "title": "PIDoc: Wiki style Literate Programming for Prolog",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  This document introduces PlDoc, a literate programming system for Prolog.\nStarting point for PlDoc was minimal distraction from the programming task and\nmaximal immediate reward, attempting to seduce the programmer to use the\nsystem. Minimal distraction is achieved using structured comments that are as\nclosely as possible related to common Prolog documentation practices. Immediate\nreward is provided by a web interface powered from the Prolog development\nenvironment that integrates searching and browsing application and system\ndocumentation. When accessed from localhost, it is possible to go from\ndocumentation shown in a browser to the source code displayed in the user's\neditor of choice.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 5 Nov 2007 12:13:12 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Wielemaker",
                "Jan",
                ""
            ],
            [
                "Anjewierden",
                "Anjo",
                ""
            ]
        ]
    },
    {
        "id": "0711.0618",
        "submitter": "Wim Vanhoof",
        "authors": "Jan Wielemaker, Anjo Anjewierden",
        "title": "PIDoc: Wiki style Literate Programming for Prolog",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  This document introduces PlDoc, a literate programming system for Prolog.\nStarting point for PlDoc was minimal distraction from the programming task and\nmaximal immediate reward, attempting to seduce the programmer to use the\nsystem. Minimal distraction is achieved using structured comments that are as\nclosely as possible related to common Prolog documentation practices. Immediate\nreward is provided by a web interface powered from the Prolog development\nenvironment that integrates searching and browsing application and system\ndocumentation. When accessed from localhost, it is possible to go from\ndocumentation shown in a browser to the source code displayed in the user's\neditor of choice.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 5 Nov 2007 12:13:12 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Wielemaker",
                "Jan",
                ""
            ],
            [
                "Anjewierden",
                "Anjo",
                ""
            ]
        ]
    },
    {
        "id": "0711.0643",
        "submitter": "Simon Portegies Zwart",
        "authors": "Simon Portegies Zwart (Amsterdam), Steve McMillan (Drexel), Derek\n  Groen (Amsterdam), Alessia Gualandris (Rochester), Michael Sipior (Astron),\n  Willem Vermin (SARA)",
        "title": "A parallel gravitational N-body kernel",
        "comments": "21 pages, New Astronomy (in press)",
        "journal-ref": null,
        "doi": "10.1016/j.newast.2007.11.002",
        "report-no": null,
        "categories": "astro-ph cs.DC",
        "license": null,
        "abstract": "  We describe source code level parallelization for the {\\tt kira} direct\ngravitational $N$-body integrator, the workhorse of the {\\tt starlab}\nproduction environment for simulating dense stellar systems. The\nparallelization strategy, called ``j-parallelization'', involves the partition\nof the computational domain by distributing all particles in the system among\nthe available processors. Partial forces on the particles to be advanced are\ncalculated in parallel by their parent processors, and are then summed in a\nfinal global operation. Once total forces are obtained, the computing elements\nproceed to the computation of their particle trajectories. We report the\nresults of timing measurements on four different parallel computers, and\ncompare them with theoretical predictions. The computers employ either a\nhigh-speed interconnect, a NUMA architecture to minimize the communication\noverhead or are distributed in a grid. The code scales well in the domain\ntested, which ranges from 1024 - 65536 stars on 1 - 128 processors, providing\nsatisfactory speedup. Running the production environment on a grid becomes\ninefficient for more than 60 processors distributed across three sites.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 5 Nov 2007 14:11:32 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Zwart",
                "Simon Portegies",
                "",
                "Amsterdam"
            ],
            [
                "McMillan",
                "Steve",
                "",
                "Drexel"
            ],
            [
                "Groen",
                "Derek",
                "",
                "Amsterdam"
            ],
            [
                "Gualandris",
                "Alessia",
                "",
                "Rochester"
            ],
            [
                "Sipior",
                "Michael",
                "",
                "Astron"
            ],
            [
                "Vermin",
                "Willem",
                "",
                "SARA"
            ]
        ]
    },
    {
        "id": "0711.0692",
        "submitter": "Jean-Yves Marion",
        "authors": "Anne Bonfante (INRIA Lorraine - LORIA), Jean-Yves Marion (INRIA\n  Lorraine - LORIA)",
        "title": "On the defence notion",
        "comments": null,
        "journal-ref": "Journal in Computer Virology 3, 4 (2007) 247-251",
        "doi": "10.1007/s11416-007-0058-9",
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  'Trojan horses', 'logic bombs', 'armoured viruses' and 'cryptovirology' are\nterms recalling war gears. In fact, concepts of attack and defence drive the\nworld of computer virology, which looks like a war universe in an information\nsociety. This war has several shapes, from invasions of a network by worms, to\nmilitary and industrial espionage ...\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 5 Nov 2007 17:02:25 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Bonfante",
                "Anne",
                "",
                "INRIA Lorraine - LORIA"
            ],
            [
                "Marion",
                "Jean-Yves",
                "",
                "INRIA\n  Lorraine - LORIA"
            ]
        ]
    },
    {
        "id": "0711.0711",
        "submitter": "Yingbin Liang",
        "authors": "Yingbin Liang, H. Vincent Poor and Shlomo Shamai (Shitz)",
        "title": "Information-Theoretic Security in Wireless Networks",
        "comments": "From the Proceedings of the 2007 Joint Workshop on Coding and\n  Communications, Durnstein, Austria, October 14 - 16, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.CR math.IT",
        "license": null,
        "abstract": "  This paper summarizes recent contributions of the authors and their\nco-workers in the area of information-theoretic security.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 5 Nov 2007 18:21:14 GMT"
            }
        ],
        "update_date": "2007-11-06",
        "authors_parsed": [
            [
                "Liang",
                "Yingbin",
                "",
                "Shitz"
            ],
            [
                "Poor",
                "H. Vincent",
                "",
                "Shitz"
            ],
            [
                "Shamai",
                "Shlomo",
                "",
                "Shitz"
            ]
        ]
    },
    {
        "id": "0711.0784",
        "submitter": "Philip Baback Alipour",
        "authors": "Philip B. Alipour",
        "title": "Addendum to Research MMMCV; A Man/Microbio/Megabio/Computer Vision",
        "comments": "LaTeX, 10 pages, 4 figures (1 algorithm); An addendum to a research\n  proposal on biovielectroluminescence and MMMCV for postgraduate academic\n  bodies and research departments. The contents of this paper are mostly\n  related and referenced to Article: [arXiv:0710.0410]",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.CE",
        "license": null,
        "abstract": "  In October 2007, a Research Proposal for the University of Sydney, Australia,\nthe author suggested that biovie-physical phenomenon as `electrodynamic\ndependant biological vision', is governed by relativistic quantum laws and\nbiovision. The phenomenon on the basis of `biovielectroluminescence', satisfies\nman/microbio/megabio/computer vision (MMMCV), as a robust candidate for\nphysical and visual sciences. The general aim of this addendum is to present a\nrefined text of Sections 1-3 of that proposal and highlighting the contents of\nits Appendix in form of a `Mechanisms' Section. We then briefly remind in an\narticle aimed for December 2007, by appending two more equations into Section\n3, a theoretical II-time scenario as a time model well-proposed for the\nphenomenon. The time model within the core of the proposal, plays a significant\nrole in emphasizing the principle points on Objectives no. 1-8, Sub-hypothesis\n3.1.2, mentioned in Article [arXiv:0710.0410]. It also expresses the time\nconcept in terms of causing quantized energy f(|E|) of time |t|, emit in regard\nto shortening the probability of particle loci as predictable patterns of\nparticle's un-occurred motion, a solution to Heisenberg's uncertainty principle\n(HUP) into a simplistic manner. We conclude that, practical frames via a time\nalgorithm to this model, fixates such predictable patterns of motion of scenery\nbodies onto recordable observation points of a MMMCV system. It even\nsuppresses/predicts superposition phenomena coming from a human subject and/or\nother bio-subjects for any decision making event, e.g., brainwave quantum\npatterns based on vision. Maintaining the existential probability of Riemann\nsurfaces of II-time scenarios in the context of biovielectroluminescence, makes\nmotion-prediction a possibility.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 6 Nov 2007 19:41:22 GMT"
            }
        ],
        "update_date": "2016-09-08",
        "authors_parsed": [
            [
                "Alipour",
                "Philip B.",
                ""
            ]
        ]
    },
    {
        "id": "0711.0784",
        "submitter": "Philip Baback Alipour",
        "authors": "Philip B. Alipour",
        "title": "Addendum to Research MMMCV; A Man/Microbio/Megabio/Computer Vision",
        "comments": "LaTeX, 10 pages, 4 figures (1 algorithm); An addendum to a research\n  proposal on biovielectroluminescence and MMMCV for postgraduate academic\n  bodies and research departments. The contents of this paper are mostly\n  related and referenced to Article: [arXiv:0710.0410]",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.CE",
        "license": null,
        "abstract": "  In October 2007, a Research Proposal for the University of Sydney, Australia,\nthe author suggested that biovie-physical phenomenon as `electrodynamic\ndependant biological vision', is governed by relativistic quantum laws and\nbiovision. The phenomenon on the basis of `biovielectroluminescence', satisfies\nman/microbio/megabio/computer vision (MMMCV), as a robust candidate for\nphysical and visual sciences. The general aim of this addendum is to present a\nrefined text of Sections 1-3 of that proposal and highlighting the contents of\nits Appendix in form of a `Mechanisms' Section. We then briefly remind in an\narticle aimed for December 2007, by appending two more equations into Section\n3, a theoretical II-time scenario as a time model well-proposed for the\nphenomenon. The time model within the core of the proposal, plays a significant\nrole in emphasizing the principle points on Objectives no. 1-8, Sub-hypothesis\n3.1.2, mentioned in Article [arXiv:0710.0410]. It also expresses the time\nconcept in terms of causing quantized energy f(|E|) of time |t|, emit in regard\nto shortening the probability of particle loci as predictable patterns of\nparticle's un-occurred motion, a solution to Heisenberg's uncertainty principle\n(HUP) into a simplistic manner. We conclude that, practical frames via a time\nalgorithm to this model, fixates such predictable patterns of motion of scenery\nbodies onto recordable observation points of a MMMCV system. It even\nsuppresses/predicts superposition phenomena coming from a human subject and/or\nother bio-subjects for any decision making event, e.g., brainwave quantum\npatterns based on vision. Maintaining the existential probability of Riemann\nsurfaces of II-time scenarios in the context of biovielectroluminescence, makes\nmotion-prediction a possibility.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 6 Nov 2007 19:41:22 GMT"
            }
        ],
        "update_date": "2016-09-08",
        "authors_parsed": [
            [
                "Alipour",
                "Philip B.",
                ""
            ]
        ]
    },
    {
        "id": "0711.0829",
        "submitter": "Kees Middelburg",
        "authors": "J. A. Bergstra, C. A. Middelburg",
        "title": "Instruction sequences with indirect jumps",
        "comments": "23 pages; typos corrected, phrasing improved, reference replaced",
        "journal-ref": "Scientific Annals of Computer Science, 17:19--46, 2007.\n  http://www.infoiasi.ro/bin/download/Annals/XVII/XVII_1.pdf",
        "doi": null,
        "report-no": "PRG0709",
        "categories": "cs.PL",
        "license": null,
        "abstract": "  We study sequential programs that are instruction sequences with direct and\nindirect jump instructions. The intuition is that indirect jump instructions\nare jump instructions where the position of the instruction to jump to is the\ncontent of some memory cell. We consider several kinds of indirect jump\ninstructions. For each kind, we define the meaning of programs with indirect\njump instructions of that kind by means of a translation into programs without\nindirect jump instructions. For each kind, the intended behaviour of a program\nwith indirect jump instructions of that kind under execution is the behaviour\nof the translated program under execution on interaction with some memory\ndevice.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 6 Nov 2007 10:26:03 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 11 Dec 2007 07:58:45 GMT"
            }
        ],
        "update_date": "2008-04-08",
        "authors_parsed": [
            [
                "Bergstra",
                "J. A.",
                ""
            ],
            [
                "Middelburg",
                "C. A.",
                ""
            ]
        ]
    },
    {
        "id": "0711.0836",
        "submitter": "Kees Middelburg",
        "authors": "J. A. Bergstra, C. A. Middelburg",
        "title": "Machine structure oriented control code logic",
        "comments": "32 pages; phrasing improved, references added, connection with\n  Janlert's \"dark programming\" explained",
        "journal-ref": "Acta Informatica, 46(5):375--401, 2009",
        "doi": "10.1007/s00236-009-0099-2",
        "report-no": "PRG0704",
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Control code is a concept that is closely related to a frequently occurring\npractitioner's view on what is a program: code that is capable of controlling\nthe behaviour of some machine. We present a logical approach to explain issues\nconcerning control codes that are independent of the details of the behaviours\nthat are controlled. Using this approach, such issues can be explained at a\nvery abstract level. We illustrate this among other things by means of an\nexample about the production of a new compiler from an existing one. The\napproach is based on abstract machine models, called machine structures. We\nintroduce a model of systems that provide execution environments for the\nexecutable codes of machine structures and use it to go into portability of\ncontrol codes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 6 Nov 2007 11:01:21 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 23 May 2008 10:12:36 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 4 Mar 2009 09:47:36 GMT"
            }
        ],
        "update_date": "2009-09-02",
        "authors_parsed": [
            [
                "Bergstra",
                "J. A.",
                ""
            ],
            [
                "Middelburg",
                "C. A.",
                ""
            ]
        ]
    },
    {
        "id": "0711.0917",
        "submitter": "Jan Wielemaker",
        "authors": "Jan Wielemaker, Zhisheng Huang and Lourens van der Meij",
        "title": "SWI-Prolog and the Web",
        "comments": "31 pages, 24 figures and 2 tables. To appear in Theory and Practice\n  of Logic Programming (TPLP)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SC",
        "license": null,
        "abstract": "  Where Prolog is commonly seen as a component in a Web application that is\neither embedded or communicates using a proprietary protocol, we propose an\narchitecture where Prolog communicates to other components in a Web application\nusing the standard HTTP protocol. By avoiding embedding in external Web servers\ndevelopment and deployment become much easier. To support this architecture, in\naddition to the transfer protocol, we must also support parsing, representing\nand generating the key Web document types such as HTML, XML and RDF.\n  This paper motivates the design decisions in the libraries and extensions to\nProlog for handling Web documents and protocols. The design has been guided by\nthe requirement to handle large documents efficiently. The described libraries\nsupport a wide range of Web applications ranging from HTML and XML documents to\nSemantic Web RDF processing.\n  To appear in Theory and Practice of Logic Programming (TPLP)\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 6 Nov 2007 16:22:39 GMT"
            }
        ],
        "update_date": "2007-11-07",
        "authors_parsed": [
            [
                "Wielemaker",
                "Jan",
                ""
            ],
            [
                "Huang",
                "Zhisheng",
                ""
            ],
            [
                "van der Meij",
                "Lourens",
                ""
            ]
        ]
    },
    {
        "id": "0711.0917",
        "submitter": "Jan Wielemaker",
        "authors": "Jan Wielemaker, Zhisheng Huang and Lourens van der Meij",
        "title": "SWI-Prolog and the Web",
        "comments": "31 pages, 24 figures and 2 tables. To appear in Theory and Practice\n  of Logic Programming (TPLP)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SC",
        "license": null,
        "abstract": "  Where Prolog is commonly seen as a component in a Web application that is\neither embedded or communicates using a proprietary protocol, we propose an\narchitecture where Prolog communicates to other components in a Web application\nusing the standard HTTP protocol. By avoiding embedding in external Web servers\ndevelopment and deployment become much easier. To support this architecture, in\naddition to the transfer protocol, we must also support parsing, representing\nand generating the key Web document types such as HTML, XML and RDF.\n  This paper motivates the design decisions in the libraries and extensions to\nProlog for handling Web documents and protocols. The design has been guided by\nthe requirement to handle large documents efficiently. The described libraries\nsupport a wide range of Web applications ranging from HTML and XML documents to\nSemantic Web RDF processing.\n  To appear in Theory and Practice of Logic Programming (TPLP)\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 6 Nov 2007 16:22:39 GMT"
            }
        ],
        "update_date": "2007-11-07",
        "authors_parsed": [
            [
                "Wielemaker",
                "Jan",
                ""
            ],
            [
                "Huang",
                "Zhisheng",
                ""
            ],
            [
                "van der Meij",
                "Lourens",
                ""
            ]
        ]
    },
    {
        "id": "0711.1226",
        "submitter": "Willemien Visser",
        "authors": "Willemien Visser (INRIA Rocquencourt)",
        "title": "Dynamic aspects of individual design activities. A cognitive ergonomics\n  viewpoint",
        "comments": null,
        "journal-ref": "Human behaviour in design Springer Verlag (Ed.) (2003) 87-96",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  This paper focuses on the use of knowledge possessed by designers. Data\ncollection was based on observations (by the cognitive ergonomics researcher)\nand simultaneous verbalisations (by the designers) in empirical studies\nconducted in the context of industrial design projects. The contribution of\nthis research is typical of cognitive ergonomics, in that it provides data on\nactual activities implemented by designers in their actual work situation\n(rather than on prescribed and/or idealised processes and methods). Data\npresented concern global strategies (the way in which designers actually\norganise their activity) and local strategies (reuse in design). Results from\ncognitive ergonomics and other research that challenges the way in which people\nare supposed to work with existing systems are generally not received warmly.\nAbundant corroboration of such results is required before industry may consider\ntaking them into account. The opportunistic organisation of design activity is\ntaken here as an example of this reluctance. The results concerning this aspect\nof design have been verified repeatedly, but only prototypes and experimental\nsystems implementing some of the requirements formulated on their basis, are\nunder development.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 8 Nov 2007 09:29:57 GMT"
            }
        ],
        "update_date": "2007-11-09",
        "authors_parsed": [
            [
                "Visser",
                "Willemien",
                "",
                "INRIA Rocquencourt"
            ]
        ]
    },
    {
        "id": "0711.1227",
        "submitter": "Willemien Visser",
        "authors": "Willemien Visser (INRIA Rocquencourt)",
        "title": "Designing as Construction of Representations: A Dynamic Viewpoint in\n  Cognitive Design Research",
        "comments": null,
        "journal-ref": "Human-Computer Interaction 21, 1 (2006) 103-152",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  This article presents a cognitively oriented viewpoint on design. It focuses\non cognitive, dynamic aspects of real design, i.e., the actual cognitive\nactivity implemented by designers during their work on professional design\nprojects. Rather than conceiving de-signing as problem solving - Simon's\nsymbolic information processing (SIP) approach - or as a reflective practice or\nsome other form of situated activity - the situativity (SIT) approach - we\nconsider that, from a cognitive viewpoint, designing is most appropriately\ncharacterised as a construction of representations. After a critical discussion\nof the SIP and SIT approaches to design, we present our view-point. This\npresentation concerns the evolving nature of representations regarding levels\nof abstraction and degrees of precision, the function of external\nrepresentations, and specific qualities of representation in collective design.\nDesigning is described at three levels: the organisation of the activity, its\nstrategies, and its design-representation construction activities (different\nways to generate, trans-form, and evaluate representations). Even if we adopt a\n\"generic design\" stance, we claim that design can take different forms\ndepending on the nature of the artefact, and we propose some candidates for\ndimensions that allow a distinction to be made between these forms of design.\nWe discuss the potential specificity of HCI design, and the lack of cognitive\ndesign research occupied with the quality of design. We close our discussion of\nrepresentational structures and activities by an outline of some directions\nregarding their functional linkages.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 8 Nov 2007 09:33:19 GMT"
            }
        ],
        "update_date": "2007-11-09",
        "authors_parsed": [
            [
                "Visser",
                "Willemien",
                "",
                "INRIA Rocquencourt"
            ]
        ]
    },
    {
        "id": "0711.1231",
        "submitter": "Veronika Rehn-Sonigo",
        "authors": "Anne Benoit (INRIA Rh\\^one-Alpes / LIP Laboratoire d'Informatique du\n  Parall\\'elisme, LIP), Veronika Rehn-Sonigo (INRIA Rh\\^one-Alpes / LIP\n  Laboratoire d'Informatique du Parall\\'elisme, LIP), Yves Robert (INRIA\n  Rh\\^one-Alpes / LIP Laboratoire d'Informatique du Parall\\'elisme, LIP)",
        "title": "Optimizing Latency and Reliability of Pipeline Workflow Applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Mapping applications onto heterogeneous platforms is a difficult challenge,\neven for simple application patterns such as pipeline graphs. The problem is\neven more complex when processors are subject to failure during the execution\nof the application. In this paper, we study the complexity of a bi-criteria\nmapping which aims at optimizing the latency (i.e., the response time) and the\nreliability (i.e., the probability that the computation will be successful) of\nthe application. Latency is minimized by using faster processors, while\nreliability is increased by replicating computations on a set of processors.\nHowever, replication increases latency (additional communications, slower\nprocessors). The application fails to be executed only if all the processors\nfail during execution. While simple polynomial algorithms can be found for\nfully homogeneous platforms, the problem becomes NP-hard when tackling\nheterogeneous platforms. This is yet another illustration of the additional\ncomplexity added by heterogeneity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 8 Nov 2007 14:45:12 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 25 Mar 2008 10:43:50 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 26 Mar 2008 09:36:18 GMT"
            }
        ],
        "update_date": "2008-03-26",
        "authors_parsed": [
            [
                "Benoit",
                "Anne",
                "",
                "INRIA Rh\u00f4ne-Alpes / LIP Laboratoire d'Informatique du\n  Parall\u00e9lisme, LIP"
            ],
            [
                "Rehn-Sonigo",
                "Veronika",
                "",
                "INRIA Rh\u00f4ne-Alpes / LIP\n  Laboratoire d'Informatique du Parall\u00e9lisme, LIP"
            ],
            [
                "Robert",
                "Yves",
                "",
                "INRIA\n  Rh\u00f4ne-Alpes / LIP Laboratoire d'Informatique du Parall\u00e9lisme, LIP"
            ]
        ]
    },
    {
        "id": "0711.1401",
        "submitter": "Keki Burjorjee",
        "authors": "Keki Burjorjee",
        "title": "Towards a Sound Theory of Adaptation for the Simple Genetic Algorithm",
        "comments": "Typos corrected. No material changes to content",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The pace of progress in the fields of Evolutionary Computation and Machine\nLearning is currently limited -- in the former field, by the improbability of\nmaking advantageous extensions to evolutionary algorithms when their capacity\nfor adaptation is poorly understood, and in the latter by the difficulty of\nfinding effective semi-principled reductions of hard real-world problems to\nrelatively simple optimization problems. In this paper we explain why a theory\nwhich can accurately explain the simple genetic algorithm's remarkable capacity\nfor adaptation has the potential to address both these limitations. We describe\nwhat we believe to be the impediments -- historic and analytic -- to the\ndiscovery of such a theory and highlight the negative role that the building\nblock hypothesis (BBH) has played. We argue based on experimental results that\na fundamental limitation which is widely believed to constrain the SGA's\nadaptive ability (and is strongly implied by the BBH) is in fact illusionary\nand does not exist. The SGA therefore turns out to be more powerful than it is\ncurrently thought to be. We give conditions under which it becomes feasible to\nnumerically approximate and study the multivariate marginals of the search\ndistribution of an infinite population SGA over multiple generations even when\nits genomes are long, and explain why this analysis is relevant to the riddle\nof the SGA's remarkable adaptive abilities.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 9 Nov 2007 02:28:12 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 3 Apr 2009 15:28:06 GMT"
            }
        ],
        "update_date": "2009-04-03",
        "authors_parsed": [
            [
                "Burjorjee",
                "Keki",
                ""
            ]
        ]
    },
    {
        "id": "0711.1669",
        "submitter": "James Cusick",
        "authors": "James Cusick",
        "title": "Applying Software Defect Estimations: Using a Risk Matrix for Tuning\n  Test Effort",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.OH",
        "license": null,
        "abstract": "  Applying software defect esimation techniques and presenting this information\nin a compact and impactful decision table can clearly illustrate to\ncollaborative groups how critical this position is in the overall development\ncycle. The Test Risk Matrix described here has proven to be a valuable addition\nto the management tools and approaches used in developing large scale software\non several releases. Use of this matrix in development planning meetings can\nclarify the attendant risks and possible consequences of carrying out or\nbypassing specific test activities.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 11 Nov 2007 18:56:53 GMT"
            }
        ],
        "update_date": "2007-11-13",
        "authors_parsed": [
            [
                "Cusick",
                "James",
                ""
            ]
        ]
    },
    {
        "id": "0711.1786",
        "submitter": "Cyril Dumont",
        "authors": "Cyril Dumont (LACL), Fabrice Mourlin (LACL)",
        "title": "A Mobile Computing Architecture for Numerical Simulation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  The domain of numerical simulation is a place where the parallelization of\nnumerical code is common. The definition of a numerical context means the\nconfiguration of resources such as memory, processor load and communication\ngraph, with an evolving feature: the resources availability. A feature is often\nmissing: the adaptability. It is not predictable and the adaptable aspect is\nessential. Without calling into question these implementations of these codes,\nwe create an adaptive use of these implementations. Because the execution has\nto be driven by the availability of main resources, the components of a numeric\ncomputation have to react when their context changes. This paper offers a new\narchitecture, a mobile computing architecture, based on mobile agents and\nJavaSpace. At the end of this paper, we apply our architecture to several case\nstudies and obtain our first results.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 12 Nov 2007 14:39:34 GMT"
            }
        ],
        "update_date": "2007-11-13",
        "authors_parsed": [
            [
                "Dumont",
                "Cyril",
                "",
                "LACL"
            ],
            [
                "Mourlin",
                "Fabrice",
                "",
                "LACL"
            ]
        ]
    },
    {
        "id": "0711.1814",
        "submitter": "Francesca A. Lisi",
        "authors": "Francesca A. Lisi",
        "title": "Building Rules on Top of Ontologies for the Semantic Web with Inductive\n  Logic Programming",
        "comments": "30 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LG",
        "license": null,
        "abstract": "  Building rules on top of ontologies is the ultimate goal of the logical layer\nof the Semantic Web. To this aim an ad-hoc mark-up language for this layer is\ncurrently under discussion. It is intended to follow the tradition of hybrid\nknowledge representation and reasoning systems such as $\\mathcal{AL}$-log that\nintegrates the description logic $\\mathcal{ALC}$ and the function-free Horn\nclausal language \\textsc{Datalog}. In this paper we consider the problem of\nautomating the acquisition of these rules for the Semantic Web. We propose a\ngeneral framework for rule induction that adopts the methodological apparatus\nof Inductive Logic Programming and relies on the expressive and deductive power\nof $\\mathcal{AL}$-log. The framework is valid whatever the scope of induction\n(description vs. prediction) is. Yet, for illustrative purposes, we also\ndiscuss an instantiation of the framework which aims at description and turns\nout to be useful in Ontology Refinement.\n  Keywords: Inductive Logic Programming, Hybrid Knowledge Representation and\nReasoning Systems, Ontologies, Semantic Web.\n  Note: To appear in Theory and Practice of Logic Programming (TPLP)\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 12 Nov 2007 17:15:34 GMT"
            }
        ],
        "update_date": "2007-11-13",
        "authors_parsed": [
            [
                "Lisi",
                "Francesca A.",
                ""
            ]
        ]
    },
    {
        "id": "0711.1856",
        "submitter": "Sumanth Gangasani",
        "authors": "Sumanth Kumar Reddy Gangasani",
        "title": "Testing Kak's Conjecture on Binary Reciprocal of Primes and\n  Cryptographic Applications",
        "comments": "5 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  This note considers reciprocal of primes in binary representation and shows\nthat the conjecture that 0s exceed 1s in most cases continues to hold for\nprimes less one million. The conjecture has also been tested for ternary\nrepresentation with similar results. Some applications of this result to\ncryptography are discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 12 Nov 2007 23:48:53 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 2 Jan 2008 21:24:44 GMT"
            }
        ],
        "update_date": "2008-01-02",
        "authors_parsed": [
            [
                "Gangasani",
                "Sumanth Kumar Reddy",
                ""
            ]
        ]
    },
    {
        "id": "0711.2023",
        "submitter": "Peter Turney",
        "authors": "Peter D. Turney (National Research Council of Canada)",
        "title": "Empirical Evaluation of Four Tensor Decomposition Algorithms",
        "comments": "related work available at http://purl.org/peter.turney/",
        "journal-ref": null,
        "doi": null,
        "report-no": "ERB-1152, NRC-49877",
        "categories": "cs.LG cs.CL cs.IR",
        "license": null,
        "abstract": "  Higher-order tensor decompositions are analogous to the familiar Singular\nValue Decomposition (SVD), but they transcend the limitations of matrices\n(second-order tensors). SVD is a powerful tool that has achieved impressive\nresults in information retrieval, collaborative filtering, computational\nlinguistics, computational vision, and other fields. However, SVD is limited to\ntwo-dimensional arrays of data (two modes), and many potential applications\nhave three or more modes, which require higher-order tensor decompositions.\nThis paper evaluates four algorithms for higher-order tensor decomposition:\nHigher-Order Singular Value Decomposition (HO-SVD), Higher-Order Orthogonal\nIteration (HOOI), Slice Projection (SP), and Multislice Projection (MP). We\nmeasure the time (elapsed run time), space (RAM and disk space requirements),\nand fit (tensor reconstruction accuracy) of the four algorithms, under a\nvariety of conditions. We find that standard implementations of HO-SVD and HOOI\ndo not scale up to larger tensors, due to increasing RAM requirements. We\nrecommend HOOI for tensors that are small enough for the available RAM and MP\nfor larger tensors.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 13 Nov 2007 16:28:47 GMT"
            }
        ],
        "update_date": "2007-11-14",
        "authors_parsed": [
            [
                "Turney",
                "Peter D.",
                "",
                "National Research Council of Canada"
            ]
        ]
    },
    {
        "id": "0711.2062",
        "submitter": "Thomas Sandholm",
        "authors": "Thomas Sandholm",
        "title": "Autoregressive Time Series Forecasting of Computational Demand",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  We study the predictive power of autoregressive moving average models when\nforecasting demand in two shared computational networks, PlanetLab and Tycoon.\nDemand in these networks is very volatile, and predictive techniques to plan\nusage in advance can improve the performance obtained drastically.\n  Our key finding is that a random walk predictor performs best for\none-step-ahead forecasts, whereas ARIMA(1,1,0) and adaptive exponential\nsmoothing models perform better for two and three-step-ahead forecasts. A Monte\nCarlo bootstrap test is proposed to evaluate the continuous prediction\nperformance of different models with arbitrary confidence and statistical\nsignificance levels. Although the prediction results differ between the Tycoon\nand PlanetLab networks, we observe very similar overall statistical properties,\nsuch as volatility dynamics.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 14 Nov 2007 00:57:13 GMT"
            }
        ],
        "update_date": "2007-11-15",
        "authors_parsed": [
            [
                "Sandholm",
                "Thomas",
                ""
            ]
        ]
    },
    {
        "id": "0711.2087",
        "submitter": "Edna Ruckhaus",
        "authors": "Edna Ruckhaus, Eduardo Ruiz, Maria-Esther Vidal",
        "title": "Query Evaluation and Optimization in the Semantic Web",
        "comments": "18 pages, 8 figures, 7 tables. Presented at the ALPSWS2006 First\n  International Workshop on Applications of Logic Programming in the Semantic\n  Web and Semantic Web Services where it got a \"Best Paper Award\". To appear in\n  Theory and Practice of Logic Programming (TPLP)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.LO",
        "license": null,
        "abstract": "  We address the problem of answering Web ontology queries efficiently. An\nontology is formalized as a Deductive Ontology Base (DOB), a deductive database\nthat comprises the ontology's inference axioms and facts. A cost-based query\noptimization technique for DOB is presented. A hybrid cost model is proposed to\nestimate the cost and cardinality of basic and inferred facts. Cardinality and\ncost of inferred facts are estimated using an adaptive sampling technique,\nwhile techniques of traditional relational cost models are used for estimating\nthe cost of basic facts and conjunctive ontology queries. Finally, we implement\na dynamic-programming optimization algorithm to identify query evaluation plans\nthat minimize the number of intermediate inferred facts. We modeled a subset of\nthe Web ontology language OWL Lite as a DOB, and performed an experimental\nstudy to analyze the predictive capacity of our cost model and the benefits of\nthe query optimization technique. Our study has been conducted over synthetic\nand real-world OWL ontologies, and shows that the techniques are accurate and\nimprove query performance. To appear in Theory and Practice of Logic\nProgramming (TPLP).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 13 Nov 2007 22:31:09 GMT"
            }
        ],
        "update_date": "2007-11-15",
        "authors_parsed": [
            [
                "Ruckhaus",
                "Edna",
                ""
            ],
            [
                "Ruiz",
                "Eduardo",
                ""
            ],
            [
                "Vidal",
                "Maria-Esther",
                ""
            ]
        ]
    },
    {
        "id": "0711.2104",
        "submitter": "Arthur Cunha",
        "authors": "Arthur Cunha, Minh Do, and Martin Vetterli",
        "title": "On the Information Rates of the Plenoptic Function",
        "comments": "submitted to IEEE Transactions in Information Theory",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.CV math.IT math.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The {\\it plenoptic function} (Adelson and Bergen, 91) describes the visual\ninformation available to an observer at any point in space and time. Samples of\nthe plenoptic function (POF) are seen in video and in general visual content,\nand represent large amounts of information. In this paper we propose a\nstochastic model to study the compression limits of the plenoptic function. In\nthe proposed framework, we isolate the two fundamental sources of information\nin the POF: the one representing the camera motion and the other representing\nthe information complexity of the \"reality\" being acquired and transmitted. The\nsources of information are combined, generating a stochastic process that we\nstudy in detail. We first propose a model for ensembles of realities that do\nnot change over time. The proposed model is simple in that it enables us to\nderive precise coding bounds in the information-theoretic sense that are sharp\nin a number of cases of practical interest. For this simple case of static\nrealities and camera motion, our results indicate that coding practice is in\naccordance with optimal coding from an information-theoretic standpoint. The\nmodel is further extended to account for visual realities that change over\ntime. We derive bounds on the lossless and lossy information rates for this\ndynamic reality model, stating conditions under which the bounds are tight.\nExamples with synthetic sources suggest that in the presence of scene dynamics,\nsimple hybrid coding using motion/displacement estimation with DPCM performs\nconsiderably suboptimally relative to the true rate-distortion bound.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 14 Nov 2007 02:54:22 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 19 Nov 2007 02:53:30 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 7 Aug 2009 18:11:32 GMT"
            }
        ],
        "update_date": "2009-08-08",
        "authors_parsed": [
            [
                "Cunha",
                "Arthur",
                ""
            ],
            [
                "Do",
                "Minh",
                ""
            ],
            [
                "Vetterli",
                "Martin",
                ""
            ]
        ]
    },
    {
        "id": "0711.2116",
        "submitter": "Frederic Vignat",
        "authors": "Fr\\'ed\\'eric Vignat (LGS), Fran\\c{c}ois Villeneuve (LGS)",
        "title": "A numerical approach for 3D manufacturing tolerances synthesis",
        "comments": null,
        "journal-ref": "Dans Proceedings of the 10th CIRP International Seminar on\n  Computer Aided Tolerancing - 10th CIRP International Seminar on Computer\n  Aided Tolerancing, Erlangen : Allemagne (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": null,
        "abstract": "  Making a product conform to the functional requirements indicated by the\ncustomer suppose to be able to manage the manufacturing process chosen to\nrealise the parts. A simulation step is generally performed to verify that the\nexpected generated deviations fit with these requirements. It is then necessary\nto assess the actual deviations of the process in progress. This is usually\ndone by the verification of the conformity of the workpiece to manufacturing\ntolerances at the end of each set-up. It is thus necessary to determine these\nmanufacturing tolerances. This step is called \"manufacturing tolerance\nsynthesis\". In this paper, a numerical method is proposed to perform 3D\nmanufacturing tolerances synthesis. This method uses the result of the\nnumerical analysis of tolerances to determine influent mall displacement of\nsurfaces. These displacements are described by small displacements torsors. An\nalgorithm is then proposed to determine suitable ISO manufacturing tolerances.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 14 Nov 2007 06:21:17 GMT"
            }
        ],
        "update_date": "2007-11-15",
        "authors_parsed": [
            [
                "Vignat",
                "Fr\u00e9d\u00e9ric",
                "",
                "LGS"
            ],
            [
                "Villeneuve",
                "Fran\u00e7ois",
                "",
                "LGS"
            ]
        ]
    },
    {
        "id": "0711.2239",
        "submitter": "Guillaume Thomann",
        "authors": "Guillaume Thomann (LGS), Jean Caelen (LIG), Morgan Verdier (LGS),\n  Brigitte Meillon (LIG)",
        "title": "Mise en place de sc\\'enarios pour la conception d'outils en Chirurgie\n  Minimalement Invasive",
        "comments": null,
        "journal-ref": "Les Syst\\`emes de Production, Lavoisier (Ed.) (2007) 93-107",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  Nowadays, more and more surgical interventions are carried out in Minimally\nInvasive Surgery, to make the post-operative constraints less painful for the\npatient. Actually, new surgical tools or medical products are designed after\ninformal discussions between surgeons and designers. The user requirements\ndocuments are the main document used by the mechanical designers to design and\nimprove the product. Medical terms, often used by surgeons and employed to\nexplain their needs, don't allow for an instantaneous understanding by\ndesigners. Unfortunately, this relation causes a dysfunction in the definition\ncycle of the product. Our aim is to work on the design process, its assistance\nthanks to methods and tools and on its organisation for better understandings\nand more complementarities between surgeons and designers. We propose on this\narticle a design method already tested in the informatics domain: User Centred\nDesign which takes the user into account more effectively in the process\ndesign. We already propose a scenario oriented design method centred on the\nuser and represented as a scenario (Scenario-Based Design). We start in this\narticle by clarifying these concepts before detailing their implementation for\nthe design of a new surgical tool.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 14 Nov 2007 16:18:42 GMT"
            }
        ],
        "update_date": "2007-11-19",
        "authors_parsed": [
            [
                "Thomann",
                "Guillaume",
                "",
                "LGS"
            ],
            [
                "Caelen",
                "Jean",
                "",
                "LIG"
            ],
            [
                "Verdier",
                "Morgan",
                "",
                "LGS"
            ],
            [
                "Meillon",
                "Brigitte",
                "",
                "LIG"
            ]
        ]
    },
    {
        "id": "0711.2478",
        "submitter": "Vasileios Barmpoutis",
        "authors": "Vasileios Barmpoutis, Gary F. Dargush",
        "title": "A Compact Self-organizing Cellular Automata-based Genetic Algorithm",
        "comments": "24 pages, 18 figures, Submitted to Evolutionary Computation",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  A Genetic Algorithm (GA) is proposed in which each member of the population\ncan change schemata only with its neighbors according to a rule. The rule\nmethodology and the neighborhood structure employ elements from the Cellular\nAutomata (CA) strategies. Each member of the GA population is assigned to a\ncell and crossover takes place only between adjacent cells, according to the\npredefined rule. Although combinations of CA and GA approaches have appeared\npreviously, here we rely on the inherent self-organizing features of CA, rather\nthan on parallelism. This conceptual shift directs us toward the evolution of\ncompact populations containing only a handful of members. We find that the\nresulting algorithm can search the design space more efficiently than\ntraditional GA strategies due to its ability to exploit mutations within this\ncompact self-organizing population. Consequently, premature convergence is\navoided and the final results often are more accurate. In order to reinforce\nthe superior mutation capability, a re-initialization strategy also is\nimplemented. Ten test functions and two benchmark structural engineering truss\ndesign problems are examined in order to demonstrate the performance of the\nmethod.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 15 Nov 2007 18:19:39 GMT"
            }
        ],
        "update_date": "2007-11-16",
        "authors_parsed": [
            [
                "Barmpoutis",
                "Vasileios",
                ""
            ],
            [
                "Dargush",
                "Gary F.",
                ""
            ]
        ]
    },
    {
        "id": "0711.2486",
        "submitter": "Jean-Francois Boujut",
        "authors": "Onur Hisarciklilar (LGS), Jean-Fran\\c{c}ois Boujut (LGS)",
        "title": "An annotation based approach to support design communication",
        "comments": null,
        "journal-ref": "Dans Proceedings of ICED'07 - International Conference on\n  Engineering Design, Paris : France (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  The aim of this paper is to propose an approach based on the concept of\nannotation for supporting design communication. In this paper, we describe a\nco-operative design case study where we analyse some annotation practices,\nmainly focused on design minutes recorded during project reviews. We point out\nspecific requirements concerning annotation needs. Based on these requirements,\nwe propose an annotation model, inspired from the Speech Act Theory (SAT) to\nsupport communication in a 3D digital environment. We define two types of\nannotations in the engineering design context, locutionary and illocutionary\nannotations. The annotations we describe in this paper are materialised by a\nset of digital artefacts, which have a semantic dimension allowing\nexpress/record elements of technical justifications, traces of contradictory\ndebates, etc. In this paper, we first clarify the semantic annotation concept,\nand we define general properties of annotations in the engineering design\ncontext, and the role of annotations in different design project situations.\nAfter the description of the case study, where we observe and analyse\nannotations usage during the design reviews and minute making, the last section\nis dedicated to present our approach. We then describe the SAT concept, and\ndefine the concept of annotation acts. We conclude with a description of basic\nannotation functionalities that are actually implemented in a software, based\non our approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 15 Nov 2007 19:22:16 GMT"
            }
        ],
        "update_date": "2007-11-16",
        "authors_parsed": [
            [
                "Hisarciklilar",
                "Onur",
                "",
                "LGS"
            ],
            [
                "Boujut",
                "Jean-Fran\u00e7ois",
                "",
                "LGS"
            ]
        ]
    },
    {
        "id": "0711.2615",
        "submitter": "Franco Bagnoli",
        "authors": "Franco Bagnoli, Francesca Di Patti",
        "title": "A Biologically Inspired Classifier",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.IR",
        "license": null,
        "abstract": "  We present a method for measuring the distance among records based on the\ncorrelations of data stored in the corresponding database entries. The original\nmethod (F. Bagnoli, A. Berrones and F. Franci. Physica A 332 (2004) 509-518)\nwas formulated in the context of opinion formation. The opinions expressed over\na set of topic originate a ``knowledge network'' among individuals, where two\nindividuals are nearer the more similar their expressed opinions are. Assuming\nthat individuals' opinions are stored in a database, the authors show that it\nis possible to anticipate an opinion using the correlations in the database.\nThis corresponds to approximating the overlap between the tastes of two\nindividuals with the correlations of their expressed opinions.\n  In this paper we extend this model to nonlinear matching functions, inspired\nby biological problems such as microarray (probe-sample pairing). We\ninvestigate numerically the error between the correlation and the overlap\nmatrix for eight sequences of reference with random probes. Results show that\nthis method is particularly robust for detecting similarities in the presence\nof translocations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 16 Nov 2007 13:38:15 GMT"
            }
        ],
        "update_date": "2007-11-19",
        "authors_parsed": [
            [
                "Bagnoli",
                "Franco",
                ""
            ],
            [
                "Di Patti",
                "Francesca",
                ""
            ]
        ]
    },
    {
        "id": "0711.2618",
        "submitter": "Krzysztof R. Apt",
        "authors": "Krzysztof R. Apt, Farhad Arbab, Huiye Ma",
        "title": "A System for Distributed Mechanisms: Design, Implementation and\n  Applications",
        "comments": "36 pages; revised and expanded version",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.GT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe here a structured system for distributed mechanism design\nappropriate for both Intranet and Internet applications. In our approach the\nplayers dynamically form a network in which they know neither their neighbours\nnor the size of the network and interact to jointly take decisions. The only\nassumption concerning the underlying communication layer is that for each pair\nof processes there is a path of neighbours connecting them. This allows us to\ndeal with arbitrary network topologies.\n  We also discuss the implementation of this system which consists of a\nsequence of layers. The lower layers deal with the operations that implement\nthe basic primitives of distributed computing, namely low level communication\nand distributed termination, while the upper layers use these primitives to\nimplement high level communication among players, including broadcasting and\nmulticasting, and distributed decision making.\n  This yields a highly flexible distributed system whose specific applications\nare realized as instances of its top layer. This design is implemented in Java.\n  The system supports at various levels fault-tolerance and includes a\nprovision for distributed policing the purpose of which is to exclude\n`dishonest' players. Also, it can be used for repeated creation of dynamically\nformed networks of players interested in a joint decision making implemented by\nmeans of a tax-based mechanism. We illustrate its flexibility by discussing a\nnumber of implemented examples.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 16 Nov 2007 14:10:16 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 20 Feb 2008 15:10:54 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 11 Jan 2010 13:23:13 GMT"
            },
            {
                "version": "v4",
                "created": "Tue, 20 Sep 2011 15:42:02 GMT"
            }
        ],
        "update_date": "2011-09-21",
        "authors_parsed": [
            [
                "Apt",
                "Krzysztof R.",
                ""
            ],
            [
                "Arbab",
                "Farhad",
                ""
            ],
            [
                "Ma",
                "Huiye",
                ""
            ]
        ]
    },
    {
        "id": "0711.2760",
        "submitter": "Vita Hinze-Hoare",
        "authors": "Vita Hinze-Hoare",
        "title": "Computer Supported Collaborative Research",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  It is suggested that a new area of CSCR (Computer Supported Collaborative\nResearch) is distinguished from CSCW (Computer Supported Collaborative Work)\nand CSCL (Computer Supported Collaborative Learning) and that the demarcation\nbetween the three areas could do with greater clarification and prescription.\n  Although the areas of Human Computer Interaction (HCI), CSCW, and CSCL are\nnow relatively well established, the related field of Computer Supported\nCollaborative Research (CSCR) is new and little understood. An analysis of the\nprinciples and issues behind CSCR is undertaken with a view to determining\nprecisely its nature and scope and to delineate it clearly from CSCW and CSCL.\nThis determination is such that it is generally applicable to the building,\ndesign and evaluation of collaborative research environments.\n  A particular instance of the CSCR domain is then examined in order to\ndetermine the requirements of a collaborative research environment for students\nand supervisors (CRESS).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 17 Nov 2007 20:08:41 GMT"
            }
        ],
        "update_date": "2007-11-20",
        "authors_parsed": [
            [
                "Hinze-Hoare",
                "Vita",
                ""
            ]
        ]
    },
    {
        "id": "0711.2801",
        "submitter": "Xinjia Chen",
        "authors": "Xinjia Chen",
        "title": "Inverse Sampling for Nonasymptotic Sequential Estimation of Bounded\n  Variable Means",
        "comments": "31 pages, 4 figures, added proofs",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.ST cs.LG math.PR stat.TH",
        "license": null,
        "abstract": "  In this paper, we consider the nonasymptotic sequential estimation of means\nof random variables bounded in between zero and one. We have rigorously\ndemonstrated that, in order to guarantee prescribed relative precision and\nconfidence level, it suffices to continue sampling until the sample sum is no\nless than a certain bound and then take the average of samples as an estimate\nfor the mean of the bounded random variable. We have developed an explicit\nformula and a bisection search method for the determination of such bound of\nsample sum, without any knowledge of the bounded variable. Moreover, we have\nderived bounds for the distribution of sample size. In the special case of\nBernoulli random variables, we have established analytical and numerical\nmethods to further reduce the bound of sample sum and thus improve the\nefficiency of sampling. Furthermore, the fallacy of existing results are\ndetected and analyzed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 18 Nov 2007 17:28:23 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 2 Dec 2007 21:59:44 GMT"
            }
        ],
        "update_date": "2013-11-05",
        "authors_parsed": [
            [
                "Chen",
                "Xinjia",
                ""
            ]
        ]
    },
    {
        "id": "0711.2811",
        "submitter": "Annie Bouyer",
        "authors": "Gilles Halin (CRAI), Sylvain Kubicki (CRAI)",
        "title": "Une approche par les mod\\`eles pour le suivi de l'activit\\'e de\n  construction d'un b\\^atiment. Bat'iViews : une interface multi-vues\n  orient\\'ee gestion de chantier",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  Cooperation between actors in design and construction activities in\narchitecture is an essential stake nowadays. In professional practices the\nactors involved in construction projects use numerous tools. The project is\nunique but the \"views\" that actors manipulate are various and sometimes\nfundamentally different. Their common characteristic is that they partially\nrepresent the cooperation context through a specific point of view.\n\"Bat'iViews\" suggests to the actors a multi-view interface of the context and\nenables to navigate through the different views. This proposition is based on a\nmodel-driven approach. We distinguish between \"context modelling\" and modelling\nof concepts represented in each \"businessview\". A model integrative\ninfrastructure allows us to develop the prototype and to manage user\ninteraction through the definition of models' transformations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 18 Nov 2007 19:10:18 GMT"
            }
        ],
        "update_date": "2007-11-20",
        "authors_parsed": [
            [
                "Halin",
                "Gilles",
                "",
                "CRAI"
            ],
            [
                "Kubicki",
                "Sylvain",
                "",
                "CRAI"
            ]
        ]
    },
    {
        "id": "0711.2895",
        "submitter": "Stephanie Wehner",
        "authors": "Stephanie Wehner, Christian Schaffner, Barbara Terhal",
        "title": "Cryptography from Noisy Storage",
        "comments": "13 pages RevTex, 2 figures. v2: more comments on implementation\n  dependent attacks, v3: published version (minor changes)",
        "journal-ref": "Phys. Rev. Lett. 100, 220502 (2008)",
        "doi": "10.1103/PhysRevLett.100.220502",
        "report-no": null,
        "categories": "quant-ph cs.CR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show how to implement cryptographic primitives based on the realistic\nassumption that quantum storage of qubits is noisy. We thereby consider\nindividual-storage attacks, i.e. the dishonest party attempts to store each\nincoming qubit separately. Our model is similar to the model of bounded-quantum\nstorage, however, we consider an explicit noise model inspired by present-day\ntechnology. To illustrate the power of this new model, we show that a protocol\nfor oblivious transfer (OT) is secure for any amount of quantum-storage noise,\nas long as honest players can perform perfect quantum operations. Our model\nalso allows the security of protocols that cope with noise in the operations of\nthe honest players and achieve more advanced tasks such as secure\nidentification.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 19 Nov 2007 16:50:08 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 29 Nov 2007 16:43:09 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 20 Jun 2008 18:48:03 GMT"
            }
        ],
        "update_date": "2008-06-20",
        "authors_parsed": [
            [
                "Wehner",
                "Stephanie",
                ""
            ],
            [
                "Schaffner",
                "Christian",
                ""
            ],
            [
                "Terhal",
                "Barbara",
                ""
            ]
        ]
    },
    {
        "id": "0711.2897",
        "submitter": "Michael Tung M.",
        "authors": "J. Izquierdo, M.M. Tung, R. Perez, F. J. Martinez",
        "title": "Estimation of fuzzy anomalies in Water Distribution Systems",
        "comments": "5 pages",
        "journal-ref": "Progress in Industrial Mathematics at ECMI 2006 (edited by L. L.\n  Bonilla, M. A. Moscoso, G. Platero, and J. M. Vega), vol. 12 of Mathematics\n  in Industry, pp. 801-805 (Springer, Berlin, 2007), ISBN 978-3-540-71991-5",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  State estimation is necessary in diagnosing anomalies in Water Demand Systems\n(WDS). In this paper we present a neural network performing such a task. State\nestimation is performed by using optimization, which tries to reconcile all the\navailable information. Quantification of the uncertainty of the input data\n(telemetry measures and demand predictions) can be achieved by means of robust\nestate estimation. Using a mathematical model of the network, fuzzy estimated\nstates for anomalous states of the network can be obtained. They are used to\ntrain a neural network capable of assessing WDS anomalies associated with\nparticular sets of measurements.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 19 Nov 2007 11:24:47 GMT"
            }
        ],
        "update_date": "2007-11-20",
        "authors_parsed": [
            [
                "Izquierdo",
                "J.",
                ""
            ],
            [
                "Tung",
                "M. M.",
                ""
            ],
            [
                "Perez",
                "R.",
                ""
            ],
            [
                "Martinez",
                "F. J.",
                ""
            ]
        ]
    },
    {
        "id": "0711.2914",
        "submitter": "Tshilidzi Marwala",
        "authors": "Gidudu Anthony, Hulley Gregg and Marwala Tshilidzi",
        "title": "Image Classification Using SVMs: One-against-One Vs One-against-All",
        "comments": "Proccedings of the 28th Asian Conference on Remote Sensing, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI cs.CV",
        "license": null,
        "abstract": "  Support Vector Machines (SVMs) are a relatively new supervised classification\ntechnique to the land cover mapping community. They have their roots in\nStatistical Learning Theory and have gained prominence because they are robust,\naccurate and are effective even when using a small training sample. By their\nnature SVMs are essentially binary classifiers, however, they can be adopted to\nhandle the multiple classification tasks common in remote sensing studies. The\ntwo approaches commonly used are the One-Against-One (1A1) and One-Against-All\n(1AA) techniques. In this paper, these approaches are evaluated in as far as\ntheir impact and implication for land cover mapping. The main finding from this\nresearch is that whereas the 1AA technique is more predisposed to yielding\nunclassified and mixed pixels, the resulting classification accuracy is not\nsignificantly different from 1A1 approach. It is the authors conclusion\ntherefore that ultimately the choice of technique adopted boils down to\npersonal preference and the uniqueness of the dataset at hand.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 19 Nov 2007 12:25:00 GMT"
            }
        ],
        "update_date": "2007-11-20",
        "authors_parsed": [
            [
                "Anthony",
                "Gidudu",
                ""
            ],
            [
                "Gregg",
                "Hulley",
                ""
            ],
            [
                "Tshilidzi",
                "Marwala",
                ""
            ]
        ]
    },
    {
        "id": "0711.2914",
        "submitter": "Tshilidzi Marwala",
        "authors": "Gidudu Anthony, Hulley Gregg and Marwala Tshilidzi",
        "title": "Image Classification Using SVMs: One-against-One Vs One-against-All",
        "comments": "Proccedings of the 28th Asian Conference on Remote Sensing, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI cs.CV",
        "license": null,
        "abstract": "  Support Vector Machines (SVMs) are a relatively new supervised classification\ntechnique to the land cover mapping community. They have their roots in\nStatistical Learning Theory and have gained prominence because they are robust,\naccurate and are effective even when using a small training sample. By their\nnature SVMs are essentially binary classifiers, however, they can be adopted to\nhandle the multiple classification tasks common in remote sensing studies. The\ntwo approaches commonly used are the One-Against-One (1A1) and One-Against-All\n(1AA) techniques. In this paper, these approaches are evaluated in as far as\ntheir impact and implication for land cover mapping. The main finding from this\nresearch is that whereas the 1AA technique is more predisposed to yielding\nunclassified and mixed pixels, the resulting classification accuracy is not\nsignificantly different from 1A1 approach. It is the authors conclusion\ntherefore that ultimately the choice of technique adopted boils down to\npersonal preference and the uniqueness of the dataset at hand.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 19 Nov 2007 12:25:00 GMT"
            }
        ],
        "update_date": "2007-11-20",
        "authors_parsed": [
            [
                "Anthony",
                "Gidudu",
                ""
            ],
            [
                "Gregg",
                "Hulley",
                ""
            ],
            [
                "Tshilidzi",
                "Marwala",
                ""
            ]
        ]
    },
    {
        "id": "0711.2971",
        "submitter": "Annie Bouyer",
        "authors": "Sylvain Kubicki (MAP / CRAI), Annie Guerri\\'ero (MAP / CRAI), Damien\n  Hanser (MAP / CRAI), Gilles Halin (MAP / CRAI)",
        "title": "IT services design to support coordination practices in the\n  Luxembourguish AEC sector",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  In the Architecture Engineering and Construction sector (AEC) cooperation\nbetween actors is essential for project success. The configuration of actors'\norganization takes different forms like the associated coordination mechanisms.\nOur approach consists in analyzing these coordination mechanisms through the\nidentification of the \"base practices\" realized by the actors of a construction\nproject to cooperate. We also try with practitioners to highlight the \"best\npractices\" of cooperation. Then we suggest here two prototypes of IT services\naiming to demonstrate the value added of IT to support cooperation. These\nprototype tools allow us to sensitize the actors through terrain experiments\nand then to bring inch by inch the Luxembourgish AEC sector towards electronic\ncooperation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 19 Nov 2007 16:36:02 GMT"
            }
        ],
        "update_date": "2007-11-20",
        "authors_parsed": [
            [
                "Kubicki",
                "Sylvain",
                "",
                "MAP / CRAI"
            ],
            [
                "Guerri\u00e9ro",
                "Annie",
                "",
                "MAP / CRAI"
            ],
            [
                "Hanser",
                "Damien",
                "",
                "MAP / CRAI"
            ],
            [
                "Halin",
                "Gilles",
                "",
                "MAP / CRAI"
            ]
        ]
    },
    {
        "id": "0711.3375",
        "submitter": "Loredana Afanasiev",
        "authors": "Loredana Afanasiev, Torsten Grust, Maarten Marx, Jan Rittinger, Jens\n  Teubner",
        "title": "An Inflationary Fixed Point Operator in XQuery",
        "comments": "11 pages, 10 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  We introduce a controlled form of recursion in XQuery, inflationary fixed\npoints, familiar in the context of relational databases. This imposes\nrestrictions on the expressible types of recursion, but we show that\ninflationary fixed points nevertheless are sufficiently versatile to capture a\nwide range of interesting use cases, including the semantics of Regular XPath\nand its core transitive closure construct.\n  While the optimization of general user-defined recursive functions in XQuery\nappears elusive, we will describe how inflationary fixed points can be\nefficiently evaluated, provided that the recursive XQuery expressions exhibit a\ndistributivity property. We show how distributivity can be assessed both,\nsyntactically and algebraically, and provide experimental evidence that XQuery\nprocessors can substantially benefit during inflationary fixed point\nevaluation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 21 Nov 2007 13:22:15 GMT"
            }
        ],
        "update_date": "2007-11-22",
        "authors_parsed": [
            [
                "Afanasiev",
                "Loredana",
                ""
            ],
            [
                "Grust",
                "Torsten",
                ""
            ],
            [
                "Marx",
                "Maarten",
                ""
            ],
            [
                "Rittinger",
                "Jan",
                ""
            ],
            [
                "Teubner",
                "Jens",
                ""
            ]
        ]
    },
    {
        "id": "0711.3500",
        "submitter": "Shiguo Lian",
        "authors": "Shiguo Lian",
        "title": "Secure Fractal Image Coding",
        "comments": "21 pages, 8 figures. To be submitted",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MM cs.CR",
        "license": null,
        "abstract": "  In recent work, various fractal image coding methods are reported, which\nadopt the self-similarity of images to compress the size of images. However,\ntill now, no solutions for the security of fractal encoded images have been\nprovided. In this paper, a secure fractal image coding scheme is proposed and\nevaluated, which encrypts some of the fractal parameters during fractal\nencoding, and thus, produces the encrypted and encoded image. The encrypted\nimage can only be recovered by the correct key. To keep secure and efficient,\nonly the suitable parameters are selected and encrypted through in-vestigating\nthe properties of various fractal parameters, including parameter space,\nparameter distribu-tion and parameter sensitivity. The encryption process does\nnot change the file format, keeps secure in perception, and costs little time\nor computational resources. These properties make it suitable for secure image\nencoding or transmission.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 22 Nov 2007 03:53:29 GMT"
            }
        ],
        "update_date": "2007-11-26",
        "authors_parsed": [
            [
                "Lian",
                "Shiguo",
                ""
            ]
        ]
    },
    {
        "id": "0711.3580",
        "submitter": "Giovanni Feverati",
        "authors": "Giovanni Feverati, Fabio Musso",
        "title": "An evolutionary model with Turing machines",
        "comments": "16 pages, 7 figures",
        "journal-ref": null,
        "doi": "10.1103/PhysRevE.77.061901",
        "report-no": "LAPTH-1217/07",
        "categories": "q-bio.QM cs.NE q-bio.GN",
        "license": null,
        "abstract": "  The development of a large non-coding fraction in eukaryotic DNA and the\nphenomenon of the code-bloat in the field of evolutionary computations show a\nstriking similarity. This seems to suggest that (in the presence of mechanisms\nof code growth) the evolution of a complex code can't be attained without\nmaintaining a large inactive fraction. To test this hypothesis we performed\ncomputer simulations of an evolutionary toy model for Turing machines, studying\nthe relations among fitness and coding/non-coding ratio while varying mutation\nand code growth rates. The results suggest that, in our model, having a large\nreservoir of non-coding states constitutes a great (long term) evolutionary\nadvantage.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 22 Nov 2007 14:47:06 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Feverati",
                "Giovanni",
                ""
            ],
            [
                "Musso",
                "Fabio",
                ""
            ]
        ]
    },
    {
        "id": "0711.3591",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin, Edmund Burke and Jingpeng Li",
        "title": "An Estimation of Distribution Algorithm with Intelligent Local Search\n  for Rule-based Nurse Rostering",
        "comments": null,
        "journal-ref": "Journal of the Operational Research Society, 58 (12), pp\n  1574-1585, 2007",
        "doi": "10.1057/palgrave.jors.2602308",
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": null,
        "abstract": "  This paper proposes a new memetic evolutionary algorithm to achieve explicit\nlearning in rule-based nurse rostering, which involves applying a set of\nheuristic rules for each nurse's assignment. The main framework of the\nalgorithm is an estimation of distribution algorithm, in which an ant-miner\nmethodology improves the individual solutions produced in each generation.\nUnlike our previous work (where learning is implicit), the learning in the\nmemetic estimation of distribution algorithm is explicit, i.e. we are able to\nidentify building blocks directly. The overall approach learns by building a\nprobabilistic model, i.e. an estimation of the probability distribution of\nindividual nurse-rule pairs that are used to construct schedules. The local\nsearch processor (i.e. the ant-miner) reinforces nurse-rule pairs that receive\nhigher rewards. A challenging real world nurse rostering problem is used as the\ntest problem. Computational results show that the proposed approach outperforms\nmost existing approaches. It is suggested that the learning methodologies\nsuggested in this paper may be applied to other scheduling problems where\nschedules are built systematically according to specific rules\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 22 Nov 2007 15:16:21 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:14:51 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Burke",
                "Edmund",
                ""
            ],
            [
                "Li",
                "Jingpeng",
                ""
            ]
        ]
    },
    {
        "id": "0711.3591",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin, Edmund Burke and Jingpeng Li",
        "title": "An Estimation of Distribution Algorithm with Intelligent Local Search\n  for Rule-based Nurse Rostering",
        "comments": null,
        "journal-ref": "Journal of the Operational Research Society, 58 (12), pp\n  1574-1585, 2007",
        "doi": "10.1057/palgrave.jors.2602308",
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": null,
        "abstract": "  This paper proposes a new memetic evolutionary algorithm to achieve explicit\nlearning in rule-based nurse rostering, which involves applying a set of\nheuristic rules for each nurse's assignment. The main framework of the\nalgorithm is an estimation of distribution algorithm, in which an ant-miner\nmethodology improves the individual solutions produced in each generation.\nUnlike our previous work (where learning is implicit), the learning in the\nmemetic estimation of distribution algorithm is explicit, i.e. we are able to\nidentify building blocks directly. The overall approach learns by building a\nprobabilistic model, i.e. an estimation of the probability distribution of\nindividual nurse-rule pairs that are used to construct schedules. The local\nsearch processor (i.e. the ant-miner) reinforces nurse-rule pairs that receive\nhigher rewards. A challenging real world nurse rostering problem is used as the\ntest problem. Computational results show that the proposed approach outperforms\nmost existing approaches. It is suggested that the learning methodologies\nsuggested in this paper may be applied to other scheduling problems where\nschedules are built systematically according to specific rules\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 22 Nov 2007 15:16:21 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:14:51 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Burke",
                "Edmund",
                ""
            ],
            [
                "Li",
                "Jingpeng",
                ""
            ]
        ]
    },
    {
        "id": "0711.3594",
        "submitter": "Chunjing Xu",
        "authors": "Chunjing Xu, Jianzhuang Liu, Xiaoou Tang",
        "title": "Clustering with Transitive Distance and K-Means Duality",
        "comments": "13 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  Recent spectral clustering methods are a propular and powerful technique for\ndata clustering. These methods need to solve the eigenproblem whose\ncomputational complexity is $O(n^3)$, where $n$ is the number of data samples.\nIn this paper, a non-eigenproblem based clustering method is proposed to deal\nwith the clustering problem. Its performance is comparable to the spectral\nclustering algorithms but it is more efficient with computational complexity\n$O(n^2)$. We show that with a transitive distance and an observed property,\ncalled K-means duality, our algorithm can be used to handle data sets with\ncomplex cluster shapes, multi-scale clusters, and noise. Moreover, no\nparameters except the number of clusters need to be set in our algorithm.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 22 Nov 2007 15:05:35 GMT"
            }
        ],
        "update_date": "2007-11-26",
        "authors_parsed": [
            [
                "Xu",
                "Chunjing",
                ""
            ],
            [
                "Liu",
                "Jianzhuang",
                ""
            ],
            [
                "Tang",
                "Xiaoou",
                ""
            ]
        ]
    },
    {
        "id": "0711.3628",
        "submitter": "Francesc Rossell\\'o",
        "authors": "Gabriel Cardona, Francesc Rossello, Gabriel Valiente",
        "title": "A Perl Package and an Alignment Tool for Phylogenetic Networks",
        "comments": "5 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-bio.PE cs.CE",
        "license": null,
        "abstract": "  Phylogenetic networks are a generalization of phylogenetic trees that allow\nfor the representation of evolutionary events acting at the population level,\nlike recombination between genes, hybridization between lineages, and lateral\ngene transfer. While most phylogenetics tools implement a wide range of\nalgorithms on phylogenetic trees, there exist only a few applications to work\nwith phylogenetic networks, and there are no open-source libraries either.\n  In order to improve this situation, we have developed a Perl package that\nrelies on the BioPerl bundle and implements many algorithms on phylogenetic\nnetworks. We have also developed a Java applet that makes use of the\naforementioned Perl package and allows the user to make simple experiments with\nphylogenetic networks without having to develop a program or Perl script by\nherself.\n  The Perl package has been accepted as part of the BioPerl bundle. It can be\ndownloaded from http://dmi.uib.es/~gcardona/BioInfo/Bio-PhyloNetwork.tgz. The\nweb-based application is available at http://dmi.uib.es/~gcardona/BioInfo/. The\nPerl package includes full documentation of all its features.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 22 Nov 2007 18:05:49 GMT"
            }
        ],
        "update_date": "2007-11-26",
        "authors_parsed": [
            [
                "Cardona",
                "Gabriel",
                ""
            ],
            [
                "Rossello",
                "Francesc",
                ""
            ],
            [
                "Valiente",
                "Gabriel",
                ""
            ]
        ]
    },
    {
        "id": "0711.3663",
        "submitter": "P.F. Wang",
        "authors": "P.F. Wang, J.P. Li",
        "title": "The one-way function based on computational uncertainty principle",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  This paper presents how to make use of the advantage of round-off error\neffect in some research areas. The float-point operation complies with the\nreproduce theorem without the external random perturbation. The computation\nuncertainty principle and the high nonlinear of chaotic system guarantee the\nnumerical error is random and departure from the analytical result. Combining\nthese two properties we can produce unilateral one-way function and provide a\ncase of utilizing this function to construct encryption algorithm. The\nmultiple-precision (MP) library is used to analyze nonlinear dynamics systems\nand achieve the code. As an example, we provide a scheme of encrypting a\nplaintext by employing the one-way function with Lorenz system. Since the\nnumerical solution used in this scheme is beyond the maximum effective\ncomputation time (MECT) and it cannot satisfy the requirements of return-map\nanalysis and phase space reconstruction, it can block some existing attacks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 23 Nov 2007 02:37:12 GMT"
            }
        ],
        "update_date": "2007-11-26",
        "authors_parsed": [
            [
                "Wang",
                "P. F.",
                ""
            ],
            [
                "Li",
                "J. P.",
                ""
            ]
        ]
    },
    {
        "id": "0711.3672",
        "submitter": "Sebastien Tixeuil",
        "authors": "St\\'ephane Devismes (LRI), S\\'ebastien Tixeuil (INRIA Futurs, LIP6),\n  Masafumi Yamashita (TCSG)",
        "title": "Weak vs. Self vs. Probabilistic Stabilization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.DS cs.NI",
        "license": null,
        "abstract": "  Self-stabilization is a strong property that guarantees that a network always\nresume correct behavior starting from an arbitrary initial state. Weaker\nguarantees have later been introduced to cope with impossibility results:\nprobabilistic stabilization only gives probabilistic convergence to a correct\nbehavior. Also, weak stabilization only gives the possibility of convergence.\nIn this paper, we investigate the relative power of weak, self, and\nprobabilistic stabilization, with respect to the set of problems that can be\nsolved. We formally prove that in that sense, weak stabilization is strictly\nstronger that self-stabilization. Also, we refine previous results on weak\nstabilization to prove that, for practical schedule instances, a deterministic\nweak-stabilizing protocol can be turned into a probabilistic self-stabilizing\none. This latter result hints at more practical use of weak-stabilization, as\nsuch algorthms are easier to design and prove than their (probabilistic)\nself-stabilizing counterparts.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 23 Nov 2007 07:17:25 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 26 Nov 2007 11:08:34 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Devismes",
                "St\u00e9phane",
                "",
                "LRI"
            ],
            [
                "Tixeuil",
                "S\u00e9bastien",
                "",
                "INRIA Futurs, LIP6"
            ],
            [
                "Yamashita",
                "Masafumi",
                "",
                "TCSG"
            ]
        ]
    },
    {
        "id": "0711.3675",
        "submitter": "Yong Wang",
        "authors": "Yong Wang, Bao-Gang Hu",
        "title": "Derivations of Normalized Mutual Information in Binary Classifications",
        "comments": "8 pages, 8 figures, and 2 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.IT math.IT",
        "license": null,
        "abstract": "  This correspondence studies the basic problem of classifications - how to\nevaluate different classifiers. Although the conventional performance indexes,\nsuch as accuracy, are commonly used in classifier selection or evaluation,\ninformation-based criteria, such as mutual information, are becoming popular in\nfeature/model selections. In this work, we propose to assess classifiers in\nterms of normalized mutual information (NI), which is novel and well defined in\na compact range for classifier evaluation. We derive close-form relations of\nnormalized mutual information with respect to accuracy, precision, and recall\nin binary classifications. By exploring the relations among them, we reveal\nthat NI is actually a set of nonlinear functions, with a concordant\npower-exponent form, to each performance index. The relations can also be\nexpressed with respect to precision and recall, or to false alarm and hitting\nrate (recall).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 23 Nov 2007 07:45:52 GMT"
            }
        ],
        "update_date": "2007-11-26",
        "authors_parsed": [
            [
                "Wang",
                "Yong",
                ""
            ],
            [
                "Hu",
                "Bao-Gang",
                ""
            ]
        ]
    },
    {
        "id": "0711.3941",
        "submitter": "David Garber",
        "authors": "David Garber",
        "title": "Braid Group Cryptography",
        "comments": "75 pages, 19 figures; An almost final version of lectures notes for\n  lectures given in Braid PRIMA school in Singapore, June 2007. This version is\n  a totally revised version",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR math.GR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the last decade, a number of public key cryptosystems based on com-\nbinatorial group theoretic problems in braid groups have been proposed. We\nsurvey these cryptosystems and some known attacks on them.\n  This survey includes: Basic facts on braid groups and on the Garside normal\nform of its elements, some known algorithms for solving the word problem in the\nbraid group, the major public-key cryptosystems based on the braid group, and\nsome of the known attacks on these cryptosystems. We conclude with a discussion\nof future directions (which includes also a description of cryptosystems which\nare based on other non-commutative groups).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 26 Nov 2007 05:16:01 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 27 Sep 2008 19:15:38 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Garber",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0711.3949",
        "submitter": "Lei Ni",
        "authors": "Lei Ni, Aaron Harwood",
        "title": "An Adaptive Checkpointing Scheme for Peer-to-Peer Based Volunteer\n  Computing Work Flows",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Volunteer Computing, sometimes called Public Resource Computing, is an\nemerging computational model that is very suitable for work-pooled parallel\nprocessing. As more complex grid applications make use of work flows in their\ndesign and deployment it is reasonable to consider the impact of work flow\ndeployment over a Volunteer Computing infrastructure. In this case, the inter\nwork flow I/O can lead to a significant increase in I/O demands at the work\npool server. A possible solution is the use of a Peer-to- Peer based parallel\ncomputing architecture to off-load this I/O demand to the workers; where the\nworkers can fulfill some aspects of work flow coordination and I/O checking,\netc. However, achieving robustness in such a large scale system is a\nchallenging hurdle towards the decentralized execution of work flows and\ngeneral parallel processes. To increase robustness, we propose and show the\nmerits of using an adaptive checkpoint scheme that efficiently checkpoints the\nstatus of the parallel processes according to the estimation of relevant\nnetwork and peer parameters. Our scheme uses statistical data observed during\nruntime to dynamically make checkpoint decisions in a completely de-\ncentralized manner. The results of simulation show support for our proposed\napproach in terms of reduced required runtime.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 26 Nov 2007 06:41:23 GMT"
            }
        ],
        "update_date": "2007-11-27",
        "authors_parsed": [
            [
                "Ni",
                "Lei",
                ""
            ],
            [
                "Harwood",
                "Aaron",
                ""
            ]
        ]
    },
    {
        "id": "0711.4071",
        "submitter": "Wim Vanhoof",
        "authors": "Pierre Deransart, Mireille Ducass\\'e, G\\'erard Ferrand",
        "title": "Observational semantics of the Prolog Resolution Box Model",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  This paper specifies an observational semantics and gives an original\npresentation of the Byrd box model. The approach accounts for the semantics of\nProlog tracers independently of a particular Prolog implementation. Prolog\ntraces are, in general, considered as rather obscure and difficult to use. The\nproposed formal presentation of its trace constitutes a simple and pedagogical\napproach for teaching Prolog or for implementing Prolog tracers. It is a form\nof declarative specification for the tracers. The trace model introduced here\nis only one example to illustrate general problems relating to tracers and\nobserving processes. Observing processes know, from observed processes, only\ntheir traces. The issue is then to be able to reconstitute, by the sole\nanalysis of the trace, part of the behaviour of the observed process, and if\npossible, without any loss of information. As a matter of fact, our approach\nhighlights qualities of the Prolog resolution box model which made its success,\nbut also its insufficiencies.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 26 Nov 2007 18:03:07 GMT"
            }
        ],
        "update_date": "2007-11-27",
        "authors_parsed": [
            [
                "Deransart",
                "Pierre",
                ""
            ],
            [
                "Ducass\u00e9",
                "Mireille",
                ""
            ],
            [
                "Ferrand",
                "G\u00e9rard",
                ""
            ]
        ]
    },
    {
        "id": "0711.4217",
        "submitter": "Kees Middelburg",
        "authors": "J. A. Bergstra, C. A. Middelburg",
        "title": "Instruction sequences with dynamically instantiated instructions",
        "comments": "25 pages; phrasing improved",
        "journal-ref": "Fundamenta Informaticae, 96(1--2):27--48, 2009",
        "doi": "10.3233/FI-2009-165",
        "report-no": "PRG0710",
        "categories": "cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study sequential programs that are instruction sequences with dynamically\ninstantiated instructions. We define the meaning of such programs in two\ndifferent ways. In either case, we give a translation by which each program\nwith dynamically instantiated instructions is turned into a program without\nthem that exhibits on execution the same behaviour by interaction with some\nservice. The complexity of the translations differ considerably, whereas the\nservices concerned are equally simple. However, the service concerned in the\ncase of the simpler translation is far more powerful than the service concerned\nin the other case.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 27 Nov 2007 10:24:34 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 11 Dec 2007 08:06:27 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 2 Jul 2008 08:08:28 GMT"
            },
            {
                "version": "v4",
                "created": "Tue, 4 Aug 2009 06:37:50 GMT"
            }
        ],
        "update_date": "2010-01-12",
        "authors_parsed": [
            [
                "Bergstra",
                "J. A.",
                ""
            ],
            [
                "Middelburg",
                "C. A.",
                ""
            ]
        ]
    },
    {
        "id": "0711.4309",
        "submitter": "Giandomenico Sica",
        "authors": "Ruqian Lu",
        "title": "Knowware: the third star after Hardware and Software",
        "comments": "109 pages, ISBN 978-88-7699-095-3 (Printed edition), ISBN\n  978-88-7699-096-0 (Electronic edition), printed edition available on Amazon\n  and on Lulu.com",
        "journal-ref": "\"Publishing studies\" book series, edited by Giandomenico Sica,\n  ISSN 1973-6061 (Printed edition), ISSN 1973-6053 (Electronic edition)",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.AI cs.CY",
        "license": null,
        "abstract": "  This book proposes to separate knowledge from software and to make it a\ncommodity that is called knowware. The architecture, representation and\nfunction of Knowware are discussed. The principles of knowware engineering and\nits three life cycle models: furnace model, crystallization model and spiral\nmodel are proposed and analyzed. Techniques of software/knowware co-engineering\nare introduced. A software component whose knowledge is replaced by knowware is\ncalled mixware. An object and component oriented development schema of mixware\nis introduced. In particular, the tower model and ladder model for mixware\ndevelopment are proposed and discussed. Finally, knowledge service and knowware\nbased Web service are introduced and compared with Web service. In summary,\nknowware, software and hardware should be considered as three equally important\nunderpinnings of IT industry.\n  Ruqian Lu is a professor of computer science of the Institute of Mathematics,\nAcademy of Mathematics and System Sciences. He is a fellow of Chinese Academy\nof Sciences. His research interests include artificial intelligence, knowledge\nengineering and knowledge based software engineering. He has published more\nthan 100 papers and 10 books. He has won two first class awards from the\nAcademia Sinica and a National second class prize from the Ministry of Science\nand Technology. He has also won the sixth Hua Loo-keng Mathematics Prize.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 27 Nov 2007 17:36:35 GMT"
            }
        ],
        "update_date": "2007-11-28",
        "authors_parsed": [
            [
                "Lu",
                "Ruqian",
                ""
            ]
        ]
    },
    {
        "id": "0711.4324",
        "submitter": "Jinshan Zhang",
        "authors": "Jinshan Zhang",
        "title": "Report on \"American Option Pricing and Hedging Strategies\"",
        "comments": "14pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.DM",
        "license": null,
        "abstract": "  This paper mainly discusses the American option's hedging strategies via\nbinomialmodel and the basic idea of pricing and hedging American option.\nAlthough the essential scheme of hedging is almost the same as European option,\nsmall differences may arise when simulating the process for American option\nholder has more rights, spelling that the option can be exercised at anytime\nbefore its maturity. Our method is dynamic-hedging method.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 27 Nov 2007 18:34:40 GMT"
            }
        ],
        "update_date": "2007-11-28",
        "authors_parsed": [
            [
                "Zhang",
                "Jinshan",
                ""
            ]
        ]
    },
    {
        "id": "0711.4444",
        "submitter": "Laurent Hascoet",
        "authors": "Moulay Hicham Tber (INRIA Sophia Antipolis), Laurent Hascoet (INRIA\n  Sophia Antipolis, SEMA), Arthur Vidard (INRIA Rh\\^one-Alpes / LJK Laboratoire\n  Jean Kuntzmann), Benjamin Dauvergne (INRIA Sophia Antipolis)",
        "title": "Building the Tangent and Adjoint codes of the Ocean General Circulation\n  Model OPA with the Automatic Differentiation tool TAPENADE",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.CE",
        "license": null,
        "abstract": "  The ocean general circulation model OPA is developed by the LODYC team at\nParis VI university. OPA has recently undergone a major rewriting, migrating to\nFORTRAN95, and its adjoint code needs to be rebuilt. For earlier versions, the\nadjoint of OPA was written by hand at a high development cost. We use the\nAutomatic Differentiation tool TAPENADE to build mechanicaly the tangent and\nadjoint codes of OPA. We validate the differentiated codes by comparison with\ndivided differences, and also with an identical twin experiment. We apply\nstate-of-the-art methods to improve the performance of the adjoint code. In\nparticular we implement the Griewank and Walther's binomial checkpointing\nalgorithm which gives us an optimal trade-off between time and memory\nconsumption. We apply a specific strategy to differentiate the iterative linear\nsolver that comes from the implicit time stepping scheme\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 28 Nov 2007 08:04:18 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 29 Nov 2007 09:09:27 GMT"
            }
        ],
        "update_date": "2007-11-29",
        "authors_parsed": [
            [
                "Tber",
                "Moulay Hicham",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Hascoet",
                "Laurent",
                "",
                "INRIA\n  Sophia Antipolis, SEMA"
            ],
            [
                "Vidard",
                "Arthur",
                "",
                "INRIA Rh\u00f4ne-Alpes / LJK Laboratoire\n  Jean Kuntzmann"
            ],
            [
                "Dauvergne",
                "Benjamin",
                "",
                "INRIA Sophia Antipolis"
            ]
        ]
    },
    {
        "id": "0711.4444",
        "submitter": "Laurent Hascoet",
        "authors": "Moulay Hicham Tber (INRIA Sophia Antipolis), Laurent Hascoet (INRIA\n  Sophia Antipolis, SEMA), Arthur Vidard (INRIA Rh\\^one-Alpes / LJK Laboratoire\n  Jean Kuntzmann), Benjamin Dauvergne (INRIA Sophia Antipolis)",
        "title": "Building the Tangent and Adjoint codes of the Ocean General Circulation\n  Model OPA with the Automatic Differentiation tool TAPENADE",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.CE",
        "license": null,
        "abstract": "  The ocean general circulation model OPA is developed by the LODYC team at\nParis VI university. OPA has recently undergone a major rewriting, migrating to\nFORTRAN95, and its adjoint code needs to be rebuilt. For earlier versions, the\nadjoint of OPA was written by hand at a high development cost. We use the\nAutomatic Differentiation tool TAPENADE to build mechanicaly the tangent and\nadjoint codes of OPA. We validate the differentiated codes by comparison with\ndivided differences, and also with an identical twin experiment. We apply\nstate-of-the-art methods to improve the performance of the adjoint code. In\nparticular we implement the Griewank and Walther's binomial checkpointing\nalgorithm which gives us an optimal trade-off between time and memory\nconsumption. We apply a specific strategy to differentiate the iterative linear\nsolver that comes from the implicit time stepping scheme\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 28 Nov 2007 08:04:18 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 29 Nov 2007 09:09:27 GMT"
            }
        ],
        "update_date": "2007-11-29",
        "authors_parsed": [
            [
                "Tber",
                "Moulay Hicham",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Hascoet",
                "Laurent",
                "",
                "INRIA\n  Sophia Antipolis, SEMA"
            ],
            [
                "Vidard",
                "Arthur",
                "",
                "INRIA Rh\u00f4ne-Alpes / LJK Laboratoire\n  Jean Kuntzmann"
            ],
            [
                "Dauvergne",
                "Benjamin",
                "",
                "INRIA Sophia Antipolis"
            ]
        ]
    },
    {
        "id": "0711.4452",
        "submitter": "Hirotaka Niitsuma",
        "authors": "Hirotaka Niitsuma and Takashi Okada",
        "title": "Covariance and PCA for Categorical Variables",
        "comments": "12 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  Covariances from categorical variables are defined using a regular simplex\nexpression for categories. The method follows the variance definition by Gini,\nand it gives the covariance as a solution of simultaneous equations. The\ncalculated results give reasonable values for test data. A method of principal\ncomponent analysis (RS-PCA) is also proposed using regular simplex expressions,\nwhich allows easy interpretation of the principal components. The proposed\nmethods apply to variable selection problem of categorical data USCensus1990\ndata. The proposed methods give appropriate criterion for the variable\nselection problem of categorical\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 28 Nov 2007 12:05:47 GMT"
            }
        ],
        "update_date": "2007-11-29",
        "authors_parsed": [
            [
                "Niitsuma",
                "Hirotaka",
                ""
            ],
            [
                "Okada",
                "Takashi",
                ""
            ]
        ]
    },
    {
        "id": "0711.4508",
        "submitter": "Hiroshi Ishikawa",
        "authors": "Hiroshi Ishikawa",
        "title": "Representation and Measure of Structural Information",
        "comments": "Second version. Revised the Introduction and added more discussion in\n  the last section. The technical content is mostly unchanged. 51 pages, 4\n  figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CC cs.CV cs.IT math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a uniform representation of general objects that captures the\nregularities with respect to their structure. It allows a representation of a\ngeneral class of objects including geometric patterns and images in a sparse,\nmodular, hierarchical, and recursive manner. The representation can exploit any\ncomputable regularity in objects to compactly describe them, while also being\ncapable of representing random objects as raw data. A set of rules uniformly\ndictates the interpretation of the representation into raw signal, which makes\nit possible to ask what pattern a given raw signal contains. Also, it allows\nsimple separation of the information that we wish to ignore from that which we\nmeasure, by using a set of maps to delineate the a priori parts of the objects,\nleaving only the information in the structure.\n  Using the representation, we introduce a measure of information in general\nobjects relative to structures defined by the set of maps. We point out that\nthe common prescription of encoding objects by strings to use Kolmogorov\ncomplexity is meaningless when, as often is the case, the encoding is not\nspecified in any way other than that it exists. Noting this, we define the\nmeasure directly in terms of the structures of the spaces in which the objects\nreside. As a result, the measure is defined relative to a set of maps that\ncharacterize the structures. It turns out that the measure is equivalent to\nKolmogorov complexity when it is defined relative to the maps characterizing\nthe structure of natural numbers. Thus, the formulation gives the larger class\nof objects a meaningful measure of information that generalizes Kolmogorov\ncomplexity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 28 Nov 2007 18:41:30 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 12 Jun 2008 03:37:52 GMT"
            }
        ],
        "update_date": "2008-06-12",
        "authors_parsed": [
            [
                "Ishikawa",
                "Hiroshi",
                ""
            ]
        ]
    },
    {
        "id": "0712.0109",
        "submitter": "Grenville Croll",
        "authors": "Raymond R. Panko",
        "title": "Recommended Practices for Spreadsheet Testing",
        "comments": "12 Pages, Extensive References",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2006 73-84\n  ISBN:1-905617-08-9",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  This paper presents the authors recommended practices for spreadsheet\ntesting. Documented spreadsheet error rates are unacceptable in corporations\ntoday. Although improvements are needed throughout the systems development life\ncycle, credible improvement programs must include comprehensive testing.\nSeveral forms of testing are possible, but logic inspection is recommended for\nmodule testing. Logic inspection appears to be feasible for spreadsheet\ndevelopers to do, and logic inspection appears to be safe and effective.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 1 Dec 2007 21:27:59 GMT"
            }
        ],
        "update_date": "2008-03-12",
        "authors_parsed": [
            [
                "Panko",
                "Raymond R.",
                ""
            ]
        ]
    },
    {
        "id": "0712.0130",
        "submitter": "Thomas M. Breuel",
        "authors": "Thomas M. Breuel",
        "title": "On the Relationship between the Posterior and Optimal Similarity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  For a classification problem described by the joint density $P(\\omega,x)$,\nmodels of $P(\\omega\\eq\\omega'|x,x')$ (the ``Bayesian similarity measure'') have\nbeen shown to be an optimal similarity measure for nearest neighbor\nclassification. This paper analyzes demonstrates several additional properties\nof that conditional distribution. The paper first shows that we can\nreconstruct, up to class labels, the class posterior distribution $P(\\omega|x)$\ngiven $P(\\omega\\eq\\omega'|x,x')$, gives a procedure for recovering the class\nlabels, and gives an asymptotically Bayes-optimal classification procedure. It\nalso shows, given such an optimal similarity measure, how to construct a\nclassifier that outperforms the nearest neighbor classifier and achieves\nBayes-optimal classification rates. The paper then analyzes Bayesian similarity\nin a framework where a classifier faces a number of related classification\ntasks (multitask learning) and illustrates that reconstruction of the class\nposterior distribution is not possible in general. Finally, the paper\nidentifies a distinct class of classification problems using\n$P(\\omega\\eq\\omega'|x,x')$ and shows that using $P(\\omega\\eq\\omega'|x,x')$ to\nsolve those problems is the Bayes optimal solution.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 2 Dec 2007 09:38:26 GMT"
            }
        ],
        "update_date": "2007-12-04",
        "authors_parsed": [
            [
                "Breuel",
                "Thomas M.",
                ""
            ]
        ]
    },
    {
        "id": "0712.0131",
        "submitter": "Thomas M. Breuel",
        "authors": "Thomas M. Breuel",
        "title": "Learning Similarity for Character Recognition and 3D Object Recognition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  I describe an approach to similarity motivated by Bayesian methods. This\nyields a similarity function that is learnable using a standard Bayesian\nmethods. The relationship of the approach to variable kernel and variable\nmetric methods is discussed. The approach is related to variable kernel\nExperimental results on character recognition and 3D object recognition are\npresented..\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 2 Dec 2007 10:02:01 GMT"
            }
        ],
        "update_date": "2007-12-04",
        "authors_parsed": [
            [
                "Breuel",
                "Thomas M.",
                ""
            ]
        ]
    },
    {
        "id": "0712.0136",
        "submitter": "Thomas M. Breuel",
        "authors": "Thomas M. Breuel",
        "title": "Learning View Generalization Functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  Learning object models from views in 3D visual object recognition is usually\nformulated either as a function approximation problem of a function describing\nthe view-manifold of an object, or as that of learning a class-conditional\ndensity. This paper describes an alternative framework for learning in visual\nobject recognition, that of learning the view-generalization function. Using\nthe view-generalization function, an observer can perform Bayes-optimal 3D\nobject recognition given one or more 2D training views directly, without the\nneed for a separate model acquisition step. The paper shows that view\ngeneralization functions can be computationally practical by restating two\nwidely-used methods, the eigenspace and linear combination of views approaches,\nin a view generalization framework. The paper relates the approach to recent\nmethods for object recognition based on non-uniform blurring. The paper\npresents results both on simulated 3D ``paperclip'' objects and real-world\nimages from the COIL-100 database showing that useful view-generalization\nfunctions can be realistically be learned from a comparatively small number of\ntraining examples.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 2 Dec 2007 10:54:40 GMT"
            }
        ],
        "update_date": "2007-12-04",
        "authors_parsed": [
            [
                "Breuel",
                "Thomas M.",
                ""
            ]
        ]
    },
    {
        "id": "0712.0137",
        "submitter": "Thomas M. Breuel",
        "authors": "Thomas M. Breuel",
        "title": "View Based Methods can achieve Bayes-Optimal 3D Recognition",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  This paper proves that visual object recognition systems using only 2D\nEuclidean similarity measurements to compare object views against previously\nseen views can achieve the same recognition performance as observers having\naccess to all coordinate information and able of using arbitrary 3D models\ninternally. Furthermore, it demonstrates that such systems do not require more\ntraining views than Bayes-optimal 3D model-based systems. For building computer\nvision systems, these results imply that using view-based or appearance-based\ntechniques with carefully constructed combination of evidence mechanisms may\nnot be at a disadvantage relative to 3D model-based systems. For computational\napproaches to human vision, they show that it is impossible to distinguish\nview-based and 3D model-based techniques for 3D object recognition solely by\ncomparing the performance achievable by human and 3D model-based systems.}\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 2 Dec 2007 11:02:37 GMT"
            }
        ],
        "update_date": "2007-12-04",
        "authors_parsed": [
            [
                "Breuel",
                "Thomas M.",
                ""
            ]
        ]
    },
    {
        "id": "0712.0411",
        "submitter": "Suresh Thippireddy",
        "authors": "Suresh Thippireddy and Sandeep Chalasani",
        "title": "Period of the d-Sequence Based Random Number Generator",
        "comments": "8 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  This paper presents an expression to compute the exact period of a recursive\nrandom number generator based on d-sequences. Using the multi-recursive version\nof this generator we can produce large number of pseudorandom sequences.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Dec 2007 23:29:42 GMT"
            }
        ],
        "update_date": "2007-12-05",
        "authors_parsed": [
            [
                "Thippireddy",
                "Suresh",
                ""
            ],
            [
                "Chalasani",
                "Sandeep",
                ""
            ]
        ]
    },
    {
        "id": "0712.0451",
        "submitter": "Alejandro Chinea Manrique De Lara",
        "authors": "Alejandro Chinea Manrique De Lara",
        "title": "A Reactive Tabu Search Algorithm for Stimuli Generation in\n  Psycholinguistics",
        "comments": "Artificial Intelligence in Science and Technology AISAT 2004\n  Conference. 8 pages, 5 figures, 3 tables",
        "journal-ref": "Artificial Intelligence in Science and Technology AISAT 2004\n  Conference",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.CC cs.DM cs.LG",
        "license": null,
        "abstract": "  The generation of meaningless \"words\" matching certain statistical and/or\nlinguistic criteria is frequently needed for experimental purposes in\nPsycholinguistics. Such stimuli receive the name of pseudowords or nonwords in\nthe Cognitive Neuroscience literatue. The process for building nonwords\nsometimes has to be based on linguistic units such as syllables or morphemes,\nresulting in a numerical explosion of combinations when the size of the\nnonwords is increased. In this paper, a reactive tabu search scheme is proposed\nto generate nonwords of variables size. The approach builds pseudowords by\nusing a modified Metaheuristic algorithm based on a local search procedure\nenhanced by a feedback-based scheme. Experimental results show that the new\nalgorithm is a practical and effective tool for nonword generation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 4 Dec 2007 08:52:46 GMT"
            }
        ],
        "update_date": "2007-12-05",
        "authors_parsed": [
            [
                "De Lara",
                "Alejandro Chinea Manrique",
                ""
            ]
        ]
    },
    {
        "id": "0712.0499",
        "submitter": "Ioannis Antonellis",
        "authors": "Ioannis Antonellis, Hector Garcia-Molina, Chi-Chao Chang",
        "title": "Simrank++: Query rewriting through link analysis of the click graph",
        "comments": "Available via http://dbpubs.stanford.edu/pub/2007-32",
        "journal-ref": null,
        "doi": null,
        "report-no": "Stanford University, Infolab TR 2007-32",
        "categories": "cs.DL cs.DB cs.IR",
        "license": null,
        "abstract": "  We focus on the problem of query rewriting for sponsored search. We base\nrewrites on a historical click graph that records the ads that have been\nclicked on in response to past user queries. Given a query q, we first consider\nSimrank as a way to identify queries similar to q, i.e., queries whose ads a\nuser may be interested in. We argue that Simrank fails to properly identify\nquery similarities in our application, and we present two enhanced version of\nSimrank: one that exploits weights on click graph edges and another that\nexploits ``evidence.'' We experimentally evaluate our new schemes against\nSimrank, using actual click graphs and queries form Yahoo!, and using a variety\nof metrics. Our results show that the enhanced methods can yield more and\nbetter query rewrites.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 4 Dec 2007 12:43:17 GMT"
            }
        ],
        "update_date": "2007-12-05",
        "authors_parsed": [
            [
                "Antonellis",
                "Ioannis",
                ""
            ],
            [
                "Garcia-Molina",
                "Hector",
                ""
            ],
            [
                "Chang",
                "Chi-Chao",
                ""
            ]
        ]
    },
    {
        "id": "0712.0653",
        "submitter": "Sumio Watanabe",
        "authors": "Sumio Watanabe",
        "title": "Equations of States in Singular Statistical Estimation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Learning machines which have hierarchical structures or hidden variables are\nsingular statistical models because they are nonidentifiable and their Fisher\ninformation matrices are singular. In singular statistical models, neither the\nBayes a posteriori distribution converges to the normal distribution nor the\nmaximum likelihood estimator satisfies asymptotic normality. This is the main\nreason why it has been difficult to predict their generalization performances\nfrom trained states. In this paper, we study four errors, (1) Bayes\ngeneralization error, (2) Bayes training error, (3) Gibbs generalization error,\nand (4) Gibbs training error, and prove that there are mathematical relations\namong these errors. The formulas proved in this paper are equations of states\nin statistical estimation because they hold for any true distribution, any\nparametric model, and any a priori distribution. Also we show that Bayes and\nGibbs generalization errors are estimated by Bayes and Gibbs training errors,\nand propose widely applicable information criteria which can be applied to both\nregular and singular statistical models.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 5 Dec 2007 05:39:07 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 11 May 2009 05:49:09 GMT"
            }
        ],
        "update_date": "2009-05-11",
        "authors_parsed": [
            [
                "Watanabe",
                "Sumio",
                ""
            ]
        ]
    },
    {
        "id": "0712.0693",
        "submitter": "Chengqing Li",
        "authors": "Chengqing Li, Dan Zhang, and Guanrong Chen",
        "title": "Cryptanalysis of an image encryption scheme based on the Hill cipher",
        "comments": "10 pages, three figures",
        "journal-ref": null,
        "doi": "10.1631/jzus.A0720102",
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  This paper studies the security of an image encryption scheme based on the\nHill cipher and reports its following problems: 1) there is a simple necessary\nand sufficient condition that makes a number of secret keys invalid; 2) it is\ninsensitive to the change of the secret key; 3) it is insensitive to the change\nof the plain-image; 4) it can be broken with only one known/chosen-plaintext;\n5) it has some other minor defects.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 5 Dec 2007 11:11:16 GMT"
            }
        ],
        "update_date": "2009-12-21",
        "authors_parsed": [
            [
                "Li",
                "Chengqing",
                ""
            ],
            [
                "Zhang",
                "Dan",
                ""
            ],
            [
                "Chen",
                "Guanrong",
                ""
            ]
        ]
    },
    {
        "id": "0712.0811",
        "submitter": "Radim Ba\\v{c}a Ing.",
        "authors": "R.Baca, V.Snasel, J.Platos, M.Kratky, E.El-Qawasmeh",
        "title": "The Fast Fibonacci Decompression Algorithm",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.OH",
        "license": null,
        "abstract": "  Data compression has been widely applied in many data processing areas.\nCompression methods use variable-size codes with the shorter codes assigned to\nsymbols or groups of symbols that appear in the data frequently. Fibonacci\ncoding, as a representative of these codes, is used for compressing small\nnumbers. Time consumption of a decompression algorithm is not usually as\nimportant as the time of a compression algorithm. However, efficiency of the\ndecompression may be a critical issue in some cases. For example, a real-time\ncompression of tree data structures follows this issue. Tree's pages are\ndecompressed during every reading from a secondary storage into the main\nmemory. In this case, the efficiency of a decompression algorithm is extremely\nimportant. We have developed a Fast Fibonacci decompression for this purpose.\nOur approach is up to $3.5\\times$ faster than the original implementation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 5 Dec 2007 19:55:16 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 19 Dec 2007 08:05:54 GMT"
            }
        ],
        "update_date": "2007-12-19",
        "authors_parsed": [
            [
                "Baca",
                "R.",
                ""
            ],
            [
                "Snasel",
                "V.",
                ""
            ],
            [
                "Platos",
                "J.",
                ""
            ],
            [
                "Kratky",
                "M.",
                ""
            ],
            [
                "El-Qawasmeh",
                "E.",
                ""
            ]
        ]
    },
    {
        "id": "0712.0840",
        "submitter": "Leonid (Aryeh) Kontorovich",
        "authors": "Leonid (Aryeh) Kontorovich",
        "title": "A Universal Kernel for Learning Regular Languages",
        "comments": "7 pages",
        "journal-ref": "The 5th International Workshop on Mining and Learning with Graphs,\n  2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.DM",
        "license": null,
        "abstract": "  We give a universal kernel that renders all the regular languages linearly\nseparable. We are not able to compute this kernel efficiently and conjecture\nthat it is intractable, but we do have an efficient $\\eps$-approximation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 5 Dec 2007 22:25:03 GMT"
            }
        ],
        "update_date": "2007-12-07",
        "authors_parsed": [
            [
                "Leonid",
                "",
                "",
                "Aryeh"
            ],
            [
                "Kontorovich",
                "",
                ""
            ]
        ]
    },
    {
        "id": "0712.0917",
        "submitter": "Inge Bethke",
        "authors": "Inge Bethke and Piet Rodenburg",
        "title": "Some properties of finite meadows",
        "comments": "8 pages, 1 table",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.RA cs.SC",
        "license": null,
        "abstract": "  The aim of this note is to describe the structure of finite meadows. We will\nshow that the class of finite meadows is the closure of the class of finite\nfields under finite products. As a corollary, we obtain a unique representation\nof minimal meadows in terms of prime fields.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Dec 2007 11:27:44 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Bethke",
                "Inge",
                ""
            ],
            [
                "Rodenburg",
                "Piet",
                ""
            ]
        ]
    },
    {
        "id": "0712.0932",
        "submitter": "Kumar Eswaran Dr.",
        "authors": "Dasika Ratna Deepthi, Sujeet Kuchibhotla and K.Eswaran",
        "title": "Dimensionality Reduction and Reconstruction using Mirroring Neural\n  Networks and Object Recognition based on Reduced Dimension Characteristic\n  Vector",
        "comments": "Presented in IEEE International Conference on Advances in Computer\n  Vision and Information Technology (ACVIT-07), Nov. 28-30 2007",
        "journal-ref": "IEEE International Conference On Advances in Computer Vision and\n  Information Tech. (IEEE, ACVIT-07), pp. 348 - 353 (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.AI cs.NE",
        "license": null,
        "abstract": "  In this paper, we present a Mirroring Neural Network architecture to perform\nnon-linear dimensionality reduction and Object Recognition using a reduced\nlowdimensional characteristic vector. In addition to dimensionality reduction,\nthe network also reconstructs (mirrors) the original high-dimensional input\nvector from the reduced low-dimensional data. The Mirroring Neural Network\narchitecture has more number of processing elements (adalines) in the outer\nlayers and the least number of elements in the central layer to form a\nconverging-diverging shape in its configuration. Since this network is able to\nreconstruct the original image from the output of the innermost layer (which\ncontains all the information about the input pattern), these outputs can be\nused as object signature to classify patterns. The network is trained to\nminimize the discrepancy between actual output and the input by back\npropagating the mean squared error from the output layer to the input layer.\nAfter successfully training the network, it can reduce the dimension of input\nvectors and mirror the patterns fed to it. The Mirroring Neural Network\narchitecture gave very good results on various test patterns.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Dec 2007 14:11:07 GMT"
            }
        ],
        "update_date": "2008-12-13",
        "authors_parsed": [
            [
                "Deepthi",
                "Dasika Ratna",
                ""
            ],
            [
                "Kuchibhotla",
                "Sujeet",
                ""
            ],
            [
                "Eswaran",
                "K.",
                ""
            ]
        ]
    },
    {
        "id": "0712.0932",
        "submitter": "Kumar Eswaran Dr.",
        "authors": "Dasika Ratna Deepthi, Sujeet Kuchibhotla and K.Eswaran",
        "title": "Dimensionality Reduction and Reconstruction using Mirroring Neural\n  Networks and Object Recognition based on Reduced Dimension Characteristic\n  Vector",
        "comments": "Presented in IEEE International Conference on Advances in Computer\n  Vision and Information Technology (ACVIT-07), Nov. 28-30 2007",
        "journal-ref": "IEEE International Conference On Advances in Computer Vision and\n  Information Tech. (IEEE, ACVIT-07), pp. 348 - 353 (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.AI cs.NE",
        "license": null,
        "abstract": "  In this paper, we present a Mirroring Neural Network architecture to perform\nnon-linear dimensionality reduction and Object Recognition using a reduced\nlowdimensional characteristic vector. In addition to dimensionality reduction,\nthe network also reconstructs (mirrors) the original high-dimensional input\nvector from the reduced low-dimensional data. The Mirroring Neural Network\narchitecture has more number of processing elements (adalines) in the outer\nlayers and the least number of elements in the central layer to form a\nconverging-diverging shape in its configuration. Since this network is able to\nreconstruct the original image from the output of the innermost layer (which\ncontains all the information about the input pattern), these outputs can be\nused as object signature to classify patterns. The network is trained to\nminimize the discrepancy between actual output and the input by back\npropagating the mean squared error from the output layer to the input layer.\nAfter successfully training the network, it can reduce the dimension of input\nvectors and mirror the patterns fed to it. The Mirroring Neural Network\narchitecture gave very good results on various test patterns.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Dec 2007 14:11:07 GMT"
            }
        ],
        "update_date": "2008-12-13",
        "authors_parsed": [
            [
                "Deepthi",
                "Dasika Ratna",
                ""
            ],
            [
                "Kuchibhotla",
                "Sujeet",
                ""
            ],
            [
                "Eswaran",
                "K.",
                ""
            ]
        ]
    },
    {
        "id": "0712.0938",
        "submitter": "Kumar Eswaran Dr.",
        "authors": "Dasika Ratna Deepthi, G.R.Aditya Krishna and K. Eswaran",
        "title": "Automatic Pattern Classification by Unsupervised Learning Using\n  Dimensionality Reduction of Data with Mirroring Neural Networks",
        "comments": "Presented in IEEE International Conference on Advances in Computer\n  Vision and Information Technology (ACVIT-07), Nov. 28-30 2007",
        "journal-ref": "IEEE International Conference on Advances in Computer Vision and\n  Information Tech. (IEEE, ACVIT-07), pp. 354 - 360 (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI cs.NE",
        "license": null,
        "abstract": "  This paper proposes an unsupervised learning technique by using Multi-layer\nMirroring Neural Network and Forgy's clustering algorithm. Multi-layer\nMirroring Neural Network is a neural network that can be trained with\ngeneralized data inputs (different categories of image patterns) to perform\nnon-linear dimensionality reduction and the resultant low-dimensional code is\nused for unsupervised pattern classification using Forgy's algorithm. By\nadapting the non-linear activation function (modified sigmoidal function) and\ninitializing the weights and bias terms to small random values, mirroring of\nthe input pattern is initiated. In training, the weights and bias terms are\nchanged in such a way that the input presented is reproduced at the output by\nback propagating the error. The mirroring neural network is capable of reducing\nthe input vector to a great degree (approximately 1/30th the original size) and\nalso able to reconstruct the input pattern at the output layer from this\nreduced code units. The feature set (output of central hidden layer) extracted\nfrom this network is fed to Forgy's algorithm, which classify input data\npatterns into distinguishable classes. In the implementation of Forgy's\nalgorithm, initial seed points are selected in such a way that they are distant\nenough to be perfectly grouped into different categories. Thus a new method of\nunsupervised learning is formulated and demonstrated in this paper. This method\ngave impressive results when applied to classification of different image\npatterns.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Dec 2007 13:52:04 GMT"
            }
        ],
        "update_date": "2008-12-15",
        "authors_parsed": [
            [
                "Deepthi",
                "Dasika Ratna",
                ""
            ],
            [
                "Krishna",
                "G. R. Aditya",
                ""
            ],
            [
                "Eswaran",
                "K.",
                ""
            ]
        ]
    },
    {
        "id": "0712.0938",
        "submitter": "Kumar Eswaran Dr.",
        "authors": "Dasika Ratna Deepthi, G.R.Aditya Krishna and K. Eswaran",
        "title": "Automatic Pattern Classification by Unsupervised Learning Using\n  Dimensionality Reduction of Data with Mirroring Neural Networks",
        "comments": "Presented in IEEE International Conference on Advances in Computer\n  Vision and Information Technology (ACVIT-07), Nov. 28-30 2007",
        "journal-ref": "IEEE International Conference on Advances in Computer Vision and\n  Information Tech. (IEEE, ACVIT-07), pp. 354 - 360 (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI cs.NE",
        "license": null,
        "abstract": "  This paper proposes an unsupervised learning technique by using Multi-layer\nMirroring Neural Network and Forgy's clustering algorithm. Multi-layer\nMirroring Neural Network is a neural network that can be trained with\ngeneralized data inputs (different categories of image patterns) to perform\nnon-linear dimensionality reduction and the resultant low-dimensional code is\nused for unsupervised pattern classification using Forgy's algorithm. By\nadapting the non-linear activation function (modified sigmoidal function) and\ninitializing the weights and bias terms to small random values, mirroring of\nthe input pattern is initiated. In training, the weights and bias terms are\nchanged in such a way that the input presented is reproduced at the output by\nback propagating the error. The mirroring neural network is capable of reducing\nthe input vector to a great degree (approximately 1/30th the original size) and\nalso able to reconstruct the input pattern at the output layer from this\nreduced code units. The feature set (output of central hidden layer) extracted\nfrom this network is fed to Forgy's algorithm, which classify input data\npatterns into distinguishable classes. In the implementation of Forgy's\nalgorithm, initial seed points are selected in such a way that they are distant\nenough to be perfectly grouped into different categories. Thus a new method of\nunsupervised learning is formulated and demonstrated in this paper. This method\ngave impressive results when applied to classification of different image\npatterns.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Dec 2007 13:52:04 GMT"
            }
        ],
        "update_date": "2008-12-15",
        "authors_parsed": [
            [
                "Deepthi",
                "Dasika Ratna",
                ""
            ],
            [
                "Krishna",
                "G. R. Aditya",
                ""
            ],
            [
                "Eswaran",
                "K.",
                ""
            ]
        ]
    },
    {
        "id": "0712.1167",
        "submitter": "Felipe Fran\\c{c}a",
        "authors": "Leandro A. J. Marzulo, Felipe M. G. Fran\\c{c}a and V\\'itor Santos\n  Costa",
        "title": "Transactional WaveCache: Towards Speculative and Out-of-Order DataFlow\n  Execution of Memory Operations",
        "comments": "Submitted to ACM International Conference on Computing Frontiers\n  2008, http://www.computingfrontiers.org/, 20 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AR cs.DC",
        "license": null,
        "abstract": "  The WaveScalar is the first DataFlow Architecture that can efficiently\nprovide the sequential memory semantics required by imperative languages. This\nwork presents an alternative memory ordering mechanism for this architecture,\nthe Transaction WaveCache. Our mechanism maintains the execution order of\nmemory operations within blocks of code, called Waves, but adds the ability to\nspeculatively execute, out-of-order, operations from different waves. This\nordering mechanism is inspired by progress in supporting Transactional\nMemories. Waves are considered as atomic regions and executed as nested\ntransactions. If a wave has finished the execution of all its memory\noperations, as soon as the previous waves are committed, it can be committed.\nIf a hazard is detected in a speculative Wave, all the following Waves\n(children) are aborted and re-executed. We evaluate the WaveCache on a set\nartificial benchmarks. If the benchmark does not access memory often, we could\nachieve speedups of around 90%. Speedups of 33.1% and 24% were observed on more\nmemory intensive applications, and slowdowns up to 16% arise if memory\nbandwidth is a bottleneck. For an application full of WAW, WAR and RAW hazards,\na speedup of 139.7% was verified.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Dec 2007 15:59:37 GMT"
            }
        ],
        "update_date": "2007-12-10",
        "authors_parsed": [
            [
                "Marzulo",
                "Leandro A. J.",
                ""
            ],
            [
                "Fran\u00e7a",
                "Felipe M. G.",
                ""
            ],
            [
                "Costa",
                "V\u00edtor Santos",
                ""
            ]
        ]
    },
    {
        "id": "0712.1189",
        "submitter": "Olivier Zendra",
        "authors": "Olivier Zendra (INRIA Lorraine - LORIA), Eric Jul (DIKU), Roland\n  Ducournau (LIRMM), Etienne Gagnon, Richard E. Jones, Chandra Krintz (RACE\n  LAB), Philippe Mulet, Jan Vitek (S3L)",
        "title": "Implementation, Compilation, Optimization of Object-Oriented Languages,\n  Programs and Systems - Report on the Workshop ICOOOLPS'2007 at ECOOP'07",
        "comments": null,
        "journal-ref": "ECOOP 2007 Workshop Reader Springer (Ed.) (2008)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  ICOOOLPS'2007 was the second edition of the ECOOP-ICOOOLPS workshop. ICOOOLPS\nintends to bring researchers and practitioners both from academia and industry\ntogether, with a spirit of openness, to try and identify and begin to address\nthe numerous and very varied issues of optimization. After a first successful\nedition, this second one put a stronger emphasis on exchanges and discussions\namongst the participants, progressing on the bases set last year in Nantes. The\nworkshop attendance was a success, since the 30-people limit we had set was\nreached about 2 weeks before the workshop itself. Some of the discussions (e.g.\nannotations) were so successful that they would required even more time than we\nwere able to dedicate to them. That's one area we plan to further improve for\nthe next edition.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Dec 2007 17:01:52 GMT"
            }
        ],
        "update_date": "2007-12-10",
        "authors_parsed": [
            [
                "Zendra",
                "Olivier",
                "",
                "INRIA Lorraine - LORIA"
            ],
            [
                "Jul",
                "Eric",
                "",
                "DIKU"
            ],
            [
                "Ducournau",
                "Roland",
                "",
                "LIRMM"
            ],
            [
                "Gagnon",
                "Etienne",
                "",
                "RACE\n  LAB"
            ],
            [
                "Jones",
                "Richard E.",
                "",
                "RACE\n  LAB"
            ],
            [
                "Krintz",
                "Chandra",
                "",
                "RACE\n  LAB"
            ],
            [
                "Mulet",
                "Philippe",
                "",
                "S3L"
            ],
            [
                "Vitek",
                "Jan",
                "",
                "S3L"
            ]
        ]
    },
    {
        "id": "0712.1189",
        "submitter": "Olivier Zendra",
        "authors": "Olivier Zendra (INRIA Lorraine - LORIA), Eric Jul (DIKU), Roland\n  Ducournau (LIRMM), Etienne Gagnon, Richard E. Jones, Chandra Krintz (RACE\n  LAB), Philippe Mulet, Jan Vitek (S3L)",
        "title": "Implementation, Compilation, Optimization of Object-Oriented Languages,\n  Programs and Systems - Report on the Workshop ICOOOLPS'2007 at ECOOP'07",
        "comments": null,
        "journal-ref": "ECOOP 2007 Workshop Reader Springer (Ed.) (2008)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  ICOOOLPS'2007 was the second edition of the ECOOP-ICOOOLPS workshop. ICOOOLPS\nintends to bring researchers and practitioners both from academia and industry\ntogether, with a spirit of openness, to try and identify and begin to address\nthe numerous and very varied issues of optimization. After a first successful\nedition, this second one put a stronger emphasis on exchanges and discussions\namongst the participants, progressing on the bases set last year in Nantes. The\nworkshop attendance was a success, since the 30-people limit we had set was\nreached about 2 weeks before the workshop itself. Some of the discussions (e.g.\nannotations) were so successful that they would required even more time than we\nwere able to dedicate to them. That's one area we plan to further improve for\nthe next edition.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Dec 2007 17:01:52 GMT"
            }
        ],
        "update_date": "2007-12-10",
        "authors_parsed": [
            [
                "Zendra",
                "Olivier",
                "",
                "INRIA Lorraine - LORIA"
            ],
            [
                "Jul",
                "Eric",
                "",
                "DIKU"
            ],
            [
                "Ducournau",
                "Roland",
                "",
                "LIRMM"
            ],
            [
                "Gagnon",
                "Etienne",
                "",
                "RACE\n  LAB"
            ],
            [
                "Jones",
                "Richard E.",
                "",
                "RACE\n  LAB"
            ],
            [
                "Krintz",
                "Chandra",
                "",
                "RACE\n  LAB"
            ],
            [
                "Mulet",
                "Philippe",
                "",
                "S3L"
            ],
            [
                "Vitek",
                "Jan",
                "",
                "S3L"
            ]
        ]
    },
    {
        "id": "0712.1205",
        "submitter": "James Riely",
        "authors": "Radha Jagadeesan, Alan Jeffrey, Corin Pitcher, James Riely",
        "title": "Lambda-RBAC: Programming with Role-Based Access Control",
        "comments": "LMCS",
        "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 1 (January 9,\n  2008) lmcs:1195",
        "doi": "10.2168/LMCS-4(1:2)2008",
        "report-no": null,
        "categories": "cs.PL cs.CR",
        "license": null,
        "abstract": "  We study mechanisms that permit program components to express role\nconstraints on clients, focusing on programmatic security mechanisms, which\npermit access controls to be expressed, in situ, as part of the code realizing\nbasic functionality. In this setting, two questions immediately arise: (1) The\nuser of a component faces the issue of safety: is a particular role sufficient\nto use the component? (2) The component designer faces the dual issue of\nprotection: is a particular role demanded in all execution paths of the\ncomponent? We provide a formal calculus and static analysis to answer both\nquestions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Dec 2007 18:58:35 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 9 Jan 2008 16:51:45 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Jagadeesan",
                "Radha",
                ""
            ],
            [
                "Jeffrey",
                "Alan",
                ""
            ],
            [
                "Pitcher",
                "Corin",
                ""
            ],
            [
                "Riely",
                "James",
                ""
            ]
        ]
    },
    {
        "id": "0712.1205",
        "submitter": "James Riely",
        "authors": "Radha Jagadeesan, Alan Jeffrey, Corin Pitcher, James Riely",
        "title": "Lambda-RBAC: Programming with Role-Based Access Control",
        "comments": "LMCS",
        "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 1 (January 9,\n  2008) lmcs:1195",
        "doi": "10.2168/LMCS-4(1:2)2008",
        "report-no": null,
        "categories": "cs.PL cs.CR",
        "license": null,
        "abstract": "  We study mechanisms that permit program components to express role\nconstraints on clients, focusing on programmatic security mechanisms, which\npermit access controls to be expressed, in situ, as part of the code realizing\nbasic functionality. In this setting, two questions immediately arise: (1) The\nuser of a component faces the issue of safety: is a particular role sufficient\nto use the component? (2) The component designer faces the dual issue of\nprotection: is a particular role demanded in all execution paths of the\ncomponent? We provide a formal calculus and static analysis to answer both\nquestions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Dec 2007 18:58:35 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 9 Jan 2008 16:51:45 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Jagadeesan",
                "Radha",
                ""
            ],
            [
                "Jeffrey",
                "Alan",
                ""
            ],
            [
                "Pitcher",
                "Corin",
                ""
            ],
            [
                "Riely",
                "James",
                ""
            ]
        ]
    },
    {
        "id": "0712.1224",
        "submitter": "Kiran Lakkaraju",
        "authors": "Kiran Lakkaraju, Adam Slagell",
        "title": "Evaluating the Utility of Anonymized Network Traces for Intrusion\n  Detection",
        "comments": "* Updated version. * 17 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Anonymization is the process of removing or hiding sensitive information in\nlogs. Anonymization allows organizations to share network logs while not\nexposing sensitive information. However, there is an inherent trade off between\nthe amount of information revealed in the log and the usefulness of the log to\nthe client (the utility of a log). There are many anonymization techniques, and\nthere are many ways to anonymize a particular log (that is, which fields to\nanonymize and how). Different anonymization policies will result in logs with\nvarying levels of utility for analysis. In this paper we explore the effect of\ndifferent anonymization policies on logs. We provide an empirical analysis of\nthe effect of varying anonymization policies by looking at the number of alerts\ngenerated by an Intrusion Detection System. This is the first work to\nthoroughly evaluate the effect of single field anonymization policies on a data\nset. Our main contributions are to determine a set of fields that have a large\nimpact on the utility of a log.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Dec 2007 20:53:22 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 27 Jun 2008 21:08:26 GMT"
            }
        ],
        "update_date": "2008-06-28",
        "authors_parsed": [
            [
                "Lakkaraju",
                "Kiran",
                ""
            ],
            [
                "Slagell",
                "Adam",
                ""
            ]
        ]
    },
    {
        "id": "0712.1400",
        "submitter": "An-Ping Li",
        "authors": "An-Ping Li",
        "title": "Birthday attack to discrete logarithm",
        "comments": "4 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  The discrete logarithm in a finite group of large order has been widely\napplied in public key cryptosystem. In this paper, we will present a\nprobabilistic algorithm for discrete logarithm.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 10 Dec 2007 06:15:49 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 11 Dec 2007 01:57:48 GMT"
            }
        ],
        "update_date": "2009-03-21",
        "authors_parsed": [
            [
                "Li",
                "An-Ping",
                ""
            ]
        ]
    },
    {
        "id": "0712.1402",
        "submitter": "Allan Sly",
        "authors": "Guy Bresler, Elchanan Mossel, Allan Sly",
        "title": "Reconstruction of Markov Random Fields from Samples: Some Easy\n  Observations and Algorithms",
        "comments": "14 pages, 0 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CC cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Markov random fields are used to model high dimensional distributions in a\nnumber of applied areas. Much recent interest has been devoted to the\nreconstruction of the dependency structure from independent samples from the\nMarkov random fields. We analyze a simple algorithm for reconstructing the\nunderlying graph defining a Markov random field on $n$ nodes and maximum degree\n$d$ given observations. We show that under mild non-degeneracy conditions it\nreconstructs the generating graph with high probability using $\\Theta(d\n\\epsilon^{-2}\\delta^{-4} \\log n)$ samples where $\\epsilon,\\delta$ depend on the\nlocal interactions. For most local interaction $\\eps,\\delta$ are of order\n$\\exp(-O(d))$.\n  Our results are optimal as a function of $n$ up to a multiplicative constant\ndepending on $d$ and the strength of the local interactions. Our results seem\nto be the first results for general models that guarantee that {\\em the}\ngenerating model is reconstructed. Furthermore, we provide explicit $O(n^{d+2}\n\\epsilon^{-2}\\delta^{-4} \\log n)$ running time bound. In cases where the\nmeasure on the graph has correlation decay, the running time is $O(n^2 \\log n)$\nfor all fixed $d$. We also discuss the effect of observing noisy samples and\nshow that as long as the noise level is low, our algorithm is effective. On the\nother hand, we construct an example where large noise implies\nnon-identifiability even for generic noise and interactions. Finally, we\nbriefly show that in some simple cases, models with hidden nodes can also be\nrecovered.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 10 Dec 2007 06:50:36 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 8 Mar 2010 19:30:26 GMT"
            }
        ],
        "update_date": "2010-03-09",
        "authors_parsed": [
            [
                "Bresler",
                "Guy",
                ""
            ],
            [
                "Mossel",
                "Elchanan",
                ""
            ],
            [
                "Sly",
                "Allan",
                ""
            ]
        ]
    },
    {
        "id": "0712.1655",
        "submitter": "Piet Hut",
        "authors": "Piet Hut (IAS, Princeton)",
        "title": "Virtual Laboratories and Virtual Worlds",
        "comments": "10 pages, 2 figures, Conference proceedings for IAUS246 'Dynamical\n  Evolution of Dense Stellar Systems', ed. E. Vesperini (Chief Editor), M.\n  Giersz, A. Sills, Capri, Sept. 2007",
        "journal-ref": null,
        "doi": "10.1017/S1743921308016153",
        "report-no": null,
        "categories": "astro-ph cs.HC physics.comp-ph",
        "license": null,
        "abstract": "  Since we cannot put stars in a laboratory, astrophysicists had to wait till\nthe invention of computers before becoming laboratory scientists. For half a\ncentury now, we have been conducting experiments in our virtual laboratories.\nHowever, we ourselves have remained behind the keyboard, with the screen of the\nmonitor separating us from the world we are simulating. Recently, 3D on-line\ntechnology, developed first for games but now deployed in virtual worlds like\nSecond Life, is beginning to make it possible for astrophysicists to enter\ntheir virtual labs themselves, in virtual form as avatars. This has several\nadvantages, from new possibilities to explore the results of the simulations to\na shared presence in a virtual lab with remote collaborators on different\ncontinents. I will report my experiences with the use of Qwaq Forums, a virtual\nworld developed by a new company (see http://www.qwaq.com)\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Dec 2007 07:05:36 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Hut",
                "Piet",
                "",
                "IAS, Princeton"
            ]
        ]
    },
    {
        "id": "0712.1658",
        "submitter": "Kees Middelburg",
        "authors": "J. A. Bergstra, C. A. Middelburg",
        "title": "Program algebra with a jump-shift instruction",
        "comments": "19 pages",
        "journal-ref": "Journal of Applied Logic, 6(4):553--563, 2008",
        "doi": "10.1016/j.jal.2008.07.001",
        "report-no": "PRG0711",
        "categories": "cs.PL",
        "license": null,
        "abstract": "  We study sequential programs that are instruction sequences with jump-shift\ninstructions in the setting of PGA (ProGram Algebra). Jump-shift instructions\npreceding a jump instruction increase the position to jump to. The jump-shift\ninstruction is not found in programming practice. Its merit is that the\nexpressive power of PGA extended with the jump-shift instruction, is not\nreduced if the reach of jump instructions is bounded. This is used to show that\nthere exists a finite-state execution mechanism that by making use of a counter\ncan produce each finite-state thread from some program that is a finite or\nperiodic infinite sequence of instructions from a finite set.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Dec 2007 08:53:49 GMT"
            }
        ],
        "update_date": "2008-11-11",
        "authors_parsed": [
            [
                "Bergstra",
                "J. A.",
                ""
            ],
            [
                "Middelburg",
                "C. A.",
                ""
            ]
        ]
    },
    {
        "id": "0712.1759",
        "submitter": "Sebastien George",
        "authors": "Madeth May (LIESP), S\\'ebastien George (LIESP), Patrick Pr\\'ev\\^ot\n  (LIESP)",
        "title": "A Web-based System for Observing and Analyzing Computer Mediated\n  Communications",
        "comments": null,
        "journal-ref": "Dans Proceedings of the IEEE/WIC/ACM International Conference on\n  Web Intelligence (WI 2006) - IEEE/WIC/ACM International Conference on Web\n  Intelligence (WI 2006, Hong Kong : Chine (2006)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  Tracking data of user's activities resulting from Computer Mediated\nCommunication (CMC) tools (forum, chat, etc.) is often carried out in an ad-hoc\nmanner, which either confines the reusability of data in different purposes or\nmakes data exploitation difficult. Our research works are biased toward\nmethodological challenges involved in designing and developing a generic system\nfor tracking user's activities while interacting with asynchronous\ncommunication tools like discussion forums. We present in this paper, an\napproach for building a Web-based system for observing and analyzing user\nactivity on any type of discussion forums.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Dec 2007 16:47:54 GMT"
            }
        ],
        "update_date": "2007-12-12",
        "authors_parsed": [
            [
                "May",
                "Madeth",
                "",
                "LIESP"
            ],
            [
                "George",
                "S\u00e9bastien",
                "",
                "LIESP"
            ],
            [
                "Pr\u00e9v\u00f4t",
                "Patrick",
                "",
                "LIESP"
            ]
        ]
    },
    {
        "id": "0712.1768",
        "submitter": "Sebastien George",
        "authors": "S\\'ebastien George (LIESP), Alain Derycke (TRIGONE)",
        "title": "Conceptions et usages des plates-formes de formation, Revue Sciences et\n  Technologies de l'Information et de la Communication pour l'\\'Education et la\n  Formation",
        "comments": null,
        "journal-ref": "Sciences et Technologies de l'Information et de la Communication\n  pour l'Education et la Formation 12 (2006) 51-64",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  Educative platforms are at the heart of the development of online education.\nThey can not only be reduced to technological aspects. Underlying models impact\nteaching and learning from the preparing of lessons to the learning sessions.\nResearch related to these platforms are numerous and their stakes are\nimportant. For these reasons, we launched a call to a special issue on \"Designs\nand uses of educative platforms\" An educative platform is a computer system\ndesigned to automate various functions relating to the organization of the\ncourse, to the management of their content, to the monitoring of learners and\nsupervision of persons in charge of various formations (Office de la langue\nfran\\c{c}aise, 2005). So educative platforms are Learning Management Systems\n(LMS) which are specific to education contexts.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Dec 2007 16:59:19 GMT"
            }
        ],
        "update_date": "2007-12-12",
        "authors_parsed": [
            [
                "George",
                "S\u00e9bastien",
                "",
                "LIESP"
            ],
            [
                "Derycke",
                "Alain",
                "",
                "TRIGONE"
            ]
        ]
    },
    {
        "id": "0712.1800",
        "submitter": "Sebastien George",
        "authors": "S\\'ebastien George (LIESP), C\\'ecile Bothorel (TECH/EASY)",
        "title": "Conception d'outils de communication sp\\'ecifiques au contexte\n  \\'educatif",
        "comments": null,
        "journal-ref": "Sciences et Technologies de l'Information et de la Communication\n  pour l'Education et la Formation 13 (2007) 317-344",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  In a distance learning context, providing usual communication tools (forum,\nchat, ...) is not always enough to create efficient interactions between\nlearners and to favour collective knowledge building. A solution consists in\nsetting-up collective activities which encourage learners to communicate. But,\neven in that case, tools can sometimes become a barrier to communication. We\npresent in this paper examples of specific tools that are designed in order to\nfavour and to guide communications in an educational context, but also to\nfoster interactions during learning activities that are not inherently\ncollaborative. We describe synchronous communication tools (semi-structured\nchat), asynchronous tools (temporally structured forum, contextual forum) and a\nsystem which promotes mutual aid between learners.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Dec 2007 19:30:46 GMT"
            }
        ],
        "update_date": "2007-12-12",
        "authors_parsed": [
            [
                "George",
                "S\u00e9bastien",
                "",
                "LIESP"
            ],
            [
                "Bothorel",
                "C\u00e9cile",
                "",
                "TECH/EASY"
            ]
        ]
    },
    {
        "id": "0712.1854",
        "submitter": "Soung Liew",
        "authors": "S.C. Liew, C. Kai, J. Leung, B. Wong",
        "title": "Back-of-the-Envelope Computation of Throughput Distributions in CSMA\n  Wireless Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": null,
        "abstract": "  This work started out with our accidental discovery of a pattern of\nthroughput distributions among links in IEEE 802.11 networks from experimental\nresults. This pattern gives rise to an easy computation method, which we term\nback-of-the-envelop (BoE) computation, because for many network configurations,\nvery accurate results can be obtained within minutes, if not seconds, by simple\nhand computation. BoE beats prior methods in terms of both speed and accuracy.\nWhile the computation procedure of BoE is simple, explaining why it works is by\nno means trivial. Indeed the majority of our investigative efforts have been\ndevoted to the construction of a theory to explain BoE. This paper models an\nideal CSMA network as a set of interacting on-off telegraph processes. In\ndeveloping the theory, we discovered a number of analytical techniques and\nobservations that have eluded prior research, such as that the carrier-sensing\ninteractions among links in an ideal CSMA network result in a system state\nevolution that is time-reversible; and that the probability distribution of the\nsystem state is insensitive to the distributions of the \"on\" and \"off\"\ndurations given their means, and is a Markov random field. We believe these\ntheoretical frameworks are useful not just for explaining BoE, but could also\nbe a foundation for a fundamental understanding of how links in CSMA networks\ninteract. Last but not least, because of their basic nature, we surmise that\nsome of the techniques and results developed in this paper may be applicable to\nnot just CSMA networks, but also to other physical and engineering systems\nconsisting of entities interacting with each other in time and space.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 12 Dec 2007 01:41:54 GMT"
            }
        ],
        "update_date": "2007-12-13",
        "authors_parsed": [
            [
                "Liew",
                "S. C.",
                ""
            ],
            [
                "Kai",
                "C.",
                ""
            ],
            [
                "Leung",
                "J.",
                ""
            ],
            [
                "Wong",
                "B.",
                ""
            ]
        ]
    },
    {
        "id": "0712.1863",
        "submitter": "Weng-Long Chang",
        "authors": "Weng-Long Chang, Michael (Shan-Hui) Ho, and Minyi Guo",
        "title": "Constructing Bio-molecular Databases on a DNA-based Computer",
        "comments": "The article includes 35 pages, several tables and figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.DB q-bio.OT",
        "license": null,
        "abstract": "  Codd [Codd 1970] wrote the first paper in which the model of a relational\ndatabase was proposed. Adleman [Adleman 1994] wrote the first paper in which\nDNA strands in a test tube were used to solve an instance of the Hamiltonian\npath problem. From [Adleman 1994], it is obviously indicated that for storing\ninformation in molecules of DNA allows for an information density of\napproximately 1 bit per cubic nm (nanometer) and a dramatic improvement over\nexisting storage media such as video tape which store information at a density\nof approximately 1 bit per 1012 cubic nanometers. This paper demonstrates that\nbiological operations can be applied to construct bio-molecular databases where\ndata records in relational tables are encoded as DNA strands. In order to\nachieve the goal, DNA algorithms are proposed to perform eight operations of\nrelational algebra (calculus) on bio-molecular relational databases, which\ninclude Cartesian product, union, set difference, selection, projection,\nintersection, join and division. Furthermore, this work presents clear evidence\nof the ability of molecular computing to perform data retrieval operations on\nbio-molecular relational databases.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 12 Dec 2007 03:58:01 GMT"
            }
        ],
        "update_date": "2007-12-13",
        "authors_parsed": [
            [
                "Chang",
                "Weng-Long",
                "",
                "Shan-Hui"
            ],
            [
                "Michael",
                "",
                "",
                "Shan-Hui"
            ],
            [
                "Ho",
                "",
                ""
            ],
            [
                "Guo",
                "Minyi",
                ""
            ]
        ]
    },
    {
        "id": "0712.1863",
        "submitter": "Weng-Long Chang",
        "authors": "Weng-Long Chang, Michael (Shan-Hui) Ho, and Minyi Guo",
        "title": "Constructing Bio-molecular Databases on a DNA-based Computer",
        "comments": "The article includes 35 pages, several tables and figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.DB q-bio.OT",
        "license": null,
        "abstract": "  Codd [Codd 1970] wrote the first paper in which the model of a relational\ndatabase was proposed. Adleman [Adleman 1994] wrote the first paper in which\nDNA strands in a test tube were used to solve an instance of the Hamiltonian\npath problem. From [Adleman 1994], it is obviously indicated that for storing\ninformation in molecules of DNA allows for an information density of\napproximately 1 bit per cubic nm (nanometer) and a dramatic improvement over\nexisting storage media such as video tape which store information at a density\nof approximately 1 bit per 1012 cubic nanometers. This paper demonstrates that\nbiological operations can be applied to construct bio-molecular databases where\ndata records in relational tables are encoded as DNA strands. In order to\nachieve the goal, DNA algorithms are proposed to perform eight operations of\nrelational algebra (calculus) on bio-molecular relational databases, which\ninclude Cartesian product, union, set difference, selection, projection,\nintersection, join and division. Furthermore, this work presents clear evidence\nof the ability of molecular computing to perform data retrieval operations on\nbio-molecular relational databases.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 12 Dec 2007 03:58:01 GMT"
            }
        ],
        "update_date": "2007-12-13",
        "authors_parsed": [
            [
                "Chang",
                "Weng-Long",
                "",
                "Shan-Hui"
            ],
            [
                "Michael",
                "",
                "",
                "Shan-Hui"
            ],
            [
                "Ho",
                "",
                ""
            ],
            [
                "Guo",
                "Minyi",
                ""
            ]
        ]
    },
    {
        "id": "0712.1878",
        "submitter": "Luc Brun",
        "authors": "Jean Hugues Pruvot (GREYC), Luc Brun (GREYC)",
        "title": "Hierarchy construction schemes within the Scale set framework",
        "comments": null,
        "journal-ref": "Dans Graph-Based Representations in Pattern Recognition - Graph\n  based Representation 2007, Alicante : Espagne (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  Segmentation algorithms based on an energy minimisation framework often\ndepend on a scale parameter which balances a fit to data and a regularising\nterm. Irregular pyramids are defined as a stack of graphs successively reduced.\nWithin this framework, the scale is often defined implicitly as the height in\nthe pyramid. However, each level of an irregular pyramid can not usually be\nreadily associated to the global optimum of an energy or a global criterion on\nthe base level graph. This last drawback is addressed by the scale set\nframework designed by Guigues. The methods designed by this author allow to\nbuild a hierarchy and to design cuts within this hierarchy which globally\nminimise an energy. This paper studies the influence of the construction scheme\nof the initial hierarchy on the resulting optimal cuts. We propose one\nsequential and one parallel method with two variations within both. Our\nsequential methods provide partitions near the global optima while parallel\nmethods require less execution times than the sequential method of Guigues even\non sequential machines.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 12 Dec 2007 07:45:08 GMT"
            }
        ],
        "update_date": "2007-12-13",
        "authors_parsed": [
            [
                "Pruvot",
                "Jean Hugues",
                "",
                "GREYC"
            ],
            [
                "Brun",
                "Luc",
                "",
                "GREYC"
            ]
        ]
    },
    {
        "id": "0712.1996",
        "submitter": "Walied Othman",
        "authors": "Bart Kuijpers, Walied Othman, Rafael Grimson",
        "title": "A case study of the difficulty of quantifier elimination in constraint\n  databases: the alibi query in moving object databases",
        "comments": "35 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.CC cs.DB",
        "license": null,
        "abstract": "  In the constraint database model, spatial and spatio-temporal data are stored\nby boolean combinations of polynomial equalities and inequalities over the real\nnumbers. The relational calculus augmented with polynomial constraints is the\nstandard first-order query language for constraint databases. Although the\nexpressive power of this query language has been studied extensively, the\ndifficulty of the efficient evaluation of queries, usually involving some form\nof quantifier elimination, has received considerably less attention. The\ninefficiency of existing quantifier-elimination software and the intrinsic\ndifficulty of quantifier elimination have proven to be a bottle-neck for for\nreal-world implementations of constraint database systems. In this paper, we\nfocus on a particular query, called the \\emph{alibi query}, that asks whether\ntwo moving objects whose positions are known at certain moments in time, could\nhave possibly met, given certain speed constraints. This query can be seen as a\nconstraint database query and its evaluation relies on the elimination of a\nblock of three existential quantifiers. Implementations of general purpose\nelimination algorithms are in the specific case, for practical purposes, too\nslow in answering the alibi query and fail completely in the parametric case.\nThe main contribution of this paper is an analytical solution to the parametric\nalibi query, which can be used to answer this query in the specific case in\nconstant time. We also give an analytic solution to the alibi query at a fixed\nmoment in time. The solutions we propose are based on geometric argumentation\nand they illustrate the fact that some practical problems require creative\nsolutions, where at least in theory, existing systems could provide a solution.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 12 Dec 2007 18:05:41 GMT"
            }
        ],
        "update_date": "2007-12-13",
        "authors_parsed": [
            [
                "Kuijpers",
                "Bart",
                ""
            ],
            [
                "Othman",
                "Walied",
                ""
            ],
            [
                "Grimson",
                "Rafael",
                ""
            ]
        ]
    },
    {
        "id": "0712.2113",
        "submitter": "Andreas U. Schmidt",
        "authors": "Andreas U. Schmidt, Nicolai Kuntze and Michael Kasper",
        "title": "On the deployment of Mobile Trusted Modules",
        "comments": "To appear in: Proceedings of the Wireless Communications and\n  Networking Conference, IEEE WCNC 2008, Las Vegas, USA, 31 March - 2 April\n  2008",
        "journal-ref": null,
        "doi": "10.1109/WCNC.2008.553",
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  In its recently published TCG Mobile Reference Architecture, the TCG Mobile\nPhone Work Group specifies a new concept to enable trust into future mobile\ndevices. For this purpose, the TCG devises a trusted mobile platform as a set\nof trusted engines on behalf of different stakeholders supported by a physical\ntrust-anchor. In this paper, we present our perception on this emerging\nspecification. We propose an approach for the practical design and\nimplementation of this concept and how to deploy it to a trustworthy operating\nplatform. In particular we propose a method for the take-ownership of a device\nby the user and the migration (i.e., portability) of user credentials between\ndevices.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Dec 2007 20:53:32 GMT"
            }
        ],
        "update_date": "2016-11-18",
        "authors_parsed": [
            [
                "Schmidt",
                "Andreas U.",
                ""
            ],
            [
                "Kuntze",
                "Nicolai",
                ""
            ],
            [
                "Kasper",
                "Michael",
                ""
            ]
        ]
    },
    {
        "id": "0712.2168",
        "submitter": "Francoise Sandoz-Guermond",
        "authors": "Marc-Eric Bobiller-Chaumon (GRePS), Michel Dubois (LIP - PC2S),\n  Fran\\c{c}oise Sandoz-Guermond (LIESP)",
        "title": "Study of conditions of use of E-services accessible to visually disabled\n  persons",
        "comments": "4 pages visible \\`a http://ceur-ws.org/Vol-285",
        "journal-ref": "Dans CEUR Workshop Proceedings - DEGAS'07 : Workshop of Design &\n  Evaluation of e-Government Applications and services, Rio de Janeiro :\n  Br\\'esil (2006)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  The aim of this paper is to determine the expectations that French-speaking\ndisabled persons have for electronic administrative sites (utility). At the\nsame time, it is a matter of identifying the difficulties of use that the\nmanipulation of these E-services poses concretely for blind people (usability)\nand of evaluating the psychosocial impacts on the way of life of these people\nwith specific needs. We show that the lack of numerical accessibility is likely\nto accentuate the social exclusion of which these people are victim by\nestablishing a numerical glass ceiling.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Dec 2007 15:14:01 GMT"
            }
        ],
        "update_date": "2007-12-14",
        "authors_parsed": [
            [
                "Bobiller-Chaumon",
                "Marc-Eric",
                "",
                "GRePS"
            ],
            [
                "Dubois",
                "Michel",
                "",
                "LIP - PC2S"
            ],
            [
                "Sandoz-Guermond",
                "Fran\u00e7oise",
                "",
                "LIESP"
            ]
        ]
    },
    {
        "id": "0712.2183",
        "submitter": "Francoise Sandoz-Guermond",
        "authors": "Marc-Eric Bobiller-Chaumon (GRePS), Fran\\c{c}oise Sandoz-Guermond\n  (LIESP)",
        "title": "Apports des d\\'emarches d'inspection et des tests d'usage dans\n  l'\\'evaluation de l'accessibilit\\'e de E-services",
        "comments": "4 pages",
        "journal-ref": "Dans Actes du congr\\`es ERGO IA'2006 - ERGO'IA : L'humain comme\n  facteur de performance des syst\\`emes complexes, Biarritz : France (2006)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  This article proposes to describe and compare the contributions of various\ntechniques of evaluation of the accessibility of E-services carried out\nstarting from (i) methods of inspection (on the basis of traditional ergonomic\ncriteria and accessibility) and (ii) of tests of use. It show that these are\nthe latter which show the best rate of identification of the problems of uses\nfor the poeple with disabilities\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Dec 2007 16:43:56 GMT"
            }
        ],
        "update_date": "2007-12-14",
        "authors_parsed": [
            [
                "Bobiller-Chaumon",
                "Marc-Eric",
                "",
                "GRePS"
            ],
            [
                "Sandoz-Guermond",
                "Fran\u00e7oise",
                "",
                "LIESP"
            ]
        ]
    },
    {
        "id": "0712.2231",
        "submitter": "Andreas U. Schmidt",
        "authors": "Andreas U. Schmidt, Nicolai Kuntze and Joerg Abendroth",
        "title": "Trust for Location-based Authorisation",
        "comments": "To appear in: Proceedings of the Wireless Communications and\n  Networking Conference, IEEE WCNC 2008, Las Vegas, USA, 31 March - 2 April\n  2008",
        "journal-ref": null,
        "doi": "10.1109/WCNC.2008.552",
        "report-no": null,
        "categories": "cs.CR",
        "license": null,
        "abstract": "  We propose a concept for authorisation using the location of a mobile device\nand the enforcement of location-based policies. Mobile devices enhanced by\nTrusted Computing capabilities operate an autonomous and secure location\ntrigger and policy enforcement entity. Location determination is two-tiered,\nintegrating cell-based triggering at handover with precision location\nmeasurement by the device.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Dec 2007 20:17:08 GMT"
            }
        ],
        "update_date": "2016-11-18",
        "authors_parsed": [
            [
                "Schmidt",
                "Andreas U.",
                ""
            ],
            [
                "Kuntze",
                "Nicolai",
                ""
            ],
            [
                "Abendroth",
                "Joerg",
                ""
            ]
        ]
    },
    {
        "id": "0712.2255",
        "submitter": "Ian T Foster",
        "authors": "Ian Foster",
        "title": "Human-Machine Symbiosis, 50 Years On",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.CE cs.HC",
        "license": null,
        "abstract": "  Licklider advocated in 1960 the construction of computers capable of working\nsymbiotically with humans to address problems not easily addressed by humans\nworking alone. Since that time, many of the advances that he envisioned have\nbeen achieved, yet the time spent by human problem solvers in mundane\nactivities remains large. I propose here four areas in which improved tools can\nfurther advance the goal of enhancing human intellect: services, provenance,\nknowledge communities, and automation of problem-solving protocols.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Dec 2007 23:00:37 GMT"
            }
        ],
        "update_date": "2007-12-17",
        "authors_parsed": [
            [
                "Foster",
                "Ian",
                ""
            ]
        ]
    },
    {
        "id": "0712.2255",
        "submitter": "Ian T Foster",
        "authors": "Ian Foster",
        "title": "Human-Machine Symbiosis, 50 Years On",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.CE cs.HC",
        "license": null,
        "abstract": "  Licklider advocated in 1960 the construction of computers capable of working\nsymbiotically with humans to address problems not easily addressed by humans\nworking alone. Since that time, many of the advances that he envisioned have\nbeen achieved, yet the time spent by human problem solvers in mundane\nactivities remains large. I propose here four areas in which improved tools can\nfurther advance the goal of enhancing human intellect: services, provenance,\nknowledge communities, and automation of problem-solving protocols.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Dec 2007 23:00:37 GMT"
            }
        ],
        "update_date": "2007-12-17",
        "authors_parsed": [
            [
                "Foster",
                "Ian",
                ""
            ]
        ]
    },
    {
        "id": "0712.2255",
        "submitter": "Ian T Foster",
        "authors": "Ian Foster",
        "title": "Human-Machine Symbiosis, 50 Years On",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.CE cs.HC",
        "license": null,
        "abstract": "  Licklider advocated in 1960 the construction of computers capable of working\nsymbiotically with humans to address problems not easily addressed by humans\nworking alone. Since that time, many of the advances that he envisioned have\nbeen achieved, yet the time spent by human problem solvers in mundane\nactivities remains large. I propose here four areas in which improved tools can\nfurther advance the goal of enhancing human intellect: services, provenance,\nknowledge communities, and automation of problem-solving protocols.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Dec 2007 23:00:37 GMT"
            }
        ],
        "update_date": "2007-12-17",
        "authors_parsed": [
            [
                "Foster",
                "Ian",
                ""
            ]
        ]
    },
    {
        "id": "0712.2262",
        "submitter": "Ian T Foster",
        "authors": "David Bernholdt, Shishir Bharathi, David Brown, Kasidit Chanchio,\n  Meili Chen, Ann Chervenak, Luca Cinquini, Bob Drach, Ian Foster, Peter Fox,\n  Jose Garcia, Carl Kesselman, Rob Markel, Don Middleton, Veronika Nefedova,\n  Line Pouchard, Arie Shoshani, Alex Sim, Gary Strand, Dean Williams",
        "title": "The Earth System Grid: Supporting the Next Generation of Climate\n  Modeling Research",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.DC cs.NI",
        "license": null,
        "abstract": "  Understanding the earth's climate system and how it might be changing is a\npreeminent scientific challenge. Global climate models are used to simulate\npast, present, and future climates, and experiments are executed continuously\non an array of distributed supercomputers. The resulting data archive, spread\nover several sites, currently contains upwards of 100 TB of simulation data and\nis growing rapidly. Looking toward mid-decade and beyond, we must anticipate\nand prepare for distributed climate research data holdings of many petabytes.\nThe Earth System Grid (ESG) is a collaborative interdisciplinary project aimed\nat addressing the challenge of enabling management, discovery, access, and\nanalysis of these critically important datasets in a distributed and\nheterogeneous computational environment. The problem is fundamentally a Grid\nproblem. Building upon the Globus toolkit and a variety of other technologies,\nESG is developing an environment that addresses authentication, authorization\nfor data access, large-scale data transport and management, services and\nabstractions for high-performance remote data access, mechanisms for scalable\ndata replication, cataloging with rich semantic and syntactic information, data\ndiscovery, distributed monitoring, and Web-based portals for using the system.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Dec 2007 23:39:04 GMT"
            }
        ],
        "update_date": "2007-12-17",
        "authors_parsed": [
            [
                "Bernholdt",
                "David",
                ""
            ],
            [
                "Bharathi",
                "Shishir",
                ""
            ],
            [
                "Brown",
                "David",
                ""
            ],
            [
                "Chanchio",
                "Kasidit",
                ""
            ],
            [
                "Chen",
                "Meili",
                ""
            ],
            [
                "Chervenak",
                "Ann",
                ""
            ],
            [
                "Cinquini",
                "Luca",
                ""
            ],
            [
                "Drach",
                "Bob",
                ""
            ],
            [
                "Foster",
                "Ian",
                ""
            ],
            [
                "Fox",
                "Peter",
                ""
            ],
            [
                "Garcia",
                "Jose",
                ""
            ],
            [
                "Kesselman",
                "Carl",
                ""
            ],
            [
                "Markel",
                "Rob",
                ""
            ],
            [
                "Middleton",
                "Don",
                ""
            ],
            [
                "Nefedova",
                "Veronika",
                ""
            ],
            [
                "Pouchard",
                "Line",
                ""
            ],
            [
                "Shoshani",
                "Arie",
                ""
            ],
            [
                "Sim",
                "Alex",
                ""
            ],
            [
                "Strand",
                "Gary",
                ""
            ],
            [
                "Williams",
                "Dean",
                ""
            ]
        ]
    },
    {
        "id": "0712.2262",
        "submitter": "Ian T Foster",
        "authors": "David Bernholdt, Shishir Bharathi, David Brown, Kasidit Chanchio,\n  Meili Chen, Ann Chervenak, Luca Cinquini, Bob Drach, Ian Foster, Peter Fox,\n  Jose Garcia, Carl Kesselman, Rob Markel, Don Middleton, Veronika Nefedova,\n  Line Pouchard, Arie Shoshani, Alex Sim, Gary Strand, Dean Williams",
        "title": "The Earth System Grid: Supporting the Next Generation of Climate\n  Modeling Research",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.DC cs.NI",
        "license": null,
        "abstract": "  Understanding the earth's climate system and how it might be changing is a\npreeminent scientific challenge. Global climate models are used to simulate\npast, present, and future climates, and experiments are executed continuously\non an array of distributed supercomputers. The resulting data archive, spread\nover several sites, currently contains upwards of 100 TB of simulation data and\nis growing rapidly. Looking toward mid-decade and beyond, we must anticipate\nand prepare for distributed climate research data holdings of many petabytes.\nThe Earth System Grid (ESG) is a collaborative interdisciplinary project aimed\nat addressing the challenge of enabling management, discovery, access, and\nanalysis of these critically important datasets in a distributed and\nheterogeneous computational environment. The problem is fundamentally a Grid\nproblem. Building upon the Globus toolkit and a variety of other technologies,\nESG is developing an environment that addresses authentication, authorization\nfor data access, large-scale data transport and management, services and\nabstractions for high-performance remote data access, mechanisms for scalable\ndata replication, cataloging with rich semantic and syntactic information, data\ndiscovery, distributed monitoring, and Web-based portals for using the system.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Dec 2007 23:39:04 GMT"
            }
        ],
        "update_date": "2007-12-17",
        "authors_parsed": [
            [
                "Bernholdt",
                "David",
                ""
            ],
            [
                "Bharathi",
                "Shishir",
                ""
            ],
            [
                "Brown",
                "David",
                ""
            ],
            [
                "Chanchio",
                "Kasidit",
                ""
            ],
            [
                "Chen",
                "Meili",
                ""
            ],
            [
                "Chervenak",
                "Ann",
                ""
            ],
            [
                "Cinquini",
                "Luca",
                ""
            ],
            [
                "Drach",
                "Bob",
                ""
            ],
            [
                "Foster",
                "Ian",
                ""
            ],
            [
                "Fox",
                "Peter",
                ""
            ],
            [
                "Garcia",
                "Jose",
                ""
            ],
            [
                "Kesselman",
                "Carl",
                ""
            ],
            [
                "Markel",
                "Rob",
                ""
            ],
            [
                "Middleton",
                "Don",
                ""
            ],
            [
                "Nefedova",
                "Veronika",
                ""
            ],
            [
                "Pouchard",
                "Line",
                ""
            ],
            [
                "Shoshani",
                "Arie",
                ""
            ],
            [
                "Sim",
                "Alex",
                ""
            ],
            [
                "Strand",
                "Gary",
                ""
            ],
            [
                "Williams",
                "Dean",
                ""
            ]
        ]
    },
    {
        "id": "0712.2302",
        "submitter": "Georg Hager",
        "authors": "Georg Hager, Thomas Zeiser, Gerhard Wellein",
        "title": "Data access optimizations for highly threaded multi-core CPUs with\n  multiple memory controllers",
        "comments": "12 pages, 7 figures. Accepted for Workshop on Large-Scale Parallel\n  Processing 2008. Revised and extended version",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.PF",
        "license": null,
        "abstract": "  Processor and system architectures that feature multiple memory controllers\nare prone to show bottlenecks and erratic performance numbers on codes with\nregular access patterns. Although such effects are well known in the form of\ncache thrashing and aliasing conflicts, they become more severe when memory\naccess is involved. Using the new Sun UltraSPARC T2 processor as a prototypical\nmulti-core design, we analyze performance patterns in low-level and application\nbenchmarks and show ways to circumvent bottlenecks by careful data layout and\npadding.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 14 Dec 2007 08:14:20 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 28 Jan 2008 16:18:50 GMT"
            }
        ],
        "update_date": "2008-01-28",
        "authors_parsed": [
            [
                "Hager",
                "Georg",
                ""
            ],
            [
                "Zeiser",
                "Thomas",
                ""
            ],
            [
                "Wellein",
                "Gerhard",
                ""
            ]
        ]
    },
    {
        "id": "0712.2302",
        "submitter": "Georg Hager",
        "authors": "Georg Hager, Thomas Zeiser, Gerhard Wellein",
        "title": "Data access optimizations for highly threaded multi-core CPUs with\n  multiple memory controllers",
        "comments": "12 pages, 7 figures. Accepted for Workshop on Large-Scale Parallel\n  Processing 2008. Revised and extended version",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.PF",
        "license": null,
        "abstract": "  Processor and system architectures that feature multiple memory controllers\nare prone to show bottlenecks and erratic performance numbers on codes with\nregular access patterns. Although such effects are well known in the form of\ncache thrashing and aliasing conflicts, they become more severe when memory\naccess is involved. Using the new Sun UltraSPARC T2 processor as a prototypical\nmulti-core design, we analyze performance patterns in low-level and application\nbenchmarks and show ways to circumvent bottlenecks by careful data layout and\npadding.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 14 Dec 2007 08:14:20 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 28 Jan 2008 16:18:50 GMT"
            }
        ],
        "update_date": "2008-01-28",
        "authors_parsed": [
            [
                "Hager",
                "Georg",
                ""
            ],
            [
                "Zeiser",
                "Thomas",
                ""
            ],
            [
                "Wellein",
                "Gerhard",
                ""
            ]
        ]
    },
    {
        "id": "0712.2497",
        "submitter": "Fangwen Fu",
        "authors": "Fangwen Fu and Mihaela van der Schaar",
        "title": "A New Theoretic Foundation for Cross-Layer Optimization",
        "comments": "39 pages, 10 figures, technical report",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.LG",
        "license": null,
        "abstract": "  Cross-layer optimization solutions have been proposed in recent years to\nimprove the performance of network users operating in a time-varying,\nerror-prone wireless environment. However, these solutions often rely on ad-hoc\noptimization approaches, which ignore the different environmental dynamics\nexperienced at various layers by a user and violate the layered network\narchitecture of the protocol stack by requiring layers to provide access to\ntheir internal protocol parameters to other layers. This paper presents a new\ntheoretic foundation for cross-layer optimization, which allows each layer to\nmake autonomous decisions individually, while maximizing the utility of the\nwireless user by optimally determining what information needs to be exchanged\namong layers. Hence, this cross-layer framework does not change the current\nlayered architecture. Specifically, because the wireless user interacts with\nthe environment at various layers of the protocol stack, the cross-layer\noptimization problem is formulated as a layered Markov decision process (MDP)\nin which each layer adapts its own protocol parameters and exchanges\ninformation (messages) with other layers in order to cooperatively maximize the\nperformance of the wireless user. The message exchange mechanism for\ndetermining the optimal cross-layer transmission strategies has been designed\nfor both off-line optimization and on-line dynamic adaptation. We also show\nthat many existing cross-layer optimization algorithms can be formulated as\nsimplified, sub-optimal, versions of our layered MDP framework.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 15 Dec 2007 06:50:43 GMT"
            }
        ],
        "update_date": "2007-12-18",
        "authors_parsed": [
            [
                "Fu",
                "Fangwen",
                ""
            ],
            [
                "van der Schaar",
                "Mihaela",
                ""
            ]
        ]
    },
    {
        "id": "0712.2591",
        "submitter": "Grenville Croll",
        "authors": "Grenville J. Croll",
        "title": "A Typical Model Audit Approach: Spreadsheet Audit Methodologies in the\n  City of London",
        "comments": "5 Pages",
        "journal-ref": "IFIP, Integrity and Internal Control in Information Systems, Vol\n  124, pp. 213-219, Kluwer, 2003",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.CY",
        "license": null,
        "abstract": "  Spreadsheet audit and review procedures are an essential part of almost all\nCity of London financial transactions. Structured processes are used to\ndiscover errors in large financial spreadsheets underpinning major transactions\nof all types. Serious errors are routinely found and are fed back to model\ndevelopment teams generally under conditions of extreme time urgency. Corrected\nmodels form the essence of the completed transaction and firms undertaking\nmodel audit and review expose themselves to significant financial liability in\nthe event of any remaining significant error. It is noteworthy that in the\nUnited Kingdom, the management of spreadsheet error is almost unheard of\noutside of the City of London despite the commercial ubiquity of the\nspreadsheet.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 16 Dec 2007 20:40:57 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Croll",
                "Grenville J.",
                ""
            ]
        ]
    },
    {
        "id": "0712.2630",
        "submitter": "Juan J. Merelo Pr.",
        "authors": "Nestor Zorzano, Daniel Merino, J.L.J. Laredo, J.P. Sevilla, Pablo\n  Garcia, J.J. Merelo",
        "title": "Evolving XSLT stylesheets",
        "comments": "First draft, preparing for WCCI 2008",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.PL",
        "license": null,
        "abstract": "  This paper introduces a procedure based on genetic programming to evolve XSLT\nprograms (usually called stylesheets or logicsheets). XSLT is a general\npurpose, document-oriented functional language, generally used to transform XML\ndocuments (or, in general, solve any problem that can be coded as an XML\ndocument). The proposed solution uses a tree representation for the stylesheets\nas well as diverse specific operators in order to obtain, in the studied cases\nand a reasonable time, a XSLT stylesheet that performs the transformation.\nSeveral types of representation have been compared, resulting in different\nperformance and degree of success.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Dec 2007 19:59:42 GMT"
            }
        ],
        "update_date": "2007-12-18",
        "authors_parsed": [
            [
                "Zorzano",
                "Nestor",
                ""
            ],
            [
                "Merino",
                "Daniel",
                ""
            ],
            [
                "Laredo",
                "J. L. J.",
                ""
            ],
            [
                "Sevilla",
                "J. P.",
                ""
            ],
            [
                "Garcia",
                "Pablo",
                ""
            ],
            [
                "Merelo",
                "J. J.",
                ""
            ]
        ]
    },
    {
        "id": "0712.2630",
        "submitter": "Juan J. Merelo Pr.",
        "authors": "Nestor Zorzano, Daniel Merino, J.L.J. Laredo, J.P. Sevilla, Pablo\n  Garcia, J.J. Merelo",
        "title": "Evolving XSLT stylesheets",
        "comments": "First draft, preparing for WCCI 2008",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.PL",
        "license": null,
        "abstract": "  This paper introduces a procedure based on genetic programming to evolve XSLT\nprograms (usually called stylesheets or logicsheets). XSLT is a general\npurpose, document-oriented functional language, generally used to transform XML\ndocuments (or, in general, solve any problem that can be coded as an XML\ndocument). The proposed solution uses a tree representation for the stylesheets\nas well as diverse specific operators in order to obtain, in the studied cases\nand a reasonable time, a XSLT stylesheet that performs the transformation.\nSeveral types of representation have been compared, resulting in different\nperformance and degree of success.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Dec 2007 19:59:42 GMT"
            }
        ],
        "update_date": "2007-12-18",
        "authors_parsed": [
            [
                "Zorzano",
                "Nestor",
                ""
            ],
            [
                "Merino",
                "Daniel",
                ""
            ],
            [
                "Laredo",
                "J. L. J.",
                ""
            ],
            [
                "Sevilla",
                "J. P.",
                ""
            ],
            [
                "Garcia",
                "Pablo",
                ""
            ],
            [
                "Merelo",
                "J. J.",
                ""
            ]
        ]
    },
    {
        "id": "0712.2643",
        "submitter": "Cyrille Bertelle",
        "authors": "Pierrick Tranouez (LITIS), Cyrille Bertelle (LITIS), Damien Olivier\n  (LITIS)",
        "title": "Changing Levels of Description in a Fluid Flow Simulation",
        "comments": null,
        "journal-ref": "Emergent Properties in Natural and Artificial Dynamical Systems,\n  Springer (Ed.) (2006) 87-99",
        "doi": null,
        "report-no": null,
        "categories": "physics.flu-dyn cs.CE",
        "license": null,
        "abstract": "  We describe here our perception of complex systems, of how we feel the\ndifferent layers of description are important part of a correct complex system\nsimulation. We describe a rough models categorization between rules based and\nlaw based, of how these categories handled the levels of descriptions or\nscales. We then describe our fluid flow simulation, which combines different\nfineness of grain in a mixed approach of these categories. This simulation is\nbuilt keeping in mind an ulterior use inside a more general aquatic ecosystem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Dec 2007 07:07:06 GMT"
            }
        ],
        "update_date": "2007-12-18",
        "authors_parsed": [
            [
                "Tranouez",
                "Pierrick",
                "",
                "LITIS"
            ],
            [
                "Bertelle",
                "Cyrille",
                "",
                "LITIS"
            ],
            [
                "Olivier",
                "Damien",
                "",
                "LITIS"
            ]
        ]
    },
    {
        "id": "0712.2671",
        "submitter": "Laurent Buse",
        "authors": "Laurent Bus\\'e (INRIA Sophia Antipolis)",
        "title": "On the equations of the moving curve ideal of a rational algebraic plane\n  curve",
        "comments": "Journal of Algebra (2009)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.AG cs.SC math.AC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a parametrization of a rational plane algebraic curve C, some explicit\nadjoint pencils on C are described in terms of determinants. Moreover, some\ngenerators of the Rees algebra associated to this parametrization are\npresented. The main ingredient developed in this paper is a detailed study of\nthe elimination ideal of two homogeneous polynomials in two homogeneous\nvariables that form a regular sequence.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Dec 2007 10:12:33 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 10 Feb 2009 10:45:19 GMT"
            }
        ],
        "update_date": "2009-02-10",
        "authors_parsed": [
            [
                "Bus\u00e9",
                "Laurent",
                "",
                "INRIA Sophia Antipolis"
            ]
        ]
    },
    {
        "id": "0712.2737",
        "submitter": "Wim Vanhoof",
        "authors": "Kim Henriksen, Gourinath Banda, John Gallagher",
        "title": "Experiments with a Convex Polyhedral Analysis Tool for Logic Programs",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  Convex polyhedral abstractions of logic programs have been found very useful\nin deriving numeric relationships between program arguments in order to prove\nprogram properties and in other areas such as termination and complexity\nanalysis. We present a tool for constructing polyhedral analyses of\n(constraint) logic programs. The aim of the tool is to make available, with a\nconvenient interface, state-of-the-art techniques for polyhedral analysis such\nas delayed widening, narrowing, \"widening up-to\", and enhanced automatic\nselection of widening points. The tool is accessible on the web, permits user\nprograms to be uploaded and analysed, and is integrated with related program\ntransformations such as size abstractions and query-answer transformation. We\nthen report some experiments using the tool, showing how it can be conveniently\nused to analyse transition systems arising from models of embedded systems, and\nan emulator for a PIC microcontroller which is used for example in wearable\ncomputing systems. We discuss issues including scalability, tradeoffs of\nprecision and computation time, and other program transformations that can\nenhance the results of analysis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Dec 2007 15:11:36 GMT"
            }
        ],
        "update_date": "2007-12-18",
        "authors_parsed": [
            [
                "Henriksen",
                "Kim",
                ""
            ],
            [
                "Banda",
                "Gourinath",
                ""
            ],
            [
                "Gallagher",
                "John",
                ""
            ]
        ]
    },
    {
        "id": "0712.2737",
        "submitter": "Wim Vanhoof",
        "authors": "Kim Henriksen, Gourinath Banda, John Gallagher",
        "title": "Experiments with a Convex Polyhedral Analysis Tool for Logic Programs",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  Convex polyhedral abstractions of logic programs have been found very useful\nin deriving numeric relationships between program arguments in order to prove\nprogram properties and in other areas such as termination and complexity\nanalysis. We present a tool for constructing polyhedral analyses of\n(constraint) logic programs. The aim of the tool is to make available, with a\nconvenient interface, state-of-the-art techniques for polyhedral analysis such\nas delayed widening, narrowing, \"widening up-to\", and enhanced automatic\nselection of widening points. The tool is accessible on the web, permits user\nprograms to be uploaded and analysed, and is integrated with related program\ntransformations such as size abstractions and query-answer transformation. We\nthen report some experiments using the tool, showing how it can be conveniently\nused to analyse transition systems arising from models of embedded systems, and\nan emulator for a PIC microcontroller which is used for example in wearable\ncomputing systems. We discuss issues including scalability, tradeoffs of\nprecision and computation time, and other program transformations that can\nenhance the results of analysis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Dec 2007 15:11:36 GMT"
            }
        ],
        "update_date": "2007-12-18",
        "authors_parsed": [
            [
                "Henriksen",
                "Kim",
                ""
            ],
            [
                "Banda",
                "Gourinath",
                ""
            ],
            [
                "Gallagher",
                "John",
                ""
            ]
        ]
    },
    {
        "id": "0712.2773",
        "submitter": "Emmanuel Cecchet",
        "authors": "Emmanuel Cecchet, George Candea, Anastasia Ailamaki",
        "title": "Middleware-based Database Replication: The Gaps between Theory and\n  Practice",
        "comments": "14 pages. Appears in Proc. ACM SIGMOD International Conference on\n  Management of Data, Vancouver, Canada, June 2008",
        "journal-ref": null,
        "doi": null,
        "report-no": "EPFL technical report DSLAB-REPORT-2007-001",
        "categories": "cs.DB cs.DC cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The need for high availability and performance in data management systems has\nbeen fueling a long running interest in database replication from both academia\nand industry. However, academic groups often attack replication problems in\nisolation, overlooking the need for completeness in their solutions, while\ncommercial teams take a holistic approach that often misses opportunities for\nfundamental innovation. This has created over time a gap between academic\nresearch and industrial practice.\n  This paper aims to characterize the gap along three axes: performance,\navailability, and administration. We build on our own experience developing and\ndeploying replication systems in commercial and academic settings, as well as\non a large body of prior related work. We sift through representative examples\nfrom the last decade of open-source, academic, and commercial database\nreplication systems and combine this material with case studies from real\nsystems deployed at Fortune 500 customers. We propose two agendas, one for\nacademic research and one for industrial R&D, which we believe can bridge the\ngap within 5-10 years. This way, we hope to both motivate and help researchers\nin making the theory and practice of middleware-based database replication more\nrelevant to each other.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Dec 2007 18:42:15 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 5 Nov 2008 20:53:51 GMT"
            }
        ],
        "update_date": "2008-11-05",
        "authors_parsed": [
            [
                "Cecchet",
                "Emmanuel",
                ""
            ],
            [
                "Candea",
                "George",
                ""
            ],
            [
                "Ailamaki",
                "Anastasia",
                ""
            ]
        ]
    },
    {
        "id": "0712.2773",
        "submitter": "Emmanuel Cecchet",
        "authors": "Emmanuel Cecchet, George Candea, Anastasia Ailamaki",
        "title": "Middleware-based Database Replication: The Gaps between Theory and\n  Practice",
        "comments": "14 pages. Appears in Proc. ACM SIGMOD International Conference on\n  Management of Data, Vancouver, Canada, June 2008",
        "journal-ref": null,
        "doi": null,
        "report-no": "EPFL technical report DSLAB-REPORT-2007-001",
        "categories": "cs.DB cs.DC cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The need for high availability and performance in data management systems has\nbeen fueling a long running interest in database replication from both academia\nand industry. However, academic groups often attack replication problems in\nisolation, overlooking the need for completeness in their solutions, while\ncommercial teams take a holistic approach that often misses opportunities for\nfundamental innovation. This has created over time a gap between academic\nresearch and industrial practice.\n  This paper aims to characterize the gap along three axes: performance,\navailability, and administration. We build on our own experience developing and\ndeploying replication systems in commercial and academic settings, as well as\non a large body of prior related work. We sift through representative examples\nfrom the last decade of open-source, academic, and commercial database\nreplication systems and combine this material with case studies from real\nsystems deployed at Fortune 500 customers. We propose two agendas, one for\nacademic research and one for industrial R&D, which we believe can bridge the\ngap within 5-10 years. This way, we hope to both motivate and help researchers\nin making the theory and practice of middleware-based database replication more\nrelevant to each other.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Dec 2007 18:42:15 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 5 Nov 2008 20:53:51 GMT"
            }
        ],
        "update_date": "2008-11-05",
        "authors_parsed": [
            [
                "Cecchet",
                "Emmanuel",
                ""
            ],
            [
                "Candea",
                "George",
                ""
            ],
            [
                "Ailamaki",
                "Anastasia",
                ""
            ]
        ]
    },
    {
        "id": "0712.2773",
        "submitter": "Emmanuel Cecchet",
        "authors": "Emmanuel Cecchet, George Candea, Anastasia Ailamaki",
        "title": "Middleware-based Database Replication: The Gaps between Theory and\n  Practice",
        "comments": "14 pages. Appears in Proc. ACM SIGMOD International Conference on\n  Management of Data, Vancouver, Canada, June 2008",
        "journal-ref": null,
        "doi": null,
        "report-no": "EPFL technical report DSLAB-REPORT-2007-001",
        "categories": "cs.DB cs.DC cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The need for high availability and performance in data management systems has\nbeen fueling a long running interest in database replication from both academia\nand industry. However, academic groups often attack replication problems in\nisolation, overlooking the need for completeness in their solutions, while\ncommercial teams take a holistic approach that often misses opportunities for\nfundamental innovation. This has created over time a gap between academic\nresearch and industrial practice.\n  This paper aims to characterize the gap along three axes: performance,\navailability, and administration. We build on our own experience developing and\ndeploying replication systems in commercial and academic settings, as well as\non a large body of prior related work. We sift through representative examples\nfrom the last decade of open-source, academic, and commercial database\nreplication systems and combine this material with case studies from real\nsystems deployed at Fortune 500 customers. We propose two agendas, one for\nacademic research and one for industrial R&D, which we believe can bridge the\ngap within 5-10 years. This way, we hope to both motivate and help researchers\nin making the theory and practice of middleware-based database replication more\nrelevant to each other.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Dec 2007 18:42:15 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 5 Nov 2008 20:53:51 GMT"
            }
        ],
        "update_date": "2008-11-05",
        "authors_parsed": [
            [
                "Cecchet",
                "Emmanuel",
                ""
            ],
            [
                "Candea",
                "George",
                ""
            ],
            [
                "Ailamaki",
                "Anastasia",
                ""
            ]
        ]
    },
    {
        "id": "0712.2789",
        "submitter": "Lester Ingber",
        "authors": "Lester Ingber",
        "title": "Trading in Risk Dimensions (TRD)",
        "comments": "This 2005 report has been withdrawn by the author as requested by the\n  publisher of \"Handbook of Technical Trading Analysis\" (Wiley, 2009) in which\n  an updated version appears",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Previous work, mostly published, developed two-shell recursive trading\nsystems. An inner-shell of Canonical Momenta Indicators (CMI) is adaptively fit\nto incoming market data. A parameterized trading-rule outer-shell uses the\nglobal optimization code Adaptive Simulated Annealing (ASA) to fit the trading\nsystem to historical data. A simple fitting algorithm, usually not requiring\nASA, is used for the inner-shell fit. An additional risk-management\nmiddle-shell has been added to create a three-shell recursive\noptimization/sampling/fitting algorithm. Portfolio-level distributions of\ncopula-transformed multivariate distributions (with constituent markets\npossessing different marginal distributions in returns space) are generated by\nMonte Carlo samplings. ASA is used to importance-sample weightings of these\nmarkets.\n  The core code, Trading in Risk Dimensions (TRD), processes Training and\nTesting trading systems on historical data, and consistently interacts with\nRealTime trading platforms at minute resolutions, but this scale can be\nmodified. This approach transforms constituent probability distributions into a\ncommon space where it makes sense to develop correlations to further develop\nprobability distributions and risk/uncertainty analyses of the full portfolio.\nASA is used for importance-sampling these distributions and for optimizing\nsystem parameters.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Dec 2007 18:11:52 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 4 Nov 2009 03:32:16 GMT"
            }
        ],
        "update_date": "2009-11-04",
        "authors_parsed": [
            [
                "Ingber",
                "Lester",
                ""
            ]
        ]
    },
    {
        "id": "0712.2869",
        "submitter": "Daniel \\v{S}tefankovi\\v{c}",
        "authors": "Satyaki Mahalanabis, Daniel Stefankovic",
        "title": "Density estimation in linear time",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  We consider the problem of choosing a density estimate from a set of\ndistributions F, minimizing the L1-distance to an unknown distribution\n(Devroye, Lugosi 2001). Devroye and Lugosi analyze two algorithms for the\nproblem: Scheffe tournament winner and minimum distance estimate. The Scheffe\ntournament estimate requires fewer computations than the minimum distance\nestimate, but has strictly weaker guarantees than the latter.\n  We focus on the computational aspect of density estimation. We present two\nalgorithms, both with the same guarantee as the minimum distance estimate. The\nfirst one, a modification of the minimum distance estimate, uses the same\nnumber (quadratic in |F|) of computations as the Scheffe tournament. The second\none, called ``efficient minimum loss-weight estimate,'' uses only a linear\nnumber of computations, assuming that F is preprocessed.\n  We also give examples showing that the guarantees of the algorithms cannot be\nimproved and explore randomized algorithms for density estimation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 18 Dec 2007 03:30:05 GMT"
            }
        ],
        "update_date": "2007-12-19",
        "authors_parsed": [
            [
                "Mahalanabis",
                "Satyaki",
                ""
            ],
            [
                "Stefankovic",
                "Daniel",
                ""
            ]
        ]
    },
    {
        "id": "0712.2870",
        "submitter": "Hari Palaiyanur",
        "authors": "Hari Palaiyanur, Cheng Chang and Anant Sahai",
        "title": "The source coding game with a cheating switcher",
        "comments": "27 pages, 11 figures. Submitted to IT Transactions",
        "journal-ref": null,
        "doi": null,
        "report-no": "EECS-2007-155",
        "categories": "cs.IT cs.CV math.IT",
        "license": null,
        "abstract": "  Motivated by the lossy compression of an active-vision video stream, we\nconsider the problem of finding the rate-distortion function of an arbitrarily\nvarying source (AVS) composed of a finite number of subsources with known\ndistributions. Berger's paper `The Source Coding Game', \\emph{IEEE Trans.\nInform. Theory}, 1971, solves this problem under the condition that the\nadversary is allowed only strictly causal access to the subsource realizations.\nWe consider the case when the adversary has access to the subsource\nrealizations non-causally. Using the type-covering lemma, this new\nrate-distortion function is determined to be the maximum of the IID\nrate-distortion function over a set of source distributions attainable by the\nadversary. We then extend the results to allow for partial or noisy\nobservations of subsource realizations. We further explore the model by\nattempting to find the rate-distortion function when the adversary is actually\nhelpful.\n  Finally, a bound is developed on the uniform continuity of the IID\nrate-distortion function for finite-alphabet sources. The bound is used to give\na sufficient number of distributions that need to be sampled to compute the\nrate-distortion function of an AVS to within a certain accuracy. The bound is\nalso used to give a rate of convergence for the estimate of the rate-distortion\nfunction for an unknown IID finite-alphabet source .\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 18 Dec 2007 03:31:32 GMT"
            }
        ],
        "update_date": "2016-09-08",
        "authors_parsed": [
            [
                "Palaiyanur",
                "Hari",
                ""
            ],
            [
                "Chang",
                "Cheng",
                ""
            ],
            [
                "Sahai",
                "Anant",
                ""
            ]
        ]
    },
    {
        "id": "0712.2923",
        "submitter": "Roumen Anguelov",
        "authors": "Roumen Anguelov, Inger Plaskitt",
        "title": "A Class of LULU Operators on Multi-Dimensional Arrays",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  The LULU operators for sequences are extended to multi-dimensional arrays via\nthe morphological concept of connection in a way which preserves their\nessential properties, e.g. they are separators and form a four element fully\nordered semi-group. The power of the operators is demonstrated by deriving a\ntotal variation preserving discrete pulse decomposition of images.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 18 Dec 2007 10:43:23 GMT"
            }
        ],
        "update_date": "2007-12-19",
        "authors_parsed": [
            [
                "Anguelov",
                "Roumen",
                ""
            ],
            [
                "Plaskitt",
                "Inger",
                ""
            ]
        ]
    },
    {
        "id": "0712.2943",
        "submitter": "Bob Diertens",
        "authors": "Bob Diertens",
        "title": "Software (Re-)Engineering with PSF",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "PRG0505",
        "categories": "cs.SE",
        "license": null,
        "abstract": "  This paper investigates the usefulness of PSF in software engineering and\nreengineering. PSF is based on ACP (Algebra of Communicating Processes) and as\nsome architectural description languages are based on process algebra, we\ninvestigate whether PSF can be used at the software architecture level, but we\nalso use PSF at lower abstract levels. As a case study we reengineer the\ncompiler from the Toolkit of PSF.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 18 Dec 2007 12:25:02 GMT"
            }
        ],
        "update_date": "2007-12-19",
        "authors_parsed": [
            [
                "Diertens",
                "Bob",
                ""
            ]
        ]
    },
    {
        "id": "0712.2958",
        "submitter": "Joel Goossens",
        "authors": "Vincent N\\'elis, Jo\\\"el Goossens, Nicolas Navet, Raymond Devillers and\n  Dragomir Milojevic",
        "title": "Power-Aware Real-Time Scheduling upon Identical Multiprocessor Platforms",
        "comments": "The manuscript corresponds to the final version of SUTC 2008\n  conference",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": null,
        "abstract": "  In this paper, we address the power-aware scheduling of sporadic\nconstrained-deadline hard real-time tasks using dynamic voltage scaling upon\nmultiprocessor platforms. We propose two distinct algorithms. Our first\nalgorithm is an off-line speed determination mechanism which provides an\nidentical speed for each processor. That speed guarantees that all deadlines\nare met if the jobs are scheduled using EDF. The second algorithm is an on-line\nand adaptive speed adjustment mechanism which reduces the energy consumption\nwhile the system is running.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 18 Dec 2007 13:42:45 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 10 Mar 2008 16:10:50 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "N\u00e9lis",
                "Vincent",
                ""
            ],
            [
                "Goossens",
                "Jo\u00ebl",
                ""
            ],
            [
                "Navet",
                "Nicolas",
                ""
            ],
            [
                "Devillers",
                "Raymond",
                ""
            ],
            [
                "Milojevic",
                "Dragomir",
                ""
            ]
        ]
    },
    {
        "id": "0712.3088",
        "submitter": "Zhaohua Luo",
        "authors": "Zhaohua Luo",
        "title": "Clones and Genoids in Lambda Calculus and First Order Logic",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A genoid is a category of two objects such that one is the product of itself\nwith the other. A genoid may be viewed as an abstract substitution algebra. It\nis a remarkable fact that such a simple concept can be applied to present a\nunified algebraic approach to lambda calculus and first order logic.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Dec 2007 20:52:48 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 19 Dec 2007 22:00:20 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 7 Sep 2012 19:46:32 GMT"
            }
        ],
        "update_date": "2012-09-10",
        "authors_parsed": [
            [
                "Luo",
                "Zhaohua",
                ""
            ]
        ]
    },
    {
        "id": "0712.3113",
        "submitter": "Wim Vanhoof",
        "authors": "Andr\\'as Gyorgy B\\'ek\\'es, P\\'eter Szeredi",
        "title": "Optimizing Queries in a Logic-based Information Integration System",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  The SINTAGMA information integration system is an infrastructure for\naccessing several different information sources together. Besides providing a\nuniform interface to the information sources (databases, web services, web\nsites, RDF resources, XML files), semantic integration is also needed. Semantic\nintegration is carried out by providing a high-level model and the mappings to\nthe models of the sources. When executing a query of the high level model, a\nquery is transformed to a low-level query plan, which is a piece of Prolog code\nthat answers the high-level query. This transformation is done in two phases.\nFirst, the Query Planner produces a plan as a logic formula expressing the\nlow-level query. Next, the Query Optimizer transforms this formula to\nexecutable Prolog code and optimizes it according to structural and statistical\ninformation about the information sources.\n  This article discusses the main ideas of the optimization algorithm and its\nimplementation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Dec 2007 08:07:30 GMT"
            }
        ],
        "update_date": "2007-12-20",
        "authors_parsed": [
            [
                "B\u00e9k\u00e9s",
                "Andr\u00e1s Gyorgy",
                ""
            ],
            [
                "Szeredi",
                "P\u00e9ter",
                ""
            ]
        ]
    },
    {
        "id": "0712.3113",
        "submitter": "Wim Vanhoof",
        "authors": "Andr\\'as Gyorgy B\\'ek\\'es, P\\'eter Szeredi",
        "title": "Optimizing Queries in a Logic-based Information Integration System",
        "comments": "Paper presented at the 17th Workshop on Logic-based Methods in\n  Programming Environments (WLPE2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  The SINTAGMA information integration system is an infrastructure for\naccessing several different information sources together. Besides providing a\nuniform interface to the information sources (databases, web services, web\nsites, RDF resources, XML files), semantic integration is also needed. Semantic\nintegration is carried out by providing a high-level model and the mappings to\nthe models of the sources. When executing a query of the high level model, a\nquery is transformed to a low-level query plan, which is a piece of Prolog code\nthat answers the high-level query. This transformation is done in two phases.\nFirst, the Query Planner produces a plan as a logic formula expressing the\nlow-level query. Next, the Query Optimizer transforms this formula to\nexecutable Prolog code and optimizes it according to structural and statistical\ninformation about the information sources.\n  This article discusses the main ideas of the optimization algorithm and its\nimplementation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Dec 2007 08:07:30 GMT"
            }
        ],
        "update_date": "2007-12-20",
        "authors_parsed": [
            [
                "B\u00e9k\u00e9s",
                "Andr\u00e1s Gyorgy",
                ""
            ],
            [
                "Szeredi",
                "P\u00e9ter",
                ""
            ]
        ]
    },
    {
        "id": "0712.3115",
        "submitter": "Bob Diertens",
        "authors": "Bob Diertens",
        "title": "Software (Re-)Engineering with PSF II: from architecture to\n  implementation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "prg0609",
        "categories": "cs.SE",
        "license": null,
        "abstract": "  This paper presents ongoing research on the application of PSF in the field\nof software engineering and reengineering. We build a new implementation for\nthe simulator of the PSF Toolkit starting from the specification in PSF of the\narchitecture of a simple simulator and extend it with features to obtain the\narchitecture of a full simulator. We apply refining and constraining techniques\non the specification of the architecture to obtain a specification low enough\nto build an implementation from.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Dec 2007 08:11:37 GMT"
            }
        ],
        "update_date": "2007-12-20",
        "authors_parsed": [
            [
                "Diertens",
                "Bob",
                ""
            ]
        ]
    },
    {
        "id": "0712.3116",
        "submitter": "Wim Vanhoof",
        "authors": "Patricia Hill, Wim Vanhoof",
        "title": "Proceedings of the 17th Workshop on Logic-based methods in Programming\n  Environments (WLPE 2007)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  This volume contains the papers presented at WLPE 2007: the 17th Workshop on\nLogic-based Methods in Programming Environments on 13th September, 2007 in\nPorto, Portugal. It was held as a satellite workshop of ICLP 2007, the 23th\nInternational Conference on Logic Programming.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Dec 2007 08:28:12 GMT"
            }
        ],
        "update_date": "2007-12-20",
        "authors_parsed": [
            [
                "Hill",
                "Patricia",
                ""
            ],
            [
                "Vanhoof",
                "Wim",
                ""
            ]
        ]
    },
    {
        "id": "0712.3116",
        "submitter": "Wim Vanhoof",
        "authors": "Patricia Hill, Wim Vanhoof",
        "title": "Proceedings of the 17th Workshop on Logic-based methods in Programming\n  Environments (WLPE 2007)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": null,
        "abstract": "  This volume contains the papers presented at WLPE 2007: the 17th Workshop on\nLogic-based Methods in Programming Environments on 13th September, 2007 in\nPorto, Portugal. It was held as a satellite workshop of ICLP 2007, the 23th\nInternational Conference on Logic Programming.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Dec 2007 08:28:12 GMT"
            }
        ],
        "update_date": "2007-12-20",
        "authors_parsed": [
            [
                "Hill",
                "Patricia",
                ""
            ],
            [
                "Vanhoof",
                "Wim",
                ""
            ]
        ]
    },
    {
        "id": "0712.3128",
        "submitter": "Bob Diertens",
        "authors": "Bob Diertens",
        "title": "Software (Re-)Engineering with PSF III: an IDE for PSF",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "prg0708",
        "categories": "cs.SE",
        "license": null,
        "abstract": "  We describe the design of an integrated development environment (IDE) for\nPSF. In the software engineering process we used process algebra in the form of\nPSF for the specification of the architecture of the IDE. This specification is\nrefined to a PSF specification of the IDE system as a ToolBus application, by\napplying vertical and horizontal implementation techniques. We implemented the\nvarious tools as specified and connected them with a ToolBus script extracted\nfrom the system specification.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Dec 2007 09:46:23 GMT"
            }
        ],
        "update_date": "2007-12-20",
        "authors_parsed": [
            [
                "Diertens",
                "Bob",
                ""
            ]
        ]
    },
    {
        "id": "0712.3215",
        "submitter": "Francoise Sandoz-Guermond",
        "authors": "Fran\\c{c}oise Sandoz-Guermond (LIESP), Marc-Eric Bobiller-Chaumon\n  (GRePS)",
        "title": "L'accessibilit\\'e des E-services aux personnes non-voyantes :\n  difficult\\'es d'usage et recommandations",
        "comments": "4 pages",
        "journal-ref": "Dans International Conference Proceedings of IHM'2006 - IIHM :\n  Interaction Homme Machine, Montr\\'eal : Canada (2006)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  While taking into account handicapped people in the design of technologies\nrepresents a social and political stake that becomes important (in particular\nwith the recent law on equal rights for all the citizens, March 2004), this\npaper aims at evaluating the level of accessibility of two sites of E-services\nthanks to tests of use and proposing a set of recommendations in order to\nincrease usability for the largest amount of people.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Dec 2007 15:54:07 GMT"
            }
        ],
        "update_date": "2007-12-20",
        "authors_parsed": [
            [
                "Sandoz-Guermond",
                "Fran\u00e7oise",
                "",
                "LIESP"
            ],
            [
                "Bobiller-Chaumon",
                "Marc-Eric",
                "",
                "GRePS"
            ]
        ]
    },
    {
        "id": "0712.3389",
        "submitter": "Georg Hager",
        "authors": "Georg Hager, Holger Stengel, Thomas Zeiser, Gerhard Wellein",
        "title": "RZBENCH: Performance evaluation of current HPC architectures using\n  low-level and application benchmarks",
        "comments": "Contribution to the HLRB/KONWIHR results and review workshop, Dec\n  3rd/4th 2007, LRZ Munich, Germany",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.PF",
        "license": null,
        "abstract": "  RZBENCH is a benchmark suite that was specifically developed to reflect the\nrequirements of scientific supercomputer users at the University of\nErlangen-Nuremberg (FAU). It comprises a number of application and low-level\ncodes under a common build infrastructure that fosters maintainability and\nexpandability. This paper reviews the structure of the suite and briefly\nintroduces the most relevant benchmarks. In addition, some widely known\nstandard benchmark codes are reviewed in order to emphasize the need for a\ncritical review of often-cited performance results. Benchmark data is presented\nfor the HLRB-II at LRZ Munich and a local InfiniBand Woodcrest cluster as well\nas two uncommon system architectures: A bandwidth-optimized InfiniBand cluster\nbased on single socket nodes (\"Port Townsend\") and an early version of Sun's\nhighly threaded T2 architecture (\"Niagara 2\").\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 20 Dec 2007 12:15:26 GMT"
            }
        ],
        "update_date": "2007-12-21",
        "authors_parsed": [
            [
                "Hager",
                "Georg",
                ""
            ],
            [
                "Stengel",
                "Holger",
                ""
            ],
            [
                "Zeiser",
                "Thomas",
                ""
            ],
            [
                "Wellein",
                "Gerhard",
                ""
            ]
        ]
    },
    {
        "id": "0712.3389",
        "submitter": "Georg Hager",
        "authors": "Georg Hager, Holger Stengel, Thomas Zeiser, Gerhard Wellein",
        "title": "RZBENCH: Performance evaluation of current HPC architectures using\n  low-level and application benchmarks",
        "comments": "Contribution to the HLRB/KONWIHR results and review workshop, Dec\n  3rd/4th 2007, LRZ Munich, Germany",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.PF",
        "license": null,
        "abstract": "  RZBENCH is a benchmark suite that was specifically developed to reflect the\nrequirements of scientific supercomputer users at the University of\nErlangen-Nuremberg (FAU). It comprises a number of application and low-level\ncodes under a common build infrastructure that fosters maintainability and\nexpandability. This paper reviews the structure of the suite and briefly\nintroduces the most relevant benchmarks. In addition, some widely known\nstandard benchmark codes are reviewed in order to emphasize the need for a\ncritical review of often-cited performance results. Benchmark data is presented\nfor the HLRB-II at LRZ Munich and a local InfiniBand Woodcrest cluster as well\nas two uncommon system architectures: A bandwidth-optimized InfiniBand cluster\nbased on single socket nodes (\"Port Townsend\") and an early version of Sun's\nhighly threaded T2 architecture (\"Niagara 2\").\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 20 Dec 2007 12:15:26 GMT"
            }
        ],
        "update_date": "2007-12-21",
        "authors_parsed": [
            [
                "Hager",
                "Georg",
                ""
            ],
            [
                "Stengel",
                "Holger",
                ""
            ],
            [
                "Zeiser",
                "Thomas",
                ""
            ],
            [
                "Wellein",
                "Gerhard",
                ""
            ]
        ]
    },
    {
        "id": "0712.3402",
        "submitter": "Francis Bach",
        "authors": "Francis Bach (WILLOW Project - Inria/Ens)",
        "title": "Graph kernels between point clouds",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  Point clouds are sets of points in two or three dimensions. Most kernel\nmethods for learning on sets of points have not yet dealt with the specific\ngeometrical invariances and practical constraints associated with point clouds\nin computer vision and graphics. In this paper, we present extensions of graph\nkernels for point clouds, which allow to use kernel methods for such ob jects\nas shapes, line drawings, or any three-dimensional point clouds. In order to\ndesign rich and numerically efficient kernels with as few free parameters as\npossible, we use kernels between covariance matrices and their factorizations\non graphical models. We derive polynomial time dynamic programming recursions\nand present applications to recognition of handwritten digits and Chinese\ncharacters from few training examples.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 20 Dec 2007 13:06:50 GMT"
            }
        ],
        "update_date": "2007-12-21",
        "authors_parsed": [
            [
                "Bach",
                "Francis",
                "",
                "WILLOW Project - Inria/Ens"
            ]
        ]
    },
    {
        "id": "0712.3423",
        "submitter": "Alban Ponse",
        "authors": "J.A. Bergstra, A. Ponse, M.B. van der Zwaag",
        "title": "Tuplix Calculus",
        "comments": "22 pages",
        "journal-ref": "Scientific Annals of Computer Science, 18:35--61, 2008",
        "doi": null,
        "report-no": "PRG0713",
        "categories": "cs.LO cs.CE",
        "license": null,
        "abstract": "  We introduce a calculus for tuplices, which are expressions that generalize\nmatrices and vectors. Tuplices have an underlying data type for quantities that\nare taken from a zero-totalized field. We start with the core tuplix calculus\nCTC for entries and tests, which are combined using conjunctive composition. We\ndefine a standard model and prove that CTC is relatively complete with respect\nto it. The core calculus is extended with operators for choice, information\nhiding, scalar multiplication, clearing and encapsulation. We provide two\nexamples of applications; one on incremental financial budgeting, and one on\nmodular financial budget design.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 20 Dec 2007 13:58:14 GMT"
            }
        ],
        "update_date": "2009-03-03",
        "authors_parsed": [
            [
                "Bergstra",
                "J. A.",
                ""
            ],
            [
                "Ponse",
                "A.",
                ""
            ],
            [
                "van der Zwaag",
                "M. B.",
                ""
            ]
        ]
    },
    {
        "id": "0712.3433",
        "submitter": "Vadim Zaliva",
        "authors": "Vadim Zaliva",
        "title": "AccelKey Selection Method for Mobile Devices",
        "comments": "15 pages, 12 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  Portable Electronic Devices usually utilize a small screen with limited\nviewing area and a keyboard with a limited number of keys. This makes it\ndifficult to perform quick searches in data arrays containing more than dozen\nitems such an address book or song list. In this article we present a new data\nselection method which allows the user to quickly select an entry from a list\nusing 4-way navigation device such as joystick, trackball or 4-way key pad.\nThis method allows for quick navigation using just one hand, without looking at\nthe screen.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Dec 2007 23:31:55 GMT"
            }
        ],
        "update_date": "2007-12-21",
        "authors_parsed": [
            [
                "Zaliva",
                "Vadim",
                ""
            ]
        ]
    },
    {
        "id": "0712.3587",
        "submitter": "Po-Hsiang Lai",
        "authors": "Po-Hsiang Lai and Joseph A. O'Sullivan",
        "title": "Pattern Recognition System Design with Linear Encoding for Discrete\n  Patterns",
        "comments": "Submitted and accepted to ISIT 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.CV math.IT",
        "license": null,
        "abstract": "  In this paper, designs and analyses of compressive recognition systems are\ndiscussed, and also a method of establishing a dual connection between designs\nof good communication codes and designs of recognition systems is presented.\nPattern recognition systems based on compressed patterns and compressed sensor\nmeasurements can be designed using low-density matrices. We examine truncation\nencoding where a subset of the patterns and measurements are stored perfectly\nwhile the rest is discarded. We also examine the use of LDPC parity check\nmatrices for compressing measurements and patterns. We show how more general\nensembles of good linear codes can be used as the basis for pattern recognition\nsystem design, yielding system design strategies for more general noise models.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 20 Dec 2007 22:46:29 GMT"
            }
        ],
        "update_date": "2007-12-24",
        "authors_parsed": [
            [
                "Lai",
                "Po-Hsiang",
                ""
            ],
            [
                "O'Sullivan",
                "Joseph A.",
                ""
            ]
        ]
    },
    {
        "id": "0712.3617",
        "submitter": "Erhan Bayraktar",
        "authors": "Erhan Bayraktar, Bo Yang",
        "title": "A Unified Framework for Pricing Credit and Equity Derivatives",
        "comments": "Keywords: Credit Default Swap, Defaultable Bond, Defaultable Stock,\n  Equity Options, Stochastic Interest Rate, Implied Volatility, Multiscale\n  Perturbation Method",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a model which can be jointly calibrated to the corporate bond term\nstructure and equity option volatility surface of the same company. Our purpose\nis to obtain explicit bond and equity option pricing formulas that can be\ncalibrated to find a risk neutral model that matches a set of observed market\nprices. This risk neutral model can then be used to price more exotic, illiquid\nor over-the-counter derivatives. We observe that the model implied credit\ndefault swap (CDS) spread matches the market CDS spread and that our model\nproduces a very desirable CDS spread term structure. This is observation is\nworth noticing since without calibrating any parameter to the CDS spread data,\nit is matched by the CDS spread that our model generates using the available\ninformation from the equity options and corporate bond markets. We also observe\nthat our model matches the equity option implied volatility surface well since\nwe properly account for the default risk premium in the implied volatility\nsurface. We demonstrate the importance of accounting for the default risk and\nstochastic interest rate in equity option pricing by comparing our results to\nFouque, Papanicolaou, Sircar and Solna (2003), which only accounts for\nstochastic volatility.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Dec 2007 02:53:38 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 20 Sep 2008 21:44:00 GMT"
            }
        ],
        "update_date": "2008-09-21",
        "authors_parsed": [
            [
                "Bayraktar",
                "Erhan",
                ""
            ],
            [
                "Yang",
                "Bo",
                ""
            ]
        ]
    },
    {
        "id": "0712.3654",
        "submitter": "Alejandro Chinea Manrique De Lara",
        "authors": "Alejandro Chinea Manrique De Lara, Juan Manuel Moreno, Arostegui Jordi\n  Madrenas, Joan Cabestany",
        "title": "Improving the Performance of PieceWise Linear Separation Incremental\n  Algorithms for Practical Hardware Implementations",
        "comments": "10 pages, 1 figure, 3 tables",
        "journal-ref": "Biological and Artificial Computation: From Neuroscience to\n  Technology, J.Mira, R.Moreno-Diaz, J.Cabestany (eds.), pp. 607-616,\n  Springer-Verlag, 1997",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI cs.LG",
        "license": null,
        "abstract": "  In this paper we shall review the common problems associated with Piecewise\nLinear Separation incremental algorithms. This kind of neural models yield poor\nperformances when dealing with some classification problems, due to the\nevolving schemes used to construct the resulting networks. So as to avoid this\nundesirable behavior we shall propose a modification criterion. It is based\nupon the definition of a function which will provide information about the\nquality of the network growth process during the learning phase. This function\nis evaluated periodically as the network structure evolves, and will permit, as\nwe shall show through exhaustive benchmarks, to considerably improve the\nperformance(measured in terms of network complexity and generalization\ncapabilities) offered by the networks generated by these incremental models.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Dec 2007 10:05:52 GMT"
            }
        ],
        "update_date": "2007-12-24",
        "authors_parsed": [
            [
                "De Lara",
                "Alejandro Chinea Manrique",
                ""
            ],
            [
                "Moreno",
                "Juan Manuel",
                ""
            ],
            [
                "Madrenas",
                "Arostegui Jordi",
                ""
            ],
            [
                "Cabestany",
                "Joan",
                ""
            ]
        ]
    },
    {
        "id": "0712.3654",
        "submitter": "Alejandro Chinea Manrique De Lara",
        "authors": "Alejandro Chinea Manrique De Lara, Juan Manuel Moreno, Arostegui Jordi\n  Madrenas, Joan Cabestany",
        "title": "Improving the Performance of PieceWise Linear Separation Incremental\n  Algorithms for Practical Hardware Implementations",
        "comments": "10 pages, 1 figure, 3 tables",
        "journal-ref": "Biological and Artificial Computation: From Neuroscience to\n  Technology, J.Mira, R.Moreno-Diaz, J.Cabestany (eds.), pp. 607-616,\n  Springer-Verlag, 1997",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI cs.LG",
        "license": null,
        "abstract": "  In this paper we shall review the common problems associated with Piecewise\nLinear Separation incremental algorithms. This kind of neural models yield poor\nperformances when dealing with some classification problems, due to the\nevolving schemes used to construct the resulting networks. So as to avoid this\nundesirable behavior we shall propose a modification criterion. It is based\nupon the definition of a function which will provide information about the\nquality of the network growth process during the learning phase. This function\nis evaluated periodically as the network structure evolves, and will permit, as\nwe shall show through exhaustive benchmarks, to considerably improve the\nperformance(measured in terms of network complexity and generalization\ncapabilities) offered by the networks generated by these incremental models.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Dec 2007 10:05:52 GMT"
            }
        ],
        "update_date": "2007-12-24",
        "authors_parsed": [
            [
                "De Lara",
                "Alejandro Chinea Manrique",
                ""
            ],
            [
                "Moreno",
                "Juan Manuel",
                ""
            ],
            [
                "Madrenas",
                "Arostegui Jordi",
                ""
            ],
            [
                "Cabestany",
                "Joan",
                ""
            ]
        ]
    },
    {
        "id": "0712.3807",
        "submitter": "Jianguo Liu",
        "authors": "Jian-Guo Liu, Bing-Hong Wang, Qiang Guo",
        "title": "Improved Collaborative Filtering Algorithm via Information\n  Transformation",
        "comments": "5 pages, 4 figures",
        "journal-ref": "Int. J. Mod. Phys. C 20(2), 285-293 (2009)",
        "doi": "10.1142/S0129183109013613",
        "report-no": null,
        "categories": "cs.LG cs.CY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a spreading activation approach for collaborative\nfiltering (SA-CF). By using the opinion spreading process, the similarity\nbetween any users can be obtained. The algorithm has remarkably higher accuracy\nthan the standard collaborative filtering (CF) using Pearson correlation.\nFurthermore, we introduce a free parameter $\\beta$ to regulate the\ncontributions of objects to user-user correlations. The numerical results\nindicate that decreasing the influence of popular objects can further improve\nthe algorithmic accuracy and personality. We argue that a better algorithm\nshould simultaneously require less computation and generate higher accuracy.\nAccordingly, we further propose an algorithm involving only the top-$N$ similar\nneighbors for each target user, which has both less computational complexity\nand higher algorithmic accuracy.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 26 Dec 2007 14:25:18 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 12 Dec 2008 16:31:13 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 14 Oct 2009 15:30:56 GMT"
            }
        ],
        "update_date": "2015-05-13",
        "authors_parsed": [
            [
                "Liu",
                "Jian-Guo",
                ""
            ],
            [
                "Wang",
                "Bing-Hong",
                ""
            ],
            [
                "Guo",
                "Qiang",
                ""
            ]
        ]
    },
    {
        "id": "0712.3830",
        "submitter": "Tom Schrijvers",
        "authors": "Tom Schrijvers, Bart Demoen, David S. Warren",
        "title": "TCHR: a framework for tabled CLP",
        "comments": "Accepted for publication in Theory and Practice of Logic Programming",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  Tabled Constraint Logic Programming is a powerful execution mechanism for\ndealing with Constraint Logic Programming without worrying about fixpoint\ncomputation. Various applications, e.g in the fields of program analysis and\nmodel checking, have been proposed. Unfortunately, a high-level system for\ndeveloping new applications is lacking, and programmers are forced to resort to\ncomplicated ad hoc solutions.\n  This papers presents TCHR, a high-level framework for tabled Constraint Logic\nProgramming. It integrates in a light-weight manner Constraint Handling Rules\n(CHR), a high-level language for constraint solvers, with tabled Logic\nProgramming. The framework is easily instantiated with new application-specific\nconstraint domains. Various high-level operations can be instantiated to\ncontrol performance. In particular, we propose a novel, generalized technique\nfor compacting answer sets.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 26 Dec 2007 15:28:16 GMT"
            }
        ],
        "update_date": "2007-12-27",
        "authors_parsed": [
            [
                "Schrijvers",
                "Tom",
                ""
            ],
            [
                "Demoen",
                "Bart",
                ""
            ],
            [
                "Warren",
                "David S.",
                ""
            ]
        ]
    },
    {
        "id": "0712.3870",
        "submitter": "Bruce Hajek",
        "authors": "Bruce Hajek",
        "title": "Substitute Valuations: Generation and Structure",
        "comments": "Revision includes more background and explanations",
        "journal-ref": null,
        "doi": "10.1016/j.peva.2008.07.001",
        "report-no": null,
        "categories": "cs.GT cs.PF",
        "license": null,
        "abstract": "  Substitute valuations (in some contexts called gross substitute valuations)\nare prominent in combinatorial auction theory. An algorithm is given in this\npaper for generating a substitute valuation through Monte Carlo simulation. In\naddition, the geometry of the set of all substitute valuations for a fixed\nnumber of goods K is investigated. The set consists of a union of polyhedrons,\nand the maximal polyhedrons are identified for K=4. It is shown that the\nmaximum dimension of the maximal polyhedrons increases with K nearly as fast as\ntwo to the power K. Consequently, under broad conditions, if a combinatorial\nalgorithm can present an arbitrary substitute valuation given a list of input\nnumbers, the list must grow nearly as fast as two to the power K.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 22 Dec 2007 16:52:39 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 13 Mar 2008 16:14:03 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 19 May 2008 14:50:58 GMT"
            }
        ],
        "update_date": "2014-08-15",
        "authors_parsed": [
            [
                "Hajek",
                "Bruce",
                ""
            ]
        ]
    },
    {
        "id": "0712.3925",
        "submitter": "Pascal Heus",
        "authors": "Pascal Heus, Richard Gomez",
        "title": "QIS-XML: A metadata specification for Quantum Information Science",
        "comments": "26 pages, 22 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.DB quant-ph",
        "license": null,
        "abstract": "  While Quantum Information Science (QIS) is still in its infancy, the ability\nfor quantum based hardware or computers to communicate and integrate with their\nclassical counterparts will be a major requirement towards their success.\nLittle attention however has been paid to this aspect of QIS. To manage and\nexchange information between systems, today's classic Information Technology\n(IT) commonly uses the eXtensible Markup Language (XML) and its related tools.\nXML is composed of numerous specifications related to various fields of\nexpertise. No such global specification however has been defined for quantum\ncomputers. QIS-XML is a proposed XML metadata specification for the description\nof fundamental components of QIS (gates & circuits) and a platform for the\ndevelopment of a hardware independent low level pseudo-code for quantum\nalgorithms. This paper lays out the general characteristics of the QIS-XML\nspecification and outlines practical applications through prototype use cases.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 23 Dec 2007 15:24:35 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Heus",
                "Pascal",
                ""
            ],
            [
                "Gomez",
                "Richard",
                ""
            ]
        ]
    },
    {
        "id": "0712.3925",
        "submitter": "Pascal Heus",
        "authors": "Pascal Heus, Richard Gomez",
        "title": "QIS-XML: A metadata specification for Quantum Information Science",
        "comments": "26 pages, 22 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.DB quant-ph",
        "license": null,
        "abstract": "  While Quantum Information Science (QIS) is still in its infancy, the ability\nfor quantum based hardware or computers to communicate and integrate with their\nclassical counterparts will be a major requirement towards their success.\nLittle attention however has been paid to this aspect of QIS. To manage and\nexchange information between systems, today's classic Information Technology\n(IT) commonly uses the eXtensible Markup Language (XML) and its related tools.\nXML is composed of numerous specifications related to various fields of\nexpertise. No such global specification however has been defined for quantum\ncomputers. QIS-XML is a proposed XML metadata specification for the description\nof fundamental components of QIS (gates & circuits) and a platform for the\ndevelopment of a hardware independent low level pseudo-code for quantum\nalgorithms. This paper lays out the general characteristics of the QIS-XML\nspecification and outlines practical applications through prototype use cases.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 23 Dec 2007 15:24:35 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Heus",
                "Pascal",
                ""
            ],
            [
                "Gomez",
                "Richard",
                ""
            ]
        ]
    },
    {
        "id": "0712.3973",
        "submitter": "Marc Schoenauer",
        "authors": "Pierre Collet (LIL), Marc Schoenauer (INRIA Rocquencourt)",
        "title": "GUIDE: Unifying Evolutionary Engines through a Graphical User Interface",
        "comments": null,
        "journal-ref": "Dans Evolution Artificielle 2936 (2003) 203-215",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  Many kinds of Evolutionary Algorithms (EAs) have been described in the\nliterature since the last 30 years. However, though most of them share a common\nstructure, no existing software package allows the user to actually shift from\none model to another by simply changing a few parameters, e.g. in a single\nwindow of a Graphical User Interface. This paper presents GUIDE, a Graphical\nUser Interface for DREAM Experiments that, among other user-friendly features,\nunifies all kinds of EAs into a single panel, as far as evolution parameters\nare concerned. Such a window can be used either to ask for one of the well\nknown ready-to-use algorithms, or to very easily explore new combinations that\nhave not yet been studied. Another advantage of grouping all necessary elements\nto describe virtually all kinds of EAs is that it creates a fantastic pedagogic\ntool to teach EAs to students and newcomers to the field.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 24 Dec 2007 07:31:58 GMT"
            }
        ],
        "update_date": "2011-11-10",
        "authors_parsed": [
            [
                "Collet",
                "Pierre",
                "",
                "LIL"
            ],
            [
                "Schoenauer",
                "Marc",
                "",
                "INRIA Rocquencourt"
            ]
        ]
    },
    {
        "id": "0712.3980",
        "submitter": "Vincent Gramoli",
        "authors": "Antonio Fernandez (LADyR), Vincent Gramoli (INRIA Futurs, IRISA),\n  Ernesto Jimenez (EUI), Anne-Marie Kermarrec (IRISA), Michel Raynal (IRISA)",
        "title": "Distributed Slicing in Dynamic Systems",
        "comments": null,
        "journal-ref": "Dans The 27th International Conference on Distributed Computing\n  Systems (ICDCS'07) (2007) 66",
        "doi": null,
        "report-no": "ICDCS07",
        "categories": "cs.DC",
        "license": null,
        "abstract": "  Peer to peer (P2P) systems are moving from application specific architectures\nto a generic service oriented design philosophy. This raises interesting\nproblems in connection with providing useful P2P middleware services capable of\ndealing with resource assignment and management in a large-scale, heterogeneous\nand unreliable environment. The slicing service, has been proposed to allow for\nan automatic partitioning of P2P networks into groups (slices) that represent a\ncontrollable amount of some resource and that are also relatively homogeneous\nwith respect to that resource. In this paper we propose two gossip-based\nalgorithms to solve the distributed slicing problem. The first algorithm speeds\nup an existing algorithm sorting a set of uniform random numbers. The second\nalgorithm statistically approximates the rank of nodes in the ordering. The\nscalability, efficiency and resilience to dynamics of both algorithms rely on\ntheir gossip-based models. These algorithms are proved viable theoretically and\nexperimentally.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 26 Dec 2007 13:55:47 GMT"
            }
        ],
        "update_date": "2007-12-27",
        "authors_parsed": [
            [
                "Fernandez",
                "Antonio",
                "",
                "LADyR"
            ],
            [
                "Gramoli",
                "Vincent",
                "",
                "INRIA Futurs, IRISA"
            ],
            [
                "Jimenez",
                "Ernesto",
                "",
                "EUI"
            ],
            [
                "Kermarrec",
                "Anne-Marie",
                "",
                "IRISA"
            ],
            [
                "Raynal",
                "Michel",
                "",
                "IRISA"
            ]
        ]
    },
    {
        "id": "0712.4015",
        "submitter": "Jayadev Acharya",
        "authors": "Sreechakra Goparaju, Jayadev Acharya, Ajoy K. Ray, Jaideva C. Goswami",
        "title": "A Fast Hierarchical Multilevel Image Segmentation Method using Unbiased\n  Estimators",
        "comments": "10 pages, 5 figures, submitted to \"IEEE Transactions on Pattern\n  Analysis and Machine Intelligence\"",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  This paper proposes a novel method for segmentation of images by hierarchical\nmultilevel thresholding. The method is global, agglomerative in nature and\ndisregards pixel locations. It involves the optimization of the ratio of the\nunbiased estimators of within class to between class variances. We obtain a\nrecursive relation at each step for the variances which expedites the process.\nThe efficacy of the method is shown in a comparison with some well-known\nmethods.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 24 Dec 2007 17:11:56 GMT"
            }
        ],
        "update_date": "2007-12-27",
        "authors_parsed": [
            [
                "Goparaju",
                "Sreechakra",
                ""
            ],
            [
                "Acharya",
                "Jayadev",
                ""
            ],
            [
                "Ray",
                "Ajoy K.",
                ""
            ],
            [
                "Goswami",
                "Jaideva C.",
                ""
            ]
        ]
    },
    {
        "id": "0712.4046",
        "submitter": "David Harvey",
        "authors": "David Harvey",
        "title": "Faster polynomial multiplication via multipoint Kronecker substitution",
        "comments": "14 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.DS",
        "license": null,
        "abstract": "  We give several new algorithms for dense polynomial multiplication based on\nthe Kronecker substitution method. For moderately sized input polynomials, the\nnew algorithms improve on the performance of the standard Kronecker\nsubstitution by a sizeable constant, both in theory and in empirical tests.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Dec 2007 04:57:04 GMT"
            }
        ],
        "update_date": "2007-12-27",
        "authors_parsed": [
            [
                "Harvey",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0712.4099",
        "submitter": "Gerard Briscoe Mr",
        "authors": "G. Briscoe, P. De Wilde",
        "title": "Digital Ecosystems: Optimisation by a Distributed Intelligence",
        "comments": "12 pages, 14 figures, conference",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Can intelligence optimise Digital Ecosystems? How could a distributed\nintelligence interact with the ecosystem dynamics? Can the software components\nthat are part of genetic selection be intelligent in themselves, as in an\nadaptive technology? We consider the effect of a distributed intelligence\nmechanism on the evolutionary and ecological dynamics of our Digital Ecosystem,\nwhich is the digital counterpart of a biological ecosystem for evolving\nsoftware services in a distributed network. We investigate Neural Networks and\nSupport Vector Machine for the learning based pattern recognition functionality\nof our distributed intelligence. Simulation results imply that the Digital\nEcosystem performs better with the application of a distributed intelligence,\nmarginally more effectively when powered by Support Vector Machine than Neural\nNetworks, and suggest that it can contribute to optimising the operation of our\nDigital Ecosystem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 26 Dec 2007 04:13:20 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 21 Feb 2009 13:38:04 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 21 Sep 2009 00:39:27 GMT"
            }
        ],
        "update_date": "2009-09-21",
        "authors_parsed": [
            [
                "Briscoe",
                "G.",
                ""
            ],
            [
                "De Wilde",
                "P.",
                ""
            ]
        ]
    },
    {
        "id": "0712.4101",
        "submitter": "Gerard Briscoe Mr",
        "authors": "P. De Wilde, G. Briscoe",
        "title": "Digital Ecosystems: Stability of Evolving Agent Populations",
        "comments": "8 pages, 6 figures, ACM Management of Emergent Digital EcoSystems\n  (MEDES) 2009",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stability is perhaps one of the most desirable features of any engineered\nsystem, given the importance of being able to predict its response to various\nenvironmental conditions prior to actual deployment. Engineered systems are\nbecoming ever more complex, approaching the same levels of biological\necosystems, and so their stability becomes ever more important, but taking on\nmore and more differential dynamics can make stability an ever more elusive\nproperty. The Chli-DeWilde definition of stability views a Multi-Agent System\nas a discrete time Markov chain with potentially unknown transition\nprobabilities. With a Multi-Agent System being considered stable when its\nstate, a stochastic process, has converged to an equilibrium distribution,\nbecause stability of a system can be understood intuitively as exhibiting\nbounded behaviour. We investigate an extension to include Multi-Agent Systems\nwith evolutionary dynamics, focusing on the evolving agent populations of our\nDigital Ecosystem. We then built upon this to construct an entropy-based\ndefinition for the degree of instability (entropy of the limit probabilities),\nwhich was later used to perform a stability analysis. The Digital Ecosystem is\nconsidered to investigate the stability of an evolving agent population through\nsimulations, for which the results were consistent with the original\nChli-DeWilde definition.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 26 Dec 2007 04:40:16 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 28 Oct 2008 10:38:12 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 19 Feb 2009 18:33:26 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 20 Feb 2009 02:43:08 GMT"
            },
            {
                "version": "v5",
                "created": "Mon, 5 Oct 2009 00:31:13 GMT"
            }
        ],
        "update_date": "2009-10-05",
        "authors_parsed": [
            [
                "De Wilde",
                "P.",
                ""
            ],
            [
                "Briscoe",
                "G.",
                ""
            ]
        ]
    },
    {
        "id": "0712.4102",
        "submitter": "Gerard Briscoe Mr",
        "authors": "G. Briscoe, P. De Wilde",
        "title": "Digital Ecosystems: Evolving Service-Oriented Architectures",
        "comments": "7 pages, 4 figures, conference",
        "journal-ref": "In IEEE First International Conference on Bio Inspired mOdels of\n  NETwork, Information and Computing Systems (BIONETICS) (2006)",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We view Digital Ecosystems to be the digital counterparts of biological\necosystems, exploiting the self-organising properties of biological ecosystems,\nwhich are considered to be robust, self-organising and scalable architectures\nthat can automatically solve complex, dynamic problems. Digital Ecosystems are\na novel optimisation technique where the optimisation works at two levels: a\nfirst optimisation, migration of agents (representing services) which are\ndistributed in a decentralised peer-to-peer network, operating continuously in\ntime; this process feeds a second optimisation based on evolutionary computing\nthat operates locally on single peers and is aimed at finding solutions to\nsatisfy locally relevant constraints. We created an Ecosystem-Oriented\nArchitecture of Digital Ecosystems by extending Service-Oriented Architectures\nwith distributed evolutionary computing, allowing services to recombine and\nevolve over time, constantly seeking to improve their effectiveness for the\nuser base. Individuals within our Digital Ecosystem will be applications\n(groups of services), created in response to user requests by using\nevolutionary optimisation to aggregate the services. These individuals will\nmigrate through the Digital Ecosystem and adapt to find niches where they are\nuseful in fulfilling other user requests for applications. Simulation results\nimply that the Digital Ecosystem performs better at large scales than a\ncomparable Service-Oriented Architecture, suggesting that incorporating ideas\nfrom theoretical ecology can contribute to useful self-organising properties in\ndigital ecosystems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 26 Dec 2007 05:44:31 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 28 Oct 2008 08:39:15 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 12 Dec 2008 08:59:13 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 13 Mar 2009 02:21:31 GMT"
            },
            {
                "version": "v5",
                "created": "Thu, 26 Mar 2009 08:32:17 GMT"
            },
            {
                "version": "v6",
                "created": "Mon, 5 Oct 2009 01:55:06 GMT"
            }
        ],
        "update_date": "2009-10-05",
        "authors_parsed": [
            [
                "Briscoe",
                "G.",
                ""
            ],
            [
                "De Wilde",
                "P.",
                ""
            ]
        ]
    },
    {
        "id": "0712.4124",
        "submitter": "Michael Singer",
        "authors": "Michael F. Singer",
        "title": "Introduction to the Galois Theory of Linear Differential Equations",
        "comments": "82 pages; some typos corrected",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.CA cs.SC",
        "license": null,
        "abstract": "  This is an expanded version of the 10 lectures given as the 2006 London\nMathematical Society Invited Lecture Series at the Heriot-Watt University 31\nJuly - 4 August 2006.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 26 Dec 2007 02:40:19 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 10 Jan 2008 00:18:53 GMT"
            }
        ],
        "update_date": "2008-01-10",
        "authors_parsed": [
            [
                "Singer",
                "Michael F.",
                ""
            ]
        ]
    },
    {
        "id": "0712.4126",
        "submitter": "Chandan Reddy",
        "authors": "Chandan K. Reddy",
        "title": "TRUST-TECH based Methods for Optimization and Learning",
        "comments": "PHD Thesis",
        "journal-ref": "Chandan K. Reddy, TRUST-TECH based Methods for Optimization and\n  Learning, PHD Thesis, Cornell University, February 2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.CE cs.MS cs.NA cs.NE",
        "license": null,
        "abstract": "  Many problems that arise in machine learning domain deal with nonlinearity\nand quite often demand users to obtain global optimal solutions rather than\nlocal optimal ones. Optimization problems are inherent in machine learning\nalgorithms and hence many methods in machine learning were inherited from the\noptimization literature. Popularly known as the initialization problem, the\nideal set of parameters required will significantly depend on the given\ninitialization values. The recently developed TRUST-TECH (TRansformation Under\nSTability-reTaining Equilibria CHaracterization) methodology systematically\nexplores the subspace of the parameters to obtain a complete set of local\noptimal solutions. In this thesis work, we propose TRUST-TECH based methods for\nsolving several optimization and machine learning problems. Two stages namely,\nthe local stage and the neighborhood-search stage, are repeated alternatively\nin the solution space to achieve improvements in the quality of the solutions.\nOur methods were tested on both synthetic and real datasets and the advantages\nof using this novel framework are clearly manifested. This framework not only\nreduces the sensitivity to initialization, but also allows the flexibility for\nthe practitioners to use various global and local methods that work well for a\nparticular problem of interest. Other hierarchical stochastic algorithms like\nevolutionary algorithms and smoothing algorithms are also studied and\nframeworks for combining these methods with TRUST-TECH have been proposed and\nevaluated on several test systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Dec 2007 03:14:32 GMT"
            }
        ],
        "update_date": "2007-12-27",
        "authors_parsed": [
            [
                "Reddy",
                "Chandan K.",
                ""
            ]
        ]
    },
    {
        "id": "0712.4126",
        "submitter": "Chandan Reddy",
        "authors": "Chandan K. Reddy",
        "title": "TRUST-TECH based Methods for Optimization and Learning",
        "comments": "PHD Thesis",
        "journal-ref": "Chandan K. Reddy, TRUST-TECH based Methods for Optimization and\n  Learning, PHD Thesis, Cornell University, February 2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.CE cs.MS cs.NA cs.NE",
        "license": null,
        "abstract": "  Many problems that arise in machine learning domain deal with nonlinearity\nand quite often demand users to obtain global optimal solutions rather than\nlocal optimal ones. Optimization problems are inherent in machine learning\nalgorithms and hence many methods in machine learning were inherited from the\noptimization literature. Popularly known as the initialization problem, the\nideal set of parameters required will significantly depend on the given\ninitialization values. The recently developed TRUST-TECH (TRansformation Under\nSTability-reTaining Equilibria CHaracterization) methodology systematically\nexplores the subspace of the parameters to obtain a complete set of local\noptimal solutions. In this thesis work, we propose TRUST-TECH based methods for\nsolving several optimization and machine learning problems. Two stages namely,\nthe local stage and the neighborhood-search stage, are repeated alternatively\nin the solution space to achieve improvements in the quality of the solutions.\nOur methods were tested on both synthetic and real datasets and the advantages\nof using this novel framework are clearly manifested. This framework not only\nreduces the sensitivity to initialization, but also allows the flexibility for\nthe practitioners to use various global and local methods that work well for a\nparticular problem of interest. Other hierarchical stochastic algorithms like\nevolutionary algorithms and smoothing algorithms are also studied and\nframeworks for combining these methods with TRUST-TECH have been proposed and\nevaluated on several test systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Dec 2007 03:14:32 GMT"
            }
        ],
        "update_date": "2007-12-27",
        "authors_parsed": [
            [
                "Reddy",
                "Chandan K.",
                ""
            ]
        ]
    },
    {
        "id": "0712.4126",
        "submitter": "Chandan Reddy",
        "authors": "Chandan K. Reddy",
        "title": "TRUST-TECH based Methods for Optimization and Learning",
        "comments": "PHD Thesis",
        "journal-ref": "Chandan K. Reddy, TRUST-TECH based Methods for Optimization and\n  Learning, PHD Thesis, Cornell University, February 2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.CE cs.MS cs.NA cs.NE",
        "license": null,
        "abstract": "  Many problems that arise in machine learning domain deal with nonlinearity\nand quite often demand users to obtain global optimal solutions rather than\nlocal optimal ones. Optimization problems are inherent in machine learning\nalgorithms and hence many methods in machine learning were inherited from the\noptimization literature. Popularly known as the initialization problem, the\nideal set of parameters required will significantly depend on the given\ninitialization values. The recently developed TRUST-TECH (TRansformation Under\nSTability-reTaining Equilibria CHaracterization) methodology systematically\nexplores the subspace of the parameters to obtain a complete set of local\noptimal solutions. In this thesis work, we propose TRUST-TECH based methods for\nsolving several optimization and machine learning problems. Two stages namely,\nthe local stage and the neighborhood-search stage, are repeated alternatively\nin the solution space to achieve improvements in the quality of the solutions.\nOur methods were tested on both synthetic and real datasets and the advantages\nof using this novel framework are clearly manifested. This framework not only\nreduces the sensitivity to initialization, but also allows the flexibility for\nthe practitioners to use various global and local methods that work well for a\nparticular problem of interest. Other hierarchical stochastic algorithms like\nevolutionary algorithms and smoothing algorithms are also studied and\nframeworks for combining these methods with TRUST-TECH have been proposed and\nevaluated on several test systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Dec 2007 03:14:32 GMT"
            }
        ],
        "update_date": "2007-12-27",
        "authors_parsed": [
            [
                "Reddy",
                "Chandan K.",
                ""
            ]
        ]
    },
    {
        "id": "0712.4153",
        "submitter": "Gerard Briscoe Mr",
        "authors": "G. Briscoe, S. Sadedin, G. Paperin",
        "title": "Biology of Applied Digital Ecosystems",
        "comments": "9 pages, 4 figure, conference",
        "journal-ref": "In IEEE First International Conference on Digital Ecosystems and\n  Technologies, 2007",
        "doi": "10.1109/DEST.2007.372015",
        "report-no": null,
        "categories": "cs.NE cs.MA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A primary motivation for our research in Digital Ecosystems is the desire to\nexploit the self-organising properties of biological ecosystems. Ecosystems are\nthought to be robust, scalable architectures that can automatically solve\ncomplex, dynamic problems. However, the biological processes that contribute to\nthese properties have not been made explicit in Digital Ecosystems research.\nHere, we discuss how biological properties contribute to the self-organising\nfeatures of biological ecosystems, including population dynamics, evolution, a\ncomplex dynamic environment, and spatial distributions for generating local\ninteractions. The potential for exploiting these properties in artificial\nsystems is then considered. We suggest that several key features of biological\necosystems have not been fully explored in existing digital ecosystems, and\ndiscuss how mimicking these features may assist in developing robust, scalable\nself-organising architectures. An example architecture, the Digital Ecosystem,\nis considered in detail. The Digital Ecosystem is then measured experimentally\nthrough simulations, with measures originating from theoretical ecology, to\nconfirm its likeness to a biological ecosystem. Including the responsiveness to\nrequests for applications from the user base, as a measure of the 'ecological\nsuccession' (development).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 26 Dec 2007 21:56:52 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 28 Oct 2008 09:19:00 GMT"
            }
        ],
        "update_date": "2016-11-18",
        "authors_parsed": [
            [
                "Briscoe",
                "G.",
                ""
            ],
            [
                "Sadedin",
                "S.",
                ""
            ],
            [
                "Paperin",
                "G.",
                ""
            ]
        ]
    },
    {
        "id": "0712.4159",
        "submitter": "Gerard Briscoe Dr",
        "authors": "G Briscoe",
        "title": "Creating a Digital Ecosystem: Service-Oriented Architectures with\n  Distributed Evolutionary Computing",
        "comments": "This has been withdrawn by the author due to an error in using\n  presentation notes in submission",
        "journal-ref": "In JavaOne Conference, 2006, BOF-0759",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We start with a discussion of the relevant literature, including Nature\nInspired Computing as a framework in which to understand this work, and the\nprocess of biomimicry to be used in mimicking the necessary biological\nprocesses to create Digital Ecosystems. We then consider the relevant\ntheoretical ecology in creating the digital counterpart of a biological\necosystem, including the topological structure of ecosystems, and evolutionary\nprocesses within distributed environments. This leads to a discussion of the\nrelevant fields from computer science for the creation of Digital Ecosystems,\nincluding evolutionary computing, Multi-Agent Systems, and Service-Oriented\nArchitectures. We then define Ecosystem-Oriented Architectures for the creation\nof Digital Ecosystems, imbibed with the properties of self-organisation and\nscalability from biological ecosystems, including a novel form of distributed\nevolutionary computing.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 26 Dec 2007 23:32:10 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 28 Oct 2008 08:56:59 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 5 Oct 2009 00:41:53 GMT"
            },
            {
                "version": "v4",
                "created": "Sat, 24 Nov 2012 01:18:27 GMT"
            },
            {
                "version": "v5",
                "created": "Tue, 27 Nov 2012 15:39:35 GMT"
            }
        ],
        "update_date": "2012-11-28",
        "authors_parsed": [
            [
                "Briscoe",
                "G",
                ""
            ]
        ]
    },
    {
        "id": "0712.4183",
        "submitter": "Daoshun Wang",
        "authors": "Dao-Shun Wang, Feng Yi and Xiaobo Li",
        "title": "Probabilistic Visual Secret Sharing Schemes for Gray-scale images and\n  Color images",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.CV",
        "license": null,
        "abstract": "  Visual secrete sharing (VSS) is an encryption technique that utilizes human\nvisual system in the recovering of the secret image and it does not require any\ncomplex calculation. Pixel expansion has been a major issue of VSS schemes. A\nnumber of probabilistic VSS schemes with minimum pixel expansion have been\nproposed for binary secret images. This paper presents a general probabilistic\n(k, n)-VSS scheme for gray-scale images and another scheme for color images.\nWith our schemes, the pixel expansion can be set to a user-defined value. When\nthis value is 1, there is no pixel expansion at all. The quality of\nreconstructed secret images, measured by Average Relative Difference, is\nequivalent to Relative Difference of existing deterministic schemes. Previous\nprobabilistic VSS schemes for black-and-white images with respect to pixel\nexpansion can be viewed as special cases of the schemes proposed here\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 27 Dec 2007 03:27:10 GMT"
            }
        ],
        "update_date": "2007-12-28",
        "authors_parsed": [
            [
                "Wang",
                "Dao-Shun",
                ""
            ],
            [
                "Yi",
                "Feng",
                ""
            ],
            [
                "Li",
                "Xiaobo",
                ""
            ]
        ]
    },
    {
        "id": "0712.4248",
        "submitter": "Reinhard Laubenbacher",
        "authors": "Reinhard Laubenbacher and Bernd Sturmfels",
        "title": "Computer algebra in systems biology",
        "comments": "to appear in American Mathematical Monthly",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC q-bio.MN q-bio.QM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Systems biology focuses on the study of entire biological systems rather than\non their individual components. With the emergence of high-throughput data\ngeneration technologies for molecular biology and the development of advanced\nmathematical modeling techniques, this field promises to provide important new\ninsights. At the same time, with the availability of increasingly powerful\ncomputers, computer algebra has developed into a useful tool for many\napplications. This article illustrates the use of computer algebra in systems\nbiology by way of a well-known gene regulatory network, the Lac Operon in the\nbacterium E. coli.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 27 Dec 2007 16:01:35 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 19 Dec 2008 02:53:23 GMT"
            }
        ],
        "update_date": "2008-12-19",
        "authors_parsed": [
            [
                "Laubenbacher",
                "Reinhard",
                ""
            ],
            [
                "Sturmfels",
                "Bernd",
                ""
            ]
        ]
    },
    {
        "id": "0712.4273",
        "submitter": "Olivier Cappe",
        "authors": "Olivier Capp\\'e (LTCI), Eric Moulines (LTCI)",
        "title": "Online EM Algorithm for Latent Data Models",
        "comments": "Version that includes the corrigendum published in volume 73, part 5\n  (2011), of the Journal of the Royal Statistical Society, Series B + the\n  correction of a typo in Eqs. (32-33)",
        "journal-ref": "Journal of the Royal Statistical Society: Series B, Royal\n  Statistical Society, 2009, 71 (3), pp.593-613",
        "doi": "10.1111/j.1467-9868.2009.00698.x",
        "report-no": null,
        "categories": "stat.CO cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this contribution, we propose a generic online (also sometimes called\nadaptive or recursive) version of the Expectation-Maximisation (EM) algorithm\napplicable to latent variable models of independent observations. Compared to\nthe algorithm of Titterington (1984), this approach is more directly connected\nto the usual EM algorithm and does not rely on integration with respect to the\ncomplete data distribution. The resulting algorithm is usually simpler and is\nshown to achieve convergence to the stationary points of the Kullback-Leibler\ndivergence between the marginal distribution of the observation and the model\ndistribution at the optimal rate, i.e., that of the maximum likelihood\nestimator. In addition, the proposed approach is also suitable for conditional\n(or regression) models, as illustrated in the case of the mixture of linear\nregressions model.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 27 Dec 2007 19:44:34 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 4 Sep 2008 14:36:55 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 2 Dec 2011 14:59:41 GMT"
            },
            {
                "version": "v4",
                "created": "Wed, 1 Mar 2017 13:40:32 GMT"
            }
        ],
        "update_date": "2017-03-02",
        "authors_parsed": [
            [
                "Capp\u00e9",
                "Olivier",
                "",
                "LTCI"
            ],
            [
                "Moulines",
                "Eric",
                "",
                "LTCI"
            ]
        ]
    },
    {
        "id": "0801.0131",
        "submitter": "Alexandr Savinov",
        "authors": "Alexandr Savinov",
        "title": "Two-Level Concept-Oriented Data Model",
        "comments": null,
        "journal-ref": "Institute of Mathematics and Computer Science, Academy of Sciences\n  of Moldova, Technical Report RT0006, 2007",
        "doi": null,
        "report-no": "Technical Report RT0006",
        "categories": "cs.DB",
        "license": null,
        "abstract": "  In this paper we describe a new approach to data modelling called the\nconcept-oriented model (CoM). This model is based on the formalism of nested\nordered sets which uses inclusion relation to produce hierarchical structure of\nsets and ordering relation to produce multi-dimensional structure among its\nelements. Nested ordered set is defined as an ordered set where an each element\ncan be itself an ordered set. Ordering relation in CoM is used to define data\nsemantics and operations with data such as projection and de-projection. This\ndata model can be applied to very different problems and the paper describes\nsome its uses such grouping with aggregation and multi-dimensional analysis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 30 Dec 2007 14:29:17 GMT"
            }
        ],
        "update_date": "2008-01-03",
        "authors_parsed": [
            [
                "Savinov",
                "Alexandr",
                ""
            ]
        ]
    },
    {
        "id": "0801.0133",
        "submitter": "Alexandr Savinov",
        "authors": "Alexandr Savinov",
        "title": "An Approach to Programming Based on Concepts",
        "comments": "49 pages. Related papers: http://conceptoriented.com",
        "journal-ref": "Institute of Mathematics and Computer Science, Academy of Sciences\n  of Moldova, Technical Report RT0005, 2007",
        "doi": null,
        "report-no": "Technical Report RT0005",
        "categories": "cs.PL",
        "license": null,
        "abstract": "  In this paper we describe a new approach to programming which generalizes\nobject-oriented programming. It is based on using a new programming construct,\ncalled concept, which generalizes classes. Concept is defined as a pair of two\nclasses: one reference class and one object class. Each concept has a parent\nconcept which is specified using inclusion relation generalizing inheritance.\nWe describe several important mechanisms such as reference resolution, context\nstack, dual methods and life-cycle management, inheritance and polymorphism.\nThis approach to programming is positioned as a new programming paradigm and\ntherefore we formulate its main principles and rules.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 30 Dec 2007 14:43:27 GMT"
            }
        ],
        "update_date": "2008-01-03",
        "authors_parsed": [
            [
                "Savinov",
                "Alexandr",
                ""
            ]
        ]
    },
    {
        "id": "0801.0135",
        "submitter": "Alexandr Savinov",
        "authors": "Alexandr Savinov",
        "title": "Concepts and their Use for Modelling Objects and References in\n  Programming Languages",
        "comments": "43 pages. Related papers: http://conceptoriented.com/",
        "journal-ref": "Institute of Mathematics and Computer Science, Academy of Sciences\n  of Moldova, Technical Report RT0004, 2007",
        "doi": null,
        "report-no": "Technical Report RT0004",
        "categories": "cs.PL",
        "license": null,
        "abstract": "  In the paper a new programming construct, called concept, is introduced.\nConcept is pair of two classes: a reference class and an object class.\nInstances of the reference classes are passed-by-value and are intended to\nrepresent objects. Instances of the object class are passed-by-reference. An\napproach to programming where concepts are used instead of classes is called\nconcept-oriented programming (CoP). In CoP objects are represented and accessed\nindirectly by means of references. The structure of concepts describes a\nhierarchical space with a virtual address system. The paper describes this new\napproach to programming including such mechanisms as reference resolution,\ncomplex references, method interception, dual methods, life-cycle management\ninheritance and polymorphism.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 30 Dec 2007 14:50:01 GMT"
            }
        ],
        "update_date": "2008-01-03",
        "authors_parsed": [
            [
                "Savinov",
                "Alexandr",
                ""
            ]
        ]
    },
    {
        "id": "0801.0136",
        "submitter": "Alexandr Savinov",
        "authors": "Alexandr Savinov",
        "title": "Indirect Object Representation and Access by Means of Concepts",
        "comments": "8 pages. Related papers: http://conceptoriented.com/",
        "journal-ref": "Institute of Mathematics and Computer Science, Academy of Sciences\n  of Moldova, Technical Report, 2006",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  The paper describes a mechanism for indirect object representation and access\n(ORA) in programming languages. The mechanism is based on using a new\nprogramming construct which is referred to as concept. Concept consists of one\nobject class and one reference class both having their fields and methods. The\nobject class is the conventional class as defined in OOP with instances passed\nby reference. Instances of the reference class are passed by value and are\nintended to represent objects. The reference classes are used to describe how\nobjects have to be represented and accessed by providing custom format for\ntheir identifiers and custom access procedures. Such an approach to programming\nwhere concepts are used instead of classes is referred to as concept-oriented\nprogramming. It generalizes OOP and its main advantage is that it allows the\nprogrammer to describe not only the functionality of target objects but also\nintermediate functions which are executed behind the scenes as an object is\nbeing accessed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 30 Dec 2007 14:56:05 GMT"
            }
        ],
        "update_date": "2008-01-03",
        "authors_parsed": [
            [
                "Savinov",
                "Alexandr",
                ""
            ]
        ]
    },
    {
        "id": "0801.0139",
        "submitter": "Alexandr Savinov",
        "authors": "Alexandr Savinov",
        "title": "Principles of the Concept-Oriented Data Model",
        "comments": "54 pages. Related papers: http://conceptoriented.com/",
        "journal-ref": "Institute of Mathematics and Computer Science, Academy of Sciences\n  of Moldova, Technical Report, 2004",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": null,
        "abstract": "  In the paper a new approach to data representation and manipulation is\ndescribed, which is called the concept-oriented data model (CODM). It is\nsupposed that items represent data units, which are stored in concepts. A\nconcept is a combination of superconcepts, which determine the concept's\ndimensionality or properties. An item is a combination of superitems taken by\none from all the superconcepts. An item stores a combination of references to\nits superitems. The references implement inclusion relation or attribute-value\nrelation among items. A concept-oriented database is defined by its concept\nstructure called syntax or schema and its item structure called semantics. The\nmodel defines formal transformations of syntax and semantics including the\ncanonical semantics where all concepts are merged and the data semantics is\nrepresented by one set of items. The concept-oriented data model treats\nrelations as subconcepts where items are instances of the relations.\nMulti-valued attributes are defined via subconcepts as a view on the database\nsemantics rather than as a built-in mechanism. The model includes\nconcept-oriented query language, which is based on collection manipulations. It\nalso has such mechanisms as aggregation and inference based on semantics\npropagation through the database schema.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 30 Dec 2007 15:04:25 GMT"
            }
        ],
        "update_date": "2008-01-03",
        "authors_parsed": [
            [
                "Savinov",
                "Alexandr",
                ""
            ]
        ]
    },
    {
        "id": "0801.0390",
        "submitter": "Richard Nock",
        "authors": "Richard Nock, Nicolas Sanz, Fred Celimene, Frank Nielsen",
        "title": "Staring at Economic Aggregators through Information Lenses",
        "comments": "18 pages, 2 tables, 3 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.LG math.IT math.OC",
        "license": null,
        "abstract": "  It is hard to exaggerate the role of economic aggregators -- functions that\nsummarize numerous and / or heterogeneous data -- in economic models since the\nearly XX$^{th}$ century. In many cases, as witnessed by the pioneering works of\nCobb and Douglas, these functions were information quantities tailored to\neconomic theories, i.e. they were built to fit economic phenomena. In this\npaper, we look at these functions from the complementary side: information. We\nuse a recent toolbox built on top of a vast class of distortions coined by\nBregman, whose application field rivals metrics' in various subfields of\nmathematics. This toolbox makes it possible to find the quality of an\naggregator (for consumptions, prices, labor, capital, wages, etc.), from the\nstandpoint of the information it carries. We prove a rather striking result.\n  From the informational standpoint, well-known economic aggregators do belong\nto the \\textit{optimal} set. As common economic assumptions enter the analysis,\nthis large set shrinks, and it essentially ends up \\textit{exactly fitting}\neither CES, or Cobb-Douglas, or both. To summarize, in the relevant economic\ncontexts, one could not have crafted better some aggregator from the\ninformation standpoint. We also discuss global economic behaviors of optimal\ninformation aggregators in general, and present a brief panorama of the links\nbetween economic and information aggregators.\n  Keywords: Economic Aggregators, CES, Cobb-Douglas, Bregman divergences\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 Jan 2008 13:23:04 GMT"
            }
        ],
        "update_date": "2008-01-03",
        "authors_parsed": [
            [
                "Nock",
                "Richard",
                ""
            ],
            [
                "Sanz",
                "Nicolas",
                ""
            ],
            [
                "Celimene",
                "Fred",
                ""
            ],
            [
                "Nielsen",
                "Frank",
                ""
            ]
        ]
    },
    {
        "id": "0801.0455",
        "submitter": "Jorg Liebeherr",
        "authors": "Jorg Liebeherr, Markus Fidler, Shahrokh Valaee",
        "title": "A System Theoretic Approach to Bandwidth Estimation",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": null,
        "abstract": "  It is shown that bandwidth estimation in packet networks can be viewed in\nterms of min-plus linear system theory. The available bandwidth of a link or\ncomplete path is expressed in terms of a {\\em service curve}, which is a\nfunction that appears in the network calculus to express the service available\nto a traffic flow. The service curve is estimated based on measurements of a\nsequence of probing packets or passive measurements of a sample path of\narrivals. It is shown that existing bandwidth estimation methods can be derived\nin the min-plus algebra of the network calculus, thus providing further\nmathematical justification for these methods. Principal difficulties of\nestimating available bandwidth from measurement of network probes are related\nto potential non-linearities of the underlying network. When networks are\nviewed as systems that operate either in a linear or in a non-linear regime, it\nis argued that probing schemes extract the most information at a point when the\nnetwork crosses from a linear to a non-linear regime. Experiments on the Emulab\ntestbed at the University of Utah evaluate the robustness of the system\ntheoretic interpretation of networks in practice. Multi-node experiments\nevaluate how well the convolution operation of the min-plus algebra provides\nestimates for the available bandwidth of a path from estimates of individual\nlinks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 Jan 2008 00:11:26 GMT"
            }
        ],
        "update_date": "2008-01-04",
        "authors_parsed": [
            [
                "Liebeherr",
                "Jorg",
                ""
            ],
            [
                "Fidler",
                "Markus",
                ""
            ],
            [
                "Valaee",
                "Shahrokh",
                ""
            ]
        ]
    },
    {
        "id": "0801.0523",
        "submitter": "Florent De Dinechin",
        "authors": "Florent De Dinechin (LIP), Christoph Quirin Lauter (LIP), Guillaume\n  Melquiond (LIP)",
        "title": "Certifying floating-point implementations using Gappa",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NA cs.MS",
        "license": null,
        "abstract": "  High confidence in floating-point programs requires proving numerical\nproperties of final and intermediate values. One may need to guarantee that a\nvalue stays within some range, or that the error relative to some ideal value\nis well bounded. Such work may require several lines of proof for each line of\ncode, and will usually be broken by the smallest change to the code (e.g. for\nmaintenance or optimization purpose). Certifying these programs by hand is\ntherefore very tedious and error-prone. This article discusses the use of the\nGappa proof assistant in this context. Gappa has two main advantages over\nprevious approaches: Its input format is very close to the actual C code to\nvalidate, and it automates error evaluation and propagation using interval\narithmetic. Besides, it can be used to incrementally prove complex mathematical\nproperties pertaining to the C code. Yet it does not require any specific\nknowledge about automatic theorem proving, and thus is accessible to a wide\ncommunity. Moreover, Gappa may generate a formal proof of the results that can\nbe checked independently by a lower-level proof assistant like Coq, hence\nproviding an even higher confidence in the certification of the numerical code.\nThe article demonstrates the use of this tool on a real-size example, an\nelementary function with correctly rounded output.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 Jan 2008 13:34:03 GMT"
            }
        ],
        "update_date": "2008-01-04",
        "authors_parsed": [
            [
                "De Dinechin",
                "Florent",
                "",
                "LIP"
            ],
            [
                "Lauter",
                "Christoph Quirin",
                "",
                "LIP"
            ],
            [
                "Melquiond",
                "Guillaume",
                "",
                "LIP"
            ]
        ]
    },
    {
        "id": "0801.0586",
        "submitter": "Daniel Perrucci",
        "authors": "Gabriela Jeronimo, Daniel Perrucci, Juan Sabia",
        "title": "On sign conditions over real multivariate polynomials",
        "comments": "extended version",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.AG cs.CG cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new probabilistic algorithm to find a finite set of points\nintersecting the closure of each connected component of the realization of\nevery sign condition over a family of real polynomials defining regular\nhypersurfaces that intersect transversally. This enables us to show a\nprobabilistic procedure to list all feasible sign conditions over the\npolynomials. In addition, we extend these results to the case of closed sign\nconditions over an arbitrary family of real multivariate polynomials. The\ncomplexity bounds for these procedures improve the known ones.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 Jan 2008 20:03:05 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 18 Dec 2008 19:58:32 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Jeronimo",
                "Gabriela",
                ""
            ],
            [
                "Perrucci",
                "Daniel",
                ""
            ],
            [
                "Sabia",
                "Juan",
                ""
            ]
        ]
    },
    {
        "id": "0801.0609",
        "submitter": "Grenville Croll",
        "authors": "Raymond J. Butler",
        "title": "Applying the CobiT Control Framework to Spreadsheet Developments",
        "comments": "6 Pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2001 7-13 ISBN:1 86166\n  179 7",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.CY",
        "license": null,
        "abstract": "  One of the problems reported by researchers and auditors in the field of\nspreadsheet risks is that of getting and keeping managements attention to the\nproblem. Since 1996, the Information Systems Audit & Control Foundation and the\nIT Governance Institute have published CobiT which brings mainstream IT control\nissues into the corporate governance arena. This paper illustrates how\nspreadsheet risk and control issues can be mapped onto the CobiT framework and\nthus brought to managers attention in a familiar format.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 Jan 2008 22:09:15 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Butler",
                "Raymond J.",
                ""
            ]
        ]
    },
    {
        "id": "0801.0678",
        "submitter": "Joel Chevrier",
        "authors": "Sylvain Marli\\`ere (ICA), Jean Loup Florens (ICA), Florence Marchi\n  (ESRF, NEEL), Annie Luciani (ICA), Joel Chevrier (ESRF, NEEL)",
        "title": "Implementation of perception and action at nanoscale",
        "comments": "Proceedings of ENACTIVE/07 4th International Conference on Enactive\n  Interfaces Grenoble, France, November 19th-22nd, 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.RO cs.HC",
        "license": null,
        "abstract": "  Real time combination of nanosensors and nanoactuators with virtual reality\nenvironment and multisensorial interfaces enable us to efficiently act and\nperceive at nanoscale. Advanced manipulation of nanoobjects and new strategies\nfor scientific education are the key motivations. We have no existing intuitive\nrepresentation of the nanoworld ruled by laws foreign to our experience. A\ncentral challenge is then the construction of nanoworld simulacrum that we can\nstart to visit and to explore. In this nanoworld simulacrum, object\nidentifications will be based on probed entity physical and chemical intrinsic\nproperties, on their interactions with sensors and on the final choices made in\nbuilding a multisensorial interface so that these objects become coherent\nelements of the human sphere of action and perception. Here we describe a 1D\nvirtual nanomanipulator, part of the Cit\\'e des Sciences EXPO NANO in Paris,\nthat is the first realization based on this program.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 4 Jan 2008 13:38:39 GMT"
            }
        ],
        "update_date": "2008-01-07",
        "authors_parsed": [
            [
                "Marli\u00e8re",
                "Sylvain",
                "",
                "ICA"
            ],
            [
                "Florens",
                "Jean Loup",
                "",
                "ICA"
            ],
            [
                "Marchi",
                "Florence",
                "",
                "ESRF, NEEL"
            ],
            [
                "Luciani",
                "Annie",
                "",
                "ICA"
            ],
            [
                "Chevrier",
                "Joel",
                "",
                "ESRF, NEEL"
            ]
        ]
    },
    {
        "id": "0801.0714",
        "submitter": "James Cheney",
        "authors": "James Cheney",
        "title": "Regular Expression Subtyping for XML Query and Update Languages",
        "comments": "ESOP 2008. Companion technical report with proofs",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.DB",
        "license": null,
        "abstract": "  XML database query languages such as XQuery employ regular expression types\nwith structural subtyping. Subtyping systems typically have two presentations,\nwhich should be equivalent: a declarative version in which the subsumption rule\nmay be used anywhere, and an algorithmic version in which the use of\nsubsumption is limited in order to make typechecking syntax-directed and\ndecidable. However, the XQuery standard type system circumvents this issue by\nusing imprecise typing rules for iteration constructs and defining only\nalgorithmic typechecking, and another extant proposal provides more precise\ntypes for iteration constructs but ignores subtyping. In this paper, we\nconsider a core XQuery-like language with a subsumption rule and prove the\ncompleteness of algorithmic typechecking; this is straightforward for XQuery\nproper but requires some care in the presence of more precise iteration typing\ndisciplines. We extend this result to an XML update language we have introduced\nin earlier work.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 4 Jan 2008 18:13:48 GMT"
            }
        ],
        "update_date": "2008-01-07",
        "authors_parsed": [
            [
                "Cheney",
                "James",
                ""
            ]
        ]
    },
    {
        "id": "0801.0714",
        "submitter": "James Cheney",
        "authors": "James Cheney",
        "title": "Regular Expression Subtyping for XML Query and Update Languages",
        "comments": "ESOP 2008. Companion technical report with proofs",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.DB",
        "license": null,
        "abstract": "  XML database query languages such as XQuery employ regular expression types\nwith structural subtyping. Subtyping systems typically have two presentations,\nwhich should be equivalent: a declarative version in which the subsumption rule\nmay be used anywhere, and an algorithmic version in which the use of\nsubsumption is limited in order to make typechecking syntax-directed and\ndecidable. However, the XQuery standard type system circumvents this issue by\nusing imprecise typing rules for iteration constructs and defining only\nalgorithmic typechecking, and another extant proposal provides more precise\ntypes for iteration constructs but ignores subtyping. In this paper, we\nconsider a core XQuery-like language with a subsumption rule and prove the\ncompleteness of algorithmic typechecking; this is straightforward for XQuery\nproper but requires some care in the presence of more precise iteration typing\ndisciplines. We extend this result to an XML update language we have introduced\nin earlier work.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 4 Jan 2008 18:13:48 GMT"
            }
        ],
        "update_date": "2008-01-07",
        "authors_parsed": [
            [
                "Cheney",
                "James",
                ""
            ]
        ]
    },
    {
        "id": "0801.0830",
        "submitter": "Atilim Gunes Baydin",
        "authors": "Atilim Gunes Baydin",
        "title": "Evolution of central pattern generators for the control of a five-link\n  bipedal walking mechanism",
        "comments": "11 pages, 9 figures; substantial revision of content, organization,\n  and quantitative results",
        "journal-ref": "Paladyn. Journal of Behavioral Robotics 3(1), 45-53 (2012)",
        "doi": "10.2478/s13230-012-0019-y",
        "report-no": null,
        "categories": "cs.NE cs.RO",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Central pattern generators (CPGs), with a basis is neurophysiological\nstudies, are a type of neural network for the generation of rhythmic motion.\nWhile CPGs are being increasingly used in robot control, most applications are\nhand-tuned for a specific task and it is acknowledged in the field that generic\nmethods and design principles for creating individual networks for a given task\nare lacking. This study presents an approach where the connectivity and\noscillatory parameters of a CPG network are determined by an evolutionary\nalgorithm with fitness evaluations in a realistic simulation with accurate\nphysics. We apply this technique to a five-link planar walking mechanism to\ndemonstrate its feasibility and performance. In addition, to see whether\nresults from simulation can be acceptably transferred to real robot hardware,\nthe best evolved CPG network is also tested on a real mechanism. Our results\nalso confirm that the biologically inspired CPG model is well suited for legged\nlocomotion, since a diverse manifestation of networks have been observed to\nsucceed in fitness simulations during evolution.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 6 Jan 2008 00:20:25 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 20 Feb 2008 19:28:42 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 6 Mar 2008 17:37:29 GMT"
            },
            {
                "version": "v4",
                "created": "Sat, 21 Nov 2009 00:04:19 GMT"
            },
            {
                "version": "v5",
                "created": "Sat, 30 Jul 2011 02:01:05 GMT"
            },
            {
                "version": "v6",
                "created": "Mon, 10 Oct 2011 17:37:47 GMT"
            },
            {
                "version": "v7",
                "created": "Tue, 11 Oct 2011 02:42:44 GMT"
            },
            {
                "version": "v8",
                "created": "Wed, 12 Oct 2011 00:42:10 GMT"
            },
            {
                "version": "v9",
                "created": "Thu, 29 Mar 2012 02:02:45 GMT"
            }
        ],
        "update_date": "2015-03-13",
        "authors_parsed": [
            [
                "Baydin",
                "Atilim Gunes",
                ""
            ]
        ]
    },
    {
        "id": "0801.0882",
        "submitter": "Nina Bohr",
        "authors": "Neil D. Jones and Nina Bohr",
        "title": "Call-by-value Termination in the Untyped lambda-calculus",
        "comments": null,
        "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 1 (March 17,\n  2008) lmcs:915",
        "doi": "10.2168/LMCS-4(1:3)2008",
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  A fully-automated algorithm is developed able to show that evaluation of a\ngiven untyped lambda-expression will terminate under CBV (call-by-value). The\n``size-change principle'' from first-order programs is extended to arbitrary\nuntyped lambda-expressions in two steps. The first step suffices to show CBV\ntermination of a single, stand-alone lambda;-expression. The second suffices to\nshow CBV termination of any member of a regular set of lambda-expressions,\ndefined by a tree grammar. (A simple example is a minimum function, when\napplied to arbitrary Church numerals.) The algorithm is sound and proven so in\nthis paper. The Halting Problem's undecidability implies that any sound\nalgorithm is necessarily incomplete: some lambda-expressions may in fact\nterminate under CBV evaluation, but not be recognised as terminating.\n  The intensional power of the termination algorithm is reasonably high. It\ncertifies as terminating many interesting and useful general recursive\nalgorithms including programs with mutual recursion and parameter exchanges,\nand Colson's ``minimum'' algorithm. Further, our type-free approach allows use\nof the Y combinator, and so can identify as terminating a substantial subset of\nPCF.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 6 Jan 2008 19:01:02 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 17 Mar 2008 12:55:44 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Jones",
                "Neil D.",
                ""
            ],
            [
                "Bohr",
                "Nina",
                ""
            ]
        ]
    },
    {
        "id": "0801.1033",
        "submitter": "George Tsibidis",
        "authors": "George Tsibidis, Theodoros N. Arvanitis and Chris Baber",
        "title": "The What, Who, Where, When, Why and How of Context-Awareness",
        "comments": "Accepted manuscript at the CHI 2000, April 3-2000, The Hague, The\n  Netherlands",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  The understanding of context and context-awareness is very important for the\nareas of handheld and ubiquitous computing. Unfortunately, at present, there\nhas not been a satisfactory definition of these two concepts that would lead to\na more effective communication in humancomputer interaction. As a result, on\nthe one hand, application designers are not able to choose what context to use\nin their applications and on the other, they cannot determine the type of\ncontext-awareness behaviours their applications should exhibit. In this work,\nwe aim to provide answers to some fundamental questions that could enlighten us\non the definition of context and its functionality.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 Jan 2008 16:05:46 GMT"
            }
        ],
        "update_date": "2008-01-08",
        "authors_parsed": [
            [
                "Tsibidis",
                "George",
                ""
            ],
            [
                "Arvanitis",
                "Theodoros N.",
                ""
            ],
            [
                "Baber",
                "Chris",
                ""
            ]
        ]
    },
    {
        "id": "0801.1063",
        "submitter": "Ivan Titov",
        "authors": "Ivan Titov and Ryan McDonald",
        "title": "Modeling Online Reviews with Multi-grain Topic Models",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IR cs.DB",
        "license": null,
        "abstract": "  In this paper we present a novel framework for extracting the ratable aspects\nof objects from online user reviews. Extracting such aspects is an important\nchallenge in automatically mining product opinions from the web and in\ngenerating opinion-based summaries of user reviews. Our models are based on\nextensions to standard topic modeling methods such as LDA and PLSA to induce\nmulti-grain topics. We argue that multi-grain models are more appropriate for\nour task since standard models tend to produce topics that correspond to global\nproperties of objects (e.g., the brand of a product type) rather than the\naspects of an object that tend to be rated by a user. The models we present not\nonly extract ratable aspects, but also cluster them into coherent topics, e.g.,\n`waitress' and `bartender' are part of the same topic `staff' for restaurants.\nThis differentiates it from much of the previous work which extracts aspects\nthrough term frequency analysis with minimal clustering. We evaluate the\nmulti-grain models both qualitatively and quantitatively to show that they\nimprove significantly upon standard topic models.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 Jan 2008 17:01:34 GMT"
            }
        ],
        "update_date": "2008-01-08",
        "authors_parsed": [
            [
                "Titov",
                "Ivan",
                ""
            ],
            [
                "McDonald",
                "Ryan",
                ""
            ]
        ]
    },
    {
        "id": "0801.1219",
        "submitter": "Andrey Breslav",
        "authors": "Andrey Breslav",
        "title": "DSL development based on target meta-models. Using AST transformations\n  for automating semantic analysis in a textual DSL framework",
        "comments": "15 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": null,
        "abstract": "  This paper describes an approach to creating textual syntax for Do-\nmain-Specific Languages (DSL). We consider target meta-model to be the main\nartifact and hence to be developed first. The key idea is to represent analysis\nof textual syntax as a sequence of transformations. This is made by explicit\nopera- tions on abstract syntax trees (ATS), for which a simple language is\nproposed. Text-to-model transformation is divided into two parts: text-to-AST\n(developed by openArchitectureWare [1]) and AST-to-model (proposed by this\npaper). Our approach simplifies semantic analysis and helps to generate as much\nas possi- ble.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 8 Jan 2008 12:28:18 GMT"
            }
        ],
        "update_date": "2008-01-09",
        "authors_parsed": [
            [
                "Breslav",
                "Andrey",
                ""
            ]
        ]
    },
    {
        "id": "0801.1251",
        "submitter": "Andrew Pitts",
        "authors": "Andrew M. Pitts and Mark R. Shinwell",
        "title": "Generative Unbinding of Names",
        "comments": null,
        "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 1 (March 18,\n  2008) lmcs:916",
        "doi": "10.2168/LMCS-4(1:4)2008",
        "report-no": null,
        "categories": "cs.PL cs.LO",
        "license": null,
        "abstract": "  This paper is concerned with the form of typed name binding used by the\nFreshML family of languages. Its characteristic feature is that a name binding\nis represented by an abstract (name,value)-pair that may only be deconstructed\nvia the generation of fresh bound names. The paper proves a new result about\nwhat operations on names can co-exist with this construct. In FreshML the only\nobservation one can make of names is to test whether or not they are equal.\nThis restricted amount of observation was thought necessary to ensure that\nthere is no observable difference between alpha-equivalent name binders. Yet\nfrom an algorithmic point of view it would be desirable to allow other\noperations and relations on names, such as a total ordering. This paper shows\nthat, contrary to expectations, one may add not just ordering, but almost any\nrelation or numerical function on names without disturbing the fundamental\ncorrectness result about this form of typed name binding (that object-level\nalpha-equivalence precisely corresponds to contextual equivalence at the\nprogramming meta-level), so long as one takes the state of dynamically created\nnames into account.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 8 Jan 2008 15:04:56 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 18 Mar 2008 18:23:41 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Pitts",
                "Andrew M.",
                ""
            ],
            [
                "Shinwell",
                "Mark R.",
                ""
            ]
        ]
    },
    {
        "id": "0801.1341",
        "submitter": "Sergey Tsarev P.",
        "authors": "S.P. Tsarev",
        "title": "Factorization in categories of systems of linear partial differential\n  equations",
        "comments": "LaTeX, 23 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC",
        "license": null,
        "abstract": "  We start with elementary algebraic theory of factorization of linear ordinary\ndifferential equations developed in the period 1880-1930. After exposing these\nclassical results we sketch more sophisticated algorithmic approaches developed\nin the last 20 years.\n  The main part of this paper is devoted to modern generalizations of the\nnotion of factorization to the case of systems of linear partial differential\nequations and their relation with explicit solvability of nonlinear partial\ndifferential equations based on some constructions from the theory of abelian\ncategories.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 Jan 2008 00:50:20 GMT"
            }
        ],
        "update_date": "2008-01-10",
        "authors_parsed": [
            [
                "Tsarev",
                "S. P.",
                ""
            ]
        ]
    },
    {
        "id": "0801.1416",
        "submitter": "Piyush Kurur",
        "authors": "Anindya De, Piyush P Kurur, Chandan Saha and Ramprasad Saptharishi",
        "title": "Fast Integer Multiplication using Modular Arithmetic",
        "comments": "fixed some typos and references",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give an $O(N\\cdot \\log N\\cdot 2^{O(\\log^*N)})$ algorithm for multiplying\ntwo $N$-bit integers that improves the $O(N\\cdot \\log N\\cdot \\log\\log N)$\nalgorithm by Sch\\\"{o}nhage-Strassen. Both these algorithms use modular\narithmetic. Recently, F\\\"{u}rer gave an $O(N\\cdot \\log N\\cdot 2^{O(\\log^*N)})$\nalgorithm which however uses arithmetic over complex numbers as opposed to\nmodular arithmetic. In this paper, we use multivariate polynomial\nmultiplication along with ideas from F\\\"{u}rer's algorithm to achieve this\nimprovement in the modular setting. Our algorithm can also be viewed as a\n$p$-adic version of F\\\"{u}rer's algorithm. Thus, we show that the two seemingly\ndifferent approaches to integer multiplication, modular and complex arithmetic,\nare similar.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 Jan 2008 12:44:55 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 6 May 2008 07:05:09 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 19 Sep 2008 06:45:16 GMT"
            }
        ],
        "update_date": "2008-09-19",
        "authors_parsed": [
            [
                "De",
                "Anindya",
                ""
            ],
            [
                "Kurur",
                "Piyush P",
                ""
            ],
            [
                "Saha",
                "Chandan",
                ""
            ],
            [
                "Saptharishi",
                "Ramprasad",
                ""
            ]
        ]
    },
    {
        "id": "0801.1676",
        "submitter": "Juan Gerardo Alcazar Arribas",
        "authors": "Juan Gerardo Alcazar",
        "title": "Analyzing the Topology Types arising in a Family of Algebraic Curves\n  Depending On Two Parameters",
        "comments": "8 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given the implicit equation $F(x,y,t,s)$ of a family of algebraic plane\ncurves depending on the parameters $t,s$, we provide an algorithm for studying\nthe topology types arising in the family. For this purpose, the algorithm\ncomputes a finite partition of the parameter space so that the topology type of\nthe family stays invariant over each element of the partition. The ideas\ncontained in the paper can be seen as a generalization of the ideas in\n\\cite{JGRS}, where the problem is solved for families of algebraic curves\ndepending on one parameter, to the two-parameters case.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 Jan 2008 21:08:22 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 2 Sep 2008 15:44:23 GMT"
            }
        ],
        "update_date": "2008-09-02",
        "authors_parsed": [
            [
                "Alcazar",
                "Juan Gerardo",
                ""
            ]
        ]
    },
    {
        "id": "0801.1715",
        "submitter": "Srivatsava Ranjit Ganta",
        "authors": "Srivatsava Ranjit Ganta, Raj Acharya",
        "title": "On Breaching Enterprise Data Privacy Through Adversarial Information\n  Fusion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.CR cs.OH",
        "license": null,
        "abstract": "  Data privacy is one of the key challenges faced by enterprises today.\nAnonymization techniques address this problem by sanitizing sensitive data such\nthat individual privacy is preserved while allowing enterprises to maintain and\nshare sensitive data. However, existing work on this problem make inherent\nassumptions about the data that are impractical in day-to-day enterprise data\nmanagement scenarios. Further, application of existing anonymization schemes on\nenterprise data could lead to adversarial attacks in which an intruder could\nuse information fusion techniques to inflict a privacy breach. In this paper,\nwe shed light on the shortcomings of current anonymization schemes in the\ncontext of enterprise data. We define and experimentally demonstrate Web-based\nInformation- Fusion Attack on anonymized enterprise data. We formulate the\nproblem of Fusion Resilient Enterprise Data Anonymization and propose a\nprototype solution to address this problem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 Jan 2008 03:21:49 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 8 Feb 2008 21:58:35 GMT"
            }
        ],
        "update_date": "2008-02-08",
        "authors_parsed": [
            [
                "Ganta",
                "Srivatsava Ranjit",
                ""
            ],
            [
                "Acharya",
                "Raj",
                ""
            ]
        ]
    },
    {
        "id": "0801.1856",
        "submitter": "Grenville Croll",
        "authors": "David A. Banks, Ann Monday",
        "title": "Interpretation as a factor in understanding flawed spreadsheets",
        "comments": "9 pages incuding references",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2002 13 21 ISBN 1 86166\n  182 7",
        "doi": null,
        "report-no": null,
        "categories": "cs.CY cs.HC",
        "license": null,
        "abstract": "  The spreadsheet has been used by the business community for many years and\nyet still raises a number of significant concerns. As educators our concern is\nto try to develop the students skills in both the development of spreadsheets\nand in taking a critical view of their potential defects. In this paper we\nconsider both the problems of mechanical production and the problems of\ntranslation of problem to spreadsheet representation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 Jan 2008 21:47:01 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Banks",
                "David A.",
                ""
            ],
            [
                "Monday",
                "Ann",
                ""
            ]
        ]
    },
    {
        "id": "0801.1883",
        "submitter": "Andras Lorincz",
        "authors": "Barnabas Poczos and Andras Lorincz",
        "title": "D-optimal Bayesian Interrogation for Parameter and Noise Identification\n  of Recurrent Neural Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.IT math.IT",
        "license": null,
        "abstract": "  We introduce a novel online Bayesian method for the identification of a\nfamily of noisy recurrent neural networks (RNNs). We develop Bayesian active\nlearning technique in order to optimize the interrogating stimuli given past\nexperiences. In particular, we consider the unknown parameters as stochastic\nvariables and use the D-optimality principle, also known as `\\emph{infomax\nmethod}', to choose optimal stimuli. We apply a greedy technique to maximize\nthe information gain concerning network parameters at each time step. We also\nderive the D-optimal estimation of the additive noise that perturbs the\ndynamical system of the RNN. Our analytical results are approximation-free. The\nanalytic derivation gives rise to attractive quadratic update rules.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 12 Jan 2008 08:02:12 GMT"
            }
        ],
        "update_date": "2008-01-15",
        "authors_parsed": [
            [
                "Poczos",
                "Barnabas",
                ""
            ],
            [
                "Lorincz",
                "Andras",
                ""
            ]
        ]
    },
    {
        "id": "0801.1925",
        "submitter": "Paul M. Aoki",
        "authors": "Rowena Luk, Melissa Ho, Paul M. Aoki",
        "title": "A Framework for Designing Teleconsultation Systems in Africa",
        "comments": "5 pages",
        "journal-ref": "Proc. Int'l Conf. on Health Informatics in Africa (HELINA),\n  Bamako, Mali, Jan. 2007, 28(1-5)",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  All of the countries within Africa experience a serious shortage of medical\nprofessionals, particularly specialists, a problem that is only exacerbated by\nhigh emigration of doctors with better prospects overseas. As a result, those\nthat remain in Africa, particularly those practicing in rural regions,\nexperience a shortage of specialists and other colleagues with whom to exchange\nideas. Telemedicine and teleconsultation are key areas that attempt to address\nthis problem by leveraging remote expertise for local problems. This paper\npresents an overview of teleconsultation in the developing world, with a\nparticular focus on how lessons learned apply to Africa. By teleconsultation,\nwe are addressing non-real-time communication between health care professionals\nfor the purposes of providing expertise and informal recommendations, without\nthe real-time, interactive requirements typical of diagnosis and patient care,\nwhich is impractical for the vast majority of existing medical practices. From\nthese previous experiences, we draw a set of guidelines and examine their\nrelevance to Ghana in particular. Based on 6 weeks of needs assessment, we\nidentify key variables that guide our framework, and then illustrate how our\nframework is used to inform the iterative design of a prototype system.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 12 Jan 2008 23:39:30 GMT"
            }
        ],
        "update_date": "2008-01-15",
        "authors_parsed": [
            [
                "Luk",
                "Rowena",
                ""
            ],
            [
                "Ho",
                "Melissa",
                ""
            ],
            [
                "Aoki",
                "Paul M.",
                ""
            ]
        ]
    },
    {
        "id": "0801.1927",
        "submitter": "Paul M. Aoki",
        "authors": "Rowena Luk, Melissa Ho, Paul M. Aoki",
        "title": "Asynchronous Remote Medical Consultation for Ghana",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": "10.1145/1357054.1357173",
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  Computer-mediated communication systems can be used to bridge the gap between\ndoctors in underserved regions with local shortages of medical expertise and\nmedical specialists worldwide. To this end, we describe the design of a\nprototype remote consultation system intended to provide the social,\ninstitutional and infrastructural context for sustained, self-organizing growth\nof a globally-distributed Ghanaian medical community. The design is grounded in\nan iterative design process that included two rounds of extended design\nfieldwork throughout Ghana and draws on three key design principles (social\nnetworks as a framework on which to build incentives within a self-organizing\nnetwork; optional and incremental integration with existing referral\nmechanisms; and a weakly-connected, distributed architecture that allows for a\nhighly interactive, responsive system despite failures in connectivity). We\ndiscuss initial experiences from an ongoing trial deployment in southern Ghana.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 12 Jan 2008 23:43:18 GMT"
            }
        ],
        "update_date": "2016-09-05",
        "authors_parsed": [
            [
                "Luk",
                "Rowena",
                ""
            ],
            [
                "Ho",
                "Melissa",
                ""
            ],
            [
                "Aoki",
                "Paul M.",
                ""
            ]
        ]
    },
    {
        "id": "0801.1988",
        "submitter": "Andras Lorincz",
        "authors": "Istvan Szita and Andras Lorincz",
        "title": "Online variants of the cross-entropy method",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  The cross-entropy method is a simple but efficient method for global\noptimization. In this paper we provide two online variants of the basic CEM,\ntogether with a proof of convergence.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 Jan 2008 06:56:42 GMT"
            }
        ],
        "update_date": "2008-01-15",
        "authors_parsed": [
            [
                "Szita",
                "Istvan",
                ""
            ],
            [
                "Lorincz",
                "Andras",
                ""
            ]
        ]
    },
    {
        "id": "0801.2069",
        "submitter": "Istvan Szita",
        "authors": "Istvan Szita and Andras Lorincz",
        "title": "Factored Value Iteration Converges",
        "comments": "17 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we propose a novel algorithm, factored value iteration (FVI),\nfor the approximate solution of factored Markov decision processes (fMDPs). The\ntraditional approximate value iteration algorithm is modified in two ways. For\none, the least-squares projection operator is modified so that it does not\nincrease max-norm, and thus preserves convergence. The other modification is\nthat we uniformly sample polynomially many samples from the (exponentially\nlarge) state space. This way, the complexity of our algorithm becomes\npolynomial in the size of the fMDP description length. We prove that the\nalgorithm is convergent. We also derive an upper bound on the difference\nbetween our approximate solution and the optimal one, and also on the error\nintroduced by sampling. We analyze various projection operators with respect to\ntheir computation complexity and their convergence when combined with\napproximate value iteration.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 Jan 2008 13:09:06 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 13 Aug 2008 15:07:08 GMT"
            }
        ],
        "update_date": "2008-08-13",
        "authors_parsed": [
            [
                "Szita",
                "Istvan",
                ""
            ],
            [
                "Lorincz",
                "Andras",
                ""
            ]
        ]
    },
    {
        "id": "0801.2201",
        "submitter": "Ed Harcourt",
        "authors": "Ed Harcourt",
        "title": "Policies of System Level Pipeline Modeling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AR cs.PL",
        "license": null,
        "abstract": "  Pipelining is a well understood and often used implementation technique for\nincreasing the performance of a hardware system. We develop several SystemC/C++\nmodeling techniques that allow us to quickly model, simulate, and evaluate\npipelines. We employ a small domain specific language (DSL) based on resource\nusage patterns that automates the drudgery of boilerplate code needed to\nconfigure connectivity in simulation models. The DSL is embedded directly in\nthe host modeling language SystemC/C++. Additionally we develop several\ntechniques for parameterizing a pipeline's behavior based on policies of\nfunction, communication, and timing (performance modeling).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 Jan 2008 15:44:28 GMT"
            }
        ],
        "update_date": "2008-01-16",
        "authors_parsed": [
            [
                "Harcourt",
                "Ed",
                ""
            ]
        ]
    },
    {
        "id": "0801.2226",
        "submitter": "Kees Middelburg",
        "authors": "J. A. Bergstra, C. A. Middelburg",
        "title": "Programming an interpreter using molecular dynamics",
        "comments": "27 pages",
        "journal-ref": "Scientific Annals of Computer Science, 17:47--81, 2007.\n  http://www.infoiasi.ro/bin/download/Annals/XVII/XVII_2.pdf",
        "doi": null,
        "report-no": "PRG0801",
        "categories": "cs.PL",
        "license": null,
        "abstract": "  PGA (ProGram Algebra) is an algebra of programs which concerns programs in\ntheir simplest form: sequences of instructions. Molecular dynamics is a simple\nmodel of computation developed in the setting of PGA, which bears on the use of\ndynamic data structures in programming. We consider the programming of an\ninterpreter for a program notation that is close to existing assembly languages\nusing PGA with the primitives of molecular dynamics as basic instructions. It\nhappens that, although primarily meant for explaining programming language\nfeatures relating to the use of dynamic data structures, the collection of\nprimitives of molecular dynamics in itself is suited to our programming wants.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 Jan 2008 07:56:12 GMT"
            }
        ],
        "update_date": "2008-04-08",
        "authors_parsed": [
            [
                "Bergstra",
                "J. A.",
                ""
            ],
            [
                "Middelburg",
                "C. A.",
                ""
            ]
        ]
    },
    {
        "id": "0801.2398",
        "submitter": "Zuoqiang Shi",
        "authors": "Thomas Y. Hou, Zuoqiang Shi",
        "title": "Removing the Stiffness of Elastic Force from the Immersed Boundary\n  Method for the 2D Stokes Equations",
        "comments": "40 pages with 8 figures",
        "journal-ref": null,
        "doi": "10.1016/j.jcp.2008.03.002",
        "report-no": null,
        "categories": "cs.CE cs.NA math.NA",
        "license": null,
        "abstract": "  The Immersed Boundary method has evolved into one of the most useful\ncomputational methods in studying fluid structure interaction. On the other\nhand, the Immersed Boundary method is also known to suffer from a severe\ntimestep stability restriction when using an explicit time discretization. In\nthis paper, we propose several efficient semi-implicit schemes to remove this\nstiffness from the Immersed Boundary method for the two-dimensional Stokes\nflow. First, we obtain a novel unconditionally stable semi-implicit\ndiscretization for the immersed boundary problem. Using this unconditionally\nstable discretization as a building block, we derive several efficient\nsemi-implicit schemes for the immersed boundary problem by applying the Small\nScale Decomposition to this unconditionally stable discretization. Our\nstability analysis and extensive numerical experiments show that our\nsemi-implicit schemes offer much better stability property than the explicit\nscheme. Unlike other implicit or semi-implicit schemes proposed in the\nliterature, our semi-implicit schemes can be solved explicitly in the spectral\nspace. Thus the computational cost of our semi-implicit schemes is comparable\nto that of an explicit scheme, but with a much better stability property.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 Jan 2008 22:22:25 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 18 Jan 2008 21:38:37 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Hou",
                "Thomas Y.",
                ""
            ],
            [
                "Shi",
                "Zuoqiang",
                ""
            ]
        ]
    },
    {
        "id": "0801.2405",
        "submitter": "Katrin Heitmann",
        "authors": "Steve Haroz, Kwan-Liu Ma, Katrin Heitmann",
        "title": "Multiple Uncertainties in Time-Variant Cosmological Particle Data",
        "comments": "8 pages, 8 figures, published in Pacific Vis 2008, project website at\n  http://steveharoz.com/research/cosmology/",
        "journal-ref": "Haroz, S; Ma, K-L; Heitmann, K, \"Multiple Uncertainties in\n  Time-Variant Cosmological Particle Data\" IEEE PacificVIS '08, pp.207-214, 5-7\n  March 2008",
        "doi": "10.1109/PACIFICVIS.2008.4475478",
        "report-no": "LAUR-08-0052",
        "categories": "astro-ph cs.GR cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Though the mediums for visualization are limited, the potential dimensions of\na dataset are not. In many areas of scientific study, understanding the\ncorrelations between those dimensions and their uncertainties is pivotal to\nmining useful information from a dataset. Obtaining this insight can\nnecessitate visualizing the many relationships among temporal, spatial, and\nother dimensionalities of data and its uncertainties. We utilize multiple views\nfor interactive dataset exploration and selection of important features, and we\napply those techniques to the unique challenges of cosmological particle\ndatasets. We show how interactivity and incorporation of multiple visualization\ntechniques help overcome the problem of limited visualization dimensions and\nallow many types of uncertainty to be seen in correlation with other variables.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 Jan 2008 22:57:41 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 25 Feb 2009 08:09:24 GMT"
            }
        ],
        "update_date": "2009-02-25",
        "authors_parsed": [
            [
                "Haroz",
                "Steve",
                ""
            ],
            [
                "Ma",
                "Kwan-Liu",
                ""
            ],
            [
                "Heitmann",
                "Katrin",
                ""
            ]
        ]
    },
    {
        "id": "0801.2618",
        "submitter": "Barry Doyle",
        "authors": "Barry Doyle (University of California, Irvine) and Cristina Videira\n  Lopes (University of California, Irvine)",
        "title": "Survey of Technologies for Web Application Development",
        "comments": "43 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.IR cs.NI",
        "license": null,
        "abstract": "  Web-based application developers face a dizzying array of platforms,\nlanguages, frameworks and technical artifacts to choose from. We survey,\nclassify, and compare technologies supporting Web application development. The\nclassification is based on (1) foundational technologies; (2)integration with\nother information sources; and (3) dynamic content generation. We further\nsurvey and classify software engineering techniques and tools that have been\nadopted from traditional programming into Web programming. We conclude that,\nalthough the infrastructure problems of the Web have largely been solved, the\ncacophony of technologies for Web-based applications reflects the lack of a\nsolid model tailored for this domain.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 17 Jan 2008 05:06:44 GMT"
            }
        ],
        "update_date": "2008-01-18",
        "authors_parsed": [
            [
                "Doyle",
                "Barry",
                "",
                "University of California, Irvine"
            ],
            [
                "Lopes",
                "Cristina Videira",
                "",
                "University of California, Irvine"
            ]
        ]
    },
    {
        "id": "0801.3046",
        "submitter": "John Stockie",
        "authors": "Michael Chapwanya, Wentao Liu and John M. Stockie",
        "title": "A model for reactive porous transport during re-wetting of hardened\n  concrete",
        "comments": "30 pages",
        "journal-ref": "Journal of Engineering Mathematics, 65(1):53-73, 2009",
        "doi": "10.1007/s10665-009-9268-0",
        "report-no": null,
        "categories": "cs.CE physics.flu-dyn",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A mathematical model is developed that captures the transport of liquid water\nin hardened concrete, as well as the chemical reactions that occur between the\nimbibed water and the residual calcium silicate compounds residing in the\nporous concrete matrix. The main hypothesis in this model is that the reaction\nproduct -- calcium silicate hydrate gel -- clogs the pores within the concrete\nthereby hindering water transport. Numerical simulations are employed to\ndetermine the sensitivity of the model solution to changes in various physical\nparameters, and compare to experimental results available in the literature.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 19 Jan 2008 18:54:01 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 26 Aug 2008 17:55:14 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 2 Jan 2009 04:15:45 GMT"
            }
        ],
        "update_date": "2009-08-12",
        "authors_parsed": [
            [
                "Chapwanya",
                "Michael",
                ""
            ],
            [
                "Liu",
                "Wentao",
                ""
            ],
            [
                "Stockie",
                "John M.",
                ""
            ]
        ]
    },
    {
        "id": "0801.3048",
        "submitter": "Franco Bagnoli",
        "authors": "Franco Bagnoli, Andrea Guazzini, Pietro Lio'",
        "title": "Human Heuristics for Autonomous Agents",
        "comments": "12 pages",
        "journal-ref": "P. Li\\'o et al. editors, BIOWIRE 2007, LNCS 5151, pages 340-351,\n  Springer--Verlag Berlin Heidelberg 2008",
        "doi": "10.1007/978-3-540-92191-2_30",
        "report-no": null,
        "categories": "cs.MA cs.HC cs.NI",
        "license": null,
        "abstract": "  We investigate the problem of autonomous agents processing pieces of\ninformation that may be corrupted (tainted). Agents have the option of\ncontacting a central database for a reliable check of the status of the\nmessage, but this procedure is costly and therefore should be used with\nparsimony. Agents have to evaluate the risk of being infected, and decide if\nand when communicating partners are affordable. Trustability is implemented as\na personal (one-to-one) record of past contacts among agents, and as a\nmean-field monitoring of the level of message corruption. Moreover, this\ninformation is slowly forgotten in time, so that at the end everybody is\nchecked against the database. We explore the behavior of a homogeneous system\nin the case of a fixed pool of spreaders of corrupted messages, and in the case\nof spontaneous appearance of corrupted messages.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 19 Jan 2008 19:36:13 GMT"
            }
        ],
        "update_date": "2011-08-22",
        "authors_parsed": [
            [
                "Bagnoli",
                "Franco",
                ""
            ],
            [
                "Guazzini",
                "Andrea",
                ""
            ],
            [
                "Lio'",
                "Pietro",
                ""
            ]
        ]
    },
    {
        "id": "0801.3102",
        "submitter": "Eloisa Bentivegna",
        "authors": "Mark Wenstrom, Eloisa Bentivegna and Ali Hurson (Pennsylvania State\n  University)",
        "title": "Balancing transparency, efficiency and security in pervasive systems",
        "comments": "52 pages, to be published in Advances in Computers",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.IR",
        "license": null,
        "abstract": "  This chapter will survey pervasive computing with a look at how its\nconstraint for transparency affects issues of resource management and security.\nThe goal of pervasive computing is to render computing transparent, such that\ncomputing resources are ubiquitously offered to the user and services are\nproactively performed for a user without his or her intervention. The task of\nintegrating computing infrastructure into everyday life without making it\nexcessively invasive brings about tradeoffs between flexibility and robustness,\nefficiency and effectiveness, as well as autonomy and reliability. As the\nfeasibility of ubiquitous computing and its real potential for mass\napplications are still a matter of controversy, this chapter will look into the\nunderlying issues of resource management and authentication to discover how\nthese can be handled in a least invasive fashion. The discussion will be closed\nby an overview of the solutions proposed by current pervasive computing\nefforts, both in the area of generic platforms and for dedicated applications\nsuch as pervasive education and healthcare.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 20 Jan 2008 19:15:50 GMT"
            }
        ],
        "update_date": "2008-01-22",
        "authors_parsed": [
            [
                "Wenstrom",
                "Mark",
                "",
                "Pennsylvania State\n  University"
            ],
            [
                "Bentivegna",
                "Eloisa",
                "",
                "Pennsylvania State\n  University"
            ],
            [
                "Hurson",
                "Ali",
                "",
                "Pennsylvania State\n  University"
            ]
        ]
    },
    {
        "id": "0801.3111",
        "submitter": "Martin Pelikan",
        "authors": "Martin Pelikan",
        "title": "Analysis of Estimation of Distribution Algorithms and Genetic Algorithms\n  on NK Landscapes",
        "comments": "Also available at the MEDAL web site, http://medal.cs.umsl.edu/",
        "journal-ref": "Proceedings of the Genetic and Evolutionary Computation Conference\n  (GECCO-2008), ACM Press, 1033-1040",
        "doi": null,
        "report-no": "MEDAL Report No. 2008001",
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  This study analyzes performance of several genetic and evolutionary\nalgorithms on randomly generated NK fitness landscapes with various values of n\nand k. A large number of NK problem instances are first generated for each n\nand k, and the global optimum of each instance is obtained using the\nbranch-and-bound algorithm. Next, the hierarchical Bayesian optimization\nalgorithm (hBOA), the univariate marginal distribution algorithm (UMDA), and\nthe simple genetic algorithm (GA) with uniform and two-point crossover\noperators are applied to all generated instances. Performance of all algorithms\nis then analyzed and compared, and the results are discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 21 Jan 2008 00:20:50 GMT"
            }
        ],
        "update_date": "2008-07-30",
        "authors_parsed": [
            [
                "Pelikan",
                "Martin",
                ""
            ]
        ]
    },
    {
        "id": "0801.3113",
        "submitter": "Martin Pelikan",
        "authors": "Martin Pelikan, Kumara Sastry, and David E. Goldberg",
        "title": "iBOA: The Incremental Bayesian Optimization Algorithm",
        "comments": "Also available at the MEDAL web site, http://medal.cs.umsl.edu/",
        "journal-ref": "Proceedings of the Genetic and Evolutionary Computation Conference\n  (GECCO-2008), ACM Press, 455-462",
        "doi": null,
        "report-no": "MEDAL Report No. 2008002",
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  This paper proposes the incremental Bayesian optimization algorithm (iBOA),\nwhich modifies standard BOA by removing the population of solutions and using\nincremental updates of the Bayesian network. iBOA is shown to be able to learn\nand exploit unrestricted Bayesian networks using incremental techniques for\nupdating both the structure as well as the parameters of the probabilistic\nmodel. This represents an important step toward the design of competent\nincremental estimation of distribution algorithms that can solve difficult\nnearly decomposable problems scalably and reliably.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 21 Jan 2008 00:34:55 GMT"
            }
        ],
        "update_date": "2008-07-30",
        "authors_parsed": [
            [
                "Pelikan",
                "Martin",
                ""
            ],
            [
                "Sastry",
                "Kumara",
                ""
            ],
            [
                "Goldberg",
                "David E.",
                ""
            ]
        ]
    },
    {
        "id": "0801.3114",
        "submitter": "Grenville Croll",
        "authors": "Raymond R. Panko",
        "title": "Thinking is Bad: Implications of Human Error Research for Spreadsheet\n  Research and Practice",
        "comments": "12 pages including references",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2007 69-80 ISBN\n  978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  In the spreadsheet error community, both academics and practitioners\ngenerally have ignored the rich findings produced by a century of human error\nresearch. These findings can suggest ways to reduce errors; we can then test\nthese suggestions empirically. In addition, research on human error seems to\nsuggest that several common prescriptions and expectations for reducing errors\nare likely to be incorrect. Among the key conclusions from human error research\nare that thinking is bad, that spreadsheets are not the cause of spreadsheet\nerrors, and that reducing errors is extremely difficult.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 21 Jan 2008 00:33:15 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Panko",
                "Raymond R.",
                ""
            ]
        ]
    },
    {
        "id": "0801.3119",
        "submitter": "Grenville Croll",
        "authors": "Mukul Madahar, Pat Cleary, David Ball",
        "title": "Categorisation of Spreadsheet Use within Organisations, Incorporating\n  Risk: A Progress Report",
        "comments": "10 pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2007 37-45 ISBN\n  978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.CY cs.HC",
        "license": null,
        "abstract": "  There has been a significant amount of research into spreadsheets over the\nlast two decades. Errors in spreadsheets are well documented. Once used mainly\nfor simple functions such as logging, tracking and totalling information,\nspreadsheets with enhanced formulas are being used for complex calculative\nmodels. There are many software packages and tools which assist in detecting\nerrors within spreadsheets. There has been very little evidence of\ninvestigation into the spreadsheet risks associated with the main stream\noperations within an organisation. This study is a part of the investigation\ninto the means of mitigating risks associated with spreadsheet use within\norganisations. In this paper the authors present and analyse three proposed\nmodels for categorisation of spreadsheet use and the level of risks involved.\nThe models are analysed in the light of current knowledge and the general risks\nassociated with organisations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 21 Jan 2008 00:57:31 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Madahar",
                "Mukul",
                ""
            ],
            [
                "Cleary",
                "Pat",
                ""
            ],
            [
                "Ball",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0801.3209",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin",
        "title": "A Pyramidal Evolutionary Algorithm with Different Inter-Agent Partnering\n  Strategies for Scheduling Problems",
        "comments": null,
        "journal-ref": "Proceedings of the Genetic and Evolutionary Computation Conference\n  (GECCO 2001), late-breaking papers volume, pp 1-8, San Francisco, USA",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": null,
        "abstract": "  This paper combines the idea of a hierarchical distributed genetic algorithm\nwith different inter-agent partnering strategies. Cascading clusters of\nsub-populations are built from bottom up, with higher-level sub-populations\noptimising larger parts of the problem. Hence higher-level sub-populations\nsearch a larger search space with a lower resolution whilst lower-level\nsub-populations search a smaller search space with a higher resolution. The\neffects of different partner selection schemes amongst the agents on solution\nquality are examined for two multiple-choice optimisation problems. It is shown\nthat partnering strategies that exploit problem-specific knowledge are superior\nand can counter inappropriate (sub-) fitness measurements.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 21 Jan 2008 15:55:22 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:10:36 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ]
        ]
    },
    {
        "id": "0801.3209",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin",
        "title": "A Pyramidal Evolutionary Algorithm with Different Inter-Agent Partnering\n  Strategies for Scheduling Problems",
        "comments": null,
        "journal-ref": "Proceedings of the Genetic and Evolutionary Computation Conference\n  (GECCO 2001), late-breaking papers volume, pp 1-8, San Francisco, USA",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": null,
        "abstract": "  This paper combines the idea of a hierarchical distributed genetic algorithm\nwith different inter-agent partnering strategies. Cascading clusters of\nsub-populations are built from bottom up, with higher-level sub-populations\noptimising larger parts of the problem. Hence higher-level sub-populations\nsearch a larger search space with a lower resolution whilst lower-level\nsub-populations search a smaller search space with a higher resolution. The\neffects of different partner selection schemes amongst the agents on solution\nquality are examined for two multiple-choice optimisation problems. It is shown\nthat partnering strategies that exploit problem-specific knowledge are superior\nand can counter inappropriate (sub-) fitness measurements.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 21 Jan 2008 15:55:22 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:10:36 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ]
        ]
    },
    {
        "id": "0801.3539",
        "submitter": "Uwe Aickelin",
        "authors": "Steve Cayzer and Uwe Aickelin",
        "title": "On the Effects of Idiotypic Interactions for Recommendation Communities\n  in Artificial Immune Systems",
        "comments": null,
        "journal-ref": "Proceedings of the 1st International Conference on Artificial\n  Immune Systems (ICARIS 2002), pp 154-160, Canterbury, UK, 2001",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  It has previously been shown that a recommender based on immune system\nidiotypic principles can out perform one based on correlation alone. This paper\nreports the results of work in progress, where we undertake some investigations\ninto the nature of this beneficial effect. The initial findings are that the\nimmune system recommender tends to produce different neighbourhoods, and that\nthe superior performance of this recommender is due partly to the different\nneighbourhoods, and partly to the way that the idiotypic effect is used to\nweight each neighbours recommendations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 23 Jan 2008 09:59:06 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:10:05 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 16 May 2008 10:42:42 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Cayzer",
                "Steve",
                ""
            ],
            [
                "Aickelin",
                "Uwe",
                ""
            ]
        ]
    },
    {
        "id": "0801.3547",
        "submitter": "Uwe Aickelin",
        "authors": "Steve Cazyer and Uwe Aickelin",
        "title": "A Recommender System based on the Immune Network",
        "comments": null,
        "journal-ref": "Proceedings of the IEEE Congress on Evolutionary Computation (CEC\n  2002), pp 807-813, Honolulu, USA, 2002",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  The immune system is a complex biological system with a highly distributed,\nadaptive and self-organising nature. This paper presents an artificial immune\nsystem (AIS) that exploits some of these characteristics and is applied to the\ntask of film recommendation by collaborative filtering (CF). Natural evolution\nand in particular the immune system have not been designed for classical\noptimisation. However, for this problem, we are not interested in finding a\nsingle optimum. Rather we intend to identify a sub-set of good matches on which\nrecommendations can be based. It is our hypothesis that an AIS built on two\ncentral aspects of the biological immune system will be an ideal candidate to\nachieve this: Antigen - antibody interaction for matching and antibody -\nantibody interaction for diversity. Computational results are presented in\nsupport of this conjecture and compared to those found by other CF techniques.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 23 Jan 2008 10:42:49 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:09:24 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Cazyer",
                "Steve",
                ""
            ],
            [
                "Aickelin",
                "Uwe",
                ""
            ]
        ]
    },
    {
        "id": "0801.3549",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin and Steve Cayzer",
        "title": "The Danger Theory and Its Application to Artificial Immune Systems",
        "comments": null,
        "journal-ref": "Proceedings of the 1st International Conference on Artificial\n  Immune Systems (ICARIS 2002), pp 141-148, Canterbury, Uk, 2002",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI cs.CR",
        "license": null,
        "abstract": "  Over the last decade, a new idea challenging the classical self-non-self\nviewpoint has become popular amongst immunologists. It is called the Danger\nTheory. In this conceptual paper, we look at this theory from the perspective\nof Artificial Immune System practitioners. An overview of the Danger Theory is\npresented with particular emphasis on analogies in the Artificial Immune\nSystems world. A number of potential application areas are then used to provide\na framing for a critical assessment of the concept, and its relevance for\nArtificial Immune Systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 23 Jan 2008 11:01:31 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:08:46 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 16 May 2008 10:45:49 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Cayzer",
                "Steve",
                ""
            ]
        ]
    },
    {
        "id": "0801.3550",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin and Larry Bull",
        "title": "Partnering Strategies for Fitness Evaluation in a Pyramidal Evolutionary\n  Algorithm",
        "comments": null,
        "journal-ref": "Proceedings of the Genetic and Evolutionary Computation Conference\n  (GECCO 2002), pp 263-270, New York, USA, 2002",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  This paper combines the idea of a hierarchical distributed genetic algorithm\nwith different inter-agent partnering strategies. Cascading clusters of\nsub-populations are built from bottom up, with higher-level sub-populations\noptimising larger parts of the problem. Hence higher-level sub-populations\nsearch a larger search space with a lower resolution whilst lower-level\nsub-populations search a smaller search space with a higher resolution. The\neffects of different partner selection schemes for (sub-)fitness evaluation\npurposes are examined for two multiple-choice optimisation problems. It is\nshown that random partnering strategies perform best by providing better\nsampling and more diversity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 23 Jan 2008 11:12:39 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:08:00 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Bull",
                "Larry",
                ""
            ]
        ]
    },
    {
        "id": "0801.3654",
        "submitter": "Mikhail Zaslavskiy",
        "authors": "Mikhail Zaslavskiy, Francis Bach, and Jean-Philippe Vert",
        "title": "A path following algorithm for the graph matching problem",
        "comments": "23 pages, 13 figures,typo correction, new results in sections 4,5,6",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.DM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a convex-concave programming approach for the labeled weighted\ngraph matching problem. The convex-concave programming formulation is obtained\nby rewriting the weighted graph matching problem as a least-square problem on\nthe set of permutation matrices and relaxing it to two different optimization\nproblems: a quadratic convex and a quadratic concave optimization problem on\nthe set of doubly stochastic matrices. The concave relaxation has the same\nglobal minimum as the initial graph matching problem, but the search for its\nglobal minimum is also a hard combinatorial problem. We therefore construct an\napproximation of the concave problem solution by following a solution path of a\nconvex-concave problem obtained by linear interpolation of the convex and\nconcave formulations, starting from the convex relaxation. This method allows\nto easily integrate the information on graph label similarities into the\noptimization problem, and therefore to perform labeled weighted graph matching.\nThe algorithm is compared with some of the best performing graph matching\nmethods on four datasets: simulated graphs, QAPLib, retina vessel images and\nhandwritten chinese characters. In all cases, the results are competitive with\nthe state-of-the-art.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 23 Jan 2008 20:20:32 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 27 Oct 2008 14:16:01 GMT"
            }
        ],
        "update_date": "2008-10-27",
        "authors_parsed": [
            [
                "Zaslavskiy",
                "Mikhail",
                ""
            ],
            [
                "Bach",
                "Francis",
                ""
            ],
            [
                "Vert",
                "Jean-Philippe",
                ""
            ]
        ]
    },
    {
        "id": "0801.3690",
        "submitter": "Grenville Croll",
        "authors": "Jocelyn Paine",
        "title": "Ensuring Spreadsheet Integrity with Model Master",
        "comments": "15 pages; substantive references; code examples",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2001 17-38 ISBN:1 86166\n  179 7",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.HC",
        "license": null,
        "abstract": "  We have developed the Model Master (MM) language for describing spreadsheets,\nand tools for converting MM programs to and from spreadsheets. The MM\ndecompiler translates a spreadsheet into an MM program which gives a concise\nsummary of its calculations, layout, and styling. This is valuable when trying\nto understand spreadsheets one has not seen before, and when checking for\nerrors. The MM compiler goes the other way, translating an MM program into a\nspreadsheet. This makes possible a new style of development, in which\nspreadsheets are generated from textual specifications. This can reduce error\nrates compared to working directly with the raw spreadsheet, and gives\nimportant facilities for code reuse. MM programs also offer advantages over\nExcel files for the interchange of spreadsheets.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 Jan 2008 00:32:29 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Paine",
                "Jocelyn",
                ""
            ]
        ]
    },
    {
        "id": "0801.3690",
        "submitter": "Grenville Croll",
        "authors": "Jocelyn Paine",
        "title": "Ensuring Spreadsheet Integrity with Model Master",
        "comments": "15 pages; substantive references; code examples",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2001 17-38 ISBN:1 86166\n  179 7",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.HC",
        "license": null,
        "abstract": "  We have developed the Model Master (MM) language for describing spreadsheets,\nand tools for converting MM programs to and from spreadsheets. The MM\ndecompiler translates a spreadsheet into an MM program which gives a concise\nsummary of its calculations, layout, and styling. This is valuable when trying\nto understand spreadsheets one has not seen before, and when checking for\nerrors. The MM compiler goes the other way, translating an MM program into a\nspreadsheet. This makes possible a new style of development, in which\nspreadsheets are generated from textual specifications. This can reduce error\nrates compared to working directly with the raw spreadsheet, and gives\nimportant facilities for code reuse. MM programs also offer advantages over\nExcel files for the interchange of spreadsheets.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 Jan 2008 00:32:29 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Paine",
                "Jocelyn",
                ""
            ]
        ]
    },
    {
        "id": "0801.3715",
        "submitter": "Annie Ressouche",
        "authors": "Annie Ressouche, Daniel Gaff\\'e (LEAT), Val\\'erie Roy",
        "title": "Modular Compilation of a Synchronous Language",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.LO",
        "license": null,
        "abstract": "  Synchronous languages rely on formal methods to ease the development of\napplications in an efficient and reusable way. Formal methods have been\nadvocated as a means of increasing the reliability of systems, especially those\nwhich are safety or business critical. It is still difficult to develop\nautomatic specification and verification tools due to limitations like state\nexplosion, undecidability, etc... In this work, we design a new specification\nmodel based on a reactive synchronous approach. Then, we benefit from a formal\nframework well suited to perform compilation and formal validation of systems.\nIn practice, we design and implement a special purpose language (LE) and its\ntwo semantics: the ehavioral semantics helps us to define a program by the set\nof its behaviors and avoid ambiguousness in programs' interpretation; the\nexecution equational semantics allows the modular compilation of programs into\nsoftware and hardware targets (c code, vhdl code, fpga synthesis, observers).\nOur approach is pertinent considering the two main requirements of critical\nrealistic applications: the modular compilation allows us to deal with large\nsystems, the model-based approach provides us with formal validation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 Jan 2008 15:24:46 GMT"
            }
        ],
        "update_date": "2009-04-20",
        "authors_parsed": [
            [
                "Ressouche",
                "Annie",
                "",
                "LEAT"
            ],
            [
                "Gaff\u00e9",
                "Daniel",
                "",
                "LEAT"
            ],
            [
                "Roy",
                "Val\u00e9rie",
                ""
            ]
        ]
    },
    {
        "id": "0801.3853",
        "submitter": "Grenville Croll",
        "authors": "Simon Murphy",
        "title": "Comparison of Spreadsheets with other Development Tools (limitations,\n  solutions, workarounds and alternatives)",
        "comments": "9 pages including references, colour diagrams and comparison tables",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2005 201208\n  ISBN:1-902724-16-X",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.CY",
        "license": null,
        "abstract": "  The spreadsheet paradigm has some unique risks and challenges that are not\npresent in more traditional development technologies. Many of the recent\nadvances in other branches of software development have bypassed spreadsheets\nand spreadsheet developers. This paper compares spreadsheets and spreadsheet\ndevelopment to more traditional platforms such as databases and procedural\nlanguages. It also considers the fundamental danger introduced in the\ntransition from paper spreadsheets to electronic. Suggestions are made to\nmanage the risks and work around the limitations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 Jan 2008 21:40:17 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Murphy",
                "Simon",
                ""
            ]
        ]
    },
    {
        "id": "0801.3875",
        "submitter": "Jan Mandel",
        "authors": "Jan Mandel, Jonathan D. Beezley, Soham Chakraborty, Janice L. Coen,\n  Craig C. Douglas, Anthony Vodacek, Zhen Wang",
        "title": "Towards a Real-Time Data Driven Wildland Fire Model",
        "comments": "5 pages, 4 figures",
        "journal-ref": "IEEE International Symposium on Parallel and Distributed\n  Processing, 2008 (IPDPS 2008), pp. 1-5",
        "doi": "10.1109/IPDPS.2008.4536414",
        "report-no": "UCD CCM Report 265",
        "categories": "physics.ao-ph cs.CE",
        "license": null,
        "abstract": "  A wildland fire model based on semi-empirical relations for the spread rate\nof a surface fire and post-frontal heat release is coupled with the Weather\nResearch and Forecasting atmospheric model (WRF). The propagation of the fire\nfront is implemented by a level set method. Data is assimilated by a morphing\nensemble Kalman filter, which provides amplitude as well as position\ncorrections. Thermal images of a fire will provide the observations and will be\ncompared to a synthetic image from the model state.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 Jan 2008 04:41:01 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 27 Jan 2008 00:40:22 GMT"
            }
        ],
        "update_date": "2015-08-03",
        "authors_parsed": [
            [
                "Mandel",
                "Jan",
                ""
            ],
            [
                "Beezley",
                "Jonathan D.",
                ""
            ],
            [
                "Chakraborty",
                "Soham",
                ""
            ],
            [
                "Coen",
                "Janice L.",
                ""
            ],
            [
                "Douglas",
                "Craig C.",
                ""
            ],
            [
                "Vodacek",
                "Anthony",
                ""
            ],
            [
                "Wang",
                "Zhen",
                ""
            ]
        ]
    },
    {
        "id": "0801.3924",
        "submitter": "Jaap-Henk Hoepman",
        "authors": "Jaap-Henk Hoepman, Bart Jacobs",
        "title": "Increased security through open source",
        "comments": null,
        "journal-ref": "Communications of the ACM, 50(1):79-83, 2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.CY cs.SE",
        "license": null,
        "abstract": "  In this paper we discuss the impact of open source on both the security and\ntransparency of a software system. We focus on the more technical aspects of\nthis issue, combining and extending arguments developed over the years. We\nstress that our discussion of the problem only applies to software for general\npurpose computing systems. For embedded systems, where the software usually\ncannot easily be patched or upgraded, different considerations may apply.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 Jan 2008 12:06:48 GMT"
            }
        ],
        "update_date": "2021-08-23",
        "authors_parsed": [
            [
                "Hoepman",
                "Jaap-Henk",
                ""
            ],
            [
                "Jacobs",
                "Bart",
                ""
            ]
        ]
    },
    {
        "id": "0801.3971",
        "submitter": "Uwe Aickelin",
        "authors": "Jingpeng Li and Uwe Aickelin",
        "title": "A Bayesian Optimisation Algorithm for the Nurse Scheduling Problem",
        "comments": null,
        "journal-ref": "Proceedings of the IEEE Congress on Evolutionary Computation (CEC\n  2003), pp 2149-2156, Canberra, Australia, 2003",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": null,
        "abstract": "  A Bayesian optimization algorithm for the nurse scheduling problem is\npresented, which involves choosing a suitable scheduling rule from a set for\neach nurses assignment. Unlike our previous work that used Gas to implement\nimplicit learning, the learning in the proposed algorithm is explicit, ie.\nEventually, we will be able to identify and mix building blocks directly. The\nBayesian optimization algorithm is applied to implement such explicit learning\nby building a Bayesian network of the joint distribution of solutions. The\nconditional probability of each variable in the network is computed according\nto an initial set of promising solutions. Subsequently, each new instance for\neach variable is generated, ie in our case, a new rule string has been\nobtained. Another set of rule strings will be generated in this way, some of\nwhich will replace previous strings based on fitness selection. If stopping\nconditions are not met, the conditional probabilities for all nodes in the\nBayesian network are updated again using the current set of promising rule\nstrings. Computational results from 52 real data instances demonstrate the\nsuccess of this approach. It is also suggested that the learning mechanism in\nthe proposed approach might be suitable for other scheduling problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 Jan 2008 16:07:25 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:07:17 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 16 May 2008 10:43:52 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Li",
                "Jingpeng",
                ""
            ],
            [
                "Aickelin",
                "Uwe",
                ""
            ]
        ]
    },
    {
        "id": "0801.3971",
        "submitter": "Uwe Aickelin",
        "authors": "Jingpeng Li and Uwe Aickelin",
        "title": "A Bayesian Optimisation Algorithm for the Nurse Scheduling Problem",
        "comments": null,
        "journal-ref": "Proceedings of the IEEE Congress on Evolutionary Computation (CEC\n  2003), pp 2149-2156, Canberra, Australia, 2003",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": null,
        "abstract": "  A Bayesian optimization algorithm for the nurse scheduling problem is\npresented, which involves choosing a suitable scheduling rule from a set for\neach nurses assignment. Unlike our previous work that used Gas to implement\nimplicit learning, the learning in the proposed algorithm is explicit, ie.\nEventually, we will be able to identify and mix building blocks directly. The\nBayesian optimization algorithm is applied to implement such explicit learning\nby building a Bayesian network of the joint distribution of solutions. The\nconditional probability of each variable in the network is computed according\nto an initial set of promising solutions. Subsequently, each new instance for\neach variable is generated, ie in our case, a new rule string has been\nobtained. Another set of rule strings will be generated in this way, some of\nwhich will replace previous strings based on fitness selection. If stopping\nconditions are not met, the conditional probabilities for all nodes in the\nBayesian network are updated again using the current set of promising rule\nstrings. Computational results from 52 real data instances demonstrate the\nsuccess of this approach. It is also suggested that the learning mechanism in\nthe proposed approach might be suitable for other scheduling problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 Jan 2008 16:07:25 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:07:17 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 16 May 2008 10:43:52 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Li",
                "Jingpeng",
                ""
            ],
            [
                "Aickelin",
                "Uwe",
                ""
            ]
        ]
    },
    {
        "id": "0801.4061",
        "submitter": "Jean-Philippe Vert",
        "authors": "Jean-Philippe Vert (CB)",
        "title": "The optimal assignment kernel is not positive definite",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  We prove that the optimal assignment kernel, proposed recently as an attempt\nto embed labeled graphs and more generally tuples of basic data to a Hilbert\nspace, is in fact not always positive definite.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 26 Jan 2008 07:32:48 GMT"
            }
        ],
        "update_date": "2008-01-29",
        "authors_parsed": [
            [
                "Vert",
                "Jean-Philippe",
                "",
                "CB"
            ]
        ]
    },
    {
        "id": "0801.4119",
        "submitter": "Uwe Aickelin",
        "authors": "Gianni Tedesco and Uwe Aickelin",
        "title": "Strategic Alert Throttling for Intrusion Detection Systems",
        "comments": null,
        "journal-ref": "4th WSEAS International Conference on Information Security (WSEAS\n  2005), Tenerife, Spain, 2005",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CR",
        "license": null,
        "abstract": "  Network intrusion detection systems are themselves becoming targets of\nattackers. Alert flood attacks may be used to conceal malicious activity by\nhiding it among a deluge of false alerts sent by the attacker. Although these\ntypes of attacks are very hard to stop completely, our aim is to present\ntechniques that improve alert throughput and capacity to such an extent that\nthe resources required to successfully mount the attack become prohibitive. The\nkey idea presented is to combine a token bucket filter with a realtime\ncorrelation algorithm. The proposed algorithm throttles alert output from the\nIDS when an attack is detected. The attack graph used in the correlation\nalgorithm is used to make sure that alerts crucial to forming strategies are\nnot discarded by throttling.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jan 2008 15:36:56 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:00:56 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 16 May 2008 10:43:30 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Tedesco",
                "Gianni",
                ""
            ],
            [
                "Aickelin",
                "Uwe",
                ""
            ]
        ]
    },
    {
        "id": "0801.4190",
        "submitter": "Sebastian Roch",
        "authors": "Constantinos Daskalakis, Elchanan Mossel, Sebastien Roch",
        "title": "Phylogenies without Branch Bounds: Contracting the Short, Pruning the\n  Deep",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-bio.PE cs.CE cs.DS math.PR math.ST stat.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new phylogenetic reconstruction algorithm which, unlike most\nprevious rigorous inference techniques, does not rely on assumptions regarding\nthe branch lengths or the depth of the tree. The algorithm returns a forest\nwhich is guaranteed to contain all edges that are: 1) sufficiently long and 2)\nsufficiently close to the leaves. How much of the true tree is recovered\ndepends on the sequence length provided. The algorithm is distance-based and\nruns in polynomial time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jan 2008 05:10:22 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 28 Jul 2009 01:48:27 GMT"
            }
        ],
        "update_date": "2011-09-30",
        "authors_parsed": [
            [
                "Daskalakis",
                "Constantinos",
                ""
            ],
            [
                "Mossel",
                "Elchanan",
                ""
            ],
            [
                "Roch",
                "Sebastien",
                ""
            ]
        ]
    },
    {
        "id": "0801.4230",
        "submitter": "Simon Perdrix",
        "authors": "Simon Perdrix",
        "title": "Quantum entanglement analysis based on abstract interpretation",
        "comments": "13 pages",
        "journal-ref": "Proc. of 15th International Static Analysis Symposium (SAS 2008).\n  LNCS 5079, pp 270-282",
        "doi": "10.1007/978-3-540-69166-2_18",
        "report-no": null,
        "categories": "cs.LO cs.PL quant-ph",
        "license": null,
        "abstract": "  Entanglement is a non local property of quantum states which has no classical\ncounterpart and plays a decisive role in quantum information theory. Several\nprotocols, like the teleportation, are based on quantum entangled states.\nMoreover, any quantum algorithm which does not create entanglement can be\nefficiently simulated on a classical computer. The exact role of the\nentanglement is nevertheless not well understood. Since an exact analysis of\nentanglement evolution induces an exponential slowdown, we consider\napproximative analysis based on the framework of abstract interpretation. In\nthis paper, a concrete quantum semantics based on superoperators is associated\nwith a simple quantum programming language. The representation of entanglement,\ni.e. the design of the abstract domain is a key issue. A representation of\nentanglement as a partition of the memory is chosen. An abstract semantics is\nintroduced, and the soundness of the approximation is proven.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jan 2008 10:45:47 GMT"
            }
        ],
        "update_date": "2008-12-08",
        "authors_parsed": [
            [
                "Perdrix",
                "Simon",
                ""
            ]
        ]
    },
    {
        "id": "0801.4274",
        "submitter": "Grenville Croll",
        "authors": "Karin Hodnigg, Markus Clermont, Roland T. Mittermeir",
        "title": "Computational Models of Spreadsheet Development: Basis for Educational\n  Approaches",
        "comments": "16 Pages, 4 figures, includes references",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2004 153-168 ISBN 1\n  902724 94 1",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.SE",
        "license": null,
        "abstract": "  Among the multiple causes of high error rates in spreadsheets, lack of proper\ntraining and of deep understanding of the computational model upon which\nspreadsheet computations rest might not be the least issue. The paper addresses\nthis problem by presenting a didactical model focussing on cell interaction,\nthus exceeding the atomicity of cell computations. The approach is motivated by\nan investigation how different spreadsheet systems handle certain computational\nissues implied from moving cells, copy-paste operations, or recursion.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jan 2008 13:55:55 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Hodnigg",
                "Karin",
                ""
            ],
            [
                "Clermont",
                "Markus",
                ""
            ],
            [
                "Mittermeir",
                "Roland T.",
                ""
            ]
        ]
    },
    {
        "id": "0801.4274",
        "submitter": "Grenville Croll",
        "authors": "Karin Hodnigg, Markus Clermont, Roland T. Mittermeir",
        "title": "Computational Models of Spreadsheet Development: Basis for Educational\n  Approaches",
        "comments": "16 Pages, 4 figures, includes references",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2004 153-168 ISBN 1\n  902724 94 1",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.SE",
        "license": null,
        "abstract": "  Among the multiple causes of high error rates in spreadsheets, lack of proper\ntraining and of deep understanding of the computational model upon which\nspreadsheet computations rest might not be the least issue. The paper addresses\nthis problem by presenting a didactical model focussing on cell interaction,\nthus exceeding the atomicity of cell computations. The approach is motivated by\nan investigation how different spreadsheet systems handle certain computational\nissues implied from moving cells, copy-paste operations, or recursion.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jan 2008 13:55:55 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Hodnigg",
                "Karin",
                ""
            ],
            [
                "Clermont",
                "Markus",
                ""
            ],
            [
                "Mittermeir",
                "Roland T.",
                ""
            ]
        ]
    },
    {
        "id": "0801.4280",
        "submitter": "Grenville Croll",
        "authors": "Yirsaw Ayalew, Roland Mittermeir",
        "title": "Spreadsheet Debugging",
        "comments": "13 Pages, 4 figues",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2003 67-79 ISBN 1 86166\n  199 1",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.PL",
        "license": null,
        "abstract": "  Spreadsheet programs, artifacts developed by non-programmers, are used for a\nvariety of important tasks and decisions. Yet a significant proportion of them\nhave severe quality problems. To address this issue, our previous work\npresented an interval-based testing methodology for spreadsheets.\nInterval-based testing rests on the observation that spreadsheets are mainly\nused for numerical computations. It also incorporates ideas from symbolic\ntesting and interval analysis. This paper addresses the issue of efficiently\ndebugging spreadsheets. Based on the interval-based testing methodology, this\npaper presents a technique for tracing faults in spreadsheet programs. The\nfault tracing technique proposed uses the dataflow information and cell marks\nto identify the most influential faulty cell(s) for a given formula cell\ncontaining a propagated fault.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jan 2008 14:07:58 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Ayalew",
                "Yirsaw",
                ""
            ],
            [
                "Mittermeir",
                "Roland",
                ""
            ]
        ]
    },
    {
        "id": "0801.4280",
        "submitter": "Grenville Croll",
        "authors": "Yirsaw Ayalew, Roland Mittermeir",
        "title": "Spreadsheet Debugging",
        "comments": "13 Pages, 4 figues",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2003 67-79 ISBN 1 86166\n  199 1",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.PL",
        "license": null,
        "abstract": "  Spreadsheet programs, artifacts developed by non-programmers, are used for a\nvariety of important tasks and decisions. Yet a significant proportion of them\nhave severe quality problems. To address this issue, our previous work\npresented an interval-based testing methodology for spreadsheets.\nInterval-based testing rests on the observation that spreadsheets are mainly\nused for numerical computations. It also incorporates ideas from symbolic\ntesting and interval analysis. This paper addresses the issue of efficiently\ndebugging spreadsheets. Based on the interval-based testing methodology, this\npaper presents a technique for tracing faults in spreadsheet programs. The\nfault tracing technique proposed uses the dataflow information and cell marks\nto identify the most influential faulty cell(s) for a given formula cell\ncontaining a propagated fault.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jan 2008 14:07:58 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Ayalew",
                "Yirsaw",
                ""
            ],
            [
                "Mittermeir",
                "Roland",
                ""
            ]
        ]
    },
    {
        "id": "0801.4287",
        "submitter": "Uwe Aickelin",
        "authors": "Qi Chen and Uwe Aickelin",
        "title": "Movie Recommendation Systems Using An Artificial Immune System",
        "comments": null,
        "journal-ref": "6th International Conference in Adaptive Computing in Design and\n  Manufacture (ACDM 2004), Bristol, UK, 2004",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  We apply the Artificial Immune System (AIS) technology to the Collaborative\nFiltering (CF) technology when we build the movie recommendation system. Two\ndifferent affinity measure algorithms of AIS, Kendall tau and Weighted Kappa,\nare used to calculate the correlation coefficients for this movie\nrecommendation system. From the testing we think that Weighted Kappa is more\nsuitable than Kendall tau for movie problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jan 2008 14:19:12 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:05:58 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Chen",
                "Qi",
                ""
            ],
            [
                "Aickelin",
                "Uwe",
                ""
            ]
        ]
    },
    {
        "id": "0801.4292",
        "submitter": "Joel Goossens",
        "authors": "Liliana Cucu and Jo\\\"el Goossens",
        "title": "Exact Feasibility Tests for Real-Time Scheduling of Periodic Tasks upon\n  Multiprocessor Platforms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": null,
        "abstract": "  In this paper we study the global scheduling of periodic task systems upon\nmultiprocessor platforms. We first show two very general properties which are\nwell-known for uniprocessor platforms and which remain for multiprocessor\nplatforms: (i) under few and not so restrictive assumptions, we show that\nfeasible schedules of periodic task systems are periodic from some point with a\nperiod equal to the least common multiple of task periods and (ii) for the\nspecific case of synchronous periodic task systems, we show that feasible\nschedules repeat from the origin. We then present our main result: we\ncharacterize, for task-level fixed-priority schedulers and for asynchronous\nconstrained or arbitrary deadline periodic task models, upper bounds of the\nfirst time instant where the schedule repeats. We show that job-level\nfixed-priority schedulers are predictable upon unrelated multiprocessor\nplatforms. For task-level fixed-priority schedulers, based on the upper bounds\nand the predictability property, we provide for asynchronous constrained or\narbitrary deadline periodic task sets, exact feasibility tests. Finally, for\nthe job-level fixed-priority EDF scheduler, for which such an upper bound\nremains unknown, we provide an exact feasibility test as well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jan 2008 14:30:34 GMT"
            }
        ],
        "update_date": "2008-01-29",
        "authors_parsed": [
            [
                "Cucu",
                "Liliana",
                ""
            ],
            [
                "Goossens",
                "Jo\u00ebl",
                ""
            ]
        ]
    },
    {
        "id": "0801.4305",
        "submitter": "Frank Schweitzer",
        "authors": "J. Emeterio Navarro Barrientos, Frank E. Walter, Frank Schweitzer",
        "title": "Risk-Seeking versus Risk-Avoiding Investments in Noisy Periodic\n  Environments",
        "comments": "27 pp. v2 with minor corrections. See http://www.sg.ethz.ch for more\n  info",
        "journal-ref": "International Journal of Modern Physics C vol. 19, no. 6 (2008)\n  971-994",
        "doi": "10.1142/S0129183108012662",
        "report-no": null,
        "categories": "q-fin.PM cs.CE physics.soc-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the performance of various agent strategies in an artificial\ninvestment scenario. Agents are equipped with a budget, $x(t)$, and at each\ntime step invest a particular fraction, $q(t)$, of their budget. The return on\ninvestment (RoI), $r(t)$, is characterized by a periodic function with\ndifferent types and levels of noise. Risk-avoiding agents choose their fraction\n$q(t)$ proportional to the expected positive RoI, while risk-seeking agents\nalways choose a maximum value $q_{max}$ if they predict the RoI to be positive\n(\"everything on red\"). In addition to these different strategies, agents have\ndifferent capabilities to predict the future $r(t)$, dependent on their\ninternal complexity. Here, we compare 'zero-intelligent' agents using technical\nanalysis (such as moving least squares) with agents using reinforcement\nlearning or genetic algorithms to predict $r(t)$. The performance of agents is\nmeasured by their average budget growth after a certain number of time steps.\nWe present results of extensive computer simulations, which show that, for our\ngiven artificial environment, (i) the risk-seeking strategy outperforms the\nrisk-avoiding one, and (ii) the genetic algorithm was able to find this optimal\nstrategy itself, and thus outperforms other prediction approaches considered.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jan 2008 15:09:58 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 7 Sep 2008 13:48:45 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Barrientos",
                "J. Emeterio Navarro",
                ""
            ],
            [
                "Walter",
                "Frank E.",
                ""
            ],
            [
                "Schweitzer",
                "Frank",
                ""
            ]
        ]
    },
    {
        "id": "0801.4307",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin and Qi Chen",
        "title": "On Affinity Measures for Artificial Immune System Movie Recommenders",
        "comments": null,
        "journal-ref": "Proceedings of the 5th International Conference on Recent Advances\n  in Soft Computing (RASC 2004), Nottingham, UK",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI cs.CY",
        "license": null,
        "abstract": "  We combine Artificial Immune Systems 'AIS', technology with Collaborative\nFiltering 'CF' and use it to build a movie recommendation system. We already\nknow that Artificial Immune Systems work well as movie recommenders from\nprevious work by Cayzer and Aickelin 3, 4, 5. Here our aim is to investigate\nthe effect of different affinity measure algorithms for the AIS. Two different\naffinity measures, Kendalls Tau and Weighted Kappa, are used to calculate the\ncorrelation coefficients for the movie recommender. We compare the results with\nthose published previously and show that Weighted Kappa is more suitable than\nothers for movie problems. We also show that AIS are generally robust movie\nrecommenders and that, as long as a suitable affinity measure is chosen,\nresults are good.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jan 2008 15:14:45 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:06:30 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 16 May 2008 10:44:44 GMT"
            }
        ],
        "update_date": "2008-05-16",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Chen",
                "Qi",
                ""
            ]
        ]
    },
    {
        "id": "0801.4312",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin, Edmund Burke and Aniza Din",
        "title": "Investigating Artificial Immune Systems For Job Shop Rescheduling In\n  Changing Environments",
        "comments": null,
        "journal-ref": "6th International Conference in Adaptive Computing in Design and\n  Manufacture (ACDM 2004), Bristol, UK, 2004",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": null,
        "abstract": "  Artificial immune system can be used to generate schedules in changing\nenvironments and it has been proven to be more robust than schedules developed\nusing a genetic algorithm. Good schedules can be produced especially when the\nnumber of the antigens is increased. However, an increase in the range of the\nantigens had somehow affected the fitness of the immune system. In this\nresearch, we are trying to improve the result of the system by rescheduling the\nsame problem using the same method while at the same time maintaining the\nrobustness of the schedules.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jan 2008 15:26:59 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:03:46 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 16 May 2008 10:43:07 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Burke",
                "Edmund",
                ""
            ],
            [
                "Din",
                "Aniza",
                ""
            ]
        ]
    },
    {
        "id": "0801.4312",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin, Edmund Burke and Aniza Din",
        "title": "Investigating Artificial Immune Systems For Job Shop Rescheduling In\n  Changing Environments",
        "comments": null,
        "journal-ref": "6th International Conference in Adaptive Computing in Design and\n  Manufacture (ACDM 2004), Bristol, UK, 2004",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": null,
        "abstract": "  Artificial immune system can be used to generate schedules in changing\nenvironments and it has been proven to be more robust than schedules developed\nusing a genetic algorithm. Good schedules can be produced especially when the\nnumber of the antigens is increased. However, an increase in the range of the\nantigens had somehow affected the fitness of the immune system. In this\nresearch, we are trying to improve the result of the system by rescheduling the\nsame problem using the same method while at the same time maintaining the\nrobustness of the schedules.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jan 2008 15:26:59 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:03:46 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 16 May 2008 10:43:07 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Burke",
                "Edmund",
                ""
            ],
            [
                "Din",
                "Aniza",
                ""
            ]
        ]
    },
    {
        "id": "0801.4314",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin",
        "title": "Artificial Immune Systems (AIS) - A New Paradigm for Heuristic Decision\n  Making",
        "comments": null,
        "journal-ref": "Invited Keynote Talk, Annual Operational Research Conference 46,\n  York, UK, 2004",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI",
        "license": null,
        "abstract": "  Over the last few years, more and more heuristic decision making techniques\nhave been inspired by nature, e.g. evolutionary algorithms, ant colony\noptimisation and simulated annealing. More recently, a novel computational\nintelligence technique inspired by immunology has emerged, called Artificial\nImmune Systems (AIS). This immune system inspired technique has already been\nuseful in solving some computational problems. In this keynote, we will very\nbriefly describe the immune system metaphors that are relevant to AIS. We will\nthen give some illustrative real-world problems suitable for AIS use and show a\nstep-by-step algorithm walkthrough. A comparison of AIS to other well-known\nalgorithms and areas for future work will round this keynote off. It should be\nnoted that as AIS is still a young and evolving field, there is not yet a fixed\nalgorithm template and hence actual implementations might differ somewhat from\nthe examples given here.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jan 2008 15:32:05 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 17:02:52 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 16 May 2008 10:46:24 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ]
        ]
    },
    {
        "id": "0801.4423",
        "submitter": "Manas Tungare",
        "authors": "Manas Tungare, Manuel Perez-Quinones",
        "title": "It's Not What You Have, But How You Use It: Compromises in Mobile Device\n  Use",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": null,
        "abstract": "  As users begin to use many more devices for personal information management\n(PIM) than just the traditional desktop computer, it is essential for HCI\nresearchers to understand how these devices are being used in the wild and\ntheir roles in users' information environments. We conducted a study of 220\nknowledge workers about their devices, the activities they performed on each,\nand the groups of devices used together. Our findings indicate that several\ndevices are often used in groups; integrated multi-function portable devices\nhave begun to replace single-function devices for communication (e.g. email and\nIM). Users use certain features opportunistically because they happen to be\ncarrying a multi-function device with them. The use of multiple devices and\nmulti-function devices is fraught with compromises as users must choose and\nmake trade-offs among various factors.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 29 Jan 2008 04:42:26 GMT"
            }
        ],
        "update_date": "2008-01-30",
        "authors_parsed": [
            [
                "Tungare",
                "Manas",
                ""
            ],
            [
                "Perez-Quinones",
                "Manuel",
                ""
            ]
        ]
    },
    {
        "id": "0801.4592",
        "submitter": "Yue Wang",
        "authors": "Yue Wang, John C. S. Lui, Dah-Ming Chiu",
        "title": "Understanding the Paradoxical Effects of Power Control on the Capacity\n  of Wireless Networks",
        "comments": "I refined the previous version in many places, including the title.\n  to appear in IEEE Transactions on Wireless Communications",
        "journal-ref": null,
        "doi": "10.1109/T-WC.2009.080142",
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recent works show conflicting results: network capacity may increase or\ndecrease with higher transmission power under different scenarios. In this\nwork, we want to understand this paradox. Specifically, we address the\nfollowing questions: (1)Theoretically, should we increase or decrease\ntransmission power to maximize network capacity? (2) Theoretically, how much\nnetwork capacity gain can we achieve by power control? (3) Under realistic\nsituations, how do power control, link scheduling and routing interact with\neach other? Under which scenarios can we expect a large capacity gain by using\nhigher transmission power? To answer these questions, firstly, we prove that\nthe optimal network capacity is a non-decreasing function of transmission\npower. Secondly, we prove that the optimal network capacity can be increased\nunlimitedly by higher transmission power in some network configurations.\nHowever, when nodes are distributed uniformly, the gain of optimal network\ncapacity by higher transmission power is upper-bounded by a positive constant.\nThirdly, we discuss why network capacity in practice may increase or decrease\nwith higher transmission power under different scenarios using carrier sensing\nand the minimum hop-count routing. Extensive simulations are carried out to\nverify our analysis.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 30 Jan 2008 02:51:11 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 25 Sep 2008 04:03:33 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Wang",
                "Yue",
                ""
            ],
            [
                "Lui",
                "John C. S.",
                ""
            ],
            [
                "Chiu",
                "Dah-Ming",
                ""
            ]
        ]
    },
    {
        "id": "0801.4774",
        "submitter": "Grenville Croll",
        "authors": "Thomas A. Grossman",
        "title": "Source Code Protection for Applications Written in Microsoft Excel and\n  Google Spreadsheet",
        "comments": "11 pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2007 81-91 ISBN\n  978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  Spreadsheets are used to develop application software that is distributed to\nusers. Unfortunately, the users often have the ability to change the\nprogramming statements (\"source code\") of the spreadsheet application. This\ncauses a host of problems. By critically examining the suitability of\nspreadsheet computer programming languages for application development, six\n\"application development features\" are identified, with source code protection\nbeing the most important. We investigate the status of these features and\ndiscuss how they might be implemented in the dominant Microsoft Excel\nspreadsheet and in the new Google Spreadsheet. Although Google Spreadsheet\ncurrently provides no source code control, its web-centric delivery model\noffers technical advantages for future provision of a rich set of features.\nExcel has a number of tools that can be combined to provide \"pretty good\nprotection\" of source code, but weak passwords reduce its robustness. User\naccess to Excel source code must be considered a programmer choice rather than\nan attribute of the spreadsheet.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 30 Jan 2008 21:35:17 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Grossman",
                "Thomas A.",
                ""
            ]
        ]
    },
    {
        "id": "0801.4775",
        "submitter": "Grenville Croll",
        "authors": "Harmen Ettema, Paul Janssen, Jacques de Swart",
        "title": "Spreadsheet Assurance by \"Control Around\" is a Viable Alternative to the\n  Traditional Approach",
        "comments": "9 pages, one colour diagram and a client case study",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2001 107-116 ISBN:1\n  86166 179 7",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  The traditional approach to spreadsheet auditing generally consists of\nauditing every distinct formula within a spreadsheet. Although tools are\ndeveloped to support auditors during this process, the approach is still very\ntime consuming and therefore relatively expensive. As an alternative to the\ntraditional \"control through\" approach, this paper discusses a \"control around\"\napproach. Within the proposed approach not all distinct formulas are audited\nseparately, but the relationship between input data and output data of a\nspreadsheet is audited through comparison with a shadow model developed in a\nmodelling language. Differences between the two models then imply possible\nerrors in the spreadsheet. This paper describes relevant issues regarding the\n\"control around\" approach and the circumstances in which this approach is\npreferred above a traditional spreadsheet audit approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 30 Jan 2008 21:53:43 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Ettema",
                "Harmen",
                ""
            ],
            [
                "Janssen",
                "Paul",
                ""
            ],
            [
                "de Swart",
                "Jacques",
                ""
            ]
        ]
    },
    {
        "id": "0801.4790",
        "submitter": "Joel Ratsaby",
        "authors": "Joel Ratsaby",
        "title": "Information Width",
        "comments": "Typo error in eq. (13)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DM cs.IT cs.LG math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Kolmogorov argued that the concept of information exists also in problems\nwith no underlying stochastic model (as Shannon's information representation)\nfor instance, the information contained in an algorithm or in the genome. He\nintroduced a combinatorial notion of entropy and information $I(x:\\sy)$\nconveyed by a binary string $x$ about the unknown value of a variable $\\sy$.\nThe current paper poses the following questions: what is the relationship\nbetween the information conveyed by $x$ about $\\sy$ to the description\ncomplexity of $x$ ? is there a notion of cost of information ? are there limits\non how efficient $x$ conveys information ?\n  To answer these questions Kolmogorov's definition is extended and a new\nconcept termed {\\em information width} which is similar to $n$-widths in\napproximation theory is introduced. Information of any input source, e.g.,\nsample-based, general side-information or a hybrid of both can be evaluated by\na single common formula. An application to the space of binary functions is\nconsidered.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 30 Jan 2008 22:49:57 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 1 Jul 2008 09:46:33 GMT"
            }
        ],
        "update_date": "2008-07-01",
        "authors_parsed": [
            [
                "Ratsaby",
                "Joel",
                ""
            ]
        ]
    },
    {
        "id": "0801.4794",
        "submitter": "Joel Ratsaby",
        "authors": "Joel Ratsaby",
        "title": "On the Complexity of Binary Samples",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DM cs.AI cs.LG",
        "license": null,
        "abstract": "  Consider a class $\\mH$ of binary functions $h: X\\to\\{-1, +1\\}$ on a finite\ninterval $X=[0, B]\\subset \\Real$. Define the {\\em sample width} of $h$ on a\nfinite subset (a sample) $S\\subset X$ as $\\w_S(h) \\equiv \\min_{x\\in S}\n|\\w_h(x)|$, where $\\w_h(x) = h(x) \\max\\{a\\geq 0: h(z)=h(x), x-a\\leq z\\leq\nx+a\\}$. Let $\\mathbb{S}_\\ell$ be the space of all samples in $X$ of cardinality\n$\\ell$ and consider sets of wide samples, i.e., {\\em hypersets} which are\ndefined as $A_{\\beta, h} = \\{S\\in \\mathbb{S}_\\ell: \\w_{S}(h) \\geq \\beta\\}$.\nThrough an application of the Sauer-Shelah result on the density of sets an\nupper estimate is obtained on the growth function (or trace) of the class\n$\\{A_{\\beta, h}: h\\in\\mH\\}$, $\\beta>0$, i.e., on the number of possible\ndichotomies obtained by intersecting all hypersets with a fixed collection of\nsamples $S\\in\\mathbb{S}_\\ell$ of cardinality $m$. The estimate is\n$2\\sum_{i=0}^{2\\lfloor B/(2\\beta)\\rfloor}{m-\\ell\\choose i}$.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 30 Jan 2008 23:14:19 GMT"
            }
        ],
        "update_date": "2008-02-01",
        "authors_parsed": [
            [
                "Ratsaby",
                "Joel",
                ""
            ]
        ]
    },
    {
        "id": "0801.4802",
        "submitter": "Grenville Croll",
        "authors": "Alan Rust, Brian Bishop, Kevin McDaid",
        "title": "Investigating the Potential of Test-Driven Development for Spreadsheet\n  Engineering",
        "comments": "11 pages, 5 colour figures, 2 case studies",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2006 95-105\n  ISBN:1-905617-08-9",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  It is widely documented that the absence of a structured approach to\nspreadsheet engineering is a key factor in the high level of spreadsheet\nerrors. In this paper we propose and investigate the application of Test-Driven\nDevelopment to the creation of spreadsheets. Test-Driven Development is an\nemerging development technique in software engineering that has been shown to\nresult in better quality software code. It has also been shown that this code\nrequires less testing and is easier to maintain. Through a pair of case studies\nwe demonstrate that Test-Driven Development can be applied to the development\nof spreadsheets. We present the detail of these studies preceded by a clear\nexplanation of the technique and its application to spreadsheet engineering. A\nsupporting tool under development by the authors is also documented along with\nproposed research to determine the effectiveness of the methodology and the\nassociated tool.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 31 Jan 2008 00:39:38 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Rust",
                "Alan",
                ""
            ],
            [
                "Bishop",
                "Brian",
                ""
            ],
            [
                "McDaid",
                "Kevin",
                ""
            ]
        ]
    },
    {
        "id": "0801.4807",
        "submitter": "Syed Ali Jafri",
        "authors": "Syed Ali Raza Jafri, Mireille Boutin, and Edward J. Delp",
        "title": "Automatic Text Area Segmentation in Natural Images",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": null,
        "abstract": "  We present a hierarchical method for segmenting text areas in natural images.\nThe method assumes that the text is written with a contrasting color on a more\nor less uniform background. But no assumption is made regarding the language or\ncharacter set used to write the text. In particular, the text can contain\nsimple graphics or symbols. The key feature of our approach is that we first\nconcentrate on finding the background of the text, before testing whether there\nis actually text on the background. Since uniform areas are easy to find in\nnatural images, and since text backgrounds define areas which contain \"holes\"\n(where the text is written) we thus look for uniform areas containing \"holes\"\nand label them as text backgrounds candidates. Each candidate area is then\nfurther tested for the presence of text within its convex hull. We tested our\nmethod on a database of 65 images including English and Urdu text. The method\ncorrectly segmented all the text areas in 63 of these images, and in only 4 of\nthese were areas that do not contain text also segmented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 31 Jan 2008 01:46:32 GMT"
            }
        ],
        "update_date": "2008-02-01",
        "authors_parsed": [
            [
                "Jafri",
                "Syed Ali Raza",
                ""
            ],
            [
                "Boutin",
                "Mireille",
                ""
            ],
            [
                "Delp",
                "Edward J.",
                ""
            ]
        ]
    },
    {
        "id": "0802.0137",
        "submitter": "Pierre Sutra",
        "authors": "Pierre Sutra (INRIA Rocquencourt), Marc Shapiro (INRIA Rocquencourt)",
        "title": "Fault-Tolerant Partial Replication in Large-Scale Database Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "RR-6440",
        "categories": "cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate a decentralised approach to committing transactions in a\nreplicated database, under partial replication. Previous protocols either\nre-execute transactions entirely and/or compute a total order of transactions.\nIn contrast, ours applies update values, and orders only conflicting\ntransactions. It results that transactions execute faster, and distributed\ndatabases commit in small committees. Both effects contribute to preserve\nscalability as the number of databases and transactions increase. Our algorithm\nensures serializability, and is live and safe in spite of faults.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 1 Feb 2008 14:47:24 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 4 Feb 2008 16:47:09 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 31 Mar 2009 14:41:43 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Sutra",
                "Pierre",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Shapiro",
                "Marc",
                "",
                "INRIA Rocquencourt"
            ]
        ]
    },
    {
        "id": "0802.0212",
        "submitter": "Christine C\\'ordula Dantas",
        "authors": "Miriam C. B. Alves, Christine C. Dantas, Nanci N. Arai, Rovedy B. da\n  Silva (Institute of Aeronautics and Space - IAE/CTA, Brazil)",
        "title": "A topological formal treatment for scenario-based software specification\n  of concurrent real-time systems",
        "comments": "20th International Conference on Software and Systems Engineering and\n  their Applications, Conservatoire des Arts & Metiers, Paris, France, 4-6\n  December 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.LO",
        "license": null,
        "abstract": "  Real-time systems are computing systems in which the meeting of their\nrequirements is vital for their correctness. Consequently, if the real-time\nrequirements of these systems are poorly understood and verified, the results\ncan be disastrous and lead to irremediable project failures at the early phases\nof development. The present work addresses the problem of detecting deadlock\nsituations early in the requirements specification phase of a concurrent real\ntime system, proposing a simple proof-of-concepts prototype that joins\nscenario-based requirements specifications and techniques based on topology.\nThe efforts are concentrated in the integration of the formal representation of\nMessage Sequence Chart scenarios into the deadlock detection algorithm of\nFajstrup et al., based on geometric and algebraic topology.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 1 Feb 2008 22:12:47 GMT"
            }
        ],
        "update_date": "2008-02-05",
        "authors_parsed": [
            [
                "Alves",
                "Miriam C. B.",
                "",
                "Institute of Aeronautics and Space - IAE/CTA, Brazil"
            ],
            [
                "Dantas",
                "Christine C.",
                "",
                "Institute of Aeronautics and Space - IAE/CTA, Brazil"
            ],
            [
                "Arai",
                "Nanci N.",
                "",
                "Institute of Aeronautics and Space - IAE/CTA, Brazil"
            ],
            [
                "da Silva",
                "Rovedy B.",
                "",
                "Institute of Aeronautics and Space - IAE/CTA, Brazil"
            ]
        ]
    },
    {
        "id": "0802.0249",
        "submitter": "Gerard Henry Edmond Duchamp",
        "authors": "G. H. E. Duchamp (LIPN), P. Blasiak (IFJ-Pan), A. Horzela (IFJ-Pan),\n  K. A. Penson (LPTMC), A. I. Solomon",
        "title": "Hopf Algebras in General and in Combinatorial Physics: a practical\n  introduction",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "quant-ph cs.SC math.CO",
        "license": null,
        "abstract": "  This tutorial is intended to give an accessible introduction to Hopf\nalgebras. The mathematical context is that of representation theory, and we\nalso illustrate the structures with examples taken from combinatorics and\nquantum physics, showing that in this latter case the axioms of Hopf algebra\narise naturally. The text contains many exercises, some taken from physics,\naimed at expanding and exemplifying the concepts introduced.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 2 Feb 2008 15:06:41 GMT"
            }
        ],
        "update_date": "2008-02-09",
        "authors_parsed": [
            [
                "Duchamp",
                "G. H. E.",
                "",
                "LIPN"
            ],
            [
                "Blasiak",
                "P.",
                "",
                "IFJ-Pan"
            ],
            [
                "Horzela",
                "A.",
                "",
                "IFJ-Pan"
            ],
            [
                "Penson",
                "K. A.",
                "",
                "LPTMC"
            ],
            [
                "Solomon",
                "A. I.",
                ""
            ]
        ]
    },
    {
        "id": "0802.0251",
        "submitter": "Fabrice Rossi",
        "authors": "Fabrice Rossi (INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE),\n  Brieuc Conan-Guez (INRIA Rocquencourt / INRIA Sophia Antipolis, LITA)",
        "title": "Multi-Layer Perceptrons and Symbolic Data",
        "comments": null,
        "journal-ref": "Symbolic Data Analysis and the SODAS Software Wiley (Ed.) (2008)\n  373-391",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  In some real world situations, linear models are not sufficient to represent\naccurately complex relations between input variables and output variables of a\nstudied system. Multilayer Perceptrons are one of the most successful\nnon-linear regression tool but they are unfortunately restricted to inputs and\noutputs that belong to a normed vector space. In this chapter, we propose a\ngeneral recoding method that allows to use symbolic data both as inputs and\noutputs to Multilayer Perceptrons. The recoding is quite simple to implement\nand yet provides a flexible framework that allows to deal with almost all\npractical cases. The proposed method is illustrated on a real world data set.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 2 Feb 2008 15:09:42 GMT"
            }
        ],
        "update_date": "2008-02-05",
        "authors_parsed": [
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE"
            ],
            [
                "Conan-Guez",
                "Brieuc",
                "",
                "INRIA Rocquencourt / INRIA Sophia Antipolis, LITA"
            ]
        ]
    },
    {
        "id": "0802.0252",
        "submitter": "Fabrice Rossi",
        "authors": "Brieuc Conan-Guez (LITA), Fabrice Rossi (INRIA Rocquencourt / INRIA\n  Sophia Antipolis)",
        "title": "Acc\\'el\\'eration des cartes auto-organisatrices sur tableau de\n  dissimilarit\\'es par s\\'eparation et \\'evaluation",
        "comments": "A para\\^itre",
        "journal-ref": "REVUE DES NOUVELLES TECHNOLOGIES DE L'INFORMATION (2008)",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  In this paper, a new implementation of the adaptation of Kohonen\nself-organising maps (SOM) to dissimilarity matrices is proposed. This\nimplementation relies on the branch and bound principle to reduce the algorithm\nrunning time. An important property of this new approach is that the obtained\nalgorithm produces exactly the same results as the standard algorithm.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 2 Feb 2008 15:10:35 GMT"
            }
        ],
        "update_date": "2008-02-05",
        "authors_parsed": [
            [
                "Conan-Guez",
                "Brieuc",
                "",
                "LITA"
            ],
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt / INRIA\n  Sophia Antipolis"
            ]
        ]
    },
    {
        "id": "0802.0287",
        "submitter": "Fabrice Rossi",
        "authors": "Catherine Krier (DICE), Fabrice Rossi (INRIA Rocquencourt / INRIA\n  Sophia Antipolis), Damien Fran\\c{c}ois (CESAME), Michel Verleysen (DICE -\n  MLG)",
        "title": "A data-driven functional projection approach for the selection of\n  feature ranges in spectra with ICA or cluster analysis",
        "comments": "A paraitre",
        "journal-ref": "Chemometrics and Intelligent Laboratory Systems (2008)",
        "doi": "10.1016/j.chemolab.2007.09.004",
        "report-no": null,
        "categories": "cs.NE",
        "license": null,
        "abstract": "  Prediction problems from spectra are largely encountered in chemometry. In\naddition to accurate predictions, it is often needed to extract information\nabout which wavelengths in the spectra contribute in an effective way to the\nquality of the prediction. This implies to select wavelengths (or wavelength\nintervals), a problem associated to variable selection. In this paper, it is\nshown how this problem may be tackled in the specific case of smooth (for\nexample infrared) spectra. The functional character of the spectra (their\nsmoothness) is taken into account through a functional variable projection\nprocedure. Contrarily to standard approaches, the projection is performed on a\nbasis that is driven by the spectra themselves, in order to best fit their\ncharacteristics. The methodology is illustrated by two examples of functional\nprojection, using Independent Component Analysis and functional variable\nclustering, respectively. The performances on two standard infrared spectra\nbenchmarks are illustrated.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 3 Feb 2008 19:02:49 GMT"
            }
        ],
        "update_date": "2008-02-05",
        "authors_parsed": [
            [
                "Krier",
                "Catherine",
                "",
                "DICE"
            ],
            [
                "Rossi",
                "Fabrice",
                "",
                "INRIA Rocquencourt / INRIA\n  Sophia Antipolis"
            ],
            [
                "Fran\u00e7ois",
                "Damien",
                "",
                "CESAME"
            ],
            [
                "Verleysen",
                "Michel",
                "",
                "DICE -\n  MLG"
            ]
        ]
    },
    {
        "id": "0802.0487",
        "submitter": "Marius Zimand",
        "authors": "Cristian Calude, Marius Zimand",
        "title": "Algorithmically independent sequences",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.SE math.AG math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Two objects are independent if they do not affect each other. Independence is\nwell-understood in classical information theory, but less in algorithmic\ninformation theory. Working in the framework of algorithmic information theory,\nthe paper proposes two types of independence for arbitrary infinite binary\nsequences and studies their properties. Our two proposed notions of\nindependence have some of the intuitive properties that one naturally expects.\nFor example, for every sequence $x$, the set of sequences that are independent\n(in the weaker of the two senses) with $x$ has measure one. For both notions of\nindependence we investigate to what extent pairs of independent sequences, can\nbe effectively constructed via Turing reductions (from one or more input\nsequences). In this respect, we prove several impossibility results. For\nexample, it is shown that there is no effective way of producing from an\narbitrary sequence with positive constructive Hausdorff dimension two sequences\nthat are independent (even in the weaker type of independence) and have\nsuper-logarithmic complexity. Finally, a few conjectures and open questions are\ndiscussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 4 Feb 2008 20:32:07 GMT"
            }
        ],
        "update_date": "2008-02-05",
        "authors_parsed": [
            [
                "Calude",
                "Cristian",
                ""
            ],
            [
                "Zimand",
                "Marius",
                ""
            ]
        ]
    },
    {
        "id": "0802.0820",
        "submitter": "Jonathan Hayman",
        "authors": "Jonathan Hayman and Glynn Winskel",
        "title": "Independence and concurrent separation logic",
        "comments": null,
        "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 1 (March 19,\n  2008) lmcs:1100",
        "doi": "10.2168/LMCS-4(1:6)2008",
        "report-no": null,
        "categories": "cs.LO cs.PL",
        "license": null,
        "abstract": "  A compositional Petri net-based semantics is given to a simple language\nallowing pointer manipulation and parallelism. The model is then applied to\ngive a notion of validity to the judgements made by concurrent separation logic\nthat emphasizes the process-environment duality inherent in such rely-guarantee\nreasoning. Soundness of the rules of concurrent separation logic with respect\nto this definition of validity is shown. The independence information retained\nby the Petri net model is then exploited to characterize the independence of\nparallel processes enforced by the logic. This is shown to permit a refinement\noperation capable of changing the granularity of atomic actions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 6 Feb 2008 15:39:20 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 19 Mar 2008 15:26:51 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Hayman",
                "Jonathan",
                ""
            ],
            [
                "Winskel",
                "Glynn",
                ""
            ]
        ]
    },
    {
        "id": "0802.0861",
        "submitter": "Paul Gazis",
        "authors": "Paul R. Gazis and Jeffrey D. Scargle",
        "title": "Using Bayesian Blocks to Partition Self-Organizing Maps",
        "comments": "9 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Self organizing maps (SOMs) are widely-used for unsupervised classification.\nFor this application, they must be combined with some partitioning scheme that\ncan identify boundaries between distinct regions in the maps they produce. We\ndiscuss a novel partitioning scheme for SOMs based on the Bayesian Blocks\nsegmentation algorithm of Scargle [1998]. This algorithm minimizes a cost\nfunction to identify contiguous regions over which the values of the attributes\ncan be represented as approximately constant. Because this cost function is\nwell-defined and largely independent of assumptions regarding the number and\nstructure of clusters in the original sample space, this partitioning scheme\noffers significant advantages over many conventional methods. Sample code is\navailable.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 6 Feb 2008 18:50:16 GMT"
            }
        ],
        "update_date": "2008-02-07",
        "authors_parsed": [
            [
                "Gazis",
                "Paul R.",
                ""
            ],
            [
                "Scargle",
                "Jeffrey D.",
                ""
            ]
        ]
    },
    {
        "id": "0802.0914",
        "submitter": "Sebastian Roch",
        "authors": "Elchanan Mossel and Sebastien Roch and Mike Steel",
        "title": "Shrinkage Effect in Ancestral Maximum Likelihood",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-bio.PE cs.CE math.PR math.ST stat.TH",
        "license": null,
        "abstract": "  Ancestral maximum likelihood (AML) is a method that simultaneously\nreconstructs a phylogenetic tree and ancestral sequences from extant data\n(sequences at the leaves). The tree and ancestral sequences maximize the\nprobability of observing the given data under a Markov model of sequence\nevolution, in which branch lengths are also optimized but constrained to take\nthe same value on any edge across all sequence sites. AML differs from the more\nusual form of maximum likelihood (ML) in phylogenetics because ML averages over\nall possible ancestral sequences. ML has long been known to be statistically\nconsistent -- that is, it converges on the correct tree with probability\napproaching 1 as the sequence length grows. However, the statistical\nconsistency of AML has not been formally determined, despite informal remarks\nin a literature that dates back 20 years. In this short note we prove a general\nresult that implies that AML is statistically inconsistent. In particular we\nshow that AML can `shrink' short edges in a tree, resulting in a tree that has\nno internal resolution as the sequence length grows. Our results apply to any\nnumber of taxa.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Feb 2008 06:52:44 GMT"
            }
        ],
        "update_date": "2017-07-24",
        "authors_parsed": [
            [
                "Mossel",
                "Elchanan",
                ""
            ],
            [
                "Roch",
                "Sebastien",
                ""
            ],
            [
                "Steel",
                "Mike",
                ""
            ]
        ]
    },
    {
        "id": "0802.1002",
        "submitter": "Xavier Bry",
        "authors": "Xavier Bry (I3M)",
        "title": "New Estimation Procedures for PLS Path Modelling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": null,
        "abstract": "  Given R groups of numerical variables X1, ... XR, we assume that each group\nis the result of one underlying latent variable, and that all latent variables\nare bound together through a linear equation system. Moreover, we assume that\nsome explanatory latent variables may interact pairwise in one or more\nequations. We basically consider PLS Path Modelling's algorithm to estimate\nboth latent variables and the model's coefficients. New \"external\" estimation\nschemes are proposed that draw latent variables towards strong group structures\nin a more flexible way. New \"internal\" estimation schemes are proposed to\nenable PLSPM to make good use of variable group complementarity and to deal\nwith interactions. Application examples are given.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Feb 2008 15:18:27 GMT"
            }
        ],
        "update_date": "2008-02-08",
        "authors_parsed": [
            [
                "Bry",
                "Xavier",
                "",
                "I3M"
            ]
        ]
    },
    {
        "id": "0802.1026",
        "submitter": "Benjamin Sach Mr",
        "authors": "Benjamin Sach and Rapha\\\"el Clifford",
        "title": "An Empirical Study of Cache-Oblivious Priority Queues and their\n  Application to the Shortest Path Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.SE",
        "license": null,
        "abstract": "  In recent years the Cache-Oblivious model of external memory computation has\nprovided an attractive theoretical basis for the analysis of algorithms on\nmassive datasets. Much progress has been made in discovering algorithms that\nare asymptotically optimal or near optimal. However, to date there are still\nrelatively few successful experimental studies. In this paper we compare two\ndifferent Cache-Oblivious priority queues based on the Funnel and Bucket Heap\nand apply them to the single source shortest path problem on graphs with\npositive edge weights. Our results show that when RAM is limited and data is\nswapping to external storage, the Cache-Oblivious priority queues achieve\norders of magnitude speedups over standard internal memory techniques. However,\nfor the single source shortest path problem both on simulated and real world\ngraph data, these speedups are markedly lower due to the time required to\naccess the graph adjacency list itself.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Feb 2008 18:02:11 GMT"
            }
        ],
        "update_date": "2008-02-08",
        "authors_parsed": [
            [
                "Sach",
                "Benjamin",
                ""
            ],
            [
                "Clifford",
                "Rapha\u00ebl",
                ""
            ]
        ]
    },
    {
        "id": "0802.1123",
        "submitter": "Sebastien Tixeuil",
        "authors": "Sylvie Dela\\\"et (LRI), St\\'ephane Devismes (LRI), Mikhail Nesterenko,\n  S\\'ebastien Tixeuil (INRIA Futurs, LIP6)",
        "title": "Snap-Stabilization in Message-Passing Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.NI cs.PF",
        "license": null,
        "abstract": "  In this paper, we tackle the open problem of snap-stabilization in\nmessage-passing systems. Snap-stabilization is a nice approach to design\nprotocols that withstand transient faults. Compared to the well-known\nself-stabilizing approach, snap-stabilization guarantees that the effect of\nfaults is contained immediately after faults cease to occur. Our contribution\nis twofold: we show that (1) snap-stabilization is impossible for a wide class\nof problems if we consider networks with finite yet unbounded channel capacity;\n(2) snap-stabilization becomes possible in the same setting if we assume\nbounded-capacity channels. We propose three snap-stabilizing protocols working\nin fully-connected networks. Our work opens exciting new research perspectives,\nas it enables the snap-stabilizing paradigm to be implemented in actual\nnetworks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Feb 2008 10:51:24 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 11 Feb 2008 08:57:19 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Dela\u00ebt",
                "Sylvie",
                "",
                "LRI"
            ],
            [
                "Devismes",
                "St\u00e9phane",
                "",
                "LRI"
            ],
            [
                "Nesterenko",
                "Mikhail",
                "",
                "INRIA Futurs, LIP6"
            ],
            [
                "Tixeuil",
                "S\u00e9bastien",
                "",
                "INRIA Futurs, LIP6"
            ]
        ]
    },
    {
        "id": "0802.1162",
        "submitter": "Gerard Henry Edmond Duchamp",
        "authors": "H. Cheballah (LIPN), G. H. E. Duchamp (LIPN), K. A. Penson (LPTMC)",
        "title": "Approximate substitutions and the normal ordering problem",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1088/1742-6596/104/1/012031",
        "report-no": null,
        "categories": "quant-ph cs.SC math.CO",
        "license": null,
        "abstract": "  In this paper, we show that the infinite generalised Stirling matrices\nassociated with boson strings with one annihilation operator are projective\nlimits of approximate substitutions, the latter being characterised by a finite\nset of algebraic equations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Feb 2008 14:52:52 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Cheballah",
                "H.",
                "",
                "LIPN"
            ],
            [
                "Duchamp",
                "G. H. E.",
                "",
                "LIPN"
            ],
            [
                "Penson",
                "K. A.",
                "",
                "LPTMC"
            ]
        ]
    },
    {
        "id": "0802.1176",
        "submitter": "Thomas Begin",
        "authors": "Thomas Begin (LIP6), Alexandre Brandwajn (UCSC)",
        "title": "Note sur les temps de service r\\'esiduels dans les syst\\`emes type M/G/c",
        "comments": null,
        "journal-ref": "Colloque Francophone sur l'Ing\\'enierie des Protocoles (CFIP), Les\n  Arcs : France (2008)",
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": null,
        "abstract": "  Approximations for the mean performance indices for the M/G/c queue rely on\nthe approximate computation of the probability that an arriving request has to\nwait for service and of the minimum of residual service times if all servers\nare found busy. Using numerical examples, we investigate properties of these\ntwo quantities. In particular, we show that the minimum of residual service\ntimes depends on higher order properties, beyond the first two moments, of the\nservice time distribution. Improved knowledge of the properties of the two\nquantities studied in this paper provides insight into avenues for improving\nthe accuracy of approximations for the M/G/c queue.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Feb 2008 16:40:26 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 14 Mar 2008 08:36:11 GMT"
            }
        ],
        "update_date": "2008-03-14",
        "authors_parsed": [
            [
                "Begin",
                "Thomas",
                "",
                "LIP6"
            ],
            [
                "Brandwajn",
                "Alexandre",
                "",
                "UCSC"
            ]
        ]
    },
    {
        "id": "0802.1244",
        "submitter": "Shuheng Zhou",
        "authors": "Shuheng Zhou",
        "title": "Learning Balanced Mixtures of Discrete Distributions with Small Sample",
        "comments": "24 Pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG stat.ML",
        "license": null,
        "abstract": "  We study the problem of partitioning a small sample of $n$ individuals from a\nmixture of $k$ product distributions over a Boolean cube $\\{0, 1\\}^K$ according\nto their distributions. Each distribution is described by a vector of allele\nfrequencies in $\\R^K$. Given two distributions, we use $\\gamma$ to denote the\naverage $\\ell_2^2$ distance in frequencies across $K$ dimensions, which\nmeasures the statistical divergence between them. We study the case assuming\nthat bits are independently distributed across $K$ dimensions. This work\ndemonstrates that, for a balanced input instance for $k = 2$, a certain\ngraph-based optimization function returns the correct partition with high\nprobability, where a weighted graph $G$ is formed over $n$ individuals, whose\npairwise hamming distances between their corresponding bit vectors define the\nedge weights, so long as $K = \\Omega(\\ln n/\\gamma)$ and $Kn = \\tilde\\Omega(\\ln\nn/\\gamma^2)$. The function computes a maximum-weight balanced cut of $G$, where\nthe weight of a cut is the sum of the weights across all edges in the cut. This\nresult demonstrates a nice property in the high-dimensional feature space: one\ncan trade off the number of features that are required with the size of the\nsample to accomplish certain tasks like clustering.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 10 Feb 2008 07:38:49 GMT"
            }
        ],
        "update_date": "2008-02-21",
        "authors_parsed": [
            [
                "Zhou",
                "Shuheng",
                ""
            ]
        ]
    },
    {
        "id": "0802.1258",
        "submitter": "Heng Lian",
        "authors": "Heng Lian",
        "title": "Bayesian Nonlinear Principal Component Analysis Using Random Fields",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel model for nonlinear dimension reduction motivated by the\nprobabilistic formulation of principal component analysis. Nonlinearity is\nachieved by specifying different transformation matrices at different locations\nof the latent space and smoothing the transformation using a Markov random\nfield type prior. The computation is made feasible by the recent advances in\nsampling from von Mises-Fisher distributions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 9 Feb 2008 12:22:47 GMT"
            }
        ],
        "update_date": "2008-02-12",
        "authors_parsed": [
            [
                "Lian",
                "Heng",
                ""
            ]
        ]
    },
    {
        "id": "0802.1258",
        "submitter": "Heng Lian",
        "authors": "Heng Lian",
        "title": "Bayesian Nonlinear Principal Component Analysis Using Random Fields",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a novel model for nonlinear dimension reduction motivated by the\nprobabilistic formulation of principal component analysis. Nonlinearity is\nachieved by specifying different transformation matrices at different locations\nof the latent space and smoothing the transformation using a Markov random\nfield type prior. The computation is made feasible by the recent advances in\nsampling from von Mises-Fisher distributions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 9 Feb 2008 12:22:47 GMT"
            }
        ],
        "update_date": "2008-02-12",
        "authors_parsed": [
            [
                "Lian",
                "Heng",
                ""
            ]
        ]
    },
    {
        "id": "0802.1274",
        "submitter": "Jose M. Martin-Garcia",
        "authors": "Jose M. Martin-Garcia, David Yllanes, Renato Portugal",
        "title": "The Invar tensor package: Differential invariants of Riemann",
        "comments": "12 pages, 1 figure, 3 tables. Package can be downloaded from\n  http://metric.iem.csic.es/Martin-Garcia/xAct/Invar/ (Mathematica version) or\n  http://www.lncc.br/~portugal/Invar.html (Maple version)",
        "journal-ref": "Comp.Phys.Commun.179:586-590,2008",
        "doi": "10.1016/j.cpc.2008.04.018",
        "report-no": null,
        "categories": "cs.SC gr-qc hep-th",
        "license": null,
        "abstract": "  The long standing problem of the relations among the scalar invariants of the\nRiemann tensor is computationally solved for all 6x10^23 objects with up to 12\nderivatives of the metric. This covers cases ranging from products of up to 6\nundifferentiated Riemann tensors to cases with up to 10 covariant derivatives\nof a single Riemann. We extend our computer algebra system Invar to produce\nwithin seconds a canonical form for any of those objects in terms of a basis.\nThe process is as follows: (1) an invariant is converted in real time into a\ncanonical form with respect to the permutation symmetries of the Riemann\ntensor; (2) Invar reads a database of more than 6x10^5 relations and applies\nthose coming from the cyclic symmetry of the Riemann tensor; (3) then applies\nthe relations coming from the Bianchi identity, (4) the relations coming from\ncommutations of covariant derivatives, (5) the dimensionally-dependent\nidentities for dimension 4, and finally (6) simplifies invariants that can be\nexpressed as product of dual invariants. Invar runs on top of the tensor\ncomputer algebra systems xTensor (for Mathematica) and Canon (for Maple).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 11 Feb 2008 16:38:05 GMT"
            }
        ],
        "update_date": "2008-11-26",
        "authors_parsed": [
            [
                "Martin-Garcia",
                "Jose M.",
                ""
            ],
            [
                "Yllanes",
                "David",
                ""
            ],
            [
                "Portugal",
                "Renato",
                ""
            ]
        ]
    },
    {
        "id": "0802.1412",
        "submitter": "Mahesh  Pal Dr.",
        "authors": "Mahesh Pal",
        "title": "Extreme Learning Machine for land cover classification",
        "comments": "6 pages, mapindia 2008 conference",
        "journal-ref": null,
        "doi": "10.1080/01431160902788636",
        "report-no": null,
        "categories": "cs.NE cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the potential of extreme learning machine based\nsupervised classification algorithm for land cover classification. In\ncomparison to a backpropagation neural network, which requires setting of\nseveral user-defined parameters and may produce local minima, extreme learning\nmachine require setting of one parameter and produce a unique solution. ETM+\nmultispectral data set (England) was used to judge the suitability of extreme\nlearning machine for remote sensing classifications. A back propagation neural\nnetwork was used to compare its performance in term of classification accuracy\nand computational cost. Results suggest that the extreme learning machine\nperform equally well to back propagation neural network in term of\nclassification accuracy with this data set. The computational cost using\nextreme learning machine is very small in comparison to back propagation neural\nnetwork.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 11 Feb 2008 11:12:06 GMT"
            }
        ],
        "update_date": "2019-07-02",
        "authors_parsed": [
            [
                "Pal",
                "Mahesh",
                ""
            ]
        ]
    },
    {
        "id": "0802.1412",
        "submitter": "Mahesh  Pal Dr.",
        "authors": "Mahesh Pal",
        "title": "Extreme Learning Machine for land cover classification",
        "comments": "6 pages, mapindia 2008 conference",
        "journal-ref": null,
        "doi": "10.1080/01431160902788636",
        "report-no": null,
        "categories": "cs.NE cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper explores the potential of extreme learning machine based\nsupervised classification algorithm for land cover classification. In\ncomparison to a backpropagation neural network, which requires setting of\nseveral user-defined parameters and may produce local minima, extreme learning\nmachine require setting of one parameter and produce a unique solution. ETM+\nmultispectral data set (England) was used to judge the suitability of extreme\nlearning machine for remote sensing classifications. A back propagation neural\nnetwork was used to compare its performance in term of classification accuracy\nand computational cost. Results suggest that the extreme learning machine\nperform equally well to back propagation neural network in term of\nclassification accuracy with this data set. The computational cost using\nextreme learning machine is very small in comparison to back propagation neural\nnetwork.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 11 Feb 2008 11:12:06 GMT"
            }
        ],
        "update_date": "2019-07-02",
        "authors_parsed": [
            [
                "Pal",
                "Mahesh",
                ""
            ]
        ]
    },
    {
        "id": "0802.1430",
        "submitter": "Francis Bach",
        "authors": "Jacob Abernethy, Francis Bach (INRIA Rocquencourt), Theodoros\n  Evgeniou, Jean-Philippe Vert (CB)",
        "title": "A New Approach to Collaborative Filtering: Operator Estimation with\n  Spectral Regularization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a general approach for collaborative filtering (CF) using spectral\nregularization to learn linear operators from \"users\" to the \"objects\" they\nrate. Recent low-rank type matrix completion approaches to CF are shown to be\nspecial cases. However, unlike existing regularization based CF methods, our\napproach can be used to also incorporate information such as attributes of the\nusers or the objects -- a limitation of existing regularization based CF\nmethods. We then provide novel representer theorems that we use to develop new\nestimation methods. We provide learning algorithms based on low-rank\ndecompositions, and test them on a standard CF dataset. The experiments\nindicate the advantages of generalizing the existing regularization based CF\nmethods to incorporate related information about users and objects. Finally, we\nshow that certain multi-task learning methods can be also seen as special cases\nof our proposed approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 11 Feb 2008 12:55:34 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 19 Dec 2008 14:05:14 GMT"
            }
        ],
        "update_date": "2008-12-19",
        "authors_parsed": [
            [
                "Abernethy",
                "Jacob",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Bach",
                "Francis",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Evgeniou",
                "Theodoros",
                "",
                "CB"
            ],
            [
                "Vert",
                "Jean-Philippe",
                "",
                "CB"
            ]
        ]
    },
    {
        "id": "0802.1578",
        "submitter": "Kees Middelburg",
        "authors": "J. A. Bergstra, C. A. Middelburg",
        "title": "Thread extraction for polyadic instruction sequences",
        "comments": "21 pages; error corrected; presentation improved",
        "journal-ref": "Scientific Annals of Computer Science, 21(2):283--310, 2011.\n  http://www.infoiasi.ro/bin/download/Annals/XXI2/XXI2_4.pdf",
        "doi": null,
        "report-no": "PRG0803",
        "categories": "cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we study the phenomenon that instruction sequences are split\ninto fragments which somehow produce a joint behaviour. In order to bring this\nphenomenon better into the picture, we formalize a simple mechanism by which\nseveral instruction sequence fragments can produce a joint behaviour. We also\nshow that, even in the case of this simple mechanism, it is a non-trivial\nmatter to explain by means of a translation into a single instruction sequence\nwhat takes place on execution of a collection of instruction sequence\nfragments.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 12 Feb 2008 07:49:27 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 28 Jan 2009 08:59:16 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 28 Jul 2009 07:07:45 GMT"
            }
        ],
        "update_date": "2012-11-20",
        "authors_parsed": [
            [
                "Bergstra",
                "J. A.",
                ""
            ],
            [
                "Middelburg",
                "C. A.",
                ""
            ]
        ]
    },
    {
        "id": "0802.1586",
        "submitter": "Mark Burgess",
        "authors": "Demissies Aredo, Mark Burgess and Simen Hagen",
        "title": "Program Promises",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": null,
        "abstract": "  The framework of promise theory offers an alternative way of understanding\nprogramming models, especially in distributed systems. We show that promise\ntheory can express some familiar constructs and resolve some problems in\nprogram interface design, using fewer and simpler concepts than the Unified\nModelling Language (UML).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 12 Feb 2008 08:40:51 GMT"
            }
        ],
        "update_date": "2008-02-13",
        "authors_parsed": [
            [
                "Aredo",
                "Demissies",
                ""
            ],
            [
                "Burgess",
                "Mark",
                ""
            ],
            [
                "Hagen",
                "Simen",
                ""
            ]
        ]
    },
    {
        "id": "0802.2001",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin and Kathryn Dowsland",
        "title": "Exploiting problem structure in a genetic algorithm approach to a nurse\n  rostering problem",
        "comments": null,
        "journal-ref": "Journal of Scheduling, 3(3), pp 139-153, 2000",
        "doi": "10.1002/(SICI)1099-1425(200005/06)3:3<139::AID-JOS41>3.0.CO;2-2",
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": null,
        "abstract": "  There is considerable interest in the use of genetic algorithms to solve\nproblems arising in the areas of scheduling and timetabling. However, the\nclassical genetic algorithm paradigm is not well equipped to handle the\nconflict between objectives and constraints that typically occurs in such\nproblems. In order to overcome this, successful implementations frequently make\nuse of problem specific knowledge. This paper is concerned with the development\nof a GA for a nurse rostering problem at a major UK hospital. The structure of\nthe constraints is used as the basis for a co-evolutionary strategy using\nco-operating sub-populations. Problem specific knowledge is also used to define\na system of incentives and disincentives, and a complementary mutation\noperator. Empirical results based on 52 weeks of live data show how these\nfeatures are able to improve an unsuccessful canonical GA to the point where it\nis able to provide a practical solution to the problem\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 14 Feb 2008 11:25:37 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 16:56:56 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 16 May 2008 10:44:23 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Dowsland",
                "Kathryn",
                ""
            ]
        ]
    },
    {
        "id": "0802.2001",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin and Kathryn Dowsland",
        "title": "Exploiting problem structure in a genetic algorithm approach to a nurse\n  rostering problem",
        "comments": null,
        "journal-ref": "Journal of Scheduling, 3(3), pp 139-153, 2000",
        "doi": "10.1002/(SICI)1099-1425(200005/06)3:3<139::AID-JOS41>3.0.CO;2-2",
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": null,
        "abstract": "  There is considerable interest in the use of genetic algorithms to solve\nproblems arising in the areas of scheduling and timetabling. However, the\nclassical genetic algorithm paradigm is not well equipped to handle the\nconflict between objectives and constraints that typically occurs in such\nproblems. In order to overcome this, successful implementations frequently make\nuse of problem specific knowledge. This paper is concerned with the development\nof a GA for a nurse rostering problem at a major UK hospital. The structure of\nthe constraints is used as the basis for a co-evolutionary strategy using\nco-operating sub-populations. Problem specific knowledge is also used to define\na system of incentives and disincentives, and a complementary mutation\noperator. Empirical results based on 52 weeks of live data show how these\nfeatures are able to improve an unsuccessful canonical GA to the point where it\nis able to provide a practical solution to the problem\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 14 Feb 2008 11:25:37 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 3 Mar 2008 16:56:56 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 16 May 2008 10:44:23 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Dowsland",
                "Kathryn",
                ""
            ]
        ]
    },
    {
        "id": "0802.2015",
        "submitter": "Steven de Rooij",
        "authors": "Wouter Koolen and Steven de Rooij",
        "title": "Combining Expert Advice Efficiently",
        "comments": "50 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.DS cs.IT math.IT",
        "license": null,
        "abstract": "  We show how models for prediction with expert advice can be defined concisely\nand clearly using hidden Markov models (HMMs); standard HMM algorithms can then\nbe used to efficiently calculate, among other things, how the expert\npredictions should be weighted according to the model. We cast many existing\nmodels as HMMs and recover the best known running times in each case. We also\ndescribe two new models: the switch distribution, which was recently developed\nto improve Bayesian/Minimum Description Length model selection, and a new\ngeneralisation of the fixed share algorithm based on run-length coding. We give\nloss bounds for all models and shed new light on their relationships.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 14 Feb 2008 14:54:57 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 15 Feb 2008 10:59:15 GMT"
            }
        ],
        "update_date": "2008-02-15",
        "authors_parsed": [
            [
                "Koolen",
                "Wouter",
                ""
            ],
            [
                "de Rooij",
                "Steven",
                ""
            ]
        ]
    },
    {
        "id": "0802.2027",
        "submitter": "Martin Ziegler",
        "authors": "Martin Ziegler and Wouter M. Koolen",
        "title": "Kolmogorov Complexity Theory over the Reals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CC cs.SC",
        "license": null,
        "abstract": "  Kolmogorov Complexity constitutes an integral part of computability theory,\ninformation theory, and computational complexity theory -- in the discrete\nsetting of bits and Turing machines. Over real numbers, on the other hand, the\nBSS-machine (aka real-RAM) has been established as a major model of\ncomputation. This real realm has turned out to exhibit natural counterparts to\nmany notions and results in classical complexity and recursion theory; although\nusually with considerably different proofs. The present work investigates\nsimilarities and differences between discrete and real Kolmogorov Complexity as\nintroduced by Montana and Pardo (1998).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 14 Feb 2008 18:30:55 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 28 Mar 2008 11:04:28 GMT"
            }
        ],
        "update_date": "2008-03-28",
        "authors_parsed": [
            [
                "Ziegler",
                "Martin",
                ""
            ],
            [
                "Koolen",
                "Wouter M.",
                ""
            ]
        ]
    },
    {
        "id": "0802.2138",
        "submitter": "Mahesh  Pal Dr.",
        "authors": "Mahesh Pal and Paul M. Mather",
        "title": "Support Vector classifiers for Land Cover Classification",
        "comments": "11 pages, 1 figure, Published in MapIndia Conference 2003",
        "journal-ref": null,
        "doi": "10.1080/01431160802007624",
        "report-no": null,
        "categories": "cs.NE cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Support vector machines represent a promising development in machine learning\nresearch that is not widely used within the remote sensing community. This\npaper reports the results of Multispectral(Landsat-7 ETM+) and Hyperspectral\nDAIS)data in which multi-class SVMs are compared with maximum likelihood and\nartificial neural network methods in terms of classification accuracy. Our\nresults show that the SVM achieves a higher level of classification accuracy\nthan either the maximum likelihood or the neural classifier, and that the\nsupport vector machine can be used with small training datasets and\nhigh-dimensional data.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 15 Feb 2008 04:53:33 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Pal",
                "Mahesh",
                ""
            ],
            [
                "Mather",
                "Paul M.",
                ""
            ]
        ]
    },
    {
        "id": "0802.2138",
        "submitter": "Mahesh  Pal Dr.",
        "authors": "Mahesh Pal and Paul M. Mather",
        "title": "Support Vector classifiers for Land Cover Classification",
        "comments": "11 pages, 1 figure, Published in MapIndia Conference 2003",
        "journal-ref": null,
        "doi": "10.1080/01431160802007624",
        "report-no": null,
        "categories": "cs.NE cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Support vector machines represent a promising development in machine learning\nresearch that is not widely used within the remote sensing community. This\npaper reports the results of Multispectral(Landsat-7 ETM+) and Hyperspectral\nDAIS)data in which multi-class SVMs are compared with maximum likelihood and\nartificial neural network methods in terms of classification accuracy. Our\nresults show that the SVM achieves a higher level of classification accuracy\nthan either the maximum likelihood or the neural classifier, and that the\nsupport vector machine can be used with small training datasets and\nhigh-dimensional data.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 15 Feb 2008 04:53:33 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Pal",
                "Mahesh",
                ""
            ],
            [
                "Mather",
                "Paul M.",
                ""
            ]
        ]
    },
    {
        "id": "0802.2158",
        "submitter": "Olivier Roustant",
        "authors": "Jessica Franco, Laurent Carraro, Olivier Roustant, Astrid Jourdan\n  (LMA-PAU)",
        "title": "A Radar-Shaped Statistic for Testing and Visualizing Uniformity\n  Properties in Computer Experiments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG math.ST stat.TH",
        "license": null,
        "abstract": "  In the study of computer codes, filling space as uniformly as possible is\nimportant to describe the complexity of the investigated phenomenon. However,\nthis property is not conserved by reducing the dimension. Some numeric\nexperiment designs are conceived in this sense as Latin hypercubes or\northogonal arrays, but they consider only the projections onto the axes or the\ncoordinate planes. In this article we introduce a statistic which allows\nstudying the good distribution of points according to all 1-dimensional\nprojections. By angularly scanning the domain, we obtain a radar type\nrepresentation, allowing the uniformity defects of a design to be identified\nwith respect to its projections onto straight lines. The advantages of this new\ntool are demonstrated on usual examples of space-filling designs (SFD) and a\nglobal statistic independent of the angle of rotation is studied.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 15 Feb 2008 09:06:25 GMT"
            }
        ],
        "update_date": "2008-02-19",
        "authors_parsed": [
            [
                "Franco",
                "Jessica",
                "",
                "LMA-PAU"
            ],
            [
                "Carraro",
                "Laurent",
                "",
                "LMA-PAU"
            ],
            [
                "Roustant",
                "Olivier",
                "",
                "LMA-PAU"
            ],
            [
                "Jourdan",
                "Astrid",
                "",
                "LMA-PAU"
            ]
        ]
    },
    {
        "id": "0802.2201",
        "submitter": "Murilo Baptista S.",
        "authors": "M. S. Baptista, C. Bohn, R. Kliegl, R. Engbert, J. Kurths",
        "title": "Reconstruction of eye movements during blinks",
        "comments": null,
        "journal-ref": "Chaos (2008)",
        "doi": "10.1063/1.2890843",
        "report-no": null,
        "categories": "cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In eye movement research in reading, the amount of data plays a crucial role\nfor the validation of results. A methodological problem for the analysis of the\neye movement in reading are blinks, when readers close their eyes. Blinking\nrate increases with increasing reading time, resulting in high data losses,\nespecially for older adults or reading impaired subjects. We present a method,\nbased on the symbolic sequence dynamics of the eye movements, that reconstructs\nthe horizontal position of the eyes while the reader blinks. The method makes\nuse of an observed fact that the movements of the eyes before closing or after\nopening contain information about the eyes movements during blinks. Test\nresults indicate that our reconstruction method is superior to methods that use\nsimpler interpolation approaches. In addition, analyses of the reconstructed\ndata show no significant deviation from the usual behavior observed in readers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 15 Feb 2008 13:37:27 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 13 Mar 2008 09:30:11 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Baptista",
                "M. S.",
                ""
            ],
            [
                "Bohn",
                "C.",
                ""
            ],
            [
                "Kliegl",
                "R.",
                ""
            ],
            [
                "Engbert",
                "R.",
                ""
            ],
            [
                "Kurths",
                "J.",
                ""
            ]
        ]
    },
    {
        "id": "0802.2258",
        "submitter": "Carlos Alberto Fernandez-y-Fernandez",
        "authors": "Anthony J. H. Simons, Carlos Alberto Fernandez-y-Fernandez",
        "title": "Using Alloy to model-check visual design notations",
        "comments": "8 pages",
        "journal-ref": "Simons, A.J.H. and Fernandez-y-Fernandez, C.A., Using Alloy to\n  model-check visual design notations. In Sixth Mexican Int. Conf. on C S,\n  (Mexico, 2005), IEEE, 121-128",
        "doi": "10.1109/ENC.2005.52",
        "report-no": null,
        "categories": "cs.SE cs.SC",
        "license": null,
        "abstract": "  This paper explores the process of validation for the abstract syntax of a\ngraphical notation. We define an unified specification for five of the UML\ndiagrams used by the Discovery Method and, in this document, we illustrate how\ndiagrams can be represented in Alloy and checked against our specification in\norder to know if these are valid under the Discovery notation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 15 Feb 2008 18:25:50 GMT"
            }
        ],
        "update_date": "2008-02-18",
        "authors_parsed": [
            [
                "Simons",
                "Anthony J. H.",
                ""
            ],
            [
                "Fernandez-y-Fernandez",
                "Carlos Alberto",
                ""
            ]
        ]
    },
    {
        "id": "0802.2258",
        "submitter": "Carlos Alberto Fernandez-y-Fernandez",
        "authors": "Anthony J. H. Simons, Carlos Alberto Fernandez-y-Fernandez",
        "title": "Using Alloy to model-check visual design notations",
        "comments": "8 pages",
        "journal-ref": "Simons, A.J.H. and Fernandez-y-Fernandez, C.A., Using Alloy to\n  model-check visual design notations. In Sixth Mexican Int. Conf. on C S,\n  (Mexico, 2005), IEEE, 121-128",
        "doi": "10.1109/ENC.2005.52",
        "report-no": null,
        "categories": "cs.SE cs.SC",
        "license": null,
        "abstract": "  This paper explores the process of validation for the abstract syntax of a\ngraphical notation. We define an unified specification for five of the UML\ndiagrams used by the Discovery Method and, in this document, we illustrate how\ndiagrams can be represented in Alloy and checked against our specification in\norder to know if these are valid under the Discovery notation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 15 Feb 2008 18:25:50 GMT"
            }
        ],
        "update_date": "2008-02-18",
        "authors_parsed": [
            [
                "Simons",
                "Anthony J. H.",
                ""
            ],
            [
                "Fernandez-y-Fernandez",
                "Carlos Alberto",
                ""
            ]
        ]
    },
    {
        "id": "0802.2305",
        "submitter": "Ping Li",
        "authors": "Ping Li",
        "title": "Compressed Counting",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.CC cs.DM cs.DS cs.LG math.IT",
        "license": null,
        "abstract": "  Counting is among the most fundamental operations in computing. For example,\ncounting the pth frequency moment has been a very active area of research, in\ntheoretical computer science, databases, and data mining. When p=1, the task\n(i.e., counting the sum) can be accomplished using a simple counter.\n  Compressed Counting (CC) is proposed for efficiently computing the pth\nfrequency moment of a data stream signal A_t, where 0<p<=2. CC is applicable if\nthe streaming data follow the Turnstile model, with the restriction that at the\ntime t for the evaluation, A_t[i]>= 0, which includes the strict Turnstile\nmodel as a special case. For natural data streams encountered in practice, this\nrestriction is minor.\n  The underly technique for CC is what we call skewed stable random\nprojections, which captures the intuition that, when p=1 a simple counter\nsuffices, and when p = 1+/\\Delta with small \\Delta, the sample complexity of a\ncounter system should be low (continuously as a function of \\Delta). We show at\nsmall \\Delta the sample complexity (number of projections) k = O(1/\\epsilon)\ninstead of O(1/\\epsilon^2).\n  Compressed Counting can serve a basic building block for other tasks in\nstatistics and computing, for example, estimation entropies of data streams,\nparameter estimations using the method of moments and maximum likelihood.\n  Finally, another contribution is an algorithm for approximating the\nlogarithmic norm, \\sum_{i=1}^D\\log A_t[i], and logarithmic distance. The\nlogarithmic distance is useful in machine learning practice with heavy-tailed\ndata.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 17 Feb 2008 16:42:52 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 24 Feb 2008 09:51:09 GMT"
            }
        ],
        "update_date": "2008-02-24",
        "authors_parsed": [
            [
                "Li",
                "Ping",
                ""
            ]
        ]
    },
    {
        "id": "0802.2306",
        "submitter": "Gareth Baxter",
        "authors": "G. J. Baxter, M. R. Frean",
        "title": "Software graphs and programmer awareness",
        "comments": "9 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Dependencies between types in object-oriented software can be viewed as\ndirected graphs, with types as nodes and dependencies as edges. The in-degree\nand out-degree distributions of such graphs have quite different forms, with\nthe former resembling a power-law distribution and the latter an exponential\ndistribution. This effect appears to be independent of application or type\nrelationship. A simple generative model is proposed to explore the proposition\nthat the difference arises because the programmer is aware of the out-degree of\na type but not of its in-degree. The model reproduces the two distributions,\nand compares reasonably well to those observed in 14 different type\nrelationships across 12 different Java applications.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 16 Feb 2008 03:38:03 GMT"
            }
        ],
        "update_date": "2008-02-19",
        "authors_parsed": [
            [
                "Baxter",
                "G. J.",
                ""
            ],
            [
                "Frean",
                "M. R.",
                ""
            ]
        ]
    },
    {
        "id": "0802.2306",
        "submitter": "Gareth Baxter",
        "authors": "G. J. Baxter, M. R. Frean",
        "title": "Software graphs and programmer awareness",
        "comments": "9 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Dependencies between types in object-oriented software can be viewed as\ndirected graphs, with types as nodes and dependencies as edges. The in-degree\nand out-degree distributions of such graphs have quite different forms, with\nthe former resembling a power-law distribution and the latter an exponential\ndistribution. This effect appears to be independent of application or type\nrelationship. A simple generative model is proposed to explore the proposition\nthat the difference arises because the programmer is aware of the out-degree of\na type but not of its in-degree. The model reproduces the two distributions,\nand compares reasonably well to those observed in 14 different type\nrelationships across 12 different Java applications.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 16 Feb 2008 03:38:03 GMT"
            }
        ],
        "update_date": "2008-02-19",
        "authors_parsed": [
            [
                "Baxter",
                "G. J.",
                ""
            ],
            [
                "Frean",
                "M. R.",
                ""
            ]
        ]
    },
    {
        "id": "0802.2371",
        "submitter": "Pierre Comon",
        "authors": "P. Comon and J. ten Berge",
        "title": "Generic and Typical Ranks of Three-Way Arrays",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "I3S report ISRN I3S/RR-2006-29-FR",
        "categories": "cs.OH cs.MS",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  The concept of tensor rank, introduced in the twenties, has been popularized\nat the beginning of the seventies. This has allowed to carry out Factor\nAnalysis on arrays with more than two indices. The generic rank may be seen as\nan upper bound to the number of factors that can be extracted from a given\ntensor. We explain in this short paper how to obtain numerically the generic\nrank of tensors of arbitrary dimensions, and compare it with the rare algebraic\nresults already known at order three. In particular, we examine the cases of\nsymmetric tensors, tensors with symmetric matrix slices, or tensors with free\nentries.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 17 Feb 2008 09:48:07 GMT"
            }
        ],
        "update_date": "2008-02-19",
        "authors_parsed": [
            [
                "Comon",
                "P.",
                ""
            ],
            [
                "Berge",
                "J. ten",
                ""
            ]
        ]
    },
    {
        "id": "0802.2411",
        "submitter": "Mahesh  Pal Dr.",
        "authors": "Mahesh Pal",
        "title": "Multiclass Approaches for Support Vector Machine Based Land Cover\n  Classification",
        "comments": "16 pages, MapIndia 2005 conference",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  SVMs were initially developed to perform binary classification; though,\napplications of binary classification are very limited. Most of the practical\napplications involve multiclass classification, especially in remote sensing\nland cover classification. A number of methods have been proposed to implement\nSVMs to produce multiclass classification. A number of methods to generate\nmulticlass SVMs from binary SVMs have been proposed by researchers and is still\na continuing research topic. This paper compares the performance of six\nmulti-class approaches to solve classification problem with remote sensing data\nin term of classification accuracy and computational cost. One vs. one, one vs.\nrest, Directed Acyclic Graph (DAG), and Error Corrected Output Coding (ECOC)\nbased multiclass approaches creates many binary classifiers and combines their\nresults to determine the class label of a test pixel. Another catogery of multi\nclass approach modify the binary class objective function and allows\nsimultaneous computation of multiclass classification by solving a single\noptimisation problem. Results from this study conclude the usefulness of One\nvs. One multi class approach in term of accuracy and computational cost over\nother multi class approaches.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 18 Feb 2008 03:47:45 GMT"
            }
        ],
        "update_date": "2008-02-19",
        "authors_parsed": [
            [
                "Pal",
                "Mahesh",
                ""
            ]
        ]
    },
    {
        "id": "0802.2411",
        "submitter": "Mahesh  Pal Dr.",
        "authors": "Mahesh Pal",
        "title": "Multiclass Approaches for Support Vector Machine Based Land Cover\n  Classification",
        "comments": "16 pages, MapIndia 2005 conference",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  SVMs were initially developed to perform binary classification; though,\napplications of binary classification are very limited. Most of the practical\napplications involve multiclass classification, especially in remote sensing\nland cover classification. A number of methods have been proposed to implement\nSVMs to produce multiclass classification. A number of methods to generate\nmulticlass SVMs from binary SVMs have been proposed by researchers and is still\na continuing research topic. This paper compares the performance of six\nmulti-class approaches to solve classification problem with remote sensing data\nin term of classification accuracy and computational cost. One vs. one, one vs.\nrest, Directed Acyclic Graph (DAG), and Error Corrected Output Coding (ECOC)\nbased multiclass approaches creates many binary classifiers and combines their\nresults to determine the class label of a test pixel. Another catogery of multi\nclass approach modify the binary class objective function and allows\nsimultaneous computation of multiclass classification by solving a single\noptimisation problem. Results from this study conclude the usefulness of One\nvs. One multi class approach in term of accuracy and computational cost over\nother multi class approaches.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 18 Feb 2008 03:47:45 GMT"
            }
        ],
        "update_date": "2008-02-19",
        "authors_parsed": [
            [
                "Pal",
                "Mahesh",
                ""
            ]
        ]
    },
    {
        "id": "0802.2428",
        "submitter": "Alexandre Benoit",
        "authors": "Oya Aran, Ismail Ari, Alexandre Benoit (GIPSA-lab), Ana Huerta\n  Carrillo, Fran\\c{c}ois-Xavier Fanard (TELE), Pavel Campr, Lale Akarun, Alice\n  Caplier (GIPSA-lab), Michele Rombaut (GIPSA-lab), Bulent Sankur",
        "title": "Sign Language Tutoring Tool",
        "comments": "eNTERFACE'06. Summer Workshop. on Multimodal Interfaces, Dubrovnik :\n  Croatie (2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.HC",
        "license": null,
        "abstract": "  In this project, we have developed a sign language tutor that lets users\nlearn isolated signs by watching recorded videos and by trying the same signs.\nThe system records the user's video and analyses it. If the sign is recognized,\nboth verbal and animated feedback is given to the user. The system is able to\nrecognize complex signs that involve both hand gestures and head movements and\nexpressions. Our performance tests yield a 99% recognition rate on signs\ninvolving only manual gestures and 85% recognition rate on signs that involve\nboth manual and non manual components, such as head movement and facial\nexpressions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 18 Feb 2008 07:28:44 GMT"
            }
        ],
        "update_date": "2008-09-11",
        "authors_parsed": [
            [
                "Aran",
                "Oya",
                "",
                "GIPSA-lab"
            ],
            [
                "Ari",
                "Ismail",
                "",
                "GIPSA-lab"
            ],
            [
                "Benoit",
                "Alexandre",
                "",
                "GIPSA-lab"
            ],
            [
                "Carrillo",
                "Ana Huerta",
                "",
                "TELE"
            ],
            [
                "Fanard",
                "Fran\u00e7ois-Xavier",
                "",
                "TELE"
            ],
            [
                "Campr",
                "Pavel",
                "",
                "GIPSA-lab"
            ],
            [
                "Akarun",
                "Lale",
                "",
                "GIPSA-lab"
            ],
            [
                "Caplier",
                "Alice",
                "",
                "GIPSA-lab"
            ],
            [
                "Rombaut",
                "Michele",
                "",
                "GIPSA-lab"
            ],
            [
                "Sankur",
                "Bulent",
                ""
            ]
        ]
    },
    {
        "id": "0802.2428",
        "submitter": "Alexandre Benoit",
        "authors": "Oya Aran, Ismail Ari, Alexandre Benoit (GIPSA-lab), Ana Huerta\n  Carrillo, Fran\\c{c}ois-Xavier Fanard (TELE), Pavel Campr, Lale Akarun, Alice\n  Caplier (GIPSA-lab), Michele Rombaut (GIPSA-lab), Bulent Sankur",
        "title": "Sign Language Tutoring Tool",
        "comments": "eNTERFACE'06. Summer Workshop. on Multimodal Interfaces, Dubrovnik :\n  Croatie (2007)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.HC",
        "license": null,
        "abstract": "  In this project, we have developed a sign language tutor that lets users\nlearn isolated signs by watching recorded videos and by trying the same signs.\nThe system records the user's video and analyses it. If the sign is recognized,\nboth verbal and animated feedback is given to the user. The system is able to\nrecognize complex signs that involve both hand gestures and head movements and\nexpressions. Our performance tests yield a 99% recognition rate on signs\ninvolving only manual gestures and 85% recognition rate on signs that involve\nboth manual and non manual components, such as head movement and facial\nexpressions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 18 Feb 2008 07:28:44 GMT"
            }
        ],
        "update_date": "2008-09-11",
        "authors_parsed": [
            [
                "Aran",
                "Oya",
                "",
                "GIPSA-lab"
            ],
            [
                "Ari",
                "Ismail",
                "",
                "GIPSA-lab"
            ],
            [
                "Benoit",
                "Alexandre",
                "",
                "GIPSA-lab"
            ],
            [
                "Carrillo",
                "Ana Huerta",
                "",
                "TELE"
            ],
            [
                "Fanard",
                "Fran\u00e7ois-Xavier",
                "",
                "TELE"
            ],
            [
                "Campr",
                "Pavel",
                "",
                "GIPSA-lab"
            ],
            [
                "Akarun",
                "Lale",
                "",
                "GIPSA-lab"
            ],
            [
                "Caplier",
                "Alice",
                "",
                "GIPSA-lab"
            ],
            [
                "Rombaut",
                "Michele",
                "",
                "GIPSA-lab"
            ],
            [
                "Sankur",
                "Bulent",
                ""
            ]
        ]
    },
    {
        "id": "0802.2543",
        "submitter": "Novella Bartolini",
        "authors": "Novella Bartolini, Giancarlo Bongiovanni, Simone Silvestri (Department\n  of Computer Science University of Rome Sapienza, Italy)",
        "title": "Self-* overload control for distributed web systems",
        "comments": "The full version of this paper, titled \"Self-* through self-learning:\n  overload control for distributed web systems\", has been published on Computer\n  Networks, Elsevier. The simulator used for the evaluation of the proposed\n  algorithm is available for download at the address:\n  http://www.dsi.uniroma1.it/~novella/qos_web/",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Unexpected increases in demand and most of all flash crowds are considered\nthe bane of every web application as they may cause intolerable delays or even\nservice unavailability. Proper quality of service policies must guarantee rapid\nreactivity and responsiveness even in such critical situations. Previous\nsolutions fail to meet common performance requirements when the system has to\nface sudden and unpredictable surges of traffic. Indeed they often rely on a\nproper setting of key parameters which requires laborious manual tuning,\npreventing a fast adaptation of the control policies. We contribute an original\nSelf-* Overload Control (SOC) policy. This allows the system to self-configure\na dynamic constraint on the rate of admitted sessions in order to respect\nservice level agreements and maximize the resource utilization at the same\ntime. Our policy does not require any prior information on the incoming traffic\nor manual configuration of key parameters. We ran extensive simulations under a\nwide range of operating conditions, showing that SOC rapidly adapts to time\nvarying traffic and self-optimizes the resource utilization. It admits as many\nnew sessions as possible in observance of the agreements, even under intense\nworkload variations. We compared our algorithm to previously proposed\napproaches highlighting a more stable behavior and a better performance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 18 Feb 2008 21:18:17 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 29 Jan 2009 15:14:20 GMT"
            }
        ],
        "update_date": "2009-01-29",
        "authors_parsed": [
            [
                "Bartolini",
                "Novella",
                "",
                "Department\n  of Computer Science University of Rome Sapienza, Italy"
            ],
            [
                "Bongiovanni",
                "Giancarlo",
                "",
                "Department\n  of Computer Science University of Rome Sapienza, Italy"
            ],
            [
                "Silvestri",
                "Simone",
                "",
                "Department\n  of Computer Science University of Rome Sapienza, Italy"
            ]
        ]
    },
    {
        "id": "0802.2655",
        "submitter": "Gilles Stoltz",
        "authors": "S\\'ebastien Bubeck (INRIA Futurs), R\\'emi Munos (INRIA Futurs), Gilles\n  Stoltz (DMA, GREGH)",
        "title": "Pure Exploration for Multi-Armed Bandit Problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.ST cs.LG stat.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the framework of stochastic multi-armed bandit problems and study\nthe possibilities and limitations of forecasters that perform an on-line\nexploration of the arms. These forecasters are assessed in terms of their\nsimple regret, a regret notion that captures the fact that exploration is only\nconstrained by the number of available rounds (not necessarily known in\nadvance), in contrast to the case when the cumulative regret is considered and\nwhen exploitation needs to be performed at the same time. We believe that this\nperformance criterion is suited to situations when the cost of pulling an arm\nis expressed in terms of resources rather than rewards. We discuss the links\nbetween the simple and the cumulative regret. One of the main results in the\ncase of a finite number of arms is a general lower bound on the simple regret\nof a forecaster in terms of its cumulative regret: the smaller the latter, the\nlarger the former. Keeping this result in mind, we then exhibit upper bounds on\nthe simple regret of some forecasters. The paper ends with a study devoted to\ncontinuous-armed bandit problems; we show that the simple regret can be\nminimized with respect to a family of probability distributions if and only if\nthe cumulative regret can be minimized for it. Based on this equivalence, we\nare able to prove that the separable metric spaces are exactly the metric\nspaces on which these regrets can be minimized with respect to the family of\nall probability distributions with continuous mean-payoff functions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 19 Feb 2008 14:05:22 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 13 Jun 2008 07:03:22 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 17 Jun 2008 07:07:03 GMT"
            },
            {
                "version": "v4",
                "created": "Thu, 19 Feb 2009 10:33:29 GMT"
            },
            {
                "version": "v5",
                "created": "Tue, 26 Jan 2010 10:10:42 GMT"
            },
            {
                "version": "v6",
                "created": "Wed, 9 Jun 2010 09:08:50 GMT"
            }
        ],
        "update_date": "2010-07-26",
        "authors_parsed": [
            [
                "Bubeck",
                "S\u00e9bastien",
                "",
                "INRIA Futurs"
            ],
            [
                "Munos",
                "R\u00e9mi",
                "",
                "INRIA Futurs"
            ],
            [
                "Stoltz",
                "Gilles",
                "",
                "DMA, GREGH"
            ]
        ]
    },
    {
        "id": "0802.2932",
        "submitter": "Grenville Croll",
        "authors": "Brian Sentence",
        "title": "A New Approach to Spreadsheet Analytics Management in Financial Markets",
        "comments": "9 Pages, 9 Colour Figures",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2006 65-72\n  ISBN:1-905617-08-9",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.CY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Spreadsheets in financial markets are frequently used as database, calculator\nand reporting application combined. This paper describes an alternative\napproach in which spreadsheet design and database technology have been brought\ntogether in order to alleviate management and regulatory concerns over the\noperational risks of spreadsheet usage. In particular, the paper focuses on the\nrapid creation and centralised deployment of statistical analytics within a\nsoftware system now in use by major investment banks, and presents a novel\ntechnique for the manipulation in spreadsheets of high volumes of intraday\nmarket data.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 20 Feb 2008 20:17:52 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Sentence",
                "Brian",
                ""
            ]
        ]
    },
    {
        "id": "0802.3235",
        "submitter": "Arturo Berrones",
        "authors": "Arturo Berrones",
        "title": "Characterization of the convergence of stationary Fokker-Planck learning",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.neucom.2008.12.042",
        "report-no": null,
        "categories": "cs.NE cond-mat.dis-nn cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The convergence properties of the stationary Fokker-Planck algorithm for the\nestimation of the asymptotic density of stochastic search processes is studied.\nTheoretical and empirical arguments for the characterization of convergence of\nthe estimation in the case of separable and nonseparable nonlinear optimization\nproblems are given. Some implications of the convergence of stationary\nFokker-Planck learning for the inference of parameters in artificial neural\nnetwork models are outlined.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 21 Feb 2008 23:41:09 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 25 Feb 2008 19:37:50 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 2 Jul 2009 17:24:41 GMT"
            }
        ],
        "update_date": "2009-07-02",
        "authors_parsed": [
            [
                "Berrones",
                "Arturo",
                ""
            ]
        ]
    },
    {
        "id": "0802.3285",
        "submitter": "Radu Arsinte",
        "authors": "Radu Arsinte, Ciprian Ilioaei",
        "title": "Some Aspects of Testing Process for Transport Streams in Digital Video\n  Broadcasting",
        "comments": "5 pages, 3 figures, 3 tables",
        "journal-ref": "Acta Technica Napocensis, Electronics and Telecommunications,\n  nr.1/2004 pp.59-74",
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.MM",
        "license": null,
        "abstract": "  This paper presents some aspects related to the DVB (Digital Video\nBroadcasting) investigation. The basic aspects of DVB are presented, with an\nemphasis on DVB-T version of standard. The main purpose of this research is to\nanalyze the way that the transmission of the transport streams is realized in\ncase of the Terrestrial Digital Video Broadcasting (DVB-T). To accomplish this,\nfirst, Digital Video Broadcasting standard is presented, and then the main\naspects of DVB testing and analysis of the transport streams are investigated.\nThe paper presents also the results obtained using two programs designed for\nDVB analysis: Mosalina and TSA.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 22 Feb 2008 10:48:44 GMT"
            }
        ],
        "update_date": "2008-02-25",
        "authors_parsed": [
            [
                "Arsinte",
                "Radu",
                ""
            ],
            [
                "Ilioaei",
                "Ciprian",
                ""
            ]
        ]
    },
    {
        "id": "0802.3288",
        "submitter": "Radu Arsinte",
        "authors": "Radu Arsinte",
        "title": "Implementing a Test Strategy for an Advanced Video Acquisition and\n  Processing Architecture",
        "comments": "5 pages, 17 figures",
        "journal-ref": "Acta Technica Napocensis, Electronics and Telecommunications,\n  nr.2/2005 pp.15-20",
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.MM",
        "license": null,
        "abstract": "  This paper presents some aspects related to test process of an advanced video\nsystem used in remote IP surveillance. The system is based on a Pentium\ncompatible architecture using the industrial standard PC104+. First the overall\narchitecture of the system is presented, involving both hardware or software\naspects. The acquisition board which is developed in a special, nonstandard\narchitecture, is also briefly presented. The main purpose of this research was\nto set a coherent set of procedures in order to test all the aspects of the\nvideo acquisition board. To accomplish this, it was necessary to set-up a\nprocedure in two steps: stand alone video board test (functional test) and an\nin-system test procedure verifying the compatibility with both OS: Linux and\nWindows. The paper presents also the results obtained using this procedure.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 22 Feb 2008 10:54:59 GMT"
            }
        ],
        "update_date": "2008-02-25",
        "authors_parsed": [
            [
                "Arsinte",
                "Radu",
                ""
            ]
        ]
    },
    {
        "id": "0802.3448",
        "submitter": "Haim Kaplan",
        "authors": "Edith Cohen and Haim Kaplan",
        "title": "Sketch-Based Estimation of Subpopulation-Weight",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.DS cs.NI cs.PF",
        "license": null,
        "abstract": "  Summaries of massive data sets support approximate query processing over the\noriginal data. A basic aggregate over a set of records is the weight of\nsubpopulations specified as a predicate over records' attributes. Bottom-k\nsketches are a powerful summarization format of weighted items that includes\npriority sampling and the classic weighted sampling without replacement. They\ncan be computed efficiently for many representations of the data including\ndistributed databases and data streams.\n  We derive novel unbiased estimators and efficient confidence bounds for\nsubpopulation weight. Our estimators and bounds are tailored by distinguishing\nbetween applications (such as data streams) where the total weight of the\nsketched set can be computed by the summarization algorithm without a\nsignificant use of additional resources, and applications (such as sketches of\nnetwork neighborhoods) where this is not the case.\n  Our rigorous derivations are based on clever applications of the\nHorvitz-Thompson estimator, and are complemented by efficient computational\nmethods. We demonstrate their benefit on a wide range of Pareto distributions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 23 Feb 2008 15:25:04 GMT"
            }
        ],
        "update_date": "2008-02-26",
        "authors_parsed": [
            [
                "Cohen",
                "Edith",
                ""
            ],
            [
                "Kaplan",
                "Haim",
                ""
            ]
        ]
    },
    {
        "id": "0802.3448",
        "submitter": "Haim Kaplan",
        "authors": "Edith Cohen and Haim Kaplan",
        "title": "Sketch-Based Estimation of Subpopulation-Weight",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.DS cs.NI cs.PF",
        "license": null,
        "abstract": "  Summaries of massive data sets support approximate query processing over the\noriginal data. A basic aggregate over a set of records is the weight of\nsubpopulations specified as a predicate over records' attributes. Bottom-k\nsketches are a powerful summarization format of weighted items that includes\npriority sampling and the classic weighted sampling without replacement. They\ncan be computed efficiently for many representations of the data including\ndistributed databases and data streams.\n  We derive novel unbiased estimators and efficient confidence bounds for\nsubpopulation weight. Our estimators and bounds are tailored by distinguishing\nbetween applications (such as data streams) where the total weight of the\nsketched set can be computed by the summarization algorithm without a\nsignificant use of additional resources, and applications (such as sketches of\nnetwork neighborhoods) where this is not the case.\n  Our rigorous derivations are based on clever applications of the\nHorvitz-Thompson estimator, and are complemented by efficient computational\nmethods. We demonstrate their benefit on a wide range of Pareto distributions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 23 Feb 2008 15:25:04 GMT"
            }
        ],
        "update_date": "2008-02-26",
        "authors_parsed": [
            [
                "Cohen",
                "Edith",
                ""
            ],
            [
                "Kaplan",
                "Haim",
                ""
            ]
        ]
    },
    {
        "id": "0802.3457",
        "submitter": "Grenville Croll",
        "authors": "Raymond R. Panko",
        "title": "Spreadsheet Errors: What We Know. What We Think We Can Do",
        "comments": "9 Pages, 2 Tables",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2000 7-17\n  ISBN:1 86166 158 4",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fifteen years of research studies have concluded unanimously that spreadsheet\nerrors are both common and non-trivial. Now we must seek ways to reduce\nspreadsheet errors. Several approaches have been suggested, some of which are\npromising and others, while appealing because they are easy to do, are not\nlikely to be effective. To date, only one technique, cell-by-cell code\ninspection, has been demonstrated to be effective. We need to conduct further\nresearch to determine the degree to which other techniques can reduce\nspreadsheet errors.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 23 Feb 2008 19:48:15 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Panko",
                "Raymond R.",
                ""
            ]
        ]
    },
    {
        "id": "0802.3457",
        "submitter": "Grenville Croll",
        "authors": "Raymond R. Panko",
        "title": "Spreadsheet Errors: What We Know. What We Think We Can Do",
        "comments": "9 Pages, 2 Tables",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2000 7-17\n  ISBN:1 86166 158 4",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Fifteen years of research studies have concluded unanimously that spreadsheet\nerrors are both common and non-trivial. Now we must seek ways to reduce\nspreadsheet errors. Several approaches have been suggested, some of which are\npromising and others, while appealing because they are easy to do, are not\nlikely to be effective. To date, only one technique, cell-by-cell code\ninspection, has been demonstrated to be effective. We need to conduct further\nresearch to determine the degree to which other techniques can reduce\nspreadsheet errors.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 23 Feb 2008 19:48:15 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Panko",
                "Raymond R.",
                ""
            ]
        ]
    },
    {
        "id": "0802.3475",
        "submitter": "Grenville Croll",
        "authors": "Patrick Kemmis, Giles Thomas",
        "title": "Spreadsheet Development Methodologies using Resolver: Moving\n  spreadsheets into the 21st Century",
        "comments": "12 pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 93-104\n  ISBN 978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We intend to demonstrate the innate problems with existing spreadsheet\nproducts and to show how to tackle these issues using a new type of spreadsheet\nprogram called Resolver. It addresses the issues head-on and thereby moves the\n1980's \"VisiCalc paradigm\" on to match the advances in computer languages and\nuser requirements. Continuous display of the spreadsheet grid and the\nequivalent computer program, together with the ability to interact and add code\nthrough either interface, provides a number of new methodologies for\nspreadsheet development.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Feb 2008 01:16:52 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Kemmis",
                "Patrick",
                ""
            ],
            [
                "Thomas",
                "Giles",
                ""
            ]
        ]
    },
    {
        "id": "0802.3475",
        "submitter": "Grenville Croll",
        "authors": "Patrick Kemmis, Giles Thomas",
        "title": "Spreadsheet Development Methodologies using Resolver: Moving\n  spreadsheets into the 21st Century",
        "comments": "12 pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 93-104\n  ISBN 978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We intend to demonstrate the innate problems with existing spreadsheet\nproducts and to show how to tackle these issues using a new type of spreadsheet\nprogram called Resolver. It addresses the issues head-on and thereby moves the\n1980's \"VisiCalc paradigm\" on to match the advances in computer languages and\nuser requirements. Continuous display of the spreadsheet grid and the\nequivalent computer program, together with the ability to interact and add code\nthrough either interface, provides a number of new methodologies for\nspreadsheet development.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Feb 2008 01:16:52 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Kemmis",
                "Patrick",
                ""
            ],
            [
                "Thomas",
                "Giles",
                ""
            ]
        ]
    },
    {
        "id": "0802.3476",
        "submitter": "Grenville Croll",
        "authors": "Jocelyn Paine",
        "title": "Fun Boy Three Were Wrong: it is what you do, not the way that you do it",
        "comments": "12 Pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 105-116\n  ISBN 978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I revisit some classic publications on modularity, to show what problems its\npioneers wanted to solve. These problems occur with spreadsheets too: to\nrecognise them may help us avoid them.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Feb 2008 01:34:36 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Paine",
                "Jocelyn",
                ""
            ]
        ]
    },
    {
        "id": "0802.3476",
        "submitter": "Grenville Croll",
        "authors": "Jocelyn Paine",
        "title": "Fun Boy Three Were Wrong: it is what you do, not the way that you do it",
        "comments": "12 Pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 105-116\n  ISBN 978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I revisit some classic publications on modularity, to show what problems its\npioneers wanted to solve. These problems occur with spreadsheets too: to\nrecognise them may help us avoid them.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Feb 2008 01:34:36 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Paine",
                "Jocelyn",
                ""
            ]
        ]
    },
    {
        "id": "0802.3477",
        "submitter": "Grenville Croll",
        "authors": "Simon R. Thorne, David Ball, Z. Lawson",
        "title": "Concerning the Feasibility of Example-driven Modelling Techniques",
        "comments": "14 Pages, 8 Figures, 1 Tables",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 117-130\n  ISBN 978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We report on a series of experiments concerning the feasibility of example\ndriven modelling. The main aim was to establish experimentally within an\nacademic environment: the relationship between error and task complexity using\na) Traditional spreadsheet modelling; b) example driven techniques. We report\non the experimental design, sampling, research methods and the tasks set for\nboth control and treatment groups. Analysis of the completed tasks allows\ncomparison of several different variables. The experimental results compare the\nperformance indicators for the treatment and control groups by comparing\naccuracy, experience, training, confidence measures, perceived difficulty and\nperceived completeness. The various results are thoroughly tested for\nstatistical significance using: the Chi squared test, Fisher's exact test for\nsignificance, Cochran's Q test and McNemar's test on difficulty.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Feb 2008 01:49:31 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Thorne",
                "Simon R.",
                ""
            ],
            [
                "Ball",
                "David",
                ""
            ],
            [
                "Lawson",
                "Z.",
                ""
            ]
        ]
    },
    {
        "id": "0802.3478",
        "submitter": "Grenville Croll",
        "authors": "Jocelyn Paine",
        "title": "It Ain't What You View, But The Way That You View It: documenting\n  spreadsheets with Excelsior, semantic wikis, and literate programming",
        "comments": "12 Pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 131-142\n  ISBN 978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I describe preliminary experiments in documenting Excelsior versions of\nspreadsheets using semantic wikis and literate programming. The objective is to\ncreate well-structured and comprehensive documentation, easy to use by those\nunfamiliar with the spreadsheets documented. I discuss why so much\ndocumentation is hard to use, and briefly explain semantic wikis and literate\nprogramming; although parts of the paper are Excelsior-specific, these sections\nmay be of more general interest.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Feb 2008 01:57:01 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Paine",
                "Jocelyn",
                ""
            ]
        ]
    },
    {
        "id": "0802.3478",
        "submitter": "Grenville Croll",
        "authors": "Jocelyn Paine",
        "title": "It Ain't What You View, But The Way That You View It: documenting\n  spreadsheets with Excelsior, semantic wikis, and literate programming",
        "comments": "12 Pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 131-142\n  ISBN 978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  I describe preliminary experiments in documenting Excelsior versions of\nspreadsheets using semantic wikis and literate programming. The objective is to\ncreate well-structured and comprehensive documentation, easy to use by those\nunfamiliar with the spreadsheets documented. I discuss why so much\ndocumentation is hard to use, and briefly explain semantic wikis and literate\nprogramming; although parts of the paper are Excelsior-specific, these sections\nmay be of more general interest.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Feb 2008 01:57:01 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Paine",
                "Jocelyn",
                ""
            ]
        ]
    },
    {
        "id": "0802.3479",
        "submitter": "Grenville Croll",
        "authors": "Brian Bishop, Kevin McDaid",
        "title": "An Empirical Study of End-User Behaviour in Spreadsheet Error Detection\n  & Correction",
        "comments": "12 Pages, 3 Figures",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 165-176\n  ISBN 978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.CY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Very little is known about the process by which end-user developers detect\nand correct spreadsheet errors. Any research pertaining to the development of\nspreadsheet testing methodologies or auditing tools would benefit from\ninformation on how end-users perform the debugging process in practice.\nThirteen industry-based professionals and thirty-four accounting & finance\nstudents took part in a current ongoing experiment designed to record and\nanalyse end-user behaviour in spreadsheet error detection and correction.\nProfessionals significantly outperformed students in correcting certain error\ntypes. Time-based cell activity analysis showed that a strong correlation\nexists between the percentage of cells inspected and the number of errors\ncorrected. The cell activity data was gathered through a purpose written VBA\nExcel plug-in that records the time and detail of all cell selection and cell\nchange actions of individuals.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Feb 2008 02:03:16 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Bishop",
                "Brian",
                ""
            ],
            [
                "McDaid",
                "Kevin",
                ""
            ]
        ]
    },
    {
        "id": "0802.3480",
        "submitter": "Grenville Croll",
        "authors": "Kath McGuire",
        "title": "Why Task-Based Training is Superior to Traditional Training Methods",
        "comments": "6 Pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 191-196\n  ISBN 978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The risks of spreadsheet use do not just come from the misuse of formulae. As\nsuch, training needs to go beyond this technical aspect of spreadsheet use and\nlook at the spreadsheet in its full business context. While standard training\nis by and large unable to do this, task-based training is perfectly suited to a\ncontextual approach to training.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Feb 2008 02:10:03 GMT"
            }
        ],
        "update_date": "2008-02-26",
        "authors_parsed": [
            [
                "McGuire",
                "Kath",
                ""
            ]
        ]
    },
    {
        "id": "0802.3481",
        "submitter": "Grenville Croll",
        "authors": "David Chadwick",
        "title": "Establishing A Minimum Generic Skill Set For Risk Management Teaching In\n  A Spreadsheet Training Course",
        "comments": "12 Pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 197-208\n  ISBN 978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Past research shows that spreadsheet models are prone to such a high\nfrequency of errors and data security implications that the risk management of\nspreadsheet development and spreadsheet use is of great importance to both\nindustry and academia. The underlying rationale for this paper is that\nspreadsheet training courses should specifically address risk management in the\ndevelopment process both from a generic and a domain-specific viewpoint. This\nresearch specifically focuses on one of these namely those generic issues of\nrisk management that should be present in a training course that attempts to\nmeet good-practice within industry. A pilot questionnaire was constructed\nshowing a possible minimum set of risk management issues and sent to academics\nand industry practitioners for feedback. The findings from this pilot survey\nwill be used to refine the questionnaire for sending to a larger body of\npossible respondents. It is expected these findings will form the basis of a\nrisk management teaching approach to be trialled in a number of selected\nongoing spreadsheet training courses.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Feb 2008 02:15:49 GMT"
            }
        ],
        "update_date": "2008-02-26",
        "authors_parsed": [
            [
                "Chadwick",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0802.3483",
        "submitter": "Grenville Croll",
        "authors": "Derek Flood, Kevin Mc Daid",
        "title": "Voice-controlled Debugging of Spreadsheets",
        "comments": "10 Pages, 4 Tables",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 155-164\n  ISBN 978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Developments in Mobile Computing are putting pressure on the software\nindustry to research new modes of interaction that do not rely on the\ntraditional keyboard and mouse combination. Computer users suffering from\nRepetitive Strain Injury also seek an alternative to keyboard and mouse devices\nto reduce suffering in wrist and finger joints. Voice-control is an alternative\napproach to spreadsheet development and debugging that has been researched and\nused successfully in other domains. While voice-control technology for\nspreadsheets is available its effectiveness has not been investigated. This\nstudy is the first to compare the performance of a set of expert spreadsheet\ndevelopers that debugged a spreadsheet using voice-control technology and\nanother set that debugged the same spreadsheet using keyboard and mouse. The\nstudy showed that voice, despite its advantages, proved to be slower and less\naccurate. However, it also revealed ways in which the technology might be\nimproved to redress this imbalance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Feb 2008 02:21:35 GMT"
            }
        ],
        "update_date": "2008-02-26",
        "authors_parsed": [
            [
                "Flood",
                "Derek",
                ""
            ],
            [
                "Daid",
                "Kevin Mc",
                ""
            ]
        ]
    },
    {
        "id": "0802.3492",
        "submitter": "Marko A. Rodriguez",
        "authors": "Marko A. Rodriguez",
        "title": "The RDF Virtual Machine",
        "comments": "keywords: Resource Description Framework, Virtual Machines,\n  Distributed Computing, Semantic Web",
        "journal-ref": "Knowledge-Based Systems, 24(6), 890-903, August 2011",
        "doi": "10.1016/j.knosys.2011.04.004",
        "report-no": "LA-UR-08-03925",
        "categories": "cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Resource Description Framework (RDF) is a semantic network data model\nthat is used to create machine-understandable descriptions of the world and is\nthe basis of the Semantic Web. This article discusses the application of RDF to\nthe representation of computer software and virtual computing machines. The\nSemantic Web is posited as not only a web of data, but also as a web of\nprograms and processes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Feb 2008 05:28:52 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 25 Mar 2010 19:06:42 GMT"
            }
        ],
        "update_date": "2011-05-26",
        "authors_parsed": [
            [
                "Rodriguez",
                "Marko A.",
                ""
            ]
        ]
    },
    {
        "id": "0802.3528",
        "submitter": "Fionn Murtagh",
        "authors": "Fionn Murtagh and Jean-Luc Starck",
        "title": "Wavelet and Curvelet Moments for Image Classification: Application to\n  Aggregate Mixture Grading",
        "comments": "Submitted to Pattern Recognition Letters",
        "journal-ref": "Pattern Recognition Letters, 29, 1557-1564, 2008",
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We show the potential for classifying images of mixtures of aggregate, based\nthemselves on varying, albeit well-defined, sizes and shapes, in order to\nprovide a far more effective approach compared to the classification of\nindividual sizes and shapes. While a dominant (additive, stationary) Gaussian\nnoise component in image data will ensure that wavelet coefficients are of\nGaussian distribution, long tailed distributions (symptomatic, for example, of\nextreme values) may well hold in practice for wavelet coefficients. Energy (2nd\norder moment) has often been used for image characterization for image\ncontent-based retrieval, and higher order moments may be important also, not\nleast for capturing long tailed distributional behavior. In this work, we\nassess 2nd, 3rd and 4th order moments of multiresolution transform -- wavelet\nand curvelet transform -- coefficients as features. As analysis methodology,\ntaking account of image types, multiresolution transforms, and moments of\ncoefficients in the scales or bands, we use correspondence analysis as well as\nk-nearest neighbors supervised classification.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Feb 2008 18:25:51 GMT"
            }
        ],
        "update_date": "2010-11-16",
        "authors_parsed": [
            [
                "Murtagh",
                "Fionn",
                ""
            ],
            [
                "Starck",
                "Jean-Luc",
                ""
            ]
        ]
    },
    {
        "id": "0802.3554",
        "submitter": "Reginald Smith",
        "authors": "Reginald D. Smith",
        "title": "Data Traffic Dynamics and Saturation on a Single Link",
        "comments": "10 pages, 5 figures",
        "journal-ref": "International Journal of Computer, Information, and Systems\n  Science, and Engineering, vol 3, no. 1, 11-16 2009",
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The dynamics of User Datagram Protocol (UDP) traffic over Ethernet between\ntwo computers are analyzed using nonlinear dynamics which shows that there are\ntwo clear regimes in the data flow: free flow and saturated. The two most\nimportant variables affecting this are the packet size and packet flow rate.\nHowever, this transition is due to a transcritical bifurcation rather than\nphase transition in models such as in vehicle traffic or theorized large-scale\ncomputer network congestion. It is hoped this model will help lay the\ngroundwork for further research on the dynamics of networks, especially\ncomputer networks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 25 Feb 2008 03:23:33 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 11 Mar 2008 01:11:26 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 20 Feb 2009 13:16:50 GMT"
            }
        ],
        "update_date": "2011-12-08",
        "authors_parsed": [
            [
                "Smith",
                "Reginald D.",
                ""
            ]
        ]
    },
    {
        "id": "0802.3582",
        "submitter": "Erich Schikuta",
        "authors": "Erich Schikuta",
        "title": "Neural Networks and Database Systems",
        "comments": "19 pages, Festschrift Informationssysteme, in honor of G. Vinek",
        "journal-ref": "pp. 133-152, 2007, publisher Austrian Computer Society",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.NE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Object-oriented database systems proved very valuable at handling and\nadministrating complex objects. In the following guidelines for embedding\nneural networks into such systems are presented. It is our goal to treat\nnetworks as normal data in the database system. From the logical point of view,\na neural network is a complex data value and can be stored as a normal data\nobject. It is generally accepted that rule-based reasoning will play an\nimportant role in future database applications. The knowledge base consists of\nfacts and rules, which are both stored and handled by the underlying database\nsystem. Neural networks can be seen as representation of intensional knowledge\nof intelligent database systems. So they are part of a rule based knowledge\npool and can be used like conventional rules. The user has a unified view about\nhis knowledge base regardless of the origin of the unique rules.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 25 Feb 2008 09:57:31 GMT"
            }
        ],
        "update_date": "2008-02-26",
        "authors_parsed": [
            [
                "Schikuta",
                "Erich",
                ""
            ]
        ]
    },
    {
        "id": "0802.3582",
        "submitter": "Erich Schikuta",
        "authors": "Erich Schikuta",
        "title": "Neural Networks and Database Systems",
        "comments": "19 pages, Festschrift Informationssysteme, in honor of G. Vinek",
        "journal-ref": "pp. 133-152, 2007, publisher Austrian Computer Society",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.NE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Object-oriented database systems proved very valuable at handling and\nadministrating complex objects. In the following guidelines for embedding\nneural networks into such systems are presented. It is our goal to treat\nnetworks as normal data in the database system. From the logical point of view,\na neural network is a complex data value and can be stored as a normal data\nobject. It is generally accepted that rule-based reasoning will play an\nimportant role in future database applications. The knowledge base consists of\nfacts and rules, which are both stored and handled by the underlying database\nsystem. Neural networks can be seen as representation of intensional knowledge\nof intelligent database systems. So they are part of a rule based knowledge\npool and can be used like conventional rules. The user has a unified view about\nhis knowledge base regardless of the origin of the unique rules.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 25 Feb 2008 09:57:31 GMT"
            }
        ],
        "update_date": "2008-02-26",
        "authors_parsed": [
            [
                "Schikuta",
                "Erich",
                ""
            ]
        ]
    },
    {
        "id": "0802.3628",
        "submitter": "Pierre Thierry",
        "authors": "Pierre Thierry and Simon E. B. Thierry",
        "title": "Dynamic data models: an application of MOP-based persistence in Common\n  Lisp",
        "comments": "Presented at the 4th European Lisp Workshop, co-located with ECOOP\n  2007. No proceedings",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The data model of an application, the nature and format of data stored across\nexecutions, is typically a very rigid part of its early specification, even\nwhen prototyping, and changing it after code that relies on it was written can\nprove quite expensive and error-prone.\n  Code and data in a running Lisp image can be dynamically modified. A\nMOP-based persistence library can bring this dynamicity to the data model. This\nenables to extend the easy prototyping way of development to the storage of\ndata and helps avoiding interruptions of service. This article presents the\nconditions to do this portably and transparently.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 25 Feb 2008 14:20:18 GMT"
            }
        ],
        "update_date": "2008-02-26",
        "authors_parsed": [
            [
                "Thierry",
                "Pierre",
                ""
            ],
            [
                "Thierry",
                "Simon E. B.",
                ""
            ]
        ]
    },
    {
        "id": "0802.3784",
        "submitter": "Jerry Overton",
        "authors": "Jerry Overton",
        "title": "Pattern-Oriented Analysis and Design (POAD) Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.IT math.IT",
        "license": null,
        "abstract": "  Pattern-Oriented Analysis and Design (POAD) is the practice of building\ncomplex software by applying proven designs to specific problem domains.\nAlthough a great deal of research and practice has been devoted to formalizing\nexisting design patterns and discovering new ones, there has been relatively\nlittle research into methods for combining these patterns into software\napplications. This is partly because the creation of complex software\napplications is so expensive. This paper proposes a mathematical model of POAD\nthat may allow future research in pattern-oriented techniques to be performed\nusing less expensive formal techniques rather than expensive, complex software\ndevelopment.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 26 Feb 2008 13:51:39 GMT"
            }
        ],
        "update_date": "2008-02-27",
        "authors_parsed": [
            [
                "Overton",
                "Jerry",
                ""
            ]
        ]
    },
    {
        "id": "0802.3789",
        "submitter": "Viviana Sica",
        "authors": "Nick Milton",
        "title": "Knowledge Technologies",
        "comments": "130 pages, ISBN 978-88-7699-099-1 (Printed edition), ISBN\n  978-88-7699-100-4 (Electronic edition), printed edition available at\n  http://stores.lulu.com/polimetrica and on http://www.amazon.com/",
        "journal-ref": "\"Publishing studies\" book series, edited by Giandomenico Sica,\n  ISSN 1973-6061 (Printed edition), ISSN 1973-6053 (Electronic edition)",
        "doi": null,
        "report-no": null,
        "categories": "cs.CY cs.AI cs.LG cs.SE",
        "license": null,
        "abstract": "  Several technologies are emerging that provide new ways to capture, store,\npresent and use knowledge. This book is the first to provide a comprehensive\nintroduction to five of the most important of these technologies: Knowledge\nEngineering, Knowledge Based Engineering, Knowledge Webs, Ontologies and\nSemantic Webs. For each of these, answers are given to a number of key\nquestions (What is it? How does it operate? How is a system developed? What can\nit be used for? What tools are available? What are the main issues?). The book\nis aimed at students, researchers and practitioners interested in Knowledge\nManagement, Artificial Intelligence, Design Engineering and Web Technologies.\n  During the 1990s, Nick worked at the University of Nottingham on the\napplication of AI techniques to knowledge management and on various knowledge\nacquisition projects to develop expert systems for military applications. In\n1999, he joined Epistemics where he worked on numerous knowledge projects and\nhelped establish knowledge management programmes at large organisations in the\nengineering, technology and legal sectors. He is author of the book \"Knowledge\nAcquisition in Practice\", which describes a step-by-step procedure for\nacquiring and implementing expertise. He maintains strong links with leading\nresearch organisations working on knowledge technologies, such as\nknowledge-based engineering, ontologies and semantic technologies.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 26 Feb 2008 11:26:09 GMT"
            }
        ],
        "update_date": "2008-02-27",
        "authors_parsed": [
            [
                "Milton",
                "Nick",
                ""
            ]
        ]
    },
    {
        "id": "0802.3789",
        "submitter": "Viviana Sica",
        "authors": "Nick Milton",
        "title": "Knowledge Technologies",
        "comments": "130 pages, ISBN 978-88-7699-099-1 (Printed edition), ISBN\n  978-88-7699-100-4 (Electronic edition), printed edition available at\n  http://stores.lulu.com/polimetrica and on http://www.amazon.com/",
        "journal-ref": "\"Publishing studies\" book series, edited by Giandomenico Sica,\n  ISSN 1973-6061 (Printed edition), ISSN 1973-6053 (Electronic edition)",
        "doi": null,
        "report-no": null,
        "categories": "cs.CY cs.AI cs.LG cs.SE",
        "license": null,
        "abstract": "  Several technologies are emerging that provide new ways to capture, store,\npresent and use knowledge. This book is the first to provide a comprehensive\nintroduction to five of the most important of these technologies: Knowledge\nEngineering, Knowledge Based Engineering, Knowledge Webs, Ontologies and\nSemantic Webs. For each of these, answers are given to a number of key\nquestions (What is it? How does it operate? How is a system developed? What can\nit be used for? What tools are available? What are the main issues?). The book\nis aimed at students, researchers and practitioners interested in Knowledge\nManagement, Artificial Intelligence, Design Engineering and Web Technologies.\n  During the 1990s, Nick worked at the University of Nottingham on the\napplication of AI techniques to knowledge management and on various knowledge\nacquisition projects to develop expert systems for military applications. In\n1999, he joined Epistemics where he worked on numerous knowledge projects and\nhelped establish knowledge management programmes at large organisations in the\nengineering, technology and legal sectors. He is author of the book \"Knowledge\nAcquisition in Practice\", which describes a step-by-step procedure for\nacquiring and implementing expertise. He maintains strong links with leading\nresearch organisations working on knowledge technologies, such as\nknowledge-based engineering, ontologies and semantic technologies.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 26 Feb 2008 11:26:09 GMT"
            }
        ],
        "update_date": "2008-02-27",
        "authors_parsed": [
            [
                "Milton",
                "Nick",
                ""
            ]
        ]
    },
    {
        "id": "0802.3895",
        "submitter": "Grenville Croll",
        "authors": "Andrej Bregar",
        "title": "Complexity Metrics for Spreadsheet Models",
        "comments": "9 pages, 5 figures",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2004 85-93\n  ISBN 1 902724 94 1",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Several complexity metrics are described which are related to logic\nstructure, data structure and size of spreadsheet models. They primarily\nconcentrate on the dispersion of cell references and cell paths. Most metrics\nare newly defined, while some are adapted from traditional software\nengineering. Their purpose is the identification of cells which are liable to\nerrors. In addition, they can be used to estimate the values of dependent\nprocess metrics, such as the development duration and effort, and especially to\nadjust the cell error rate in accordance with the contents of each individual\ncell, in order to accurately asses the reliability of a model. Finally, two\nconceptual constructs - the reference branching condition cell and the\ncondition block - are discussed, aiming at improving the reliability,\nmodifiability, auditability and comprehensibility of logical tests.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Feb 2008 00:37:56 GMT"
            }
        ],
        "update_date": "2008-02-28",
        "authors_parsed": [
            [
                "Bregar",
                "Andrej",
                ""
            ]
        ]
    },
    {
        "id": "0802.3919",
        "submitter": "Grenville Croll",
        "authors": "Thomas A. Grossman, Ozgur Ozluk",
        "title": "A Paradigm for Spreadsheet Engineering Methodologies",
        "comments": "11 Pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2004 23-33\n  ISBN 1 902724 94 1",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Spreadsheet engineering methodologies are diverse and sometimes\ncontradictory. It is difficult for spreadsheet developers to identify a\nspreadsheet engineering methodology that is appropriate for their class of\nspreadsheet, with its unique combination of goals, type of problem, and\navailable time and resources. There is a lack of well-organized, proven\nmethodologies with known costs and benefits for well-defined spreadsheet\nclasses. It is difficult to compare and critically evaluate methodologies. We\npresent a paradigm for organizing and interpreting spreadsheet engineering\nrecommendations. It systematically addresses the myriad choices made when\ndeveloping a spreadsheet, and explicitly considers resource constraints and\nother development parameters. This paradigm provides a framework for\nevaluation, comparison, and selection of methodologies, and a list of essential\nelements for developers or codifiers of new methodologies. This paradigm\nidentifies gaps in our knowledge that merit further research.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 26 Feb 2008 21:58:26 GMT"
            }
        ],
        "update_date": "2008-02-28",
        "authors_parsed": [
            [
                "Grossman",
                "Thomas A.",
                ""
            ],
            [
                "Ozluk",
                "Ozgur",
                ""
            ]
        ]
    },
    {
        "id": "0802.3924",
        "submitter": "Grenville Croll",
        "authors": "Markus Clermont",
        "title": "A Toolkit for Scalable Spreadsheet Visualization",
        "comments": "12 Pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2004 95-106\n  ISBN 1 902724 94 1",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a toolkit for spreadsheet visualization based on logical\nareas, semantic classes and data modules. Logical areas, semantic classes and\ndata modules are abstract representations of spreadsheet programs that are\nmeant to reduce the auditing and comprehension effort, especially for large and\nregular spreadsheets. The toolkit is integrated as a plug-in in the Gnumeric\nspreadsheet system for Linux. It can process large, industry scale spreadsheet\nprograms in reasonable time and is tightly integrated with its host spreadsheet\nsystem. Users can generate hierarchical and graph-based representations of\ntheir spreadsheets. This allows them to spot conceptual similarities in\ndifferent regions of the spreadsheet, that would otherwise not fit on a screen.\nAs it is assumed that the learning effort for effective use of such a tool\nshould be kept low, we aim for intuitive handling of most of the tool's\nfunctions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 26 Feb 2008 22:11:24 GMT"
            }
        ],
        "update_date": "2008-02-28",
        "authors_parsed": [
            [
                "Clermont",
                "Markus",
                ""
            ]
        ]
    },
    {
        "id": "0802.3939",
        "submitter": "Grenville Croll",
        "authors": "Sabine Hipfl",
        "title": "Using Layout Information for Spreadsheet Visualization",
        "comments": "13 pages, 3 colour figures",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2004 107-119\n  ISBN 1 902724 94 1",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper extends a spreadsheet visualization technique by using layout\ninformation. The original approach identifies logically or semantically related\ncells by relying exclusively on the content of cells for identifying semantic\nclasses. A disadvantage of semantic classes is that users have to supply\nparameters which describe the possible shapes of these blocks. The correct\nparametrization requires a certain degree of experience and is thus not\nsuitable for untrained users. To avoid this constraint, the approach reported\nin this paper uses row/column-labels as well as common format information for\nlocating areas with common, recurring semantics. Heuristics are provided to\ndistinguish between cell groups with intended common semantics and cell groups\nrelated in an ad-hoc manner.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Feb 2008 00:14:34 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Hipfl",
                "Sabine",
                ""
            ]
        ]
    },
    {
        "id": "0802.3940",
        "submitter": "Grenville Croll",
        "authors": "Jocelyn Paine",
        "title": "Spreadsheet Structure Discovery with Logic Programming",
        "comments": "11 pages, code fragments",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2004 121-133\n  ISBN 1 902724 94 1",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our term \"structure discovery\" denotes the recovery of structure, such as the\ngrouping of cells, that was intended by a spreadsheet's author but is not\nexplicit in the spreadsheet. We are implementing structure discovery tools in\nthe logic-programming language Prolog for our spreadsheet analysis program\nModel Master, by writing grammars for spreadsheet structures. The objective is\nan \"intelligent structure monitor\" to run beside Excel, allowing users to\nreconfigure spreadsheets to the representational needs of the task at hand.\nThis could revolutionise spreadsheet \"best practice\". We also describe a\nformulation of spreadsheet reverse-engineering based on \"arrows\".\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Feb 2008 00:25:47 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Paine",
                "Jocelyn",
                ""
            ]
        ]
    },
    {
        "id": "0802.4018",
        "submitter": "Luc Maranget",
        "authors": "Qin Ma and Luc Maranget",
        "title": "Algebraic Pattern Matching in Join Calculus",
        "comments": null,
        "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 1 (March 21,\n  2008) lmcs:770",
        "doi": "10.2168/LMCS-4(1:7)2008",
        "report-no": null,
        "categories": "cs.PL cs.DC",
        "license": null,
        "abstract": "  We propose an extension of the join calculus with pattern matching on\nalgebraic data types. Our initial motivation is twofold: to provide an\nintuitive semantics of the interaction between concurrency and pattern\nmatching; to define a practical compilation scheme from extended join\ndefinitions into ordinary ones plus ML pattern matching. To assess the\ncorrectness of our compilation scheme, we develop a theory of the applied join\ncalculus, a calculus with value passing and value matching. We implement this\ncalculus as an extension of the current JoCaml system.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Feb 2008 13:21:51 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 21 Mar 2008 11:22:39 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Ma",
                "Qin",
                ""
            ],
            [
                "Maranget",
                "Luc",
                ""
            ]
        ]
    },
    {
        "id": "0802.4126",
        "submitter": "Alexei Botchkarev",
        "authors": "Peter Andru, Alexei Botchkarev",
        "title": "Hospital Case Cost Estimates Modelling - Algorithm Comparison",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.DB",
        "license": "http://creativecommons.org/licenses/publicdomain/",
        "abstract": "  Ontario (Canada) Health System stakeholders support the idea and necessity of\nthe integrated source of data that would include both clinical (e.g. diagnosis,\nintervention, length of stay, case mix group) and financial (e.g. cost per\nweighted case, cost per diem) characteristics of the Ontario healthcare system\nactivities at the patient-specific level. At present, the actual patient-level\ncase costs in the explicit form are not available in the financial databases\nfor all hospitals. The goal of this research effort is to develop financial\nmodels that will assign each clinical case in the patient-specific data\nwarehouse a dollar value, representing the cost incurred by the Ontario health\ncare facility which treated the patient. Five mathematical models have been\ndeveloped and verified using real dataset. All models can be classified into\ntwo groups based on their underlying method: 1. Models based on using relative\nintensity weights of the cases, and 2. Models based on using cost per diem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 28 Feb 2008 04:56:48 GMT"
            }
        ],
        "update_date": "2008-02-29",
        "authors_parsed": [
            [
                "Andru",
                "Peter",
                ""
            ],
            [
                "Botchkarev",
                "Alexei",
                ""
            ]
        ]
    },
    {
        "id": "0802.4126",
        "submitter": "Alexei Botchkarev",
        "authors": "Peter Andru, Alexei Botchkarev",
        "title": "Hospital Case Cost Estimates Modelling - Algorithm Comparison",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.DB",
        "license": "http://creativecommons.org/licenses/publicdomain/",
        "abstract": "  Ontario (Canada) Health System stakeholders support the idea and necessity of\nthe integrated source of data that would include both clinical (e.g. diagnosis,\nintervention, length of stay, case mix group) and financial (e.g. cost per\nweighted case, cost per diem) characteristics of the Ontario healthcare system\nactivities at the patient-specific level. At present, the actual patient-level\ncase costs in the explicit form are not available in the financial databases\nfor all hospitals. The goal of this research effort is to develop financial\nmodels that will assign each clinical case in the patient-specific data\nwarehouse a dollar value, representing the cost incurred by the Ontario health\ncare facility which treated the patient. Five mathematical models have been\ndeveloped and verified using real dataset. All models can be classified into\ntwo groups based on their underlying method: 1. Models based on using relative\nintensity weights of the cases, and 2. Models based on using cost per diem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 28 Feb 2008 04:56:48 GMT"
            }
        ],
        "update_date": "2008-02-29",
        "authors_parsed": [
            [
                "Andru",
                "Peter",
                ""
            ],
            [
                "Botchkarev",
                "Alexei",
                ""
            ]
        ]
    },
    {
        "id": "0802.4191",
        "submitter": "Christine Plumejeaud",
        "authors": "Christine Plumejeaud (INRIA Rh\\^one-Alpes / LIG Laboratoire\n  d'Informatique de Grenoble), Jean-Marc Vincent (INRIA Rh\\^one-Alpes / LIG\n  laboratoire d'Informatique de Grenoble), Claude Grasland (GC, RIATE),\n  J\\'er\\^ome Gensel (LSR - IMAG), H\\'el\\`ene Mathian (GC), Serge Guelton (INRIA\n  Rh\\^one-Alpes / LIG laboratoire d'Informatique de Grenoble), Jo\\\"el Boulier\n  (GC)",
        "title": "HyperSmooth : calcul et visualisation de cartes de potentiel\n  interactives",
        "comments": null,
        "journal-ref": "Dans SAGEO 2007, Rencontres internationales G\\'eomatique et\n  territoire. CdRom. - SAGEO 2007, Rencontres internationales G\\'eomatique et\n  territoire, France (2007)",
        "doi": null,
        "report-no": null,
        "categories": "stat.AP cs.HC",
        "license": null,
        "abstract": "  The HyperCarte research group wishes to offer a new cartographic tool for\nspatial analysis of social data, using the potential smoothing method. The\npurpose of this method is to view the spreading of phenomena's in a continuous\nway, at a macroscopic scale, basing on data sampled on administrative areas. We\naim to offer an interactive tool, accessible via the Web, but guarantying the\nconfidentiality of data. The major difficulty is induced by the high complexity\nof the calculus, working on a great amount of data. We present our solution to\nsuch a technical challenge, and our perspectives of enhancements.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 28 Feb 2008 12:36:41 GMT"
            }
        ],
        "update_date": "2008-02-29",
        "authors_parsed": [
            [
                "Plumejeaud",
                "Christine",
                "",
                "INRIA Rh\u00f4ne-Alpes / LIG Laboratoire\n  d'Informatique de Grenoble"
            ],
            [
                "Vincent",
                "Jean-Marc",
                "",
                "INRIA Rh\u00f4ne-Alpes / LIG\n  laboratoire d'Informatique de Grenoble"
            ],
            [
                "Grasland",
                "Claude",
                "",
                "GC, RIATE"
            ],
            [
                "Gensel",
                "J\u00e9r\u00f4me",
                "",
                "LSR - IMAG"
            ],
            [
                "Mathian",
                "H\u00e9l\u00e8ne",
                "",
                "GC"
            ],
            [
                "Guelton",
                "Serge",
                "",
                "INRIA\n  Rh\u00f4ne-Alpes / LIG laboratoire d'Informatique de Grenoble"
            ],
            [
                "Boulier",
                "Jo\u00ebl",
                "",
                "GC"
            ]
        ]
    },
    {
        "id": "0803.0014",
        "submitter": "Peter Schneider-Kamp",
        "authors": "P. Schneider-Kamp, J. Giesl, A. Serebrenik, R. Thiemann",
        "title": "Automated Termination Proofs for Logic Programs by Term Rewriting",
        "comments": "49 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.AI cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are two kinds of approaches for termination analysis of logic programs:\n\"transformational\" and \"direct\" ones. Direct approaches prove termination\ndirectly on the basis of the logic program. Transformational approaches\ntransform a logic program into a term rewrite system (TRS) and then analyze\ntermination of the resulting TRS instead. Thus, transformational approaches\nmake all methods previously developed for TRSs available for logic programs as\nwell. However, the applicability of most existing transformations is quite\nrestricted, as they can only be used for certain subclasses of logic programs.\n(Most of them are restricted to well-moded programs.) In this paper we improve\nthese transformations such that they become applicable for any definite logic\nprogram. To simulate the behavior of logic programs by TRSs, we slightly modify\nthe notion of rewriting by permitting infinite terms. We show that our\ntransformation results in TRSs which are indeed suitable for automated\ntermination analysis. In contrast to most other methods for termination of\nlogic programs, our technique is also sound for logic programming without occur\ncheck, which is typically used in practice. We implemented our approach in the\ntermination prover AProVE and successfully evaluated it on a large collection\nof examples.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 2 Mar 2008 14:53:01 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 1 Sep 2008 09:50:07 GMT"
            }
        ],
        "update_date": "2008-09-01",
        "authors_parsed": [
            [
                "Schneider-Kamp",
                "P.",
                ""
            ],
            [
                "Giesl",
                "J.",
                ""
            ],
            [
                "Serebrenik",
                "A.",
                ""
            ],
            [
                "Thiemann",
                "R.",
                ""
            ]
        ]
    },
    {
        "id": "0803.0015",
        "submitter": "Grenville Croll",
        "authors": "Simon Murphy",
        "title": "EuSpRIG 2006 Commercial Spreadsheet Review",
        "comments": "8 Pages, 9 Colour Diagrams",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2006 45-52\n  ISBN:1-905617-08-9",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This management summary provides an outline of a commercial spreadsheet\nreview process. The aim of this process is to ensure remedial or enhancement\nwork can safely be undertaken on a spreadsheet with a commercially acceptable\nlevel of risk of introducing new errors.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 29 Feb 2008 21:49:18 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Murphy",
                "Simon",
                ""
            ]
        ]
    },
    {
        "id": "0803.0032",
        "submitter": "Srivatsava Ranjit Ganta",
        "authors": "Srivatsava Ranjit Ganta, Shiva Prasad Kasiviswanathan, Adam Smith",
        "title": "Composition Attacks and Auxiliary Information in Data Privacy",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.CR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Privacy is an increasingly important aspect of data publishing. Reasoning\nabout privacy, however, is fraught with pitfalls. One of the most significant\nis the auxiliary information (also called external knowledge, background\nknowledge, or side information) that an adversary gleans from other channels\nsuch as the web, public records, or domain knowledge. This paper explores how\none can reason about privacy in the face of rich, realistic sources of\nauxiliary information. Specifically, we investigate the effectiveness of\ncurrent anonymization schemes in preserving privacy when multiple organizations\nindependently release anonymized data about overlapping populations. 1. We\ninvestigate composition attacks, in which an adversary uses independent\nanonymized releases to breach privacy. We explain why recently proposed models\nof limited auxiliary information fail to capture composition attacks. Our\nexperiments demonstrate that even a simple instance of a composition attack can\nbreach privacy in practice, for a large class of currently proposed techniques.\nThe class includes k-anonymity and several recent variants. 2. On a more\npositive note, certain randomization-based notions of privacy (such as\ndifferential privacy) provably resist composition attacks and, in fact, the use\nof arbitrary side information. This resistance enables stand-alone design of\nanonymization schemes, without the need for explicitly keeping track of other\nreleases. We provide a precise formulation of this property, and prove that an\nimportant class of relaxations of differential privacy also satisfy the\nproperty. This significantly enlarges the class of protocols known to enable\nmodular design.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 1 Mar 2008 00:36:12 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 31 Mar 2008 16:23:40 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Ganta",
                "Srivatsava Ranjit",
                ""
            ],
            [
                "Kasiviswanathan",
                "Shiva Prasad",
                ""
            ],
            [
                "Smith",
                "Adam",
                ""
            ]
        ]
    },
    {
        "id": "0803.0146",
        "submitter": "Dorit Hochbaum",
        "authors": "Dorit S. Hochbaum",
        "title": "Polynomial time algorithms for bi-criteria, multi-objective and ratio\n  problems in clustering and imaging. Part I: Normalized cut and ratio regions",
        "comments": "15 pages, 4 figures",
        "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  May 2010 32:5 889-898",
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.DM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Partitioning and grouping of similar objects plays a fundamental role in\nimage segmentation and in clustering problems. In such problems a typical goal\nis to group together similar objects, or pixels in the case of image\nprocessing. At the same time another goal is to have each group distinctly\ndissimilar from the rest and possibly to have the group size fairly large.\nThese goals are often combined as a ratio optimization problem. One example of\nsuch problem is the normalized cut problem, another is the ratio regions\nproblem. We devise here the first polynomial time algorithms solving these\nproblems optimally. The algorithms are efficient and combinatorial. This\ncontrasts with the heuristic approaches used in the image segmentation\nliterature that formulate those problems as nonlinear optimization problems,\nwhich are then relaxed and solved with spectral techniques in real numbers.\nThese approaches not only fail to deliver an optimal solution, but they are\nalso computationally expensive. The algorithms presented here use as a\nsubroutine a minimum $s,t-cut procedure on a related graph which is of\npolynomial size. The output consists of the optimal solution to the respective\nratio problem, as well as a sequence of nested solution with respect to any\nrelative weighting of the objectives of the numerator and denominator.\n  An extension of the results here to bi-criteria and multi-criteria objective\nfunctions is presented in part II.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 2 Mar 2008 21:30:58 GMT"
            }
        ],
        "update_date": "2010-10-12",
        "authors_parsed": [
            [
                "Hochbaum",
                "Dorit S.",
                ""
            ]
        ]
    },
    {
        "id": "0803.0162",
        "submitter": "Grenville Croll",
        "authors": "Andrew Kumiega, Ben Van Vliet",
        "title": "A Software Development Methodology for Research and Prototyping in\n  Financial Markets",
        "comments": "22 Pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2006 107-127\n  ISBN:1-905617-08-9",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of this paper is to develop a standardized methodology for\nsoftware development in the very unique industry and culture of financial\nmarkets. The prototyping process we present allows the development team to\ndeliver for review and comment intermediate-level models based upon clearly\ndefined customer requirements. This spreadsheet development methodology is\npresented within a larger business context, that of trading system development,\nthe subject of an upcoming book by the authors of this paper.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Mar 2008 01:05:40 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Kumiega",
                "Andrew",
                ""
            ],
            [
                "Van Vliet",
                "Ben",
                ""
            ]
        ]
    },
    {
        "id": "0803.0163",
        "submitter": "Grenville Croll",
        "authors": "Jocelyn Paine, Emre Tek, Duncan Williamson",
        "title": "Rapid Spreadsheet Reshaping with Excelsior: multiple drastic changes to\n  content and layout are easy when you represent enough structure",
        "comments": "18 Pages, code examples",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2006 129-146\n  ISBN:1-905617-08-9",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Spreadsheets often need changing in ways made tedious and risky by Excel. For\nexample: simultaneously altering many tables' size, orientation, and position;\ninserting cross-tabulations; moving data between sheets; splitting and merging\nsheets. A safer, faster restructuring tool is, we claim, Excelsior. The result\nof a research project into reducing spreadsheet risk, Excelsior is the first\never tool for modularising spreadsheets; i.e. for building them from components\nwhich can be independently created, tested, debugged, and updated. It\nrepresents spreadsheets in a way that makes these components explicit,\nseparates them from layout, and allows both components and layout to be changed\nwithout breaking dependent formulae. Here, we report experiments to test that\nthis does indeed make such changes easier. In one, we automatically generated a\ncross-tabulation and added it to a spreadsheet. In the other, we generated new\nversions of a 10,000-cell housing-finance spreadsheet containing many\ninterconnected 20*40 tables. We varied table sizes from 5*10 to 200*2,000;\nmoved tables between sheets; and flipped table orientations. Each change\ngenerated a spreadsheet with different structure but identical outputs; each\nchange took just a few minutes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Mar 2008 01:11:39 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Paine",
                "Jocelyn",
                ""
            ],
            [
                "Tek",
                "Emre",
                ""
            ],
            [
                "Williamson",
                "Duncan",
                ""
            ]
        ]
    },
    {
        "id": "0803.0164",
        "submitter": "Grenville Croll",
        "authors": "Simon Thorne, David Ball",
        "title": "Considering Functional Spreadsheet Operator Usage Suggests the Value of\n  Example Driven Modelling for Decision Support Systems",
        "comments": "12 Pages, 6 Figures, 3 Tables",
        "journal-ref": "roc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2006 147-158\n  ISBN:1-905617-08-9",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most spreadsheet surveys both for reporting use and error focus on the\npractical application of the spreadsheet in a particular industry. Typically\nthese studies will illustrate that a particular percentage of spreadsheets are\nused for optimisation and a further percentage are used for 'What if' analysis.\nMuch less common is examining the classes of function, as defined by the\nvendor, used by modellers to build their spreadsheet models. This alternative\nanalysis allows further insight into the programming nature of spreadsheets and\nmay assist researchers in targeting particular structures in spreadsheet\nsoftware for further investigation. Further, understanding the functional\nmake-up of spreadsheets allows effective evaluation of novel approaches from a\nprogramming point of view. It allows greater insight into studies that report\nwhat spreadsheets are used for since it is explicit which functional structures\nare in use in spreadsheets. We conclude that a deeper understanding of the use\nof operators and the operator's relationship to error would provide fresh\ninsight into the spreadsheet error problem. Considering functional spreadsheet\noperator usage suggests the value of Example Driven Modelling for Decision\nSupport Systems\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Mar 2008 01:25:41 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Thorne",
                "Simon",
                ""
            ],
            [
                "Ball",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0803.0164",
        "submitter": "Grenville Croll",
        "authors": "Simon Thorne, David Ball",
        "title": "Considering Functional Spreadsheet Operator Usage Suggests the Value of\n  Example Driven Modelling for Decision Support Systems",
        "comments": "12 Pages, 6 Figures, 3 Tables",
        "journal-ref": "roc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2006 147-158\n  ISBN:1-905617-08-9",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most spreadsheet surveys both for reporting use and error focus on the\npractical application of the spreadsheet in a particular industry. Typically\nthese studies will illustrate that a particular percentage of spreadsheets are\nused for optimisation and a further percentage are used for 'What if' analysis.\nMuch less common is examining the classes of function, as defined by the\nvendor, used by modellers to build their spreadsheet models. This alternative\nanalysis allows further insight into the programming nature of spreadsheets and\nmay assist researchers in targeting particular structures in spreadsheet\nsoftware for further investigation. Further, understanding the functional\nmake-up of spreadsheets allows effective evaluation of novel approaches from a\nprogramming point of view. It allows greater insight into studies that report\nwhat spreadsheets are used for since it is explicit which functional structures\nare in use in spreadsheets. We conclude that a deeper understanding of the use\nof operators and the operator's relationship to error would provide fresh\ninsight into the spreadsheet error problem. Considering functional spreadsheet\noperator usage suggests the value of Example Driven Modelling for Decision\nSupport Systems\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Mar 2008 01:25:41 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Thorne",
                "Simon",
                ""
            ],
            [
                "Ball",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0803.0165",
        "submitter": "Grenville Croll",
        "authors": "Raymond Payette",
        "title": "Documenting Spreadsheets",
        "comments": "12 Pages, 15 screen shots",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2006 163-173\n  ISBN:1-905617-08-9",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper discusses spreadsheets documentation and new means to achieve this\nend by using Excel's built-in \"Comment\" function. By structuring comments, they\ncan be used as an essential tool to fully explain spreadsheet. This will\ngreatly facilitate spreadsheet change control, risk management and auditing. It\nwill fill a crucial gap in corporate governance by adding essential information\nthat can be managed in order to satisfy internal controls and accountability\nstandards.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Mar 2008 01:34:30 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Payette",
                "Raymond",
                ""
            ]
        ]
    },
    {
        "id": "0803.0166",
        "submitter": "Grenville Croll",
        "authors": "Richard Brath, Michael Peters",
        "title": "Spreadsheet Validation and Analysis through Content Visualization",
        "comments": "10 Pages, 11 Colour Figures",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2006 175-183\n  ISBN:1-905617-08-9",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Visualizing spreadsheet content provides analytic insight and visual\nvalidation of large amounts of spreadsheet data. Oculus Excel Visualizer is a\npoint and click data visualization experiment which directly visualizes Excel\ndata and re-uses the layout and formatting already present in the spreadsheet.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Mar 2008 01:40:31 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Brath",
                "Richard",
                ""
            ],
            [
                "Peters",
                "Michael",
                ""
            ]
        ]
    },
    {
        "id": "0803.0167",
        "submitter": "Grenville Croll",
        "authors": "Michael Purser, David Chadwick",
        "title": "Does an awareness of differing types of spreadsheet errors aid end-users\n  in identifying spreadsheets errors?",
        "comments": "20 Pages, 14 Tables and Figures, many in colour",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2006 185-204\n  ISBN:1-905617-08-9",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The research presented in this paper establishes a valid, and simplified,\nrevision of previous spreadsheet error classifications. This investigation is\nconcerned with the results of a web survey and two web-based gender and\ndomain-knowledge free spreadsheet error identification exercises. The\nparticipants of the survey and exercises were a test group of professionals\n(all of whom regularly use spreadsheets) and a control group of students from\nthe University of Greenwich (UK). The findings show that over 85% of users are\nalso the spreadsheet's developer, supporting the revised spreadsheet error\nclassification. The findings also show that spreadsheet error identification\nability is directly affected both by spreadsheet experience and by error-type\nawareness. In particular, that spreadsheet error-type awareness significantly\nimproves the user's ability to identify, the more surreptitious, qualitative\nerror.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Mar 2008 01:49:03 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Purser",
                "Michael",
                ""
            ],
            [
                "Chadwick",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0803.0168",
        "submitter": "Grenville Croll",
        "authors": "Kenneth R. Baker, Stephen G. Powell, Barry Lawson, and Lynn\n  Foster-Johnson",
        "title": "Comparison of Characteristics and Practices amongst Spreadsheet Users\n  with Different Levels of Experience",
        "comments": "16 Pages, 11 Tables",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2006 205-219\n  ISBN:1-905617-08-9",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We developed an internet-based questionnaire on spreadsheet use that we\nadministered to a large number of users in several companies and organizations\nto document how spreadsheets are currently being developed and used in\nbusiness. In this paper, we discuss the results drawn from of a comparison of\nresponses from individuals with the most experience and expertise with those\nfrom individuals with the least. These results describe two views of\nspreadsheet design and use in organizations, and reflect gaps between these two\ngroups and between these groups and the entire population of nearly 1600\nrespondents. Moreover, our results indicate that these gaps have multiple\ndimensions: they reflect not only the context, skill, and practices of\nindividual users but also the policies of large organizations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Mar 2008 01:55:40 GMT"
            }
        ],
        "update_date": "2008-03-10",
        "authors_parsed": [
            [
                "Baker",
                "Kenneth R.",
                ""
            ],
            [
                "Powell",
                "Stephen G.",
                ""
            ],
            [
                "Lawson",
                "Barry",
                ""
            ],
            [
                "Foster-Johnson",
                "Lynn",
                ""
            ]
        ]
    },
    {
        "id": "0803.0189",
        "submitter": "Sebastien Tixeuil",
        "authors": "Toshimitsu Masuzawa, S\\'ebastien Tixeuil (LIP6, INRIA Futurs)",
        "title": "Quiescence of Self-stabilizing Gossiping among Mobile Agents in Graphs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.PF cs.RO",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers gossiping among mobile agents in graphs: agents move on\nthe graph and have to disseminate their initial information to every other\nagent. We focus on self-stabilizing solutions for the gossip problem, where\nagents may start from arbitrary locations in arbitrary states.\nSelf-stabilization requires (some of the) participating agents to keep moving\nforever, hinting at maximizing the number of agents that could be allowed to\nstop moving eventually. This paper formalizes the self-stabilizing agent gossip\nproblem, introduces the quiescence number (i.e., the maximum number of\neventually stopping agents) of self-stabilizing solutions and investigates the\nquiescence number with respect to several assumptions related to agent\nanonymity, synchrony, link duplex capacity, and whiteboard capacity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Mar 2008 09:14:21 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 4 Mar 2008 19:19:25 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Masuzawa",
                "Toshimitsu",
                "",
                "LIP6, INRIA Futurs"
            ],
            [
                "Tixeuil",
                "S\u00e9bastien",
                "",
                "LIP6, INRIA Futurs"
            ]
        ]
    },
    {
        "id": "0803.0194",
        "submitter": "Radu Arsinte",
        "authors": "Radu Arsinte, Costin Miron",
        "title": "Acquisition Accuracy Evaluation in Visual Inspection Systems - a\n  Practical Approach",
        "comments": "6 pages, 4 figures, ETc'96 Conference paper",
        "journal-ref": "Proceeedings of ETc '96 Conference, 1996, Timisoara, Romania",
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.MM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper draws a proposal of a set of parameters and methods for accuracy\nevaluation of visual inspection systems. The case of a monochrome board is\ntreated, but practically all conclusions and methods may be extended for colour\nacquisition. Basically, the proposed parameters are grouped in five sets as\nfollows:Internal noise;Video ADC cuantisation parameters;Analogue processing\nsection parameters;Dominant frequencies;Synchronisation (lock-in) accuracy. On\nbasis of this set of parameters was developed a software environment, in\nconjunction with a test signal generator that allows the \"test\" images. The\npaper also presents conclusions of evaluation for two types of video\nacquisition boards\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 3 Mar 2008 08:57:10 GMT"
            }
        ],
        "update_date": "2008-03-04",
        "authors_parsed": [
            [
                "Arsinte",
                "Radu",
                ""
            ],
            [
                "Miron",
                "Costin",
                ""
            ]
        ]
    },
    {
        "id": "0803.0439",
        "submitter": "Christoph Lauter",
        "authors": "Florent De Dinechin (LIP), Christoph Quirin Lauter (LIP)",
        "title": "Optimizing polynomials for floating-point implementation",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NA cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The floating-point implementation of a function on an interval often reduces\nto polynomial approximation, the polynomial being typically provided by Remez\nalgorithm. However, the floating-point evaluation of a Remez polynomial\nsometimes leads to catastrophic cancellations. This happens when some of the\npolynomial coefficients are very small in magnitude with respects to others. In\nthis case, it is better to force these coefficients to zero, which also reduces\nthe operation count. This technique, classically used for odd or even\nfunctions, may be generalized to a much larger class of functions. An algorithm\nis presented that forces to zero the smaller coefficients of the initial\npolynomial thanks to a modified Remez algorithm targeting an incomplete\nmonomial basis. One advantage of this technique is that it is purely numerical,\nthe function being used as a numerical black box. This algorithm is implemented\nwithin a larger polynomial implementation tool that is demonstrated on a range\nof examples, resulting in polynomials with less coefficients than those\nobtained the usual way.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 4 Mar 2008 13:49:44 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "De Dinechin",
                "Florent",
                "",
                "LIP"
            ],
            [
                "Lauter",
                "Christoph Quirin",
                "",
                "LIP"
            ]
        ]
    },
    {
        "id": "0803.0450",
        "submitter": "Debprakash Patnaik",
        "authors": "Debprakash Patnaik (Electical Engg. Dept., Indian Institute of\n  Science, Bangalore), and P. S. Sastry (Electrical Engg. Dept., Indian\n  Institute of Science, Bangalore), and K. P. Unnikrishnan (General Motors R&D,\n  Warren)",
        "title": "Inferring Neuronal Network Connectivity from Spike Data: A Temporal\n  Datamining Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB q-bio.NC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Understanding the functioning of a neural system in terms of its underlying\ncircuitry is an important problem in neuroscience. Recent developments in\nelectrophysiology and imaging allow one to simultaneously record activities of\nhundreds of neurons. Inferring the underlying neuronal connectivity patterns\nfrom such multi-neuronal spike train data streams is a challenging statistical\nand computational problem. This task involves finding significant temporal\npatterns from vast amounts of symbolic time series data. In this paper we show\nthat the frequent episode mining methods from the field of temporal data mining\ncan be very useful in this context. In the frequent episode discovery\nframework, the data is viewed as a sequence of events, each of which is\ncharacterized by an event type and its time of occurrence and episodes are\ncertain types of temporal patterns in such data. Here we show that, using the\nset of discovered frequent episodes from multi-neuronal data, one can infer\ndifferent types of connectivity patterns in the neural system that generated\nit. For this purpose, we introduce the notion of mining for frequent episodes\nunder certain temporal constraints; the structure of these temporal constraints\nis motivated by the application. We present algorithms for discovering serial\nand parallel episodes under these temporal constraints. Through extensive\nsimulation studies we demonstrate that these methods are useful for unearthing\npatterns of neuronal network connectivity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 4 Mar 2008 14:11:38 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 11 Mar 2008 02:24:13 GMT"
            }
        ],
        "update_date": "2008-03-11",
        "authors_parsed": [
            [
                "Patnaik",
                "Debprakash",
                "",
                "Electical Engg. Dept., Indian Institute of\n  Science, Bangalore"
            ],
            [
                "Sastry",
                "P. S.",
                "",
                "Electrical Engg. Dept., Indian\n  Institute of Science, Bangalore"
            ],
            [
                "Unnikrishnan",
                "K. P.",
                "",
                "General Motors R&D,\n  Warren"
            ]
        ]
    },
    {
        "id": "0803.0666",
        "submitter": "Soumaya El Kadiri",
        "authors": "Soumaya El Kadiri (LIESP), Philippe Pernelle (LIESP), Miguel Delattre\n  (LIESP), Abdelaziz Bouras (LIESP)",
        "title": "An approach to control collaborative processes in PLM systems",
        "comments": null,
        "journal-ref": "Dans Workshop on Extended Product and Process Analysis aNd Design\n  - Extended Product and Process Analysis aNd Design, Bordeaux : France (2008)",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Companies that collaborate within the product development processes need to\nimplement an effective management of their collaborative activities. Despite\nthe implementation of a PLM system, the collaborative activities are not\nefficient as it might be expected. This paper presents an analysis of the\nproblems related to the collaborative work using a PLM system. From this\nanalysis, we propose an approach for improving collaborative processes within a\nPLM system, based on monitoring indicators. This approach leads to identify and\ntherefore to mitigate the brakes of the collaborative work.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 5 Mar 2008 14:04:19 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Kadiri",
                "Soumaya El",
                "",
                "LIESP"
            ],
            [
                "Pernelle",
                "Philippe",
                "",
                "LIESP"
            ],
            [
                "Delattre",
                "Miguel",
                "",
                "LIESP"
            ],
            [
                "Bouras",
                "Abdelaziz",
                "",
                "LIESP"
            ]
        ]
    },
    {
        "id": "0803.0862",
        "submitter": "Jose M. Martin-Garcia",
        "authors": "Jose M. Martin-Garcia",
        "title": "xPerm: fast index canonicalization for tensor computer algebra",
        "comments": "16 pages, 3 figures. Package can be downloaded from\n  http://metric.iem.csic.es/Martin-Garcia/xAct/",
        "journal-ref": "Comp. Phys. Commun. 179 (2008) 597-603",
        "doi": "10.1016/j.cpc.2008.05.009",
        "report-no": null,
        "categories": "cs.SC gr-qc hep-th",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a very fast implementation of the Butler-Portugal algorithm for\nindex canonicalization with respect to permutation symmetries. It is called\nxPerm, and has been written as a combination of a Mathematica package and a C\nsubroutine. The latter performs the most demanding parts of the computations\nand can be linked from any other program or computer algebra system. We\ndemonstrate with tests and timings the effectively polynomial performance of\nthe Butler-Portugal algorithm with respect to the number of indices, though we\nalso show a case in which it is exponential. Our implementation handles generic\ntensorial expressions with several dozen indices in hundredths of a second, or\none hundred indices in a few seconds, clearly outperforming all other current\ncanonicalizers. The code has been already under intensive testing for several\nyears and has been essential in recent investigations in large-scale tensor\ncomputer algebra.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Mar 2008 13:26:32 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Martin-Garcia",
                "Jose M.",
                ""
            ]
        ]
    },
    {
        "id": "0803.0874",
        "submitter": "Milan Batista",
        "authors": "Milan Batista",
        "title": "A Method for Solving Cyclic Block Penta-diagonal Systems of Linear\n  Equations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A method for solving cyclic block three-diagonal systems of equations is\ngeneralized for solving a block cyclic penta-diagonal system of equations.\nIntroducing a special form of two new variables the original system is split\ninto three block pentagonal systems, which can be solved by the known methods.\nAs such method belongs to class of direct methods without pivoting.\nImplementation of the algorithm is discussed in some details and the numerical\nexamples are present.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Mar 2008 18:45:39 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 8 Mar 2008 11:03:56 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 14 Mar 2008 07:35:16 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Batista",
                "Milan",
                ""
            ]
        ]
    },
    {
        "id": "0803.0924",
        "submitter": "Shiva Kasiviswanathan",
        "authors": "Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya\n  Raskhodnikova, and Adam Smith",
        "title": "What Can We Learn Privately?",
        "comments": "35 pages, 2 figures",
        "journal-ref": "SIAM Journal of Computing 40(3) (2011) 793-826",
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.CC cs.CR cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Learning problems form an important category of computational tasks that\ngeneralizes many of the computations researchers apply to large real-life data\nsets. We ask: what concept classes can be learned privately, namely, by an\nalgorithm whose output does not depend too heavily on any one input or specific\ntraining example? More precisely, we investigate learning algorithms that\nsatisfy differential privacy, a notion that provides strong confidentiality\nguarantees in contexts where aggregate information is released about a database\ncontaining sensitive information about individuals. We demonstrate that,\nignoring computational constraints, it is possible to privately agnostically\nlearn any concept class using a sample size approximately logarithmic in the\ncardinality of the concept class. Therefore, almost anything learnable is\nlearnable privately: specifically, if a concept class is learnable by a\n(non-private) algorithm with polynomial sample complexity and output size, then\nit can be learned privately using a polynomial number of samples. We also\npresent a computationally efficient private PAC learner for the class of parity\nfunctions. Local (or randomized response) algorithms are a practical class of\nprivate algorithms that have received extensive investigation. We provide a\nprecise characterization of local private learning algorithms. We show that a\nconcept class is learnable by a local algorithm if and only if it is learnable\nin the statistical query (SQ) model. Finally, we present a separation between\nthe power of interactive and noninteractive local learning algorithms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Mar 2008 17:50:07 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 14 Apr 2008 16:18:44 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 19 Feb 2010 01:47:02 GMT"
            }
        ],
        "update_date": "2012-10-10",
        "authors_parsed": [
            [
                "Kasiviswanathan",
                "Shiva Prasad",
                ""
            ],
            [
                "Lee",
                "Homin K.",
                ""
            ],
            [
                "Nissim",
                "Kobbi",
                ""
            ],
            [
                "Raskhodnikova",
                "Sofya",
                ""
            ],
            [
                "Smith",
                "Adam",
                ""
            ]
        ]
    },
    {
        "id": "0803.0924",
        "submitter": "Shiva Kasiviswanathan",
        "authors": "Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya\n  Raskhodnikova, and Adam Smith",
        "title": "What Can We Learn Privately?",
        "comments": "35 pages, 2 figures",
        "journal-ref": "SIAM Journal of Computing 40(3) (2011) 793-826",
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.CC cs.CR cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Learning problems form an important category of computational tasks that\ngeneralizes many of the computations researchers apply to large real-life data\nsets. We ask: what concept classes can be learned privately, namely, by an\nalgorithm whose output does not depend too heavily on any one input or specific\ntraining example? More precisely, we investigate learning algorithms that\nsatisfy differential privacy, a notion that provides strong confidentiality\nguarantees in contexts where aggregate information is released about a database\ncontaining sensitive information about individuals. We demonstrate that,\nignoring computational constraints, it is possible to privately agnostically\nlearn any concept class using a sample size approximately logarithmic in the\ncardinality of the concept class. Therefore, almost anything learnable is\nlearnable privately: specifically, if a concept class is learnable by a\n(non-private) algorithm with polynomial sample complexity and output size, then\nit can be learned privately using a polynomial number of samples. We also\npresent a computationally efficient private PAC learner for the class of parity\nfunctions. Local (or randomized response) algorithms are a practical class of\nprivate algorithms that have received extensive investigation. We provide a\nprecise characterization of local private learning algorithms. We show that a\nconcept class is learnable by a local algorithm if and only if it is learnable\nin the statistical query (SQ) model. Finally, we present a separation between\nthe power of interactive and noninteractive local learning algorithms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Mar 2008 17:50:07 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 14 Apr 2008 16:18:44 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 19 Feb 2010 01:47:02 GMT"
            }
        ],
        "update_date": "2012-10-10",
        "authors_parsed": [
            [
                "Kasiviswanathan",
                "Shiva Prasad",
                ""
            ],
            [
                "Lee",
                "Homin K.",
                ""
            ],
            [
                "Nissim",
                "Kobbi",
                ""
            ],
            [
                "Raskhodnikova",
                "Sofya",
                ""
            ],
            [
                "Smith",
                "Adam",
                ""
            ]
        ]
    },
    {
        "id": "0803.0954",
        "submitter": "Michael Hahsler",
        "authors": "Michael Hahsler, Christian Buchta, and Kurt Hornik",
        "title": "Selective association rule generation",
        "comments": null,
        "journal-ref": "Computational Statistics, 2007. Online First, Published: 25 July\n  2007",
        "doi": "10.1007/s00180-007-0062-z",
        "report-no": null,
        "categories": "cs.DB cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mining association rules is a popular and well researched method for\ndiscovering interesting relations between variables in large databases. A\npractical problem is that at medium to low support values often a large number\nof frequent itemsets and an even larger number of association rules are found\nin a database. A widely used approach is to gradually increase minimum support\nand minimum confidence or to filter the found rules using increasingly strict\nconstraints on additional measures of interestingness until the set of rules\nfound is reduced to a manageable size. In this paper we describe a different\napproach which is based on the idea to first define a set of ``interesting''\nitemsets (e.g., by a mixture of mining and expert knowledge) and then, in a\nsecond step to selectively generate rules for only these itemsets. The main\nadvantage of this approach over increasing thresholds or filtering rules is\nthat the number of rules found is significantly reduced while at the same time\nit is not necessary to increase the support and confidence thresholds which\nmight lead to missing important information in the database.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Mar 2008 19:43:35 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Hahsler",
                "Michael",
                ""
            ],
            [
                "Buchta",
                "Christian",
                ""
            ],
            [
                "Hornik",
                "Kurt",
                ""
            ]
        ]
    },
    {
        "id": "0803.0966",
        "submitter": "Michael Hahsler",
        "authors": "Michael Hahsler and Kurt Hornik",
        "title": "New probabilistic interest measures for association rules",
        "comments": null,
        "journal-ref": "Intelligent Data Analysis, 11(5):437-455, 2007",
        "doi": "10.3233/IDA-2007-11502",
        "report-no": null,
        "categories": "cs.DB stat.ML",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mining association rules is an important technique for discovering meaningful\npatterns in transaction databases. Many different measures of interestingness\nhave been proposed for association rules. However, these measures fail to take\nthe probabilistic properties of the mined data into account. In this paper, we\nstart with presenting a simple probabilistic framework for transaction data\nwhich can be used to simulate transaction data when no associations are\npresent. We use such data and a real-world database from a grocery outlet to\nexplore the behavior of confidence and lift, two popular interest measures used\nfor rule mining. The results show that confidence is systematically influenced\nby the frequency of the items in the left hand side of rules and that lift\nperforms poorly to filter random noise in transaction data. Based on the\nprobabilistic framework we develop two new interest measures, hyper-lift and\nhyper-confidence, which can be used to filter or order mined association rules.\nThe new measures show significantly better performance than lift for\napplications where spurious rules are problematic.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Mar 2008 20:17:19 GMT"
            }
        ],
        "update_date": "2024-01-01",
        "authors_parsed": [
            [
                "Hahsler",
                "Michael",
                ""
            ],
            [
                "Hornik",
                "Kurt",
                ""
            ]
        ]
    },
    {
        "id": "0803.1110",
        "submitter": "Olivier Ruatta",
        "authors": "Daouda Niang Diatta (XLIM), Bernard Mourrain (INRIA Sophia Antipolis),\n  Olivier Ruatta (XLIM)",
        "title": "On the Computation of the Topology of a Non-Reduced Implicit Space Curve",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.AC cs.CG cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An algorithm is presented for the computation of the topology of a\nnon-reduced space curve defined as the intersection of two implicit algebraic\nsurfaces. It computes a Piecewise Linear Structure (PLS) isotopic to the\noriginal space curve. The algorithm is designed to provide the exact result for\nall inputs. It's a symbolic-numeric algorithm based on subresultant\ncomputation. Simple algebraic criteria are given to certify the output of the\nalgorithm. The algorithm uses only one projection of the non-reduced space\ncurve augmented with adjacency information around some \"particular points\" of\nthe space curve. The algorithm is implemented with the Mathemagix Computer\nAlgebra System (CAS) using the SYNAPS library as a backend.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Mar 2008 15:28:52 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Diatta",
                "Daouda Niang",
                "",
                "XLIM"
            ],
            [
                "Mourrain",
                "Bernard",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Ruatta",
                "Olivier",
                "",
                "XLIM"
            ]
        ]
    },
    {
        "id": "0803.1555",
        "submitter": "Bart Moelans",
        "authors": "Bart Kuijpers, Vanessa Lemmens, Bart Moelans and Karl Tuyls",
        "title": "Privacy Preserving ID3 over Horizontally, Vertically and Grid\n  Partitioned Data",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider privacy preserving decision tree induction via ID3 in the case\nwhere the training data is horizontally or vertically distributed. Furthermore,\nwe consider the same problem in the case where the data is both horizontally\nand vertically distributed, a situation we refer to as grid partitioned data.\nWe give an algorithm for privacy preserving ID3 over horizontally partitioned\ndata involving more than two parties. For grid partitioned data, we discuss two\ndifferent evaluation methods for preserving privacy ID3, namely, first merging\nhorizontally and developing vertically or first merging vertically and next\ndeveloping horizontally. Next to introducing privacy preserving data mining\nover grid-partitioned data, the main contribution of this paper is that we\nshow, by means of a complexity analysis that the former evaluation method is\nthe more efficient.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Mar 2008 11:18:52 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Kuijpers",
                "Bart",
                ""
            ],
            [
                "Lemmens",
                "Vanessa",
                ""
            ],
            [
                "Moelans",
                "Bart",
                ""
            ],
            [
                "Tuyls",
                "Karl",
                ""
            ]
        ]
    },
    {
        "id": "0803.1555",
        "submitter": "Bart Moelans",
        "authors": "Bart Kuijpers, Vanessa Lemmens, Bart Moelans and Karl Tuyls",
        "title": "Privacy Preserving ID3 over Horizontally, Vertically and Grid\n  Partitioned Data",
        "comments": "25 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider privacy preserving decision tree induction via ID3 in the case\nwhere the training data is horizontally or vertically distributed. Furthermore,\nwe consider the same problem in the case where the data is both horizontally\nand vertically distributed, a situation we refer to as grid partitioned data.\nWe give an algorithm for privacy preserving ID3 over horizontally partitioned\ndata involving more than two parties. For grid partitioned data, we discuss two\ndifferent evaluation methods for preserving privacy ID3, namely, first merging\nhorizontally and developing vertically or first merging vertically and next\ndeveloping horizontally. Next to introducing privacy preserving data mining\nover grid-partitioned data, the main contribution of this paper is that we\nshow, by means of a complexity analysis that the former evaluation method is\nthe more efficient.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Mar 2008 11:18:52 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Kuijpers",
                "Bart",
                ""
            ],
            [
                "Lemmens",
                "Vanessa",
                ""
            ],
            [
                "Moelans",
                "Bart",
                ""
            ],
            [
                "Tuyls",
                "Karl",
                ""
            ]
        ]
    },
    {
        "id": "0803.1576",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin and Adrian Adewunmi",
        "title": "Simulation Optimization of the Crossdock Door Assignment Problem",
        "comments": null,
        "journal-ref": "UK Operational Research Society Simulation Workshop 2006 (SW\n  2006), Leamington Spa, UK 2006",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The purpose of this report is to present the Crossdock Door Assignment\nProblem, which involves assigning destinations to outbound dock doors of\nCrossdock centres such that travel distance by material handling equipment is\nminimized. We propose a two fold solution; simulation and optimization of the\nsimulation model simulation optimization. The novel aspect of our solution\napproach is that we intend to use simulation to derive a more realistic\nobjective function and use Memetic algorithms to find an optimal solution. The\nmain advantage of using Memetic algorithms is that it combines a local search\nwith Genetic Algorithms. The Crossdock Door Assignment Problem is a new domain\napplication to Memetic Algorithms and it is yet unknown how it will perform.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Mar 2008 12:56:51 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Adewunmi",
                "Adrian",
                ""
            ]
        ]
    },
    {
        "id": "0803.1586",
        "submitter": "Jeroen Vendrig",
        "authors": "Jarrad Springett, Jeroen Vendrig",
        "title": "Spatio-activity based object detection",
        "comments": "To be submitted to: AVSS 2008 conference",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present the SAMMI lightweight object detection method which has a high\nlevel of accuracy and robustness, and which is able to operate in an\nenvironment with a large number of cameras. Background modeling is based on DCT\ncoefficients provided by cameras. Foreground detection uses similarity in\ntemporal characteristics of adjacent blocks of pixels, which is a\ncomputationally inexpensive way to make use of object coherence. Scene model\nupdating uses the approximated median method for improved performance.\nEvaluation at pixel level and application level shows that SAMMI object\ndetection performs better and faster than the conventional Mixture of Gaussians\nmethod.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Mar 2008 13:40:42 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Springett",
                "Jarrad",
                ""
            ],
            [
                "Vendrig",
                "Jeroen",
                ""
            ]
        ]
    },
    {
        "id": "0803.1604",
        "submitter": "Uwe Aickelin",
        "authors": "Peer-Olaf Siebers, Uwe Aickelin, Helen Celia and Christopher Clegg",
        "title": "Using Intelligent Agents to Understand Management Practices and Retail\n  Productivity",
        "comments": null,
        "journal-ref": "Proceedings of the Winter Simulation Conference (WSC 2007), pp\n  2212-2220, Washington, USA 2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CE cs.MA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Intelligent agents offer a new and exciting way of understanding the world of\nwork. In this paper we apply agent-based modeling and simulation to investigate\na set of problems in a retail context. Specifically, we are working to\nunderstand the relationship between human resource management practices and\nretail productivity. Despite the fact we are working within a relatively novel\nand complex domain, it is clear that intelligent agents could offer potential\nfor fostering sustainable organizational capabilities in the future. The\nproject is still at an early stage. So far we have conducted a case study in a\nUK department store to collect data and capture impressions about operations\nand actors within departments. Furthermore, based on our case study we have\nbuilt and tested our first version of a retail branch simulator which we will\npresent in this paper.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Mar 2008 14:55:58 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Siebers",
                "Peer-Olaf",
                ""
            ],
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Celia",
                "Helen",
                ""
            ],
            [
                "Clegg",
                "Christopher",
                ""
            ]
        ]
    },
    {
        "id": "0803.1621",
        "submitter": "Uwe Aickelin",
        "authors": "Peer-Olaf Siebers, Uwe Aickelin, Helen Celia and Christopher Clegg",
        "title": "An Agent-Based Simulation of In-Store Customer Experiences",
        "comments": null,
        "journal-ref": "Operational Research Society 4th Simulation Workshop (SW08), in\n  print, pp, Worcestershire, UK 2008",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CE cs.MA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Agent-based modelling and simulation offers a new and exciting way of\nunderstanding the world of work. In this paper we describe the development of\nan agent-based simulation model, designed to help to understand the\nrelationship between human resource management practices and retail\nproductivity. We report on the current development of our simulation model\nwhich includes new features concerning the evolution of customers over time. To\ntest some of these features we have conducted a series of experiments dealing\nwith customer pool sizes, standard and noise reduction modes, and the spread of\nthe word of mouth. Our multi-disciplinary research team draws upon expertise\nfrom work psychologists and computer scientists. Despite the fact we are\nworking within a relatively novel and complex domain, it is clear that\nintelligent agents offer potential for fostering sustainable organisational\ncapabilities in the future.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Mar 2008 16:11:34 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Siebers",
                "Peer-Olaf",
                ""
            ],
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Celia",
                "Helen",
                ""
            ],
            [
                "Clegg",
                "Christopher",
                ""
            ]
        ]
    },
    {
        "id": "0803.1723",
        "submitter": "Andrei Sukhov M",
        "authors": "A. P. Platonov, D. I. Sidelnikov, M. V. Strizhov, A. M. Sukhov",
        "title": "Estimation of available bandwidth and measurement infrastructure for\n  Russian segment of Internet",
        "comments": "8 pages, 4 figures, submitted to Telecommunication (www.nait.ru, in\n  Russian)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In paper the method for estimation of available bandwidth is supposed which\ndoes not demand the advanced utilities. Our method is based on the measurement\nof network delay $D$ for packets of different sizes $W$. The simple expression\nfor available bandwidth $B_{av} =(W_2-W_1)/(D_2-D_1)$ is substantiated. For the\nexperimental testing the measurement infrastructure for Russian segment of\nInternet was installed in framework of RFBR grant 06-07-89074.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 12 Mar 2008 08:57:50 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 2 Jul 2008 08:39:05 GMT"
            }
        ],
        "update_date": "2008-07-02",
        "authors_parsed": [
            [
                "Platonov",
                "A. P.",
                ""
            ],
            [
                "Sidelnikov",
                "D. I.",
                ""
            ],
            [
                "Strizhov",
                "M. V.",
                ""
            ],
            [
                "Sukhov",
                "A. M.",
                ""
            ]
        ]
    },
    {
        "id": "0803.1728",
        "submitter": "Uwe Aickelin",
        "authors": "Salwani Abdullah, Uwe Aickelin, Edmund Burke, Aniza Din and Rong Qu",
        "title": "Investigating a Hybrid Metaheuristic For Job Shop Rescheduling",
        "comments": null,
        "journal-ref": "Proceedings of the 3rd Australian Conference on Artificial Life\n  (ACAL07), Lecture Notes in Computer Science 4828, pp 357-368, Gold Coast,\n  Australia 2007",
        "doi": "10.1007/978-3-540-76931-6_31",
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Previous research has shown that artificial immune systems can be used to\nproduce robust schedules in a manufacturing environment. The main goal is to\ndevelop building blocks (antibodies) of partial schedules that can be used to\nconstruct backup solutions (antigens) when disturbances occur during\nproduction. The building blocks are created based upon underpinning ideas from\nartificial immune systems and evolved using a genetic algorithm (Phase I). Each\npartial schedule (antibody) is assigned a fitness value and the best partial\nschedules are selected to be converted into complete schedules (antigens). We\nfurther investigate whether simulated annealing and the great deluge algorithm\ncan improve the results when hybridised with our artificial immune system\n(Phase II). We use ten fixed solutions as our target and measure how well we\ncover these specific scenarios.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 12 Mar 2008 09:26:47 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Abdullah",
                "Salwani",
                ""
            ],
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Burke",
                "Edmund",
                ""
            ],
            [
                "Din",
                "Aniza",
                ""
            ],
            [
                "Qu",
                "Rong",
                ""
            ]
        ]
    },
    {
        "id": "0803.1748",
        "submitter": "Grenville Croll",
        "authors": "Yusuf Jafry, Fredrika Sidoroff, Roger Chi",
        "title": "A Computational Framework for the Near Elimination of Spreadsheet Risk",
        "comments": "9 pages, 3 colour figures",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2006 85-93\n  ISBN:1-905617-08-9",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.CY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present Risk Integrated's Enterprise Spreadsheet Platform (ESP), a\ntechnical approach to the near-elimination of spreadsheet risk in the\nenterprise computing environment, whilst maintaining the full flexibility of\nspreadsheets for modelling complex financial structures and processes. In its\nBasic Mode of use, the system comprises a secure and robust centralised\nspreadsheet management framework. In Advanced Mode, the system can be viewed as\na robust computational framework whereby users can \"submit jobs\" to the\nspreadsheet, and retrieve the results from the computations, but with no direct\naccess to the underlying spreadsheet. An example application, Monte Carlo\nsimulation, is presented to highlight the benefits of this approach with regard\nto mitigating spreadsheet risk in complex, mission-critical, financial\ncalculations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 12 Mar 2008 11:22:12 GMT"
            }
        ],
        "update_date": "2008-03-13",
        "authors_parsed": [
            [
                "Jafry",
                "Yusuf",
                ""
            ],
            [
                "Sidoroff",
                "Fredrika",
                ""
            ],
            [
                "Chi",
                "Roger",
                ""
            ]
        ]
    },
    {
        "id": "0803.1751",
        "submitter": "Grenville Croll",
        "authors": "John Nash, Andy Adler, Neil Smith",
        "title": "TellTable Spreadsheet Audit: from Technical Possibility to Operating\n  Prototype",
        "comments": "11 pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2004\n  45-55ISBN 1 902724 94 1",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  At the 2003 EuSpRIG meeting, we presented a framework and software\ninfrastructure to generate and analyse an audit trail for a spreadsheet file.\nThis report describes the results of a pilot implementation of this software\n(now called TellTable; see www.telltable.com), along with developments in the\nserver infrastructure and availability, extensions to other \"Office Suite\"\nfiles, integration of the audit tool into the server interface, and related\ndevelopments, licensing and reports. We continue to seek collaborators and\npartners in what is primarily an open-source project with some shared-source\ncomponents.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 12 Mar 2008 11:36:41 GMT"
            }
        ],
        "update_date": "2008-03-13",
        "authors_parsed": [
            [
                "Nash",
                "John",
                ""
            ],
            [
                "Adler",
                "Andy",
                ""
            ],
            [
                "Smith",
                "Neil",
                ""
            ]
        ]
    },
    {
        "id": "0803.1975",
        "submitter": "Jean-Guillaume Dumas",
        "authors": "Jean-Guillaume Dumas (LJK), Laurent Fousse (LJK), Bruno Salvy (INRIA\n  Rocquencourt)",
        "title": "Compressed Modular Matrix Multiplication",
        "comments": "Published in: MICA'2008 : Milestones in Computer Algebra, Tobago :\n  Trinit\\'e-et-Tobago (2008)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose to store several integers modulo a small prime into a single\nmachine word. Modular addition is performed by addition and possibly\nsubtraction of a word containing several times the modulo. Modular\nMultiplication is not directly accessible but modular dot product can be\nperformed by an integer multiplication by the reverse integer. Modular\nmultiplication by a word containing a single residue is a also possible.\nTherefore matrix multiplication can be performed on such a compressed storage.\nWe here give bounds on the sizes of primes and matrices for which such a\ncompression is possible. We also explicit the details of the required\ncompressed arithmetic routines.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Mar 2008 19:15:42 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Dumas",
                "Jean-Guillaume",
                "",
                "LJK"
            ],
            [
                "Fousse",
                "Laurent",
                "",
                "LJK"
            ],
            [
                "Salvy",
                "Bruno",
                "",
                "INRIA\n  Rocquencourt"
            ]
        ]
    },
    {
        "id": "0803.1985",
        "submitter": "Uwe Aickelin",
        "authors": "Adrian Adewunmi, Uwe Aickelin and Mike Byrne",
        "title": "An Investigation of the Sequential Sampling Method for Crossdocking\n  Simulation Output Variance Reduction",
        "comments": null,
        "journal-ref": "Operational Research Society 4th Simulation Workshop (SW08) in\n  print, pp, Worcestershire, UK 2008",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the reduction of variance associated with a\nsimulation output performance measure, using the Sequential Sampling method\nwhile applying minimum simulation replications, for a class of JIT (Just in\nTime) warehousing system called crossdocking. We initially used the Sequential\nSampling method to attain a desired 95% confidence interval half width of\nplus/minus 0.5 for our chosen performance measure (Total usage cost, given the\nmean maximum level of 157,000 pounds and a mean minimum level of 149,000\npounds). From our results, we achieved a 95% confidence interval half width of\nplus/minus 2.8 for our chosen performance measure (Total usage cost, with an\naverage mean value of 115,000 pounds). However, the Sequential Sampling method\nrequires a huge number of simulation replications to reduce variance for our\nsimulation output value to the target level. Arena (version 11) simulation\nsoftware was used to conduct this study.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Mar 2008 15:02:48 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Adewunmi",
                "Adrian",
                ""
            ],
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Byrne",
                "Mike",
                ""
            ]
        ]
    },
    {
        "id": "0803.1993",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin, Edmund Burke and Jingpeng Li",
        "title": "Improved Squeaky Wheel Optimisation for Driver Scheduling",
        "comments": null,
        "journal-ref": "Proceedings of the 9th International Conference on Parallel\n  Problem Solving from Nature (PPSN IX), Lecture Notes in Computer Science\n  4193, pp 182-191, Reykjavik, Iceland, 2006",
        "doi": "10.1007/11844297_19",
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a technique called Improved Squeaky Wheel Optimisation\nfor driver scheduling problems. It improves the original Squeaky Wheel\nOptimisations effectiveness and execution speed by incorporating two additional\nsteps of Selection and Mutation which implement evolution within a single\nsolution. In the ISWO, a cycle of\nAnalysis-Selection-Mutation-Prioritization-Construction continues until\nstopping conditions are reached. The Analysis step first computes the fitness\nof a current solution to identify troublesome components. The Selection step\nthen discards these troublesome components probabilistically by using the\nfitness measure, and the Mutation step follows to further discard a small\nnumber of components at random. After the above steps, an input solution\nbecomes partial and thus the resulting partial solution needs to be repaired.\nThe repair is carried out by using the Prioritization step to first produce\npriorities that determine an order by which the following Construction step\nthen schedules the remaining components. Therefore, the optimisation in the\nISWO is achieved by solution disruption, iterative improvement and an iterative\nconstructive repair process performed. Encouraging experimental results are\nreported.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Mar 2008 15:28:06 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Burke",
                "Edmund",
                ""
            ],
            [
                "Li",
                "Jingpeng",
                ""
            ]
        ]
    },
    {
        "id": "0803.1994",
        "submitter": "Uwe Aickelin",
        "authors": "Jingpeng Li and Uwe Aickelin",
        "title": "The Application of Bayesian Optimization and Classifier Systems in Nurse\n  Scheduling",
        "comments": null,
        "journal-ref": "Proceedings of the 8th International Conference on Parallel\n  Problem Solving from Nature (PPSN VIII), Lecture Notes in Computer Science\n  3242, pp 581-590, Birmingham, UK 2004",
        "doi": "10.1007/b100601",
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Two ideas taken from Bayesian optimization and classifier systems are\npresented for personnel scheduling based on choosing a suitable scheduling rule\nfrom a set for each persons assignment. Unlike our previous work of using\ngenetic algorithms whose learning is implicit, the learning in both approaches\nis explicit, i.e. we are able to identify building blocks directly. To achieve\nthis target, the Bayesian optimization algorithm builds a Bayesian network of\nthe joint probability distribution of the rules used to construct solutions,\nwhile the adapted classifier system assigns each rule a strength value that is\nconstantly updated according to its usefulness in the current situation.\nComputational results from 52 real data instances of nurse scheduling\ndemonstrate the success of both approaches. It is also suggested that the\nlearning mechanism in the proposed approaches might be suitable for other\nscheduling problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Mar 2008 15:43:34 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Li",
                "Jingpeng",
                ""
            ],
            [
                "Aickelin",
                "Uwe",
                ""
            ]
        ]
    },
    {
        "id": "0803.2027",
        "submitter": "Grenville Croll",
        "authors": "Jocelyn Paine",
        "title": "Excelsior: Bringing the Benefits of Modularisation to Excel",
        "comments": "13 Pages, Code Fragments",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2005 173-184\n  ISBN:1-902724-16-X",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Excel lacks features for modular design. Had it such features, as do most\nprogramming languages, they would save time, avoid unneeded programming, make\nmistakes less likely, make code-control easier, help organisations adopt a\nuniform house style, and open business opportunities in buying and selling\nspreadsheet modules. I present Excelsior, a system for bringing these benefits\nto Excel.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 13 Mar 2008 18:41:21 GMT"
            }
        ],
        "update_date": "2008-03-14",
        "authors_parsed": [
            [
                "Paine",
                "Jocelyn",
                ""
            ]
        ]
    },
    {
        "id": "0803.2093",
        "submitter": "Yoann Pigne",
        "authors": "Yoann Pign\\'e (LITIS), Antoine Dutot (LITIS), Fr\\'ed\\'eric Guinand\n  (LITIS), Damien Olivier (LITIS)",
        "title": "GraphStream: A Tool for bridging the gap between Complex Systems and\n  Dynamic Graphs",
        "comments": null,
        "journal-ref": "Emergent Properties in Natural and Artificial Complex Systems.\n  Satellite Conference within the 4th European Conference on Complex Systems\n  (ECCS'2007), Dresden : Allemagne (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The notion of complex systems is common to many domains, from Biology to\nEconomy, Computer Science, Physics, etc. Often, these systems are made of sets\nof entities moving in an evolving environment. One of their major\ncharacteristics is the emergence of some global properties stemmed from local\ninteractions between the entities themselves and between the entities and the\nenvironment. The structure of these systems as sets of interacting entities\nleads researchers to model them as graphs. However, their understanding\nrequires most often to consider the dynamics of their evolution. It is indeed\nnot relevant to study some properties out of any temporal consideration. Thus,\ndynamic graphs seem to be a very suitable model for investigating the emergence\nand the conservation of some properties. GraphStream is a Java-based library\nwhose main purpose is to help researchers and developers in their daily tasks\nof dynamic problem modeling and of classical graph management tasks: creation,\nprocessing, display, etc. It may also be used, and is indeed already used, for\nteaching purpose. GraphStream relies on an event-based engine allowing several\nevent sources. Events may be included in the core of the application, read from\na file or received from an event handler.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 14 Mar 2008 07:09:13 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Pign\u00e9",
                "Yoann",
                "",
                "LITIS"
            ],
            [
                "Dutot",
                "Antoine",
                "",
                "LITIS"
            ],
            [
                "Guinand",
                "Fr\u00e9d\u00e9ric",
                "",
                "LITIS"
            ],
            [
                "Olivier",
                "Damien",
                "",
                "LITIS"
            ]
        ]
    },
    {
        "id": "0803.2212",
        "submitter": "Dan Olteanu",
        "authors": "Christoph Koch and Dan Olteanu",
        "title": "Conditioning Probabilistic Databases",
        "comments": "13 pages, 13 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Past research on probabilistic databases has studied the problem of answering\nqueries on a static database. Application scenarios of probabilistic databases\nhowever often involve the conditioning of a database using additional\ninformation in the form of new evidence. The conditioning problem is thus to\ntransform a probabilistic database of priors into a posterior probabilistic\ndatabase which is materialized for subsequent query processing or further\nrefinement. It turns out that the conditioning problem is closely related to\nthe problem of computing exact tuple confidence values.\n  It is known that exact confidence computation is an NP-hard problem. This has\nled researchers to consider approximation techniques for confidence\ncomputation. However, neither conditioning nor exact confidence computation can\nbe solved using such techniques.\n  In this paper we present efficient techniques for both problems. We study\nseveral problem decomposition methods and heuristics that are based on the most\nsuccessful search techniques from constraint satisfaction, such as the\nDavis-Putnam algorithm. We complement this with a thorough experimental\nevaluation of the algorithms proposed. Our experiments show that our exact\nalgorithms scale well to realistic database sizes and can in some scenarios\ncompete with the most efficient previous approximation algorithms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 14 Mar 2008 17:23:34 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 16 Jun 2008 16:20:06 GMT"
            }
        ],
        "update_date": "2008-06-16",
        "authors_parsed": [
            [
                "Koch",
                "Christoph",
                ""
            ],
            [
                "Olteanu",
                "Dan",
                ""
            ]
        ]
    },
    {
        "id": "0803.2305",
        "submitter": "Andrew Gacek",
        "authors": "Andrew Gacek",
        "title": "The Abella Interactive Theorem Prover (System Description)",
        "comments": "7 pages, to appear in IJCAR'08",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Abella is an interactive system for reasoning about aspects of object\nlanguages that have been formally presented through recursive rules based on\nsyntactic structure. Abella utilizes a two-level logic approach to\nspecification and reasoning. One level is defined by a specification logic\nwhich supports a transparent encoding of structural semantics rules and also\nenables their execution. The second level, called the reasoning logic, embeds\nthe specification logic and allows the development of proofs of properties\nabout specifications. An important characteristic of both logics is that they\nexploit the lambda tree syntax approach to treating binding in object\nlanguages. Amongst other things, Abella has been used to prove normalizability\nproperties of the lambda calculus, cut admissibility for a sequent calculus and\ntype uniqueness and subject reduction properties. This paper discusses the\nlogical foundations of Abella, outlines the style of theorem proving that it\nsupports and finally describes some of its recent applications.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 15 Mar 2008 16:15:10 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 23 May 2008 15:28:08 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Gacek",
                "Andrew",
                ""
            ]
        ]
    },
    {
        "id": "0803.2317",
        "submitter": "Jorge Sousa Pinto",
        "authors": "Joao Gomes and Daniel Martins and Simao Melo de Sousa and Jorge Sousa\n  Pinto",
        "title": "Lissom, a Source Level Proof Carrying Code Platform",
        "comments": "Poster presented at the International Workshop on Proof-Carrying Code\n  (PCC 06), 2006",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces a proposal for a Proof Carrying Code (PCC) architecture\ncalled Lissom. Started as a challenge for final year Computing students, Lissom\nwas thought as a mean to prove to a sceptic community, and in particular to\nstudents, that formal verification tools can be put to practice in a realistic\nenvironment, and be used to solve complex and concrete problems. The\nattractiveness of the problems that PCC addresses has already brought students\nto show interest in this project.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 15 Mar 2008 18:53:06 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Gomes",
                "Joao",
                ""
            ],
            [
                "Martins",
                "Daniel",
                ""
            ],
            [
                "de Sousa",
                "Simao Melo",
                ""
            ],
            [
                "Pinto",
                "Jorge Sousa",
                ""
            ]
        ]
    },
    {
        "id": "0803.2319",
        "submitter": "AbdelRahman Karawia Dr.",
        "authors": "A. A. Karawia",
        "title": "Two Algorithms for Solving A General Backward Pentadiagonal Linear\n  Systems",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present an efficient computational and symbolic algorithms\nfor solving a backward pentadiagonal linear systems. The implementation of the\nalgorithms using Computer Algebra Systems (CAS) such as MAPLE, MACSYMA,\nMATHEMATICA, and MATLAB are straightforward. An examples are given in order to\nillustrate the algorithms. The symbolic algorithm is competitive the other\nmethods for solving a backward pentadiagonal linear systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 15 Mar 2008 20:04:18 GMT"
            }
        ],
        "update_date": "2008-03-18",
        "authors_parsed": [
            [
                "Karawia",
                "A. A.",
                ""
            ]
        ]
    },
    {
        "id": "0803.2363",
        "submitter": "Li Chen",
        "authors": "Li Chen",
        "title": "lambda-Connectedness Determination for Image Segmentation",
        "comments": "9 pages, 36th Applied Image Pattern Recognition Workshop (AIPR 2007),\n  October 2007, Washington, DC, USA",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.DM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Image segmentation is to separate an image into distinct homogeneous regions\nbelonging to different objects. It is an essential step in image analysis and\ncomputer vision. This paper compares some segmentation technologies and\nattempts to find an automated way to better determine the parameters for image\nsegmentation, especially the connectivity value of $\\lambda$ in\n$\\lambda$-connected segmentation.\n  Based on the theories on the maximum entropy method and Otsu's minimum\nvariance method, we propose:(1)maximum entropy connectedness determination: a\nmethod that uses maximum entropy to determine the best $\\lambda$ value in\n$\\lambda$-connected segmentation, and (2) minimum variance connectedness\ndetermination: a method that uses the principle of minimum variance to\ndetermine $\\lambda$ value. Applying these optimization techniques in real\nimages, the experimental results have shown great promise in the development of\nthe new methods. In the end, we extend the above method to more general case in\norder to compare it with the famous Mumford-Shah method that uses variational\nprinciple and geometric measure.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 16 Mar 2008 17:18:19 GMT"
            }
        ],
        "update_date": "2008-03-18",
        "authors_parsed": [
            [
                "Chen",
                "Li",
                ""
            ]
        ]
    },
    {
        "id": "0803.2365",
        "submitter": "Ganesh Narayan",
        "authors": "V Sriram, Ganesh Narayan, K Gopinath",
        "title": "SAFIUS - A secure and accountable filesystem over untrusted storage",
        "comments": "11pt, 12 pages, 16 figures",
        "journal-ref": "Fourth International IEEE Security in Storage Workshop, 2007 -\n  SISW '07. Publication Date: 27-27 Sept. 2007 On page(s): 34-45",
        "doi": "10.1109/SISW.2007.7",
        "report-no": null,
        "categories": "cs.OS cs.CR cs.DC cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe SAFIUS, a secure accountable file system that resides over an\nuntrusted storage. SAFIUS provides strong security guarantees like\nconfidentiality, integrity, prevention from rollback attacks, and\naccountability. SAFIUS also enables read/write sharing of data and provides the\nstandard UNIX-like interface for applications. To achieve accountability with\ngood performance, it uses asynchronous signatures; to reduce the space required\nfor storing these signatures, a novel signature pruning mechanism is used.\nSAFIUS has been implemented on a GNU/Linux based system modifying OpenGFS.\nPreliminary performance studies show that SAFIUS has a tolerable overhead for\nproviding secure storage: while it has an overhead of about 50% of OpenGFS in\ndata intensive workloads (due to the overhead of performing\nencryption/decryption in software), it is comparable (or better in some cases)\nto OpenGFS in metadata intensive workloads.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 16 Mar 2008 18:24:13 GMT"
            }
        ],
        "update_date": "2016-11-18",
        "authors_parsed": [
            [
                "Sriram",
                "V",
                ""
            ],
            [
                "Narayan",
                "Ganesh",
                ""
            ],
            [
                "Gopinath",
                "K",
                ""
            ]
        ]
    },
    {
        "id": "0803.2365",
        "submitter": "Ganesh Narayan",
        "authors": "V Sriram, Ganesh Narayan, K Gopinath",
        "title": "SAFIUS - A secure and accountable filesystem over untrusted storage",
        "comments": "11pt, 12 pages, 16 figures",
        "journal-ref": "Fourth International IEEE Security in Storage Workshop, 2007 -\n  SISW '07. Publication Date: 27-27 Sept. 2007 On page(s): 34-45",
        "doi": "10.1109/SISW.2007.7",
        "report-no": null,
        "categories": "cs.OS cs.CR cs.DC cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe SAFIUS, a secure accountable file system that resides over an\nuntrusted storage. SAFIUS provides strong security guarantees like\nconfidentiality, integrity, prevention from rollback attacks, and\naccountability. SAFIUS also enables read/write sharing of data and provides the\nstandard UNIX-like interface for applications. To achieve accountability with\ngood performance, it uses asynchronous signatures; to reduce the space required\nfor storing these signatures, a novel signature pruning mechanism is used.\nSAFIUS has been implemented on a GNU/Linux based system modifying OpenGFS.\nPreliminary performance studies show that SAFIUS has a tolerable overhead for\nproviding secure storage: while it has an overhead of about 50% of OpenGFS in\ndata intensive workloads (due to the overhead of performing\nencryption/decryption in software), it is comparable (or better in some cases)\nto OpenGFS in metadata intensive workloads.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 16 Mar 2008 18:24:13 GMT"
            }
        ],
        "update_date": "2016-11-18",
        "authors_parsed": [
            [
                "Sriram",
                "V",
                ""
            ],
            [
                "Narayan",
                "Ganesh",
                ""
            ],
            [
                "Gopinath",
                "K",
                ""
            ]
        ]
    },
    {
        "id": "0803.2386",
        "submitter": "James Raynolds",
        "authors": "Lenore R. Mullin and James E. Raynolds",
        "title": "Conformal Computing: Algebraically connecting the hardware/software\n  boundary using a uniform approach to high-performance computation for\n  software and hardware applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a systematic, algebraically based, design methodology for\nefficient implementation of computer programs optimized over multiple levels of\nthe processor/memory and network hierarchy. Using a common formalism to\ndescribe the problem and the partitioning of data over processors and memory\nlevels allows one to mathematically prove the efficiency and correctness of a\ngiven algorithm as measured in terms of a set of metrics (such as\nprocessor/network speeds, etc.). The approach allows the average programmer to\nachieve high-level optimizations similar to those used by compiler writers\n(e.g. the notion of \"tiling\").\n  The approach presented in this monograph makes use of A Mathematics of Arrays\n(MoA, Mullin 1988) and an indexing calculus (i.e. the psi-calculus) to enable\nthe programmer to develop algorithms using high-level compiler-like\noptimizations through the ability to algebraically compose and reduce sequences\nof array operations. Extensive discussion and benchmark results are presented\nfor the Fast Fourier Transform and other important algorithms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 Mar 2008 02:38:49 GMT"
            }
        ],
        "update_date": "2008-03-18",
        "authors_parsed": [
            [
                "Mullin",
                "Lenore R.",
                ""
            ],
            [
                "Raynolds",
                "James E.",
                ""
            ]
        ]
    },
    {
        "id": "0803.2559",
        "submitter": "James Bailey",
        "authors": "James Bailey and Guozhu Dong and Anthony Widjaja To",
        "title": "Logical Queries over Views: Decidability and Expressiveness",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of deciding satisfiability of first order logic queries\nover views, our aim being to delimit the boundary between the decidable and the\nundecidable fragments of this language. Views currently occupy a central place\nin database research, due to their role in applications such as information\nintegration and data warehousing. Our main result is the identification of a\ndecidable class of first order queries over unary conjunctive views that\ngeneralises the decidability of the classical class of first order sentences\nover unary relations, known as the Lowenheim class. We then demonstrate how\nvarious extensions of this class lead to undecidability and also provide some\nexpressivity results. Besides its theoretical interest, our new decidable class\nis potentially interesting for use in applications such as deciding implication\nof complex dependencies, analysis of a restricted class of active database\nrules, and ontology reasoning.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 18 Mar 2008 02:07:12 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Bailey",
                "James",
                ""
            ],
            [
                "Dong",
                "Guozhu",
                ""
            ],
            [
                "To",
                "Anthony Widjaja",
                ""
            ]
        ]
    },
    {
        "id": "0803.2695",
        "submitter": "Juan J. Merelo Pr.",
        "authors": "C. Fernandes, A.M. Mora, J.J. Merelo, V. Ramos, J.L.J. Laredo",
        "title": "KohonAnts: A Self-Organizing Ant Algorithm for Clustering and Pattern\n  Classification",
        "comments": "Submitted to ALIFE XI",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a new ant-based method that takes advantage of the\ncooperative self-organization of Ant Colony Systems to create a naturally\ninspired clustering and pattern recognition method. The approach considers each\ndata item as an ant, which moves inside a grid changing the cells it goes\nthrough, in a fashion similar to Kohonen's Self-Organizing Maps. The resulting\nalgorithm is conceptually more simple, takes less free parameters than other\nant-based clustering algorithms, and, after some parameter tuning, yields very\ngood results on some benchmark problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 18 Mar 2008 18:27:14 GMT"
            }
        ],
        "update_date": "2008-03-19",
        "authors_parsed": [
            [
                "Fernandes",
                "C.",
                ""
            ],
            [
                "Mora",
                "A. M.",
                ""
            ],
            [
                "Merelo",
                "J. J.",
                ""
            ],
            [
                "Ramos",
                "V.",
                ""
            ],
            [
                "Laredo",
                "J. L. J.",
                ""
            ]
        ]
    },
    {
        "id": "0803.2812",
        "submitter": "Mikhail Konnik",
        "authors": "Mikhail V. Konnik",
        "title": "Using Spatially Varying Pixels Exposures and Bayer-covered Photosensors\n  for High Dynamic Range Imaging",
        "comments": "Typos corrected",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The method of a linear high dynamic range imaging using solid-state\nphotosensors with Bayer colour filters array is provided in this paper. Using\ninformation from neighbour pixels, it is possible to reconstruct linear images\nwith wide dynamic range from the oversaturated images. Bayer colour filters\narray is considered as an array of neutral filters in a quasimonochromatic\nlight. If the camera's response function to the desirable light source is known\nthen one can calculate correction coefficients to reconstruct oversaturated\nimages. Reconstructed images are linearized in order to provide a linear high\ndynamic range images for optical-digital imaging systems. The calibration\nprocedure for obtaining the camera's response function to the desired light\nsource is described. Experimental results of the reconstruction of the images\nfrom the oversaturated images are presented for red, green, and blue\nquasimonochromatic light sources. Quantitative analysis of the accuracy of the\nreconstructed images is provided.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Mar 2008 14:55:15 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 24 Mar 2008 07:04:39 GMT"
            }
        ],
        "update_date": "2008-03-24",
        "authors_parsed": [
            [
                "Konnik",
                "Mikhail V.",
                ""
            ]
        ]
    },
    {
        "id": "0803.2856",
        "submitter": "Christoph Schommer",
        "authors": "T. Rothenberger, S. Oez, E. Tahirovic, C. Schommer",
        "title": "Figuring out Actors in Text Streams: Using Collocations to establish\n  Incremental Mind-maps",
        "comments": "10 pages, 3 Figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CL cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The recognition, involvement, and description of main actors influences the\nstory line of the whole text. This is of higher importance as the text per se\nrepresents a flow of words and expressions that once it is read it is lost. In\nthis respect, the understanding of a text and moreover on how the actor exactly\nbehaves is not only a major concern: as human beings try to store a given input\non short-term memory while associating diverse aspects and actors with\nincidents, the following approach represents a virtual architecture, where\ncollocations are concerned and taken as the associative completion of the\nactors' acting. Once that collocations are discovered, they become managed in\nseparated memory blocks broken down by the actors. As for human beings, the\nmemory blocks refer to associative mind-maps. We then present several priority\nfunctions to represent the actual temporal situation inside a mind-map to\nenable the user to reconstruct the recent events from the discovered temporal\nresults.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Mar 2008 18:00:19 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Rothenberger",
                "T.",
                ""
            ],
            [
                "Oez",
                "S.",
                ""
            ],
            [
                "Tahirovic",
                "E.",
                ""
            ],
            [
                "Schommer",
                "C.",
                ""
            ]
        ]
    },
    {
        "id": "0803.2904",
        "submitter": "Gabriel Cardona",
        "authors": "Gabriel Cardona, Merce Llabres, Francesc Rossello, Gabriel Valiente",
        "title": "A Distance Metric for Tree-Sibling Time Consistent Phylogenetic Networks",
        "comments": "16 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-bio.PE cs.CE cs.DM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The presence of reticulate evolutionary events in phylogenies turn\nphylogenetic trees into phylogenetic networks. These events imply in particular\nthat there may exist multiple evolutionary paths from a non-extant species to\nan extant one, and this multiplicity makes the comparison of phylogenetic\nnetworks much more difficult than the comparison of phylogenetic trees. In\nfact, all attempts to define a sound distance measure on the class of all\nphylogenetic networks have failed so far. Thus, the only practical solutions\nhave been either the use of rough estimates of similarity (based on comparison\nof the trees embedded in the networks), or narrowing the class of phylogenetic\nnetworks to a certain class where such a distance is known and can be\nefficiently computed. The first approach has the problem that one may identify\ntwo networks as equivalent, when they are not; the second one has the drawback\nthat there may not exist algorithms to reconstruct such networks from\nbiological sequences.\n  We present in this paper a distance measure on the class of tree-sibling time\nconsistent phylogenetic networks, which generalize tree-child time consistent\nphylogenetic networks, and thus also galled-trees. The practical interest of\nthis distance measure is twofold: it can be computed in polynomial time by\nmeans of simple algorithms, and there also exist polynomial-time algorithms for\nreconstructing networks of this class from DNA sequence data.\n  The Perl package Bio::PhyloNetwork, included in the BioPerl bundle,\nimplements many algorithms on phylogenetic networks, including the computation\nof the distance presented in this paper.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Mar 2008 22:24:11 GMT"
            }
        ],
        "update_date": "2008-03-21",
        "authors_parsed": [
            [
                "Cardona",
                "Gabriel",
                ""
            ],
            [
                "Llabres",
                "Merce",
                ""
            ],
            [
                "Rossello",
                "Francesc",
                ""
            ],
            [
                "Valiente",
                "Gabriel",
                ""
            ]
        ]
    },
    {
        "id": "0803.2957",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin and Kathryn Dowsland",
        "title": "Enhanced Direct and Indirect Genetic Algorithm Approaches for a Mall\n  Layout and Tenant Selection Problem",
        "comments": null,
        "journal-ref": "Journal of Heuristics, 8(5), pp 503-514, 2002",
        "doi": "10.1023/A:1016536623961,",
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  During our earlier research, it was recognised that in order to be successful\nwith an indirect genetic algorithm approach using a decoder, the decoder has to\nstrike a balance between being an optimiser in its own right and finding\nfeasible solutions. Previously this balance was achieved manually. Here we\nextend this by presenting an automated approach where the genetic algorithm\nitself, simultaneously to solving the problem, sets weights to balance the\ncomponents out. Subsequently we were able to solve a complex and non-linear\nscheduling problem better than with a standard direct genetic algorithm\nimplementation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 20 Mar 2008 10:19:01 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Dowsland",
                "Kathryn",
                ""
            ]
        ]
    },
    {
        "id": "0803.2967",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin and Paul White",
        "title": "Building Better Nurse Scheduling Algorithms",
        "comments": null,
        "journal-ref": "Annals of Operations Research, 128, pp 159-177, 2004",
        "doi": "10.1023/B:ANOR.0000019103.31340.a6",
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The aim of this research is twofold: Firstly, to model and solve a complex\nnurse scheduling problem with an integer programming formulation and\nevolutionary algorithms. Secondly, to detail a novel statistical method of\ncomparing and hence building better scheduling algorithms by identifying\nsuccessful algorithm modifications. The comparison method captures the results\nof algorithms in a single figure that can then be compared using traditional\nstatistical techniques. Thus, the proposed method of comparing algorithms is an\nobjective procedure designed to assist in the process of improving an\nalgorithm. This is achieved even when some results are non-numeric or missing\ndue to infeasibility. The final algorithm outperforms all previous evolutionary\nalgorithms, which relied on human expertise for modification.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 20 Mar 2008 11:15:37 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "White",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0803.2969",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin and Kathryn Dowsland",
        "title": "An Indirect Genetic Algorithm for a Nurse Scheduling Problem",
        "comments": null,
        "journal-ref": "Computers & Operations Research, 31(5), pp 761-778, 2004",
        "doi": "10.1016/S0305-0548(03)00034-0",
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes a Genetic Algorithms approach to a manpower-scheduling\nproblem arising at a major UK hospital. Although Genetic Algorithms have been\nsuccessfully used for similar problems in the past, they always had to overcome\nthe limitations of the classical Genetic Algorithms paradigm in handling the\nconflict between objectives and constraints. The approach taken here is to use\nan indirect coding based on permutations of the nurses, and a heuristic decoder\nthat builds schedules from these permutations. Computational experiments based\non 52 weeks of live data are used to evaluate three different decoders with\nvarying levels of intelligence, and four well-known crossover operators.\nResults are further enhanced by introducing a hybrid crossover operator and by\nmaking use of simple bounds to reduce the size of the solution space. The\nresults reveal that the proposed algorithm is able to find high quality\nsolutions and is both faster and more flexible than a recently published Tabu\nSearch approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 20 Mar 2008 11:21:19 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Dowsland",
                "Kathryn",
                ""
            ]
        ]
    },
    {
        "id": "0803.2975",
        "submitter": "Uwe Aickelin",
        "authors": "Uwe Aickelin and Jingpeng Li",
        "title": "An Estimation of Distribution Algorithm for Nurse Scheduling",
        "comments": null,
        "journal-ref": "Annals of Operations Research, 155 (1), pp 289-309, 2007",
        "doi": "10.1007/s10479-007-0214-0",
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Schedules can be built in a similar way to a human scheduler by using a set\nof rules that involve domain knowledge. This paper presents an Estimation of\nDistribution Algorithm (eda) for the nurse scheduling problem, which involves\nchoosing a suitable scheduling rule from a set for the assignment of each\nnurse. Unlike previous work that used Genetic Algorithms (ga) to implement\nimplicit learning, the learning in the proposed algorithm is explicit, i.e. we\nidentify and mix building blocks directly. The eda is applied to implement such\nexplicit learning by building a Bayesian network of the joint distribution of\nsolutions. The conditional probability of each variable in the network is\ncomputed according to an initial set of promising solutions. Subsequently, each\nnew instance for each variable is generated by using the corresponding\nconditional probabilities, until all variables have been generated, i.e. in our\ncase, a new rule string has been obtained. Another set of rule strings will be\ngenerated in this way, some of which will replace previous strings based on\nfitness selection. If stopping conditions are not met, the conditional\nprobabilities for all nodes in the Bayesian network are updated again using the\ncurrent set of promising rule strings. Computational results from 52 real data\ninstances demonstrate the success of this approach. It is also suggested that\nthe learning mechanism in the proposed approach might be suitable for other\nscheduling problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 20 Mar 2008 12:07:26 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 16 May 2008 10:46:10 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Li",
                "Jingpeng",
                ""
            ]
        ]
    },
    {
        "id": "0803.3027",
        "submitter": "Adrien Poteaux",
        "authors": "Adrien Poteaux, Marc Rybowicz",
        "title": "Towards a Symbolic-Numeric Method to Compute Puiseux Series: The Modular\n  Part",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We have designed a new symbolic-numeric strategy to compute efficiently and\naccurately floating point Puiseux series defined by a bivariate polynomial over\nan algebraic number field. In essence, computations modulo a well chosen prime\n$p$ are used to obtain the exact information required to guide floating point\ncomputations. In this paper, we detail the symbolic part of our algorithm:\nFirst of all, we study modular reduction of Puiseux series and give a good\nreduction criterion to ensure that the information required by the numerical\npart is preserved. To establish our results, we introduce a simple modification\nof classical Newton polygons, that we call \"generic Newton polygons\", which\nhappen to be very convenient. Then, we estimate the arithmetic complexity of\ncomputing Puiseux series over finite fields and improve known bounds. Finally,\nwe give bit-complexity bounds for deterministic and randomized versions of the\nsymbolic part. The details of the numerical part will be described in a\nforthcoming paper.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 20 Mar 2008 16:23:40 GMT"
            }
        ],
        "update_date": "2008-03-21",
        "authors_parsed": [
            [
                "Poteaux",
                "Adrien",
                ""
            ],
            [
                "Rybowicz",
                "Marc",
                ""
            ]
        ]
    },
    {
        "id": "0803.3099",
        "submitter": "Mark Burgin",
        "authors": "Mark Burgin a and Marc L. Smith",
        "title": "Concurrent Composition and Algebras of Events, Actions, and Processes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There are many different models of concurrent processes. The goal of this\nwork is to introduce a common formalized framework for current research in this\narea and to eliminate shortcomings of existing models of concurrency. Following\nup the previous research of the authors and other researchers on concurrency,\nhere we build a high-level metamodel EAP (event-action-process) for concurrent\nprocesses. This metamodel comprises a variety of other models of concurrent\nprocesses. We shape mathematical models for, and study events, actions, and\nprocesses in relation to important practical problems, such as communication in\nnetworks, concurrent programming, and distributed computations. In the third\nsection of the work, a three-level algebra of events, actions and processes is\nconstructed and studied as a new stage of algebra for concurrent processes.\nRelations between EAP process algebra and other models of concurrency are\nconsidered in the fourth section of this work.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Mar 2008 00:27:38 GMT"
            }
        ],
        "update_date": "2008-03-24",
        "authors_parsed": [
            [
                "a",
                "Mark Burgin",
                ""
            ],
            [
                "Smith",
                "Marc L.",
                ""
            ]
        ]
    },
    {
        "id": "0803.3224",
        "submitter": "Michael Hahsler",
        "authors": "Michael Hahsler",
        "title": "A Model-Based Frequency Constraint for Mining Associations from\n  Transaction Data",
        "comments": null,
        "journal-ref": "Michael Hahsler. A model-based frequency constraint for mining\n  associations from transaction data. Data Mining and Knowledge Discovery,\n  13(2):137-166, September 2006",
        "doi": "10.1007/s10618-005-0026-2",
        "report-no": null,
        "categories": "cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mining frequent itemsets is a popular method for finding associated items in\ndatabases. For this method, support, the co-occurrence frequency of the items\nwhich form an association, is used as the primary indicator of the\nassociations's significance. A single user-specified support threshold is used\nto decided if associations should be further investigated. Support has some\nknown problems with rare items, favors shorter itemsets and sometimes produces\nmisleading associations.\n  In this paper we develop a novel model-based frequency constraint as an\nalternative to a single, user-specified minimum support. The constraint\nutilizes knowledge of the process generating transaction data by applying a\nsimple stochastic mixture model (the NB model) which allows for transaction\ndata's typically highly skewed item frequency distribution. A user-specified\nprecision threshold is used together with the model to find local frequency\nthresholds for groups of itemsets. Based on the constraint we develop the\nnotion of NB-frequent itemsets and adapt a mining algorithm to find all\nNB-frequent itemsets in a database. In experiments with publicly available\ntransaction databases we show that the new constraint provides improvements\nover a single minimum support threshold and that the precision threshold is\nmore robust and easier to set and interpret by the user.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Mar 2008 20:39:53 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Hahsler",
                "Michael",
                ""
            ]
        ]
    },
    {
        "id": "0803.3230",
        "submitter": "Avik Chaudhuri",
        "authors": "Avik Chaudhuri, Prasad Naldurg, and Sriram Rajamani",
        "title": "A Type System for Data-Flow Integrity on Windows Vista",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.OS cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Windows Vista operating system implements an interesting model of\nmulti-level integrity. We observe that in this model, trusted code can be\nblamed for any information-flow attack; thus, it is possible to eliminate such\nattacks by static analysis of trusted code. We formalize this model by\ndesigning a type system that can efficiently enforce data-flow integrity on\nWindows Vista. Typechecking guarantees that objects whose contents are\nstatically trusted never contain untrusted values, regardless of what untrusted\ncode runs in the environment. Some of Windows Vista's runtime access checks are\nnecessary for soundness; others are redundant and can be optimized away.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Mar 2008 21:28:16 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 8 May 2008 02:55:57 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Chaudhuri",
                "Avik",
                ""
            ],
            [
                "Naldurg",
                "Prasad",
                ""
            ],
            [
                "Rajamani",
                "Sriram",
                ""
            ]
        ]
    },
    {
        "id": "0803.3230",
        "submitter": "Avik Chaudhuri",
        "authors": "Avik Chaudhuri, Prasad Naldurg, and Sriram Rajamani",
        "title": "A Type System for Data-Flow Integrity on Windows Vista",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.OS cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Windows Vista operating system implements an interesting model of\nmulti-level integrity. We observe that in this model, trusted code can be\nblamed for any information-flow attack; thus, it is possible to eliminate such\nattacks by static analysis of trusted code. We formalize this model by\ndesigning a type system that can efficiently enforce data-flow integrity on\nWindows Vista. Typechecking guarantees that objects whose contents are\nstatically trusted never contain untrusted values, regardless of what untrusted\ncode runs in the environment. Some of Windows Vista's runtime access checks are\nnecessary for soundness; others are redundant and can be optimized away.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Mar 2008 21:28:16 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 8 May 2008 02:55:57 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Chaudhuri",
                "Avik",
                ""
            ],
            [
                "Naldurg",
                "Prasad",
                ""
            ],
            [
                "Rajamani",
                "Sriram",
                ""
            ]
        ]
    },
    {
        "id": "0803.3231",
        "submitter": "Grenville Croll",
        "authors": "Victoria Lemieux",
        "title": "Archiving: The Overlooked Spreadsheet Risk",
        "comments": "16 Pages. Recovered by OCR from paper conference proceedings",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2005 213-226\n  ISBN:1-902724-16-X",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.CY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper maintains that archiving has been overlooked as a key spreadsheet\ninternal control. The case of failed Jamaican commercial banks demonstrates how\npoor archiving can lead to weaknesses in spreadsheet control that contribute to\noperational risk. In addition, the Sarbanes-0xley Act contains a number of\nprovisions that require tighter control over the archiving of spreadsheets. To\nmitigate operational risks and achieve compliance with the records-related\nprovisions of Sarbanes-Oxley, the author argues that organisations should\nintroduce records management programmes that provide control over the archiving\nof spreadsheets. At a minimum, spreadsheet archiving controls should identify\nand ensure compliance with retention requirements, support document production\nin the event of regulatory inquiries or litigation, and prevent unauthorised\ndestruction of records.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Mar 2008 21:35:40 GMT"
            }
        ],
        "update_date": "2008-03-25",
        "authors_parsed": [
            [
                "Lemieux",
                "Victoria",
                ""
            ]
        ]
    },
    {
        "id": "0803.3338",
        "submitter": "Ganesh M. Narayan",
        "authors": "Bhargava Kumar K, Ganesh M. Narayan, K. Gopinath",
        "title": "Performance Evaluation of Multiple TCP connections in iSCSI",
        "comments": "10pt, 11 pages, two column, 15 figures",
        "journal-ref": "Proceedings of the 24th IEEE Conference on Mass Storage Systems\n  and Technologies, 2007 - MSST '07",
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.DC cs.OS cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Scaling data storage is a significant concern in enterprise systems and\nStorage Area Networks (SANs) are deployed as a means to scale enterprise\nstorage. SANs based on Fibre Channel have been used extensively in the last\ndecade while iSCSI is fast becoming a serious contender due to its reduced\ncosts and unified infrastructure. This work examines the performance of iSCSI\nwith multiple TCP connections. Multiple TCP connections are often used to\nrealize higher bandwidth but there may be no fairness in how bandwidth is\ndistributed. We propose a mechanism to share congestion information across\nmultiple flows in ``Fair-TCP'' for improved performance. Our results show that\nFair-TCP significantly improves the performance for I/O intensive workloads.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 23 Mar 2008 19:10:00 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "K",
                "Bhargava Kumar",
                ""
            ],
            [
                "Narayan",
                "Ganesh M.",
                ""
            ],
            [
                "Gopinath",
                "K.",
                ""
            ]
        ]
    },
    {
        "id": "0803.3338",
        "submitter": "Ganesh M. Narayan",
        "authors": "Bhargava Kumar K, Ganesh M. Narayan, K. Gopinath",
        "title": "Performance Evaluation of Multiple TCP connections in iSCSI",
        "comments": "10pt, 11 pages, two column, 15 figures",
        "journal-ref": "Proceedings of the 24th IEEE Conference on Mass Storage Systems\n  and Technologies, 2007 - MSST '07",
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.DC cs.OS cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Scaling data storage is a significant concern in enterprise systems and\nStorage Area Networks (SANs) are deployed as a means to scale enterprise\nstorage. SANs based on Fibre Channel have been used extensively in the last\ndecade while iSCSI is fast becoming a serious contender due to its reduced\ncosts and unified infrastructure. This work examines the performance of iSCSI\nwith multiple TCP connections. Multiple TCP connections are often used to\nrealize higher bandwidth but there may be no fairness in how bandwidth is\ndistributed. We propose a mechanism to share congestion information across\nmultiple flows in ``Fair-TCP'' for improved performance. Our results show that\nFair-TCP significantly improves the performance for I/O intensive workloads.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 23 Mar 2008 19:10:00 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "K",
                "Bhargava Kumar",
                ""
            ],
            [
                "Narayan",
                "Ganesh M.",
                ""
            ],
            [
                "Gopinath",
                "K.",
                ""
            ]
        ]
    },
    {
        "id": "0803.3404",
        "submitter": "Wesley Calvert",
        "authors": "Wesley Calvert and John E. Porter",
        "title": "Some results on $\\mathbb{R}$-computable structures",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.LO math.LO",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This survey paper examines the effective model theory obtained with the BSS\nmodel of real number computation. It treats the following topics: computable\nordinals, satisfaction of computable infinitary formulas, forcing as a\nconstruction technique, effective categoricity, effective topology, and\nrelations with other models for the effective theory of uncountable structures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 24 Mar 2008 13:59:53 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 9 Jun 2009 17:03:07 GMT"
            }
        ],
        "update_date": "2009-06-09",
        "authors_parsed": [
            [
                "Calvert",
                "Wesley",
                ""
            ],
            [
                "Porter",
                "John E.",
                ""
            ]
        ]
    },
    {
        "id": "0803.3419",
        "submitter": "David Sevilla",
        "authors": "John McKay, David Sevilla",
        "title": "Decomposing replicable functions",
        "comments": "25 pages, 2 data tables; to be published in LMS Journal of\n  Computation and Mathematics. v2: page format adjustment",
        "journal-ref": "LMS Journal of Computation and Mathematics 11 (June 2008), p.\n  146-171. ISSN 1461-1570.",
        "doi": null,
        "report-no": null,
        "categories": "math.NT cs.SC math.RT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe an algorithm to decompose rational functions from which we\ndetermine the poset of groups fixing these functions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 24 Mar 2008 16:02:26 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 25 Mar 2008 07:18:19 GMT"
            }
        ],
        "update_date": "2008-08-21",
        "authors_parsed": [
            [
                "McKay",
                "John",
                ""
            ],
            [
                "Sevilla",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0803.3435",
        "submitter": "Tomas Rokicki",
        "authors": "Tomas Rokicki",
        "title": "Twenty-Five Moves Suffice for Rubik's Cube",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.DM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  How many moves does it take to solve Rubik's Cube? Positions are known that\nrequire 20 moves, and it has already been shown that there are no positions\nthat require 27 or more moves; this is a surprisingly large gap. This paper\ndescribes a program that is able to find solutions of length 20 or less at a\nrate of more than 16 million positions a second. We use this program, along\nwith some new ideas and incremental improvements in other techniques, to show\nthat there is no position that requires 26 moves.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 24 Mar 2008 19:37:09 GMT"
            }
        ],
        "update_date": "2008-03-25",
        "authors_parsed": [
            [
                "Rokicki",
                "Tomas",
                ""
            ]
        ]
    },
    {
        "id": "0803.3459",
        "submitter": "Franklin Marquezino",
        "authors": "F.L. Marquezino, R. Portugal",
        "title": "The QWalk Simulator of Quantum Walks",
        "comments": "21 pages, 11 figures. Accepted in Computer Physics Communications.\n  Simulator can be downloaded from http://qubit.lncc.br/qwalk",
        "journal-ref": "Computer Physics Communications, Volume 179, Issue 5, Pages\n  359-369. (2008)",
        "doi": "10.1016/j.cpc.2008.02.019",
        "report-no": null,
        "categories": "quant-ph cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Several research groups are giving special attention to quantum walks\nrecently, because this research area have been used with success in the\ndevelopment of new efficient quantum algorithms. A general simulator of quantum\nwalks is very important for the development of this area, since it allows the\nresearchers to focus on the mathematical and physical aspects of the research\ninstead of deviating the efforts to the implementation of specific numerical\nsimulations. In this paper we present QWalk, a quantum walk simulator for one-\nand two-dimensional lattices. Finite two-dimensional lattices with generic\ntopologies can be used. Decoherence can be simulated by performing measurements\nor by breaking links of the lattice. We use examples to explain the usage of\nthe software and to show some recent results of the literature that are easily\nreproduced by the simulator.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 24 Mar 2008 20:25:49 GMT"
            }
        ],
        "update_date": "2012-05-18",
        "authors_parsed": [
            [
                "Marquezino",
                "F. L.",
                ""
            ],
            [
                "Portugal",
                "R.",
                ""
            ]
        ]
    },
    {
        "id": "0803.3490",
        "submitter": "Huan Xu Mr.",
        "authors": "Huan Xu, Constantine Caramanis and Shie Mannor",
        "title": "Robustness and Regularization of Support Vector Machines",
        "comments": null,
        "journal-ref": "Journal of Machine Learning Research, vol 10, 1485-1510, year 2009",
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider regularized support vector machines (SVMs) and show that they are\nprecisely equivalent to a new robust optimization formulation. We show that\nthis equivalence of robust optimization and regularization has implications for\nboth algorithms, and analysis. In terms of algorithms, the equivalence suggests\nmore general SVM-like algorithms for classification that explicitly build in\nprotection to noise, and at the same time control overfitting. On the analysis\nfront, the equivalence of robustness and regularization, provides a robust\noptimization interpretation for the success of regularized SVMs. We use the\nthis new robustness interpretation of SVMs to give a new proof of consistency\nof (kernelized) SVMs, thus establishing robustness as the reason regularized\nSVMs generalize well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Mar 2008 03:51:59 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 11 Nov 2008 22:36:47 GMT"
            }
        ],
        "update_date": "2010-02-25",
        "authors_parsed": [
            [
                "Xu",
                "Huan",
                ""
            ],
            [
                "Caramanis",
                "Constantine",
                ""
            ],
            [
                "Mannor",
                "Shie",
                ""
            ]
        ]
    },
    {
        "id": "0803.3632",
        "submitter": "Mikhail Nesterenko",
        "authors": "Mikhail Nesterenko, Adnan Vora",
        "title": "Void Traversal for Guaranteed Delivery in Geometric Routing",
        "comments": null,
        "journal-ref": "The 2nd IEEE International Conference on Mobile Ad-hoc and Sensor\n  Systems (MASS 2005), Washington, DC, November, 2005",
        "doi": "10.1109/MAHSS.2005.1542862",
        "report-no": null,
        "categories": "cs.OS cs.DC cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Geometric routing algorithms like GFG (GPSR) are lightweight, scalable\nalgorithms that can be used to route in resource-constrained ad hoc wireless\nnetworks. However, such algorithms run on planar graphs only. To efficiently\nconstruct a planar graph, they require a unit-disk graph. To make the topology\nunit-disk, the maximum link length in the network has to be selected\nconservatively. In practical setting this leads to the designs where the node\ndensity is rather high. Moreover, the network diameter of a planar subgraph is\ngreater than the original graph, which leads to longer routes. To remedy this\nproblem, we propose a void traversal algorithm that works on arbitrary\ngeometric graphs. We describe how to use this algorithm for geometric routing\nwith guaranteed delivery and compare its performance with GFG.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Mar 2008 20:52:17 GMT"
            }
        ],
        "update_date": "2016-11-15",
        "authors_parsed": [
            [
                "Nesterenko",
                "Mikhail",
                ""
            ],
            [
                "Vora",
                "Adnan",
                ""
            ]
        ]
    },
    {
        "id": "0803.3693",
        "submitter": "Rasmus Pagh",
        "authors": "Martin Dietzfelbinger and Rasmus Pagh",
        "title": "Succinct Data Structures for Retrieval and Approximate Membership",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.DB cs.IR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The retrieval problem is the problem of associating data with keys in a set.\nFormally, the data structure must store a function f: U ->{0,1}^r that has\nspecified values on the elements of a given set S, a subset of U, |S|=n, but\nmay have any value on elements outside S. Minimal perfect hashing makes it\npossible to avoid storing the set S, but this induces a space overhead of\nTheta(n) bits in addition to the nr bits needed for function values. In this\npaper we show how to eliminate this overhead. Moreover, we show that for any k\nquery time O(k) can be achieved using space that is within a factor 1+e^{-k} of\noptimal, asymptotically for large n. If we allow logarithmic evaluation time,\nthe additive overhead can be reduced to O(log log n) bits whp. The time to\nconstruct the data structure is O(n), expected. A main technical ingredient is\nto utilize existing tight bounds on the probability of almost square random\nmatrices with rows of low weight to have full row rank. In addition to direct\nconstructions, we point out a close connection between retrieval structures and\nhash tables where keys are stored in an array and some kind of probing scheme\nis used. Further, we propose a general reduction that transfers the results on\nretrieval into analogous results on approximate membership, a problem\ntraditionally addressed using Bloom filters. Again, we show how to eliminate\nthe space overhead present in previously known methods, and get arbitrarily\nclose to the lower bound. The evaluation procedures of our data structures are\nextremely simple (similar to a Bloom filter). For the results stated above we\nassume free access to fully random hash functions. However, we show how to\njustify this assumption using extra space o(n) to simulate full randomness on a\nRAM.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 26 Mar 2008 10:53:49 GMT"
            }
        ],
        "update_date": "2008-03-27",
        "authors_parsed": [
            [
                "Dietzfelbinger",
                "Martin",
                ""
            ],
            [
                "Pagh",
                "Rasmus",
                ""
            ]
        ]
    },
    {
        "id": "0803.3812",
        "submitter": "Juan Carlos Nieves",
        "authors": "Juan Carlos Nieves, Mauricio Osorio, Ulises Cort\\'es",
        "title": "Preferred extensions as stable models",
        "comments": "17 pages, 2 Figures; to appear in Theory and Practice of Logic\n  Programming (TPLP)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given an argumentation framework AF, we introduce a mapping function that\nconstructs a disjunctive logic program P, such that the preferred extensions of\nAF correspond to the stable models of P, after intersecting each stable model\nwith the relevant atoms. The given mapping function is of polynomial size\nw.r.t. AF. In particular, we identify that there is a direct relationship\nbetween the minimal models of a propositional formula and the preferred\nextensions of an argumentation framework by working on representing the\ndefeated arguments. Then we show how to infer the preferred extensions of an\nargumentation framework by using UNSAT algorithms and disjunctive stable model\nsolvers. The relevance of this result is that we define a direct relationship\nbetween one of the most satisfactory argumentation semantics and one of the\nmost successful approach of non-monotonic reasoning i.e., logic programming\nwith the stable model semantics.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 26 Mar 2008 20:08:31 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Nieves",
                "Juan Carlos",
                ""
            ],
            [
                "Osorio",
                "Mauricio",
                ""
            ],
            [
                "Cort\u00e9s",
                "Ulises",
                ""
            ]
        ]
    },
    {
        "id": "0803.3838",
        "submitter": "Ted Dunning",
        "authors": "Ted Dunning",
        "title": "Recorded Step Directional Mutation for Faster Convergence",
        "comments": "15 pages, 4 figures, presented at EP-98",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.LG",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  Two meta-evolutionary optimization strategies described in this paper\naccelerate the convergence of evolutionary programming algorithms while still\nretaining much of their ability to deal with multi-modal problems. The\nstrategies, called directional mutation and recorded step in this paper, can\noperate independently but together they greatly enhance the ability of\nevolutionary programming algorithms to deal with fitness landscapes\ncharacterized by long narrow valleys. The directional mutation aspect of this\ncombined method uses correlated meta-mutation but does not introduce a full\ncovariance matrix. These new methods are thus much more economical in terms of\nstorage for problems with high dimensionality. Additionally, directional\nmutation is rotationally invariant which is a substantial advantage over\nself-adaptive methods which use a single variance per coordinate for problems\nwhere the natural orientation of the problem is not oriented along the axes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 26 Mar 2008 22:49:40 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 26 Mar 2009 20:37:59 GMT"
            }
        ],
        "update_date": "2009-03-26",
        "authors_parsed": [
            [
                "Dunning",
                "Ted",
                ""
            ]
        ]
    },
    {
        "id": "0803.3900",
        "submitter": "Uwe Aickelin",
        "authors": "Jingpeng Li, Uwe Aickelin and Edmund Burke",
        "title": "A Component Based Heuristic Search method with Adaptive Perturbations\n  for Hospital Personnel Scheduling",
        "comments": null,
        "journal-ref": "Technical Report NOTTCS-TR-2007-4, University of Nottingham, UK,,\n  2006",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Nurse rostering is a complex scheduling problem that affects hospital\npersonnel on a daily basis all over the world. This paper presents a new\ncomponent-based approach with adaptive perturbations, for a nurse scheduling\nproblem arising at a major UK hospital. The main idea behind this technique is\nto decompose a schedule into its components (i.e. the allocated shift pattern\nof each nurse), and then mimic a natural evolutionary process on these\ncomponents to iteratively deliver better schedules. The worthiness of all\ncomponents in the schedule has to be continuously demonstrated in order for\nthem to remain there. This demonstration employs a dynamic evaluation function\nwhich evaluates how well each component contributes towards the final\nobjective. Two perturbation steps are then applied: the first perturbation\neliminates a number of components that are deemed not worthy to stay in the\ncurrent schedule; the second perturbation may also throw out, with a low level\nof probability, some worthy components. The eliminated components are\nreplenished with new ones using a set of constructive heuristics using local\noptimality criteria. Computational results using 52 data instances demonstrate\nthe applicability of the proposed approach in solving real-world problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 27 Mar 2008 12:15:43 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Li",
                "Jingpeng",
                ""
            ],
            [
                "Aickelin",
                "Uwe",
                ""
            ],
            [
                "Burke",
                "Edmund",
                ""
            ]
        ]
    },
    {
        "id": "0803.3946",
        "submitter": "Adam Smith",
        "authors": "Shiva Prasad Kasiviswanathan and Adam Smith",
        "title": "On the `Semantics' of Differential Privacy: A Bayesian Formulation",
        "comments": "Older version of this paper was titled: \"A Note on Differential\n  Privacy: Defining Resistance to Arbitrary Side Information\"",
        "journal-ref": "Journal of Privacy and Confidentiality, 6 (1), 2014",
        "doi": "10.29012/jpc.v6i1.634",
        "report-no": null,
        "categories": "cs.CR cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Differential privacy is a definition of \"privacy'\" for algorithms that\nanalyze and publish information about statistical databases. It is often\nclaimed that differential privacy provides guarantees against adversaries with\narbitrary side information. In this paper, we provide a precise formulation of\nthese guarantees in terms of the inferences drawn by a Bayesian adversary. We\nshow that this formulation is satisfied by both \"vanilla\" differential privacy\nas well as a relaxation known as (epsilon,delta)-differential privacy. Our\nformulation follows the ideas originally due to Dwork and McSherry [Dwork\n2006]. This paper is, to our knowledge, the first place such a formulation\nappears explicitly. The analysis of the relaxed definition is new to this\npaper, and provides some concrete guidance for setting parameters when using\n(epsilon,delta)-differential privacy.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 27 Mar 2008 15:00:45 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 21 Sep 2013 00:15:57 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 22 Dec 2015 20:58:26 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 23 Jan 2023 03:31:32 GMT"
            }
        ],
        "update_date": "2023-01-24",
        "authors_parsed": [
            [
                "Kasiviswanathan",
                "Shiva Prasad",
                ""
            ],
            [
                "Smith",
                "Adam",
                ""
            ]
        ]
    },
    {
        "id": "0803.3976",
        "submitter": "David Sevilla",
        "authors": "Jaime Gutierrez, David Sevilla",
        "title": "On Ritt's decomposition Theorem in the case of finite fields",
        "comments": "13 pages. v2: added comment from reader and additional references",
        "journal-ref": "Finite Fields Appl. 12 (2006), no. 3, 403--412. MR2229324\n  (2007a:13024)",
        "doi": "10.1016/j.ffa.2005.08.004",
        "report-no": null,
        "categories": "cs.SC math.AC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A classical theorem by Ritt states that all the complete decomposition chains\nof a univariate polynomial satisfying a certain tameness condition have the\nsame length. In this paper we present our conclusions about the generalization\nof these theorem in the case of finite coefficient fields when the tameness\ncondition is dropped.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 27 Mar 2008 16:39:03 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 10 Apr 2008 09:01:01 GMT"
            }
        ],
        "update_date": "2008-04-10",
        "authors_parsed": [
            [
                "Gutierrez",
                "Jaime",
                ""
            ],
            [
                "Sevilla",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0803.4025",
        "submitter": "Ganesh M. Narayan",
        "authors": "Ganesh M. Narayan, K. Gopinath, V. Sridhar",
        "title": "Structure and Interpretation of Computer Programs",
        "comments": "9 pages, 10pt, double column, 15 figures",
        "journal-ref": "2nd IEEE International Symposium on Theoretical Aspects of\n  Software Engineering, 2008, Nanjing, China",
        "doi": "10.1109/TASE.2008.40",
        "report-no": null,
        "categories": "cs.SE cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Call graphs depict the static, caller-callee relation between \"functions\" in\na program. With most source/target languages supporting functions as the\nprimitive unit of composition, call graphs naturally form the fundamental\ncontrol flow representation available to understand/develop software. They are\nalso the substrate on which various interprocedural analyses are performed and\nare integral part of program comprehension/testing. Given their universality\nand usefulness, it is imperative to ask if call graphs exhibit any intrinsic\ngraph theoretic features -- across versions, program domains and source\nlanguages. This work is an attempt to answer these questions: we present and\ninvestigate a set of meaningful graph measures that help us understand call\ngraphs better; we establish how these measures correlate, if any, across\ndifferent languages and program domains; we also assess the overall, language\nindependent software quality by suitably interpreting these measures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 27 Mar 2008 22:58:43 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Narayan",
                "Ganesh M.",
                ""
            ],
            [
                "Gopinath",
                "K.",
                ""
            ],
            [
                "Sridhar",
                "V.",
                ""
            ]
        ]
    },
    {
        "id": "0803.4025",
        "submitter": "Ganesh M. Narayan",
        "authors": "Ganesh M. Narayan, K. Gopinath, V. Sridhar",
        "title": "Structure and Interpretation of Computer Programs",
        "comments": "9 pages, 10pt, double column, 15 figures",
        "journal-ref": "2nd IEEE International Symposium on Theoretical Aspects of\n  Software Engineering, 2008, Nanjing, China",
        "doi": "10.1109/TASE.2008.40",
        "report-no": null,
        "categories": "cs.SE cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Call graphs depict the static, caller-callee relation between \"functions\" in\na program. With most source/target languages supporting functions as the\nprimitive unit of composition, call graphs naturally form the fundamental\ncontrol flow representation available to understand/develop software. They are\nalso the substrate on which various interprocedural analyses are performed and\nare integral part of program comprehension/testing. Given their universality\nand usefulness, it is imperative to ask if call graphs exhibit any intrinsic\ngraph theoretic features -- across versions, program domains and source\nlanguages. This work is an attempt to answer these questions: we present and\ninvestigate a set of meaningful graph measures that help us understand call\ngraphs better; we establish how these measures correlate, if any, across\ndifferent languages and program domains; we also assess the overall, language\nindependent software quality by suitably interpreting these measures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 27 Mar 2008 22:58:43 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Narayan",
                "Ganesh M.",
                ""
            ],
            [
                "Gopinath",
                "K.",
                ""
            ],
            [
                "Sridhar",
                "V.",
                ""
            ]
        ]
    },
    {
        "id": "0803.4308",
        "submitter": "Vandy Berten",
        "authors": "Vandy Berten, Chi-Ju Chang, Tei-Wei Kuo",
        "title": "Discrete Frequency Selection of Frame-Based Stochastic Real-Time Tasks",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Energy-efficient real-time task scheduling has been actively explored in the\npast decade. Different from the past work, this paper considers schedulability\nconditions for stochastic real-time tasks. A schedulability condition is first\npresented for frame-based stochastic real-time tasks, and several algorithms\nare also examined to check the schedulability of a given strategy. An approach\nis then proposed based on the schedulability condition to adapt a\ncontinuous-speed-based method to a discrete-speed system. The approach is able\nto stay as close as possible to the continuous-speed-based method, but still\nguaranteeing the schedulability. It is shown by simulations that the energy\nsaving can be more than 20% for some system configurations\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 30 Mar 2008 09:26:38 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 7 Apr 2008 07:55:38 GMT"
            }
        ],
        "update_date": "2008-04-07",
        "authors_parsed": [
            [
                "Berten",
                "Vandy",
                ""
            ],
            [
                "Chang",
                "Chi-Ju",
                ""
            ],
            [
                "Kuo",
                "Tei-Wei",
                ""
            ]
        ]
    },
    {
        "id": "0804.0188",
        "submitter": "Alexandre d'Aspremont",
        "authors": "Ronny Luss, Alexandre d'Aspremont",
        "title": "Support Vector Machine Classification with Indefinite Kernels",
        "comments": "Final journal version. A few typos fixed",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a method for support vector machine classification using\nindefinite kernels. Instead of directly minimizing or stabilizing a nonconvex\nloss function, our algorithm simultaneously computes support vectors and a\nproxy kernel matrix used in forming the loss. This can be interpreted as a\npenalized kernel learning problem where indefinite kernel matrices are treated\nas a noisy observations of a true Mercer kernel. Our formulation keeps the\nproblem convex and relatively large problems can be solved efficiently using\nthe projected gradient or analytic center cutting plane methods. We compare the\nperformance of our technique with other methods on several classic data sets.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 Apr 2008 14:55:33 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 4 Aug 2009 11:48:14 GMT"
            }
        ],
        "update_date": "2009-08-04",
        "authors_parsed": [
            [
                "Luss",
                "Ronny",
                ""
            ],
            [
                "d'Aspremont",
                "Alexandre",
                ""
            ]
        ]
    },
    {
        "id": "0804.0366",
        "submitter": "Patrick Ch\\'enais",
        "authors": "Patrick Ch\\'enais",
        "title": "Merging Object and Process Diagrams for Business Information Modeling",
        "comments": "20 small format pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While developing an information system for the University of Bern, we were\nfaced with two major issues: managing software changes and adapting Business\nInformation Models. Software techniques well-suited to software development\nteams exist, yet the models obtained are often too complex for the business\nuser. We will first highlight the conceptual problems encountered while\ndesigning the Business Information Model. We will then propose merging class\ndiagrams and business process modeling to achieve a necessary transparency. We\nwill finally present a modeling tool we developed which, using pilot case\nstudies, helps to show some of the advantages of a dual model approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 2 Apr 2008 14:50:29 GMT"
            }
        ],
        "update_date": "2008-04-03",
        "authors_parsed": [
            [
                "Ch\u00e9nais",
                "Patrick",
                ""
            ]
        ]
    },
    {
        "id": "0804.0524",
        "submitter": "Uwe Aickelin",
        "authors": "Jingpeng Li and Uwe Aickelin",
        "title": "Bayesian Optimisation Algorithm for Nurse Scheduling",
        "comments": null,
        "journal-ref": "Scalable Optimization via Probabilistic Modeling: From Algorithms\n  to Applications (Studies in Computational Intelligence), edited by M Pelikan,\n  K Sastry and E Cantu Paz, Chapter 17, pp 315-332, Springer, 2006",
        "doi": "10.1007/978-3-540-34954-9_14",
        "report-no": null,
        "categories": "cs.NE cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our research has shown that schedules can be built mimicking a human\nscheduler by using a set of rules that involve domain knowledge. This chapter\npresents a Bayesian Optimization Algorithm (BOA) for the nurse scheduling\nproblem that chooses such suitable scheduling rules from a set for each nurses\nassignment. Based on the idea of using probabilistic models, the BOA builds a\nBayesian network for the set of promising solutions and samples these networks\nto generate new candidate solutions. Computational results from 52 real data\ninstances demonstrate the success of this approach. It is also suggested that\nthe learning mechanism in the proposed algorithm may be suitable for other\nscheduling problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 Apr 2008 11:14:11 GMT"
            }
        ],
        "update_date": "2010-07-05",
        "authors_parsed": [
            [
                "Li",
                "Jingpeng",
                ""
            ],
            [
                "Aickelin",
                "Uwe",
                ""
            ]
        ]
    },
    {
        "id": "0804.0797",
        "submitter": "Grenville Croll",
        "authors": "Raymond R. Panko, Nicholas Ordway",
        "title": "Sarbanes-Oxley: What About all the Spreadsheets?",
        "comments": "45 pages, 7 figures",
        "journal-ref": "European Spreadsheet Risks Int. Grp. (EuSpRIG) 2005 15-47\n  ISBN:1-902724-16-X",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.CY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Sarbanes-Oxley Act of 2002 has finally forced corporations to examine the\nvalidity of their spreadsheets. They are beginning to understand the\nspreadsheet error literature, including what it tells them about the need for\ncomprehensive spreadsheet testing. However, controlling for fraud will require\na completely new set of capabilities, and a great deal of new research will be\nneeded to develop fraud control capabilities. This paper discusses the\nriskiness of spreadsheets, which can now be quantified to a considerable\ndegree. It then discusses how to use control frameworks to reduce the dangers\ncreated by spreadsheets. It focuses especially on testing, which appears to be\nthe most crucial element in spreadsheet controls.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 6 Apr 2008 23:42:29 GMT"
            }
        ],
        "update_date": "2008-04-08",
        "authors_parsed": [
            [
                "Panko",
                "Raymond R.",
                ""
            ],
            [
                "Ordway",
                "Nicholas",
                ""
            ]
        ]
    },
    {
        "id": "0804.0876",
        "submitter": "Andreas Abel",
        "authors": "Andreas Abel",
        "title": "Semi-continuous Sized Types and Termination",
        "comments": "33 pages, extended version of CSL'06",
        "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 2 (April 10,\n  2008) lmcs:1236",
        "doi": "10.2168/LMCS-4(2:3)2008",
        "report-no": null,
        "categories": "cs.PL cs.LO",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Some type-based approaches to termination use sized types: an ordinal bound\nfor the size of a data structure is stored in its type. A recursive function\nover a sized type is accepted if it is visible in the type system that\nrecursive calls occur just at a smaller size. This approach is only sound if\nthe type of the recursive function is admissible, i.e., depends on the size\nindex in a certain way. To explore the space of admissible functions in the\npresence of higher-kinded data types and impredicative polymorphism, a\nsemantics is developed where sized types are interpreted as functions from\nordinals into sets of strongly normalizing terms. It is shown that upper\nsemi-continuity of such functions is a sufficient semantic criterion for\nadmissibility. To provide a syntactical criterion, a calculus for\nsemi-continuous functions is developed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 5 Apr 2008 22:27:29 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 10 Apr 2008 15:26:49 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Abel",
                "Andreas",
                ""
            ]
        ]
    },
    {
        "id": "0804.0924",
        "submitter": "Ratthachat Chatpatanasiri",
        "authors": "Ratthachat Chatpatanasiri and Boonserm Kijsirikul",
        "title": "A Unified Semi-Supervised Dimensionality Reduction Framework for\n  Manifold Learning",
        "comments": "22 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a general framework of semi-supervised dimensionality reduction\nfor manifold learning which naturally generalizes existing supervised and\nunsupervised learning frameworks which apply the spectral decomposition.\nAlgorithms derived under our framework are able to employ both labeled and\nunlabeled examples and are able to handle complex problems where data form\nseparate clusters of manifolds. Our framework offers simple views, explains\nrelationships among existing frameworks and provides further extensions which\ncan improve existing algorithms. Furthermore, a new semi-supervised\nkernelization framework called ``KPCA trick'' is proposed to handle non-linear\nproblems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 6 Apr 2008 18:14:34 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 29 Jul 2009 04:25:24 GMT"
            }
        ],
        "update_date": "2009-07-29",
        "authors_parsed": [
            [
                "Chatpatanasiri",
                "Ratthachat",
                ""
            ],
            [
                "Kijsirikul",
                "Boonserm",
                ""
            ]
        ]
    },
    {
        "id": "0804.0941",
        "submitter": "Grenville Croll",
        "authors": "Raymond R. Panko",
        "title": "Reducing Overconfidence in Spreadsheet Development",
        "comments": null,
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. 2003 49-66 ISBN 1 86166\n  199 1",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Despite strong evidence of widespread errors, spreadsheet developers rarely\nsubject their spreadsheets to post-development testing to reduce errors. This\nmay be because spreadsheet developers are overconfident in the accuracy of\ntheir spreadsheets. This conjecture is plausible because overconfidence is\npresent in a wide variety of human cognitive domains, even among experts. This\npaper describes two experiments in overconfidence in spreadsheet development.\nThe first is a pilot study to determine the existence of overconfidence. The\nsecond tests a manipulation to reduce overconfidence and errors. The\nmanipulation is modestly successful, indicating that overconfidence reduction\nis a promising avenue to pursue.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 Apr 2008 00:20:24 GMT"
            }
        ],
        "update_date": "2008-04-08",
        "authors_parsed": [
            [
                "Panko",
                "Raymond R.",
                ""
            ]
        ]
    },
    {
        "id": "0804.0943",
        "submitter": "Grenville Croll",
        "authors": "Richard J. Irons",
        "title": "The Wall and The Ball: A Study of Domain Referent Spreadsheet Errors",
        "comments": "15 pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2003 33-48\n  ISBN 1 86166 199 1",
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Cell Error Rate in simple spreadsheets averages about 2% to 5%. This CER\nhas been measured in domain free environments. This paper compares the CERs\noccurring in domain free and applied domain tasks. The applied domain task\nrequires the application of simple linear algebra to a costing problem. The\nresults show that domain referent knowledge influences participants' approaches\nto spreadsheet creation and spreadsheet usage. The conclusion is that\nspreadsheet error making is influenced by domain knowledge and domain\nperception. Qualitative findings also suggest that spreadsheet error making is\na part of overall human behaviour, and ought to be analyzed against this wider\ncanvas.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 Apr 2008 00:46:41 GMT"
            }
        ],
        "update_date": "2008-04-08",
        "authors_parsed": [
            [
                "Irons",
                "Richard J.",
                ""
            ]
        ]
    },
    {
        "id": "0804.0970",
        "submitter": "Marie-Claude Gaudel",
        "authors": "Marie-Claude Gaudel (LRI), Pascale Le Gall (IBISC)",
        "title": "Testing data types implementations from algebraic specifications",
        "comments": null,
        "journal-ref": "Formal Methods and Testing, Springer-Verlag (Ed.) (2008) 209-239",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Algebraic specifications of data types provide a natural basis for testing\ndata types implementations. In this framework, the conformance relation is\nbased on the satisfaction of axioms. This makes it possible to formally state\nthe fundamental concepts of testing: exhaustive test set, testability\nhypotheses, oracle. Various criteria for selecting finite test sets have been\nproposed. They depend on the form of the axioms, and on the possibilities of\nobservation of the implementation under test. This last point is related to the\nwell-known oracle problem. As the main interest of algebraic specifications is\ndata type abstraction, testing a concrete implementation raises the issue of\nthe gap between the abstract description and the concrete representation. The\nobservational semantics of algebraic specifications bring solutions on the\nbasis of the so-called observable contexts. After a description of testing\nmethods based on algebraic specifications, the chapter gives a brief\npresentation of some tools and case studies, and presents some applications to\nother formal methods involving datatypes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 Apr 2008 06:35:44 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Gaudel",
                "Marie-Claude",
                "",
                "LRI"
            ],
            [
                "Gall",
                "Pascale Le",
                "",
                "IBISC"
            ]
        ]
    },
    {
        "id": "0804.1021",
        "submitter": "Gilles Villard",
        "authors": "Gilles Villard (LIP)",
        "title": "Differentiation of Kaltofen's division-free determinant algorithm",
        "comments": null,
        "journal-ref": "Journal of Symbolic Computation 7, 46 (2011) 773-790",
        "doi": "10.1016/j.jsc.2010.08.012",
        "report-no": null,
        "categories": "cs.SC cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Kaltofen has proposed a new approach in [Kaltofen 1992] for computing matrix\ndeterminants. The algorithm is based on a baby steps/giant steps construction\nof Krylov subspaces, and computes the determinant as the constant term of a\ncharacteristic polynomial. For matrices over an abstract field and by the\nresults of Baur and Strassen 1983, the determinant algorithm, actually a\nstraight-line program, leads to an algorithm with the same complexity for\ncomputing the adjoint of a matrix [Kaltofen 1992]. However, the latter is\nobtained by the reverse mode of automatic differentiation and somehow is not\n``explicit''. We study this adjoint algorithm, show how it can be implemented\n(without resorting to an automatic transformation), and demonstrate its use on\npolynomial matrices.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 Apr 2008 12:37:43 GMT"
            }
        ],
        "update_date": "2011-12-13",
        "authors_parsed": [
            [
                "Villard",
                "Gilles",
                "",
                "LIP"
            ]
        ]
    },
    {
        "id": "0804.1021",
        "submitter": "Gilles Villard",
        "authors": "Gilles Villard (LIP)",
        "title": "Differentiation of Kaltofen's division-free determinant algorithm",
        "comments": null,
        "journal-ref": "Journal of Symbolic Computation 7, 46 (2011) 773-790",
        "doi": "10.1016/j.jsc.2010.08.012",
        "report-no": null,
        "categories": "cs.SC cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Kaltofen has proposed a new approach in [Kaltofen 1992] for computing matrix\ndeterminants. The algorithm is based on a baby steps/giant steps construction\nof Krylov subspaces, and computes the determinant as the constant term of a\ncharacteristic polynomial. For matrices over an abstract field and by the\nresults of Baur and Strassen 1983, the determinant algorithm, actually a\nstraight-line program, leads to an algorithm with the same complexity for\ncomputing the adjoint of a matrix [Kaltofen 1992]. However, the latter is\nobtained by the reverse mode of automatic differentiation and somehow is not\n``explicit''. We study this adjoint algorithm, show how it can be implemented\n(without resorting to an automatic transformation), and demonstrate its use on\npolynomial matrices.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 Apr 2008 12:37:43 GMT"
            }
        ],
        "update_date": "2011-12-13",
        "authors_parsed": [
            [
                "Villard",
                "Gilles",
                "",
                "LIP"
            ]
        ]
    },
    {
        "id": "0804.1046",
        "submitter": "Xu Zhiqiang",
        "authors": "Zhiqiang Xu, Guoliang Xu",
        "title": "Discrete schemes for Gaussian curvature and their convergence",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.CG cs.GR cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, several discrete schemes for Gaussian curvature are surveyed.\nThe convergence property of a modified discrete scheme for the Gaussian\ncurvature is considered. Furthermore, a new discrete scheme for Gaussian\ncurvature is resented. We prove that the new scheme converges at the regular\nvertex with valence not less than 5. By constructing a counterexample, we also\nshow that it is impossible for building a discrete scheme for Gaussian\ncurvature which converges over the regular vertex with valence 4. Finally,\nasymptotic errors of several discrete scheme for Gaussian curvature are\ncompared.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 Apr 2008 14:47:03 GMT"
            }
        ],
        "update_date": "2008-04-09",
        "authors_parsed": [
            [
                "Xu",
                "Zhiqiang",
                ""
            ],
            [
                "Xu",
                "Guoliang",
                ""
            ]
        ]
    },
    {
        "id": "0804.1118",
        "submitter": "Donald Sofge",
        "authors": "Donald A. Sofge",
        "title": "A Survey of Quantum Programming Languages: History, Methods, and Tools",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Quantum computer programming is emerging as a new subject domain from\nmultidisciplinary research in quantum computing, computer science, mathematics\n(especially quantum logic, lambda calculi, and linear logic), and engineering\nattempts to build the first non-trivial quantum computer. This paper briefly\nsurveys the history, methods, and proposed tools for programming quantum\ncomputers circa late 2007. It is intended to provide an extensive but\nnon-exhaustive look at work leading up to the current state-of-the-art in\nquantum computer programming. Further, it is an attempt to analyze the needed\nprogramming tools for quantum programmers, to use this analysis to predict the\ndirection in which the field is moving, and to make recommendations for further\ndevelopment of quantum programming language tools.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 Apr 2008 19:48:31 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Sofge",
                "Donald A.",
                ""
            ]
        ]
    },
    {
        "id": "0804.1187",
        "submitter": "Marianne Viallet",
        "authors": "Marianne Viallet (LTDS), G\\'erald Poum\\'erol, Olivier Dessombz (LTDS),\n  Louis Jezequel (LTDS)",
        "title": "M\\'ethode de calcul du rayonnement acoustique de structures complexes",
        "comments": null,
        "journal-ref": "Dans Actes du huiti\\`eme colloque national en Calcul des\n  structures - GIENS 2007 - Huiti\\`eme colloque national en Calcul des\n  structures, Giens : France (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the automotive industry, predicting noise during design cycle is a\nnecessary step. Well-known methods exist to answer this issue in low frequency\ndomain. Among these, Finite Element Methods, adapted to closed domains, are\nquite easy to implement whereas Boundary Element Methods are more adapted to\ninfinite domains, but may induce singularity problems. In this article, the\ndescribed method, the SDM, allows to use both methods in their best application\ndomain. A new method is also presented to solve the SDM exterior problem.\nInstead of using Boundary Element Methods, an original use of Finite Elements\nis made. Efficiency of this new version of the Substructure Deletion Method is\ndiscussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 8 Apr 2008 06:24:49 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Viallet",
                "Marianne",
                "",
                "LTDS"
            ],
            [
                "Poum\u00e9rol",
                "G\u00e9rald",
                "",
                "LTDS"
            ],
            [
                "Dessombz",
                "Olivier",
                "",
                "LTDS"
            ],
            [
                "Jezequel",
                "Louis",
                "",
                "LTDS"
            ]
        ]
    },
    {
        "id": "0804.1302",
        "submitter": "Francis Bach",
        "authors": "Francis Bach (INRIA Rocquencourt)",
        "title": "Bolasso: model consistent Lasso estimation through the bootstrap",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG math.ST stat.ML stat.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the least-square linear regression problem with regularization by\nthe l1-norm, a problem usually referred to as the Lasso. In this paper, we\npresent a detailed asymptotic analysis of model consistency of the Lasso. For\nvarious decays of the regularization parameter, we compute asymptotic\nequivalents of the probability of correct model selection (i.e., variable\nselection). For a specific rate decay, we show that the Lasso selects all the\nvariables that should enter the model with probability tending to one\nexponentially fast, while it selects all other variables with strictly positive\nprobability. We show that this property implies that if we run the Lasso for\nseveral bootstrapped replications of a given sample, then intersecting the\nsupports of the Lasso bootstrap estimates leads to consistent model selection.\nThis novel variable selection algorithm, referred to as the Bolasso, is\ncompared favorably to other linear regression methods on synthetic data and\ndatasets from the UCI machine learning repository.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 8 Apr 2008 15:40:03 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Bach",
                "Francis",
                "",
                "INRIA Rocquencourt"
            ]
        ]
    },
    {
        "id": "0804.1409",
        "submitter": "Murat Ali Bayir Mr.",
        "authors": "Murat Ali Bayir, Ismail Hakki Toroslu, Ahmet Cosar, Guven Fidan",
        "title": "Discovering More Accurate Frequent Web Usage Patterns",
        "comments": "19 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Web usage mining is a type of web mining, which exploits data mining\ntechniques to discover valuable information from navigation behavior of World\nWide Web users. As in classical data mining, data preparation and pattern\ndiscovery are the main issues in web usage mining. The first phase of web usage\nmining is the data processing phase, which includes the session reconstruction\noperation from server logs. Session reconstruction success directly affects the\nquality of the frequent patterns discovered in the next phase. In reactive web\nusage mining techniques, the source data is web server logs and the topology of\nthe web pages served by the web server domain. Other kinds of information\ncollected during the interactive browsing of web site by user, such as cookies\nor web logs containing similar information, are not used. The next phase of web\nusage mining is discovering frequent user navigation patterns. In this phase,\npattern discovery methods are applied on the reconstructed sessions obtained in\nthe first phase in order to discover frequent user patterns. In this paper, we\npropose a frequent web usage pattern discovery method that can be applied after\nsession reconstruction phase. In order to compare accuracy performance of\nsession reconstruction phase and pattern discovery phase, we have used an agent\nsimulator, which models behavior of web users and generates web user navigation\nas well as the log data kept by the web server.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 Apr 2008 05:46:26 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Bayir",
                "Murat Ali",
                ""
            ],
            [
                "Toroslu",
                "Ismail Hakki",
                ""
            ],
            [
                "Cosar",
                "Ahmet",
                ""
            ],
            [
                "Fidan",
                "Guven",
                ""
            ]
        ]
    },
    {
        "id": "0804.1435",
        "submitter": "Marc de Falco",
        "authors": "Marc de Falco",
        "title": "The Geometry of Interaction of Differential Interaction Nets",
        "comments": "20 pagee, to be published in the proceedings of LICS08",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Geometry of Interaction purpose is to give a semantic of proofs or\nprograms accounting for their dynamics. The initial presentation, translated as\nan algebraic weighting of paths in proofnets, led to a better characterization\nof the lambda-calculus optimal reduction. Recently Ehrhard and Regnier have\nintroduced an extension of the Multiplicative Exponential fragment of Linear\nLogic (MELL) that is able to express non-deterministic behaviour of programs\nand a proofnet-like calculus: Differential Interaction Nets. This paper\nconstructs a proper Geometry of Interaction (GoI) for this extension. We\nconsider it both as an algebraic theory and as a concrete reversible\ncomputation. We draw links between this GoI and the one of MELL. As a\nby-product we give for the first time an equational theory suitable for the GoI\nof the Multiplicative Additive fragment of Linear Logic.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 Apr 2008 08:57:47 GMT"
            }
        ],
        "update_date": "2008-04-10",
        "authors_parsed": [
            [
                "de Falco",
                "Marc",
                ""
            ]
        ]
    },
    {
        "id": "0804.1441",
        "submitter": "Ratthachat Chatpatanasiri",
        "authors": "Ratthachat Chatpatanasiri, Teesid Korsrilabutr, Pasakorn\n  Tangchanachaianan and Boonserm Kijsirikul",
        "title": "On Kernelization of Supervised Mahalanobis Distance Learners",
        "comments": "23 pages, 5 figures. There is a seriously wrong formula in derivation\n  of a gradient formula of the \"kernel NCA\" in the two previous versions. In\n  this new version, a new theoretical result is provided to properly account\n  kernel NCA",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper focuses on the problem of kernelizing an existing supervised\nMahalanobis distance learner. The following features are included in the paper.\nFirstly, three popular learners, namely, \"neighborhood component analysis\",\n\"large margin nearest neighbors\" and \"discriminant neighborhood embedding\",\nwhich do not have kernel versions are kernelized in order to improve their\nclassification performances. Secondly, an alternative kernelization framework\ncalled \"KPCA trick\" is presented. Implementing a learner in the new framework\ngains several advantages over the standard framework, e.g. no mathematical\nformulas and no reprogramming are required for a kernel implementation, the\nframework avoids troublesome problems such as singularity, etc. Thirdly, while\nthe truths of representer theorems are just assumptions in previous papers\nrelated to ours, here, representer theorems are formally proven. The proofs\nvalidate both the kernel trick and the KPCA trick in the context of Mahalanobis\ndistance learning. Fourthly, unlike previous works which always apply brute\nforce methods to select a kernel, we investigate two approaches which can be\nefficiently adopted to construct an appropriate kernel for a given dataset.\nFinally, numerical results on various real-world datasets are presented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 Apr 2008 09:40:51 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 20 Dec 2008 09:51:46 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 30 Jan 2009 02:19:27 GMT"
            }
        ],
        "update_date": "2009-01-30",
        "authors_parsed": [
            [
                "Chatpatanasiri",
                "Ratthachat",
                ""
            ],
            [
                "Korsrilabutr",
                "Teesid",
                ""
            ],
            [
                "Tangchanachaianan",
                "Pasakorn",
                ""
            ],
            [
                "Kijsirikul",
                "Boonserm",
                ""
            ]
        ]
    },
    {
        "id": "0804.1448",
        "submitter": "Vincent Garcia",
        "authors": "Vincent Garcia, Eric Debreuve and Michel Barlaud",
        "title": "Fast k Nearest Neighbor Search using GPU",
        "comments": "13 pages, 2figures, submitted to CVGPU 2008",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.DC",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  The recent improvements of graphics processing units (GPU) offer to the\ncomputer vision community a powerful processing platform. Indeed, a lot of\nhighly-parallelizable computer vision problems can be significantly accelerated\nusing GPU architecture. Among these algorithms, the k nearest neighbor search\n(KNN) is a well-known problem linked with many applications such as\nclassification, estimation of statistical properties, etc. The main drawback of\nthis task lies in its computation burden, as it grows polynomially with the\ndata size. In this paper, we show that the use of the NVIDIA CUDA API\naccelerates the search for the KNN up to a factor of 120.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 Apr 2008 10:06:15 GMT"
            }
        ],
        "update_date": "2008-04-10",
        "authors_parsed": [
            [
                "Garcia",
                "Vincent",
                ""
            ],
            [
                "Debreuve",
                "Eric",
                ""
            ],
            [
                "Barlaud",
                "Michel",
                ""
            ]
        ]
    },
    {
        "id": "0804.1649",
        "submitter": "David Sevilla",
        "authors": "Jaime Gutierrez, David Sevilla",
        "title": "On decomposition of tame polynomials and rational functions",
        "comments": "9 pages",
        "journal-ref": "Computer algebra in scientific computing, 219--226, Lecture Notes\n  in Comput. Sci., 4194, Springer, Berlin, 2006. MR2279795 (2008d:12001)",
        "doi": null,
        "report-no": null,
        "categories": "cs.SC math.AC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present algorithmic considerations and theoretical results\nabout the relation between the orders of certain groups associated to the\ncomponents of a polynomial and the order of the group that corresponds to the\npolynomial, proving it for arbitrary tame polynomials, and considering the case\nof rational functions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 Apr 2008 09:31:04 GMT"
            }
        ],
        "update_date": "2008-05-15",
        "authors_parsed": [
            [
                "Gutierrez",
                "Jaime",
                ""
            ],
            [
                "Sevilla",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0804.1679",
        "submitter": "David Sevilla",
        "authors": "Jaime Gutierrez, David Sevilla",
        "title": "Computation of unirational fields",
        "comments": "28 pages",
        "journal-ref": "Computation of unirational fields. J. Symbolic Comput. 41 (2006),\n  no. 11, 1222--1244. MR2267134 (2007g:12003)",
        "doi": null,
        "report-no": null,
        "categories": "cs.SC math.AC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the main contributions which Volker Weispfenning made to mathematics\nis related to Groebner bases theory. In this paper we present an algorithm for\ncomputing all algebraic intermediate subfields in a separably generated\nunirational field extension (which in particular includes the zero\ncharacteristic case). One of the main tools is Groebner bases theory. Our\nalgorithm also requires computing primitive elements and factoring over\nalgebraic extensions. Moreover, the method can be extended to finitely\ngenerated K-algebras.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 Apr 2008 11:59:15 GMT"
            }
        ],
        "update_date": "2008-05-15",
        "authors_parsed": [
            [
                "Gutierrez",
                "Jaime",
                ""
            ],
            [
                "Sevilla",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0804.1687",
        "submitter": "David Sevilla",
        "authors": "Jaime Gutierrez, David Sevilla",
        "title": "Building counterexamples to generalizations for rational functions of\n  Ritt's decomposition theorem",
        "comments": "17 pages",
        "journal-ref": "J. Algebra 303 (2006), no. 2, 655--667. MR2255128 (2007e:13032)",
        "doi": null,
        "report-no": null,
        "categories": "math.AC cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The classical Ritt's Theorems state several properties of univariate\npolynomial decomposition. In this paper we present new counterexamples to\nRitt's first theorem, which states the equality of length of decomposition\nchains of a polynomial, in the case of rational functions. Namely, we provide\nan explicit example of a rational function with coefficients in Q and two\ndecompositions of different length.\n  Another aspect is the use of some techniques that could allow for other\ncounterexamples, namely, relating groups and decompositions and using the fact\nthat the alternating group A_4 has two subgroup chains of different lengths;\nand we provide more information about the generalizations of another property\nof polynomial decomposition: the stability of the base field. We also present\nan algorithm for computing the fixing group of a rational function providing\nthe complexity over Q.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 Apr 2008 12:42:16 GMT"
            }
        ],
        "update_date": "2008-05-15",
        "authors_parsed": [
            [
                "Gutierrez",
                "Jaime",
                ""
            ],
            [
                "Sevilla",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0804.1696",
        "submitter": "Freddy Munoz",
        "authors": "Freddy Munoz (IRISA), Benoit Baudry (IRISA), Olivier Barais (IRISA)",
        "title": "A classification of invasive patterns in AOP",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "RR-6501",
        "categories": "cs.PL cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Aspect-Oriented Programming (AOP) improves modularity by encapsulating\ncrosscutting concerns into aspects. Some mechanisms to compose aspects allow\ninvasiveness as a mean to integrate concerns. Invasiveness means that AOP\nlanguages have unrestricted access to program properties. Such kind of\nlanguages are interesting because they allow performing complex operations and\nbetter introduce functionalities. In this report we present a classification of\ninvasive patterns in AOP. This classification characterizes the aspects\ninvasive behavior and allows developers to abstract about the aspect incidence\nover the program they crosscut.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 Apr 2008 13:21:32 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 24 Apr 2008 15:46:28 GMT"
            }
        ],
        "update_date": "2009-04-20",
        "authors_parsed": [
            [
                "Munoz",
                "Freddy",
                "",
                "IRISA"
            ],
            [
                "Baudry",
                "Benoit",
                "",
                "IRISA"
            ],
            [
                "Barais",
                "Olivier",
                "",
                "IRISA"
            ]
        ]
    },
    {
        "id": "0804.1696",
        "submitter": "Freddy Munoz",
        "authors": "Freddy Munoz (IRISA), Benoit Baudry (IRISA), Olivier Barais (IRISA)",
        "title": "A classification of invasive patterns in AOP",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "RR-6501",
        "categories": "cs.PL cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Aspect-Oriented Programming (AOP) improves modularity by encapsulating\ncrosscutting concerns into aspects. Some mechanisms to compose aspects allow\ninvasiveness as a mean to integrate concerns. Invasiveness means that AOP\nlanguages have unrestricted access to program properties. Such kind of\nlanguages are interesting because they allow performing complex operations and\nbetter introduce functionalities. In this report we present a classification of\ninvasive patterns in AOP. This classification characterizes the aspects\ninvasive behavior and allows developers to abstract about the aspect incidence\nover the program they crosscut.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 Apr 2008 13:21:32 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 24 Apr 2008 15:46:28 GMT"
            }
        ],
        "update_date": "2009-04-20",
        "authors_parsed": [
            [
                "Munoz",
                "Freddy",
                "",
                "IRISA"
            ],
            [
                "Baudry",
                "Benoit",
                "",
                "IRISA"
            ],
            [
                "Barais",
                "Olivier",
                "",
                "IRISA"
            ]
        ]
    },
    {
        "id": "0804.1707",
        "submitter": "David Sevilla",
        "authors": "Jaime Gutierrez, David Sevilla",
        "title": "Computation of unirational fields (extended abstract)",
        "comments": "6 pages, 2 images (pictures of authors)",
        "journal-ref": "Proceedings of the 2005 Algorithmic Algebra and Logic (A3L), p.\n  129--134, BOD Norderstedt, Germany, 2005. ISBN 3-8334-2669-1",
        "doi": null,
        "report-no": null,
        "categories": "cs.SC math.AC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present an algorithm for computing all algebraic\nintermediate subfields in a separably generated unirational field extension\n(which in particular includes the zero characteristic case). One of the main\ntools is Groebner bases theory. Our algorithm also requires computing computing\nprimitive elements and factoring over algebraic extensions. Moreover, the\nmethod can be extended to finitely generated K-algebras.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 Apr 2008 14:00:10 GMT"
            }
        ],
        "update_date": "2008-05-15",
        "authors_parsed": [
            [
                "Gutierrez",
                "Jaime",
                ""
            ],
            [
                "Sevilla",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0804.1845",
        "submitter": "Ely Porat",
        "authors": "Ely Porat",
        "title": "An Optimal Bloom Filter Replacement Based on Matrix Solving",
        "comments": "A lectureon this paper will be available in Google video",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We suggest a method for holding a dictionary data structure, which maps keys\nto values, in the spirit of Bloom Filters. The space requirements of the\ndictionary we suggest are much smaller than those of a hashtable. We allow\nstoring n keys, each mapped to value which is a string of k bits. Our suggested\nmethod requires nk + o(n) bits space to store the dictionary, and O(n) time to\nproduce the data structure, and allows answering a membership query in O(1)\nmemory probes. The dictionary size does not depend on the size of the keys.\nHowever, reducing the space requirements of the data structure comes at a\ncertain cost. Our dictionary has a small probability of a one sided error. When\nattempting to obtain the value for a key that is stored in the dictionary we\nalways get the correct answer. However, when testing for membership of an\nelement that is not stored in the dictionary, we may get an incorrect answer,\nand when requesting the value of such an element we may get a certain random\nvalue. Our method is based on solving equations in GF(2^k) and using several\nhash functions. Another significant advantage of our suggested method is that\nwe do not require using sophisticated hash functions. We only require pairwise\nindependent hash functions. We also suggest a data structure that requires only\nnk bits space, has O(n2) preprocessing time, and has a O(log n) query time.\nHowever, this data structures requires a uniform hash functions. In order\nreplace a Bloom Filter of n elements with an error proability of 2^{-k}, we\nrequire nk + o(n) memory bits, O(1) query time, O(n) preprocessing time, and\nonly pairwise independent hash function. Even the most advanced previously\nknown Bloom Filter would require nk+O(n) space, and a uniform hash functions,\nso our method is significantly less space consuming especially when k is small.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 Apr 2008 11:24:04 GMT"
            }
        ],
        "update_date": "2008-04-14",
        "authors_parsed": [
            [
                "Porat",
                "Ely",
                ""
            ]
        ]
    },
    {
        "id": "0804.1974",
        "submitter": "Nitin Saxena",
        "authors": "G\\'abor Ivanyos, Marek Karpinski, Nitin Saxena",
        "title": "Schemes for Deterministic Polynomial Factoring",
        "comments": "14 pages, preliminary version",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CC cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we relate the deterministic complexity of factoring polynomials\n(over finite fields) to certain combinatorial objects we call m-schemes. We\nextend the known conditional deterministic subexponential time polynomial\nfactoring algorithm for finite fields to get an underlying m-scheme. We\ndemonstrate how the properties of m-schemes relate to improvements in the\ndeterministic complexity of factoring polynomials over finite fields assuming\nthe generalized Riemann Hypothesis (GRH). In particular, we give the first\ndeterministic polynomial time algorithm (assuming GRH) to find a nontrivial\nfactor of a polynomial of prime degree n where (n-1) is a smooth number.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 Apr 2008 23:04:17 GMT"
            }
        ],
        "update_date": "2008-04-15",
        "authors_parsed": [
            [
                "Ivanyos",
                "G\u00e1bor",
                ""
            ],
            [
                "Karpinski",
                "Marek",
                ""
            ],
            [
                "Saxena",
                "Nitin",
                ""
            ]
        ]
    },
    {
        "id": "0804.1982",
        "submitter": "Li Chen",
        "authors": "Li Chen, Yongwu Rong",
        "title": "Linear Time Recognition Algorithms for Topological Invariants in 3D",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we design linear time algorithms to recognize and determine\ntopological invariants such as the genus and homology groups in 3D. These\nproperties can be used to identify patterns in 3D image recognition. This has\ntremendous amount of applications in 3D medical image analysis. Our method is\nbased on cubical images with direct adjacency, also called (6,26)-connectivity\nimages in discrete geometry. According to the fact that there are only six\ntypes of local surface points in 3D and a discrete version of the well-known\nGauss-Bonnett Theorem in differential geometry, we first determine the genus of\na closed 2D-connected component (a closed digital surface). Then, we use\nAlexander duality to obtain the homology groups of a 3D object in 3D space.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 12 Apr 2008 03:13:33 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 4 Aug 2008 05:06:51 GMT"
            }
        ],
        "update_date": "2008-08-04",
        "authors_parsed": [
            [
                "Chen",
                "Li",
                ""
            ],
            [
                "Rong",
                "Yongwu",
                ""
            ]
        ]
    },
    {
        "id": "0804.2095",
        "submitter": "Paul Tarau",
        "authors": "Paul Tarau and Brenda Luderman",
        "title": "A Logic Programming Framework for Combinational Circuit Synthesis",
        "comments": null,
        "journal-ref": "23rd International Conference on Logic Programming (ICLP), LNCS\n  4670, 2007, pages 180-194",
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.CE cs.DM cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Logic Programming languages and combinational circuit synthesis tools share a\ncommon \"combinatorial search over logic formulae\" background. This paper\nattempts to reconnect the two fields with a fresh look at Prolog encodings for\nthe combinatorial objects involved in circuit synthesis. While benefiting from\nProlog's fast unification algorithm and built-in backtracking mechanism,\nefficiency of our search algorithm is ensured by using parallel bitstring\noperations together with logic variable equality propagation, as a mapping\nmechanism from primary inputs to the leaves of candidate Leaf-DAGs implementing\na combinational circuit specification. After an exhaustive expressiveness\ncomparison of various minimal libraries, a surprising first-runner, Strict\nBoolean Inequality \"<\" together with constant function \"1\" also turns out to\nhave small transistor-count implementations, competitive to NAND-only or\nNOR-only libraries. As a practical outcome, a more realistic circuit\nsynthesizer is implemented that combines rewriting-based simplification of\n(<,1) circuits with exhaustive Leaf-DAG circuit search.\n  Keywords: logic programming and circuit design, combinatorial object\ngeneration, exact combinational circuit synthesis, universal boolean logic\nlibraries, symbolic rewriting, minimal transistor-count circuit synthesis\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 Apr 2008 02:40:31 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Tarau",
                "Paul",
                ""
            ],
            [
                "Luderman",
                "Brenda",
                ""
            ]
        ]
    },
    {
        "id": "0804.2095",
        "submitter": "Paul Tarau",
        "authors": "Paul Tarau and Brenda Luderman",
        "title": "A Logic Programming Framework for Combinational Circuit Synthesis",
        "comments": null,
        "journal-ref": "23rd International Conference on Logic Programming (ICLP), LNCS\n  4670, 2007, pages 180-194",
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.CE cs.DM cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Logic Programming languages and combinational circuit synthesis tools share a\ncommon \"combinatorial search over logic formulae\" background. This paper\nattempts to reconnect the two fields with a fresh look at Prolog encodings for\nthe combinatorial objects involved in circuit synthesis. While benefiting from\nProlog's fast unification algorithm and built-in backtracking mechanism,\nefficiency of our search algorithm is ensured by using parallel bitstring\noperations together with logic variable equality propagation, as a mapping\nmechanism from primary inputs to the leaves of candidate Leaf-DAGs implementing\na combinational circuit specification. After an exhaustive expressiveness\ncomparison of various minimal libraries, a surprising first-runner, Strict\nBoolean Inequality \"<\" together with constant function \"1\" also turns out to\nhave small transistor-count implementations, competitive to NAND-only or\nNOR-only libraries. As a practical outcome, a more realistic circuit\nsynthesizer is implemented that combines rewriting-based simplification of\n(<,1) circuits with exhaustive Leaf-DAG circuit search.\n  Keywords: logic programming and circuit design, combinatorial object\ngeneration, exact combinational circuit synthesis, universal boolean logic\nlibraries, symbolic rewriting, minimal transistor-count circuit synthesis\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 Apr 2008 02:40:31 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Tarau",
                "Paul",
                ""
            ],
            [
                "Luderman",
                "Brenda",
                ""
            ]
        ]
    },
    {
        "id": "0804.2181",
        "submitter": "Frederic Chyzak",
        "authors": "Alin Bostan (INRIA Rocquencourt), Fr\\'ed\\'eric Chyzak (INRIA\n  Rocquencourt), Nicolas Le Roux (INRIA Rocquencourt)",
        "title": "Products of Ordinary Differential Operators by Evaluation and\n  Interpolation",
        "comments": null,
        "journal-ref": "Dans ISSAC'08 (2008)",
        "doi": null,
        "report-no": null,
        "categories": "cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is known that multiplication of linear differential operators over ground\nfields of characteristic zero can be reduced to a constant number of matrix\nproducts. We give a new algorithm by evaluation and interpolation which is\nfaster than the previously-known one by a constant factor, and prove that in\ncharacteristic zero, multiplication of differential operators and of matrices\nare computationally equivalent problems. In positive characteristic, we show\nthat differential operators can be multiplied in nearly optimal time.\nTheoretical results are validated by intensive experiments.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 Apr 2008 13:37:14 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Bostan",
                "Alin",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Chyzak",
                "Fr\u00e9d\u00e9ric",
                "",
                "INRIA\n  Rocquencourt"
            ],
            [
                "Roux",
                "Nicolas Le",
                "",
                "INRIA Rocquencourt"
            ]
        ]
    },
    {
        "id": "0804.2337",
        "submitter": "Bruno Salvy",
        "authors": "Alin Bostan (INRIA Rocquencourt), Bruno Salvy (INRIA Rocquencourt),\n  \\'Eric Schost",
        "title": "Power Series Composition and Change of Basis",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/1390768.1390806",
        "report-no": null,
        "categories": "cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Efficient algorithms are known for many operations on truncated power series\n(multiplication, powering, exponential, ...). Composition is a more complex\ntask. We isolate a large class of power series for which composition can be\nperformed efficiently. We deduce fast algorithms for converting polynomials\nbetween various bases, including Euler, Bernoulli, Fibonacci, and the\northogonal Laguerre, Hermite, Jacobi, Krawtchouk, Meixner and\nMeixner-Pollaczek.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 Apr 2008 09:43:27 GMT"
            }
        ],
        "update_date": "2013-06-19",
        "authors_parsed": [
            [
                "Bostan",
                "Alin",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Salvy",
                "Bruno",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Schost",
                "\u00c9ric",
                ""
            ]
        ]
    },
    {
        "id": "0804.2346",
        "submitter": "Sudhakar Sahoo",
        "authors": "Pabitra Pal Choudhury, Birendra Kumar Nayak, Sudhakar Sahoo, Sunil\n  Pankaj Rath",
        "title": "Theory and Applications of Two-dimensional, Null-boundary,\n  Nine-Neighborhood, Cellular Automata Linear rules",
        "comments": "17 pages, 41 figures, a portion of this paper is accepted in the\n  journal as well as proceedings at WSEAS,2006",
        "journal-ref": null,
        "doi": null,
        "report-no": "Tech.Report No. ASD/2005/4, 13 May 2005",
        "categories": "cs.DM cs.CC cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper deals with the theory and application of 2-Dimensional,\nnine-neighborhood, null- boundary, uniform as well as hybrid Cellular Automata\n(2D CA) linear rules in image processing. These rules are classified into nine\ngroups depending upon the number of neighboring cells influences the cell under\nconsideration. All the Uniform rules have been found to be rendering multiple\ncopies of a given image depending on the groups to which they belong where as\nHybrid rules are also shown to be characterizing the phenomena of zooming in,\nzooming out, thickening and thinning of a given image. Further, using hybrid CA\nrules a new searching algorithm is developed called Sweepers algorithm which is\nfound to be applicable to simulate many inter disciplinary research areas like\nmigration of organisms towards a single point destination, Single Attractor and\nMultiple Attractor Cellular Automata Theory, Pattern Classification and\nClustering Problem, Image compression, Encryption and Decryption problems,\nDensity Classification problem etc.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 Apr 2008 10:17:35 GMT"
            }
        ],
        "update_date": "2008-04-16",
        "authors_parsed": [
            [
                "Choudhury",
                "Pabitra Pal",
                ""
            ],
            [
                "Nayak",
                "Birendra Kumar",
                ""
            ],
            [
                "Sahoo",
                "Sudhakar",
                ""
            ],
            [
                "Rath",
                "Sunil Pankaj",
                ""
            ]
        ]
    },
    {
        "id": "0804.2373",
        "submitter": "Alin Bostan",
        "authors": "Alin Bostan (INRIA Rocquencourt), Bruno Salvy (INRIA Rocquencourt),\n  \\'Eric Schost",
        "title": "Fast Conversion Algorithms for Orthogonal Polynomials",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.laa.2009.08.002",
        "report-no": null,
        "categories": "cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss efficient conversion algorithms for orthogonal polynomials. We\ndescribe a known conversion algorithm from an arbitrary orthogonal basis to the\nmonomial basis, and deduce a new algorithm of the same complexity for the\nconverse operation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 Apr 2008 12:47:14 GMT"
            }
        ],
        "update_date": "2013-06-19",
        "authors_parsed": [
            [
                "Bostan",
                "Alin",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Salvy",
                "Bruno",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Schost",
                "\u00c9ric",
                ""
            ]
        ]
    },
    {
        "id": "0804.2852",
        "submitter": "Chris Kimble",
        "authors": "David King and Chris Kimble",
        "title": "Philosophical Smoke Signals: Theory and Practice in Information Systems\n  Design",
        "comments": "in Proceedings of 10th UKAIS Conference, (May 2005), Northumbria\n  University, UK",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.GL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although the gulf between the theory and practice in Information Systems is\nmuch lamented, few researchers have offered a way forward except through a\nnumber of (failed) attempts to develop a single systematic theory for\nInformation Systems. In this paper, we encourage researchers to re-examine the\npractical consequences of their theoretical arguments. By examining these\narguments we may be able to form a number of more rigorous theories of\nInformation Systems, allowing us to draw theory and practice together without\nundertaking yet another attempt at the holy grail of a single unified\nsystematic theory of Information Systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 17 Apr 2008 16:46:55 GMT"
            }
        ],
        "update_date": "2008-04-18",
        "authors_parsed": [
            [
                "King",
                "David",
                ""
            ],
            [
                "Kimble",
                "Chris",
                ""
            ]
        ]
    },
    {
        "id": "0804.2992",
        "submitter": "Eberhard H.-A. Gerbracht",
        "authors": "Eberhard H.-A. Gerbracht",
        "title": "\"E pluribus unum\" or How to Derive Single-equation Descriptions for\n  Output-quantities in Nonlinear Circuits using Differential Algebra",
        "comments": "V1: documentclass IEEEtran, 7 pages, 10 figures. Re-release of the\n  printed version, with some minor typographical errors corrected",
        "journal-ref": "Proceedings of the 7th International Workshop on Symbolic Methods\n  and Applications to Circuit Design, SMACD 2002, Sinaia, Romania, October\n  10-11, 2002; pp. 65-70; ISBN 973-85072-5-1",
        "doi": null,
        "report-no": null,
        "categories": "cs.SC math.CA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we describe by a number of examples how to deduce one single\ncharacterizing higher order differential equation for output quantities of an\nanalog circuit.\n  In the linear case, we apply basic \"symbolic\" methods from linear algebra to\nthe system of differential equations which is used to model the analog circuit.\nFor nonlinear circuits and their corresponding nonlinear differential\nequations, we show how to employ computer algebra tools implemented in Maple,\nwhich are based on differential algebra.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 18 Apr 2008 19:55:58 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Gerbracht",
                "Eberhard H. -A.",
                ""
            ]
        ]
    },
    {
        "id": "0804.3023",
        "submitter": "Abdessamad Imine",
        "authors": "Hanifa Boucheneb (VeriForm), Abdessamad Imine (INRIA Lorraine - LORIA\n  / LIFC)",
        "title": "Experiments in Model-Checking Optimistic Replication Algorithms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "RR-6510",
        "categories": "cs.LO cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes a series of model-checking experiments to verify\noptimistic replication algorithms based on Operational Transformation (OT)\napproach used for supporting collaborative edition. We formally define, using\ntool UPPAAL, the behavior and the main consistency requirement (i.e.\nconvergence property) of the collaborative editing systems, as well as the\nabstract behavior of the environment where these systems are supposed to\noperate. Due to data replication and the unpredictable nature of user\ninteractions, such systems have infinitely many states. So, we show how to\nexploit some features of the UPPAAL specification language to attenuate the\nsevere state explosion problem. Two models are proposed. The first one, called\nconcrete model, is very close to the system implementation but runs up against\na severe explosion of states. The second model, called symbolic model, aims to\novercome the limitation of the concrete model by delaying the effective\nselection and execution of editing operations until the construction of\nsymbolic execution traces of all sites is completed. Experimental results have\nshown that the symbolic model allows a significant gain in both space and time.\nUsing the symbolic model, we have been able to show that if the number of sites\nexceeds 2 then the convergence property is not satisfied for all OT algorithms\nconsidered here. A counterexample is provided for every algorithm.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 18 Apr 2008 14:04:38 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 21 Apr 2008 08:51:52 GMT"
            }
        ],
        "update_date": "2009-04-20",
        "authors_parsed": [
            [
                "Boucheneb",
                "Hanifa",
                "",
                "VeriForm"
            ],
            [
                "Imine",
                "Abdessamad",
                "",
                "INRIA Lorraine - LORIA\n  / LIFC"
            ]
        ]
    },
    {
        "id": "0804.3171",
        "submitter": "Prashanth Alluvada",
        "authors": "Prashanth Alluvada",
        "title": "Optimization Approach for Detecting the Critical Data on a Database",
        "comments": "6 pages, 1 figure, 3 tables. corrected typos, added remarks",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Through purposeful introduction of malicious transactions (tracking\ntransactions) into randomly select nodes of a (database) graph, soiled and\nclean segments are identified. Soiled and clean measures corresponding those\nsegments are then computed. These measures are used to repose the problem of\ncritical database elements detection as an optimization problem over the graph.\nThis method is universally applicable over a large class of graphs (including\ndirected, weighted, disconnected, cyclic) that occur in several contexts of\ndatabases. A generalization argument is presented which extends the critical\ndata problem to abstract settings.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 20 Apr 2008 03:23:38 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 27 Apr 2008 19:32:31 GMT"
            }
        ],
        "update_date": "2008-04-27",
        "authors_parsed": [
            [
                "Alluvada",
                "Prashanth",
                ""
            ]
        ]
    },
    {
        "id": "0804.3193",
        "submitter": "Diego Conti",
        "authors": "Diego Conti",
        "title": "Symbolic computations in differential geometry",
        "comments": "14 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.DG cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce the C++ library Wedge, based on GiNaC, for symbolic computations\nin differential geometry. We show how Wedge makes it possible to use the\nlanguage C++ to perform such computations, and illustrate some advantages of\nthis approach with explicit examples. In particular, we describe a short\nprogram to determine whether a given linear exterior differential system is\ninvolutive.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 20 Apr 2008 14:55:56 GMT"
            }
        ],
        "update_date": "2008-04-22",
        "authors_parsed": [
            [
                "Conti",
                "Diego",
                ""
            ]
        ]
    },
    {
        "id": "0804.3234",
        "submitter": "Jorge Leandro",
        "authors": "J. J. G. Leandro (1), R. M. Cesar Jr (1) and L. da F. Costa (2) ((1)\n  Institute of Mathematics and Statistics - USP - Brazil, (2) Instituto de\n  F\\'isica de S\\~ao Carlos - USP - Brazil)",
        "title": "Technical Report - Automatic Contour Extraction from 2D Neuron Images",
        "comments": "40 pages, 22 figures and 02 tables. Typos corrected, references\n  added, figures added, new experiments added. Submitted to Elsevier-Journal of\n  Neuronscience Methods. For associated video demonstration of the system, see\n  http://www.vision.ime.usp.br/creativision/demos/branch/alfa-bta-bscea-fast.html",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV q-bio.NC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work describes a novel methodology for automatic contour extraction from\n2D images of 3D neurons (e.g. camera lucida images and other types of 2D\nmicroscopy). Most contour-based shape analysis methods can not be used to\ncharacterize such cells because of overlaps between neuronal processes. The\nproposed framework is specifically aimed at the problem of contour following\neven in presence of multiple overlaps. First, the input image is preprocessed\nin order to obtain an 8-connected skeleton with one-pixel-wide branches, as\nwell as a set of critical regions (i.e., bifurcations and crossings). Next, for\neach subtree, the tracking stage iteratively labels all valid pixel of\nbranches, up to a critical region, where it determines the suitable direction\nto proceed. Finally, the labeled skeleton segments are followed in order to\nyield the parametric contour of the neuronal shape under analysis. The reported\nsystem was successfully tested with respect to several images and the results\nfrom a set of three neuron images are presented here, each pertaining to a\ndifferent class, i.e. alpha, delta and epsilon ganglion cells, containing a\ntotal of 34 crossings. The algorithms successfully got across all these\noverlaps. The method has also been found to exhibit robustness even for images\nwith close parallel segments. The proposed method is robust and may be\nimplemented in an efficient manner. The introduction of this approach should\npave the way for more systematic application of contour-based shape analysis\nmethods in neuronal morphology.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 21 Apr 2008 04:03:48 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 24 Oct 2008 16:02:03 GMT"
            }
        ],
        "update_date": "2008-10-24",
        "authors_parsed": [
            [
                "Leandro",
                "J. J. G.",
                ""
            ],
            [
                "Cesar",
                "R. M.",
                "Jr"
            ],
            [
                "Costa",
                "L. da F.",
                ""
            ]
        ]
    },
    {
        "id": "0804.3361",
        "submitter": "Forrest Bao",
        "authors": "Forrest Sheng Bao, Donald Yu-Chun Lie, Yuanlin Zhang",
        "title": "A New Approach to Automated Epileptic Diagnosis Using EEG and\n  Probabilistic Neural Network",
        "comments": "5 pages, 6 figures, 1 table, submitted to IEEE ICTAI 2008",
        "journal-ref": null,
        "doi": "10.1109/ICTAI.2008.99",
        "report-no": null,
        "categories": "cs.AI cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Epilepsy is one of the most common neurological disorders that greatly impair\npatient' daily lives. Traditional epileptic diagnosis relies on tedious visual\nscreening by neurologists from lengthy EEG recording that requires the presence\nof seizure (ictal) activities. Nowadays, there are many systems helping the\nneurologists to quickly find interesting segments of the lengthy signal by\nautomatic seizure detection. However, we notice that it is very difficult, if\nnot impossible, to obtain long-term EEG data with seizure activities for\nepilepsy patients in areas lack of medical resources and trained neurologists.\nTherefore, we propose to study automated epileptic diagnosis using interictal\nEEG data that is much easier to collect than ictal data. The authors are not\naware of any report on automated EEG diagnostic system that can accurately\ndistinguish patients' interictal EEG from the EEG of normal people. The\nresearch presented in this paper, therefore, aims to develop an automated\ndiagnostic system that can use interictal EEG data to diagnose whether the\nperson is epileptic. Such a system should also detect seizure activities for\nfurther investigation by doctors and potential patient monitoring. To develop\nsuch a system, we extract four classes of features from the EEG data and build\na Probabilistic Neural Network (PNN) fed with these features. Leave-one-out\ncross-validation (LOO-CV) on a widely used epileptic-normal data set reflects\nan impressive 99.5% accuracy of our system on distinguishing normal people's\nEEG from patient's interictal EEG. We also find our system can be used in\npatient monitoring (seizure detection) and seizure focus localization, with\n96.7% and 77.5% accuracy respectively on the data set.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 21 Apr 2008 17:07:59 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 5 Jul 2008 01:45:56 GMT"
            }
        ],
        "update_date": "2016-11-18",
        "authors_parsed": [
            [
                "Bao",
                "Forrest Sheng",
                ""
            ],
            [
                "Lie",
                "Donald Yu-Chun",
                ""
            ],
            [
                "Zhang",
                "Yuanlin",
                ""
            ]
        ]
    },
    {
        "id": "0804.3500",
        "submitter": "Claudia Landi",
        "authors": "M. d'Amico, P. Frosini and C.Landi",
        "title": "Natural pseudo-distance and optimal matching between reduced size\n  functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CG cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper studies the properties of a new lower bound for the natural\npseudo-distance. The natural pseudo-distance is a dissimilarity measure between\nshapes, where a shape is viewed as a topological space endowed with a\nreal-valued continuous function. Measuring dissimilarity amounts to minimizing\nthe change in the functions due to the application of homeomorphisms between\ntopological spaces, with respect to the $L_\\infty$-norm. In order to obtain the\nlower bound, a suitable metric between size functions, called matching\ndistance, is introduced. It compares size functions by solving an optimal\nmatching problem between countable point sets. The matching distance is shown\nto be resistant to perturbations, implying that it is always smaller than the\nnatural pseudo-distance. We also prove that the lower bound so obtained is\nsharp and cannot be improved by any other distance between size functions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 22 Apr 2008 11:25:11 GMT"
            }
        ],
        "update_date": "2008-04-23",
        "authors_parsed": [
            [
                "d'Amico",
                "M.",
                ""
            ],
            [
                "Frosini",
                "P.",
                ""
            ],
            [
                "Landi",
                "C.",
                ""
            ]
        ]
    },
    {
        "id": "0804.3575",
        "submitter": "S. Charles Brubaker",
        "authors": "S. Charles Brubaker and Santosh S. Vempala",
        "title": "Isotropic PCA and Affine-Invariant Clustering",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.CG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new algorithm for clustering points in R^n. The key property of\nthe algorithm is that it is affine-invariant, i.e., it produces the same\npartition for any affine transformation of the input. It has strong guarantees\nwhen the input is drawn from a mixture model. For a mixture of two arbitrary\nGaussians, the algorithm correctly classifies the sample assuming only that the\ntwo components are separable by a hyperplane, i.e., there exists a halfspace\nthat contains most of one Gaussian and almost none of the other in probability\nmass. This is nearly the best possible, improving known results substantially.\nFor k > 2 components, the algorithm requires only that there be some\n(k-1)-dimensional subspace in which the emoverlap in every direction is small.\nHere we define overlap to be the ratio of the following two quantities: 1) the\naverage squared distance between a point and the mean of its component, and 2)\nthe average squared distance between a point and the mean of the mixture. The\nmain result may also be stated in the language of linear discriminant analysis:\nif the standard Fisher discriminant is small enough, labels are not needed to\nestimate the optimal subspace for projection. Our main tools are isotropic\ntransformation, spectral projection and a simple reweighting technique. We call\nthis combination isotropic PCA.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 22 Apr 2008 17:59:03 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 4 Aug 2008 19:28:46 GMT"
            }
        ],
        "update_date": "2008-08-04",
        "authors_parsed": [
            [
                "Brubaker",
                "S. Charles",
                ""
            ],
            [
                "Vempala",
                "Santosh S.",
                ""
            ]
        ]
    },
    {
        "id": "0804.3814",
        "submitter": "Arun  Kumar  S P",
        "authors": "Arun Kumar S P, Diganta Baishya, Amrendra Kumar",
        "title": "Link Enhancer for Vehicular Wireless ATM Communications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Majority of the applications used in defense are voice, video and data\noriented and has strict QoS requirements. One of the technologies that enabled\nthis is Asynchronous Transfer Mode (ATM) networking. Traditional ATM networks\nare wired networks. But Tactical networks are meant to be mobile and this\nnecessitates the use of radio relays for Vehicle-to-Infrastructure (V2I) and\nVehicle-to-Vehicle (V2V) communications. ATM networks assume a physical link\nlayer BER of 10^-9 or better because of the availability of reliable media like\noptical fiber links. But this assumption is no longer valid when ATM switches\nare connected through radio relay where error rates are in the rage of 10^-3.\nThis paper presents the architecture of a Link Enhancer meant to improve the\nBit Error Rate of the Wireless links used for V2I and V2V communications from 1\nin 10^4 to 1 in 10^8\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 Apr 2008 18:54:11 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "P",
                "Arun Kumar S",
                ""
            ],
            [
                "Baishya",
                "Diganta",
                ""
            ],
            [
                "Kumar",
                "Amrendra",
                ""
            ]
        ]
    },
    {
        "id": "0804.3817",
        "submitter": "Jan Arpe",
        "authors": "Jan Arpe and Elchanan Mossel",
        "title": "Multiple Random Oracles Are Better Than One",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of learning k-juntas given access to examples drawn from\na number of different product distributions. Thus we wish to learn a function f\n: {-1,1}^n -> {-1,1} that depends on k (unknown) coordinates. While the best\nknown algorithms for the general problem of learning a k-junta require running\ntime of n^k * poly(n,2^k), we show that given access to k different product\ndistributions with biases separated by \\gamma>0, the functions may be learned\nin time poly(n,2^k,\\gamma^{-k}). More generally, given access to t <= k\ndifferent product distributions, the functions may be learned in time n^{k/t} *\npoly(n,2^k,\\gamma^{-k}). Our techniques involve novel results in Fourier\nanalysis relating Fourier expansions with respect to different biases and a\ngeneralization of Russo's formula.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 23 Apr 2008 23:18:00 GMT"
            }
        ],
        "update_date": "2008-04-25",
        "authors_parsed": [
            [
                "Arpe",
                "Jan",
                ""
            ],
            [
                "Mossel",
                "Elchanan",
                ""
            ]
        ]
    },
    {
        "id": "0804.3914",
        "submitter": "Andrew Gacek",
        "authors": "Andrew Gacek, Dale Miller, Gopalan Nadathur",
        "title": "Reasoning in Abella about Structural Operational Semantics\n  Specifications",
        "comments": "15 pages. To appear in LFMTP'08",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The approach to reasoning about structural operational semantics style\nspecifications supported by the Abella system is discussed. This approach uses\nlambda tree syntax to treat object language binding and encodes binding related\nproperties in generic judgments. Further, object language specifications are\nembedded directly into the reasoning framework through recursive definitions.\nThe treatment of binding via generic judgments implicitly enforces distinctness\nand atomicity in the names used for bound variables. These properties must,\nhowever, be made explicit in reasoning tasks. This objective can be achieved by\nallowing recursive definitions to also specify generic properties of atomic\npredicates. The utility of these various logical features in the Abella system\nis demonstrated through actual reasoning tasks. Brief comparisons with a few\nother logic based approaches are also made.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 Apr 2008 15:22:02 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 3 Jun 2008 01:09:16 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Gacek",
                "Andrew",
                ""
            ],
            [
                "Miller",
                "Dale",
                ""
            ],
            [
                "Nadathur",
                "Gopalan",
                ""
            ]
        ]
    },
    {
        "id": "0804.4039",
        "submitter": "Ioannis Chatzigiannakis",
        "authors": "Ioannis Chatzigiannakis, Georgios Giannoulis and Paul G. Spirakis",
        "title": "Energy and Time Efficient Scheduling of Tasks with Dependencies on\n  Asymmetric Multiprocessors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "RACTI-RU1-2008-10",
        "categories": "cs.DC cs.DS cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this work we study the problem of scheduling tasks with dependencies in\nmultiprocessor architectures where processors have different speeds. We present\nthe preemptive algorithm \"Save-Energy\" that given a schedule of tasks it post\nprocesses it to improve the energy efficiency without any deterioration of the\nmakespan. In terms of time efficiency, we show that preemptive scheduling in an\nasymmetric system can achieve the same or better optimal makespan than in a\nsymmetric system. Motivited by real multiprocessor systems, we investigate\narchitectures that exhibit limited asymmetry: there are two essentially\ndifferent speeds. Interestingly, this special case has not been studied in the\nfield of parallel computing and scheduling theory; only the general case was\nstudied where processors have $K$ essentially different speeds. We present the\nnon-preemptive algorithm ``Remnants'' that achieves almost optimal makespan. We\nprovide a refined analysis of a recent scheduling method. Based on this\nanalysis, we specialize the scheduling policy and provide an algorithm of $(3 +\no(1))$ expected approximation factor. Note that this improves the previous best\nfactor (6 for two speeds). We believe that our work will convince researchers\nto revisit this well studied scheduling problem for these simple, yet\nrealistic, asymmetric multiprocessor architectures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 Apr 2008 03:16:21 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 6 Jun 2008 14:21:18 GMT"
            }
        ],
        "update_date": "2008-06-09",
        "authors_parsed": [
            [
                "Chatzigiannakis",
                "Ioannis",
                ""
            ],
            [
                "Giannoulis",
                "Georgios",
                ""
            ],
            [
                "Spirakis",
                "Paul G.",
                ""
            ]
        ]
    },
    {
        "id": "0804.4116",
        "submitter": "Mireille Ducasse",
        "authors": "Ludovic Langevine and Mireille Ducasse",
        "title": "Design and Implementation of a Tracer Driver: Easy and Efficient Dynamic\n  Analyses of Constraint Logic Programs",
        "comments": "To appear in Theory and Practice of Logic Programming (TPLP),\n  Cambridge University Press. 30 pages,",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Tracers provide users with useful information about program executions. In\nthis article, we propose a ``tracer driver''. From a single tracer, it provides\na powerful front-end enabling multiple dynamic analysis tools to be easily\nimplemented, while limiting the overhead of the trace generation. The relevant\nexecution events are specified by flexible event patterns and a large variety\nof trace data can be given either systematically or ``on demand''. The proposed\ntracer driver has been designed in the context of constraint logic programming;\nexperiments have been made within GNU-Prolog. Execution views provided by\nexisting tools have been easily emulated with a negligible overhead.\nExperimental measures show that the flexibility and power of the described\narchitecture lead to good performance. The tracer driver overhead is inversely\nproportional to the average time between two traced events. Whereas the\nprinciples of the tracer driver are independent of the traced programming\nlanguage, it is best suited for high-level languages, such as constraint logic\nprogramming, where each traced execution event encompasses numerous low-level\nexecution steps. Furthermore, constraint logic programming is especially hard\nto debug. The current environments do not provide all the useful dynamic\nanalysis tools. They can significantly benefit from our tracer driver which\nenables dynamic analyses to be integrated at a very low cost.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 Apr 2008 14:05:36 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Langevine",
                "Ludovic",
                ""
            ],
            [
                "Ducasse",
                "Mireille",
                ""
            ]
        ]
    },
    {
        "id": "0804.4116",
        "submitter": "Mireille Ducasse",
        "authors": "Ludovic Langevine and Mireille Ducasse",
        "title": "Design and Implementation of a Tracer Driver: Easy and Efficient Dynamic\n  Analyses of Constraint Logic Programs",
        "comments": "To appear in Theory and Practice of Logic Programming (TPLP),\n  Cambridge University Press. 30 pages,",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Tracers provide users with useful information about program executions. In\nthis article, we propose a ``tracer driver''. From a single tracer, it provides\na powerful front-end enabling multiple dynamic analysis tools to be easily\nimplemented, while limiting the overhead of the trace generation. The relevant\nexecution events are specified by flexible event patterns and a large variety\nof trace data can be given either systematically or ``on demand''. The proposed\ntracer driver has been designed in the context of constraint logic programming;\nexperiments have been made within GNU-Prolog. Execution views provided by\nexisting tools have been easily emulated with a negligible overhead.\nExperimental measures show that the flexibility and power of the described\narchitecture lead to good performance. The tracer driver overhead is inversely\nproportional to the average time between two traced events. Whereas the\nprinciples of the tracer driver are independent of the traced programming\nlanguage, it is best suited for high-level languages, such as constraint logic\nprogramming, where each traced execution event encompasses numerous low-level\nexecution steps. Furthermore, constraint logic programming is especially hard\nto debug. The current environments do not provide all the useful dynamic\nanalysis tools. They can significantly benefit from our tracer driver which\nenables dynamic analyses to be integrated at a very low cost.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 Apr 2008 14:05:36 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Langevine",
                "Ludovic",
                ""
            ],
            [
                "Ducasse",
                "Mireille",
                ""
            ]
        ]
    },
    {
        "id": "0804.4324",
        "submitter": "Frederic Butin",
        "authors": "Fr\\'ed\\'eric Butin",
        "title": "Hochschild Homology and Cohomology of Klein Surfaces",
        "comments": "This is a contribution to the Special Issue on Deformation\n  Quantization, published in SIGMA (Symmetry, Integrability and Geometry:\n  Methods and Applications) at http://www.emis.de/journals/SIGMA/",
        "journal-ref": "SIGMA 4 (2008), 064, 26 pages",
        "doi": "10.3842/SIGMA.2008.064",
        "report-no": null,
        "categories": "math-ph cs.SC math.AC math.MP math.QA math.RA",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  Within the framework of deformation quantization, a first step towards the\nstudy of star-products is the calculation of Hochschild cohomology. The aim of\nthis article is precisely to determine the Hochschild homology and cohomology\nin two cases of algebraic varieties. On the one hand, we consider singular\ncurves of the plane; here we recover, in a different way, a result proved by\nFronsdal and make it more precise. On the other hand, we are interested in\nKlein surfaces. The use of a complex suggested by Kontsevich and the help of\nGroebner bases allow us to solve the problem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Apr 2008 06:06:07 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 17 Sep 2008 06:07:07 GMT"
            }
        ],
        "update_date": "2008-09-17",
        "authors_parsed": [
            [
                "Butin",
                "Fr\u00e9d\u00e9ric",
                ""
            ]
        ]
    },
    {
        "id": "0804.4525",
        "submitter": "Krishnendu Chatterjee",
        "authors": "Krishnendu Chatterjee, Luca de Alfaro and Rupak Majumdar",
        "title": "The Complexity of Coverage",
        "comments": "15 Pages, 1 Figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of generating a test sequence that achieves maximal\ncoverage for a reactive system under test. We formulate the problem as a\nrepeated game between the tester and the system, where the system state space\nis partitioned according to some coverage criterion and the objective of the\ntester is to maximize the set of partitions (or coverage goals) visited during\nthe game. We show the complexity of the maximal coverage problem for\nnon-deterministic systems is PSPACE-complete, but is NP-complete for\ndeterministic systems. For the special case of non-deterministic systems with a\nre-initializing ``reset'' action, which represent running a new test input on a\nre-initialized system, we show that the complexity is again co-NP-complete. Our\nproof technique for reset games uses randomized testing strategies that\ncircumvent the exponentially large memory requirement in the deterministic\ncase.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 29 Apr 2008 04:26:08 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Chatterjee",
                "Krishnendu",
                ""
            ],
            [
                "de Alfaro",
                "Luca",
                ""
            ],
            [
                "Majumdar",
                "Rupak",
                ""
            ]
        ]
    },
    {
        "id": "0804.4525",
        "submitter": "Krishnendu Chatterjee",
        "authors": "Krishnendu Chatterjee, Luca de Alfaro and Rupak Majumdar",
        "title": "The Complexity of Coverage",
        "comments": "15 Pages, 1 Figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the problem of generating a test sequence that achieves maximal\ncoverage for a reactive system under test. We formulate the problem as a\nrepeated game between the tester and the system, where the system state space\nis partitioned according to some coverage criterion and the objective of the\ntester is to maximize the set of partitions (or coverage goals) visited during\nthe game. We show the complexity of the maximal coverage problem for\nnon-deterministic systems is PSPACE-complete, but is NP-complete for\ndeterministic systems. For the special case of non-deterministic systems with a\nre-initializing ``reset'' action, which represent running a new test input on a\nre-initialized system, we show that the complexity is again co-NP-complete. Our\nproof technique for reset games uses randomized testing strategies that\ncircumvent the exponentially large memory requirement in the deterministic\ncase.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 29 Apr 2008 04:26:08 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Chatterjee",
                "Krishnendu",
                ""
            ],
            [
                "de Alfaro",
                "Luca",
                ""
            ],
            [
                "Majumdar",
                "Rupak",
                ""
            ]
        ]
    },
    {
        "id": "0804.4740",
        "submitter": "Bart Kuijpers",
        "authors": "Sofie Haesevoets and Bart Kuijpers",
        "title": "An Affine-invariant Time-dependent Triangulation of Spatio-temporal Data",
        "comments": "40 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CG cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the geometric data model for spatio-temporal data, introduced by Chomicki\nand Revesz, spatio-temporal data are modelled as a finite collection of\ntriangles that are transformed by time-dependent affinities of the plane. To\nfacilitate querying and animation of spatio-temporal data, we present a normal\nform for data in the geometric data model. We propose an algorithm for\nconstructing this normal form via a spatio-temporal triangulation of geometric\ndata objects. This triangulation algorithm generates new geometric data objects\nthat partition the given objects both in space and in time. A particular\nproperty of the proposed partition is that it is invariant under time-dependent\naffine transformations, and hence independent of the particular choice of\ncoordinate system used to describe he spatio-temporal data in. We can show that\nour algorithm works correctly and has a polynomial time complexity (of\nreasonably low degree in the number of input triangles and the maximal degree\nof the polynomial functions that describe the transformation functions). We\nalso discuss several possible applications of this spatio-temporal\ntriangulation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 30 Apr 2008 06:02:56 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Haesevoets",
                "Sofie",
                ""
            ],
            [
                "Kuijpers",
                "Bart",
                ""
            ]
        ]
    },
    {
        "id": "0805.0192",
        "submitter": "Xavier Gonze J",
        "authors": "X. Gonze (1,2), C.-O. Almbladh (1,3), A. Cucca (1,4), D. Caliste\n  (1,2,5), C. Freysoldt (1,6), M. A. L. Marques (1,7,8), V. Olevano (1,4,9), Y.\n  Pouillon (1,2,10), M.J. Verstraete (1,11) ((1) European Theoretical\n  Spectroscopy Facility, (2) Universit\\'e Catholique de Louvain,\n  Louvain-la-Neuve, Belgium (3) University of Lund, Lund, Sweden (4) LSI,\n  CNRS-CEA, Ecole Polytechnique, Palaiseau, France, (5) C.E.A. Grenoble,\n  Grenoble, France, (6) Fritz-Haber-Institut, Berlin, Germany, (7) U. Lyon 1,\n  Villeurbanne, France, (8) U. Coimbra, Coimbra, Portugal, (9) Institut NEEL,\n  CNRS and U. Joseph Fourier, Grenoble, France, (10) Universidad del Pais Vasco\n  UPV/EHU, Donostia-San Sebasti\\`an, Spain, (11) U. York, York, United Kingdom)",
        "title": "Specification of an extensible and portable file format for electronic\n  structure and crystallographic data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DL cond-mat.mtrl-sci cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order to allow different software applications, in constant evolution, to\ninteract and exchange data, flexible file formats are needed. A file format\nspecification for different types of content has been elaborated to allow\ncommunication of data for the software developed within the European Network of\nExcellence \"NANOQUANTA\", focusing on first-principles calculations of materials\nand nanosystems. It might be used by other software as well, and is described\nhere in detail. The format relies on the NetCDF binary input/output library,\nalready used in many different scientific communities, that provides\nflexibility as well as portability accross languages and platforms. Thanks to\nNetCDF, the content can be accessed by keywords, ensuring the file format is\nextensible and backward compatible.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 May 2008 08:48:26 GMT"
            }
        ],
        "update_date": "2008-05-05",
        "authors_parsed": [
            [
                "Gonze",
                "X.",
                ""
            ],
            [
                "Almbladh",
                "C. -O.",
                ""
            ],
            [
                "Cucca",
                "A.",
                ""
            ],
            [
                "Caliste",
                "D.",
                ""
            ],
            [
                "Freysoldt",
                "C.",
                ""
            ],
            [
                "Marques",
                "M. A. L.",
                ""
            ],
            [
                "Olevano",
                "V.",
                ""
            ],
            [
                "Pouillon",
                "Y.",
                ""
            ],
            [
                "Verstraete",
                "M. J.",
                ""
            ]
        ]
    },
    {
        "id": "0805.0200",
        "submitter": "Joel Goossens",
        "authors": "Jo\\\"el Goossens",
        "title": "(m,k)-firm constraints and DBP scheduling: impact of the initial\n  k-sequence and exact schedulability test",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the scheduling of (m,k)-firm synchronous periodic task\nsystems using the Distance Based Priority (DBP) scheduler. We first show three\nphenomena: (i) choosing, for each task, the initial k-sequence 1^k is not\noptimal, (ii) we can even start the scheduling from a (fictive) error state (in\nregard to the initial k-sequence) and (iii) the period of feasible\nDBP-schedules is not necessarily the task hyper-period. We then show that any\nfeasible DBP-schedule is periodic and we upper-bound the length of that period.\nLastly, based on our periodicity result we provide an exact schedulability\ntest.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 May 2008 09:32:03 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 8 Sep 2008 09:22:24 GMT"
            }
        ],
        "update_date": "2008-09-08",
        "authors_parsed": [
            [
                "Goossens",
                "Jo\u00ebl",
                ""
            ]
        ]
    },
    {
        "id": "0805.0330",
        "submitter": "Ranko Lazic",
        "authors": "Marcin Jurdzinski and Ranko Lazic",
        "title": "Alternating Automata on Data Trees and XPath Satisfiability",
        "comments": "23 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.DB cs.FL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A data tree is an unranked ordered tree whose every node is labelled by a\nletter from a finite alphabet and an element (\"datum\") from an infinite set,\nwhere the latter can only be compared for equality. The article considers\nalternating automata on data trees that can move downward and rightward, and\nhave one register for storing data. The main results are that nonemptiness over\nfinite data trees is decidable but not primitive recursive, and that\nnonemptiness of safety automata is decidable but not elementary. The proofs use\nnondeterministic tree automata with faulty counters. Allowing upward moves,\nleftward moves, or two registers, each causes undecidability. As corollaries,\ndecidability is obtained for two data-sensitive fragments of the XPath query\nlanguage.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 3 May 2008 00:12:15 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 23 Jun 2009 13:21:17 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 1 Mar 2010 15:20:58 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 14 Jun 2010 14:13:49 GMT"
            }
        ],
        "update_date": "2010-06-15",
        "authors_parsed": [
            [
                "Jurdzinski",
                "Marcin",
                ""
            ],
            [
                "Lazic",
                "Ranko",
                ""
            ]
        ]
    },
    {
        "id": "0805.0360",
        "submitter": "Peter Harding Mr",
        "authors": "Peter J. Harding, Martyn Amos and Steve Gwynne",
        "title": "Prediction and Mitigation of Crush Conditions in Emergency Evacuations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.MA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Several simulation environments exist for the simulation of large-scale\nevacuations of buildings, ships, or other enclosed spaces. These offer\nsophisticated tools for the study of human behaviour, the recreation of\nenvironmental factors such as fire or smoke, and the inclusion of architectural\nor structural features, such as elevators, pillars and exits. Although such\nsimulation environments can provide insights into crowd behaviour, they lack\nthe ability to examine potentially dangerous forces building up within a crowd.\nThese are commonly referred to as crush conditions, and are a common cause of\ndeath in emergency evacuations.\n  In this paper, we describe a methodology for the prediction and mitigation of\ncrush conditions. The paper is organised as follows. We first establish the\nneed for such a model, defining the main factors that lead to crush conditions,\nand describing several exemplar case studies. We then examine current methods\nfor studying crush, and describe their limitations. From this, we develop a\nthree-stage hybrid approach, using a combination of techniques. We conclude\nwith a brief discussion of the potential benefits of our approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 3 May 2008 13:00:42 GMT"
            }
        ],
        "update_date": "2008-05-06",
        "authors_parsed": [
            [
                "Harding",
                "Peter J.",
                ""
            ],
            [
                "Amos",
                "Martyn",
                ""
            ],
            [
                "Gwynne",
                "Steve",
                ""
            ]
        ]
    },
    {
        "id": "0805.0650",
        "submitter": "Gesine Milde",
        "authors": "Lutz Prechelt",
        "title": "Plat_Forms -- a contest: The web development platform comparison",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": "Technical Report TR-B-06-11",
        "categories": "cs.SE",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  \"Plat_Forms\" is a competition in which top-class teams of three programmers\ncompete to implement the same requirements for a web-based system within 30\nhours, each team using a different technology platform (Java EE, .NET, PHP,\nPerl, Python, or Ruby on Rails). The results will provide new insights into the\nreal (rather than purported) pros, cons, and emergent properties of each\nplatform. The evaluation will analyze many aspects of each solution, both\nexternal (usability, functionality, reliability, performance, etc.) and\ninternal (structure, understandability, flexibility, etc.).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 6 May 2008 06:53:01 GMT"
            }
        ],
        "update_date": "2008-05-07",
        "authors_parsed": [
            [
                "Prechelt",
                "Lutz",
                ""
            ]
        ]
    },
    {
        "id": "0805.0747",
        "submitter": "Daniel Lemire",
        "authors": "Hazel Webb, Owen Kaser, Daniel Lemire",
        "title": "Pruning Attribute Values From Data Cubes with Diamond Dicing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "TR-08-011 (UNB Saint John)",
        "categories": "cs.DB cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Data stored in a data warehouse are inherently multidimensional, but most\ndata-pruning techniques (such as iceberg and top-k queries) are unidimensional.\nHowever, analysts need to issue multidimensional queries. For example, an\nanalyst may need to select not just the most profitable stores\nor--separately--the most profitable products, but simultaneous sets of stores\nand products fulfilling some profitability constraints. To fill this need, we\npropose a new operator, the diamond dice. Because of the interaction between\ndimensions, the computation of diamonds is challenging. We present the first\ndiamond-dicing experiments on large data sets. Experiments show that we can\ncompute diamond cubes over fact tables containing 100 million facts in less\nthan 35 minutes using a standard PC.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 6 May 2008 15:45:15 GMT"
            }
        ],
        "update_date": "2008-05-07",
        "authors_parsed": [
            [
                "Webb",
                "Hazel",
                ""
            ],
            [
                "Kaser",
                "Owen",
                ""
            ],
            [
                "Lemire",
                "Daniel",
                ""
            ]
        ]
    },
    {
        "id": "0805.1296",
        "submitter": "Christoph Schommer",
        "authors": "Christoph Schommer",
        "title": "A Simple Dynamic Mind-map Framework To Discover Associative\n  Relationships in Transactional Data Streams",
        "comments": "12 pages, 8 Figures. Updated version of a paper presented at the\n  Workshop on Symbolic Networks, ECAI 2004, Valencia, Spain",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we informally introduce dynamic mind-maps that represent a new\napproach on the basis of a dynamic construction of connectionist structures\nduring the processing of a data stream. This allows the representation and\nprocessing of recursively defined structures and avoids the problem of a more\ntraditional, fixed-size architecture with the processing of input structures of\nunknown size. For a data stream analysis with association discovery, the\nincremental analysis of data leads to results on demand. Here, we describe a\nframework that uses symbolic cells to calculate associations based on\ntransactional data streams as it exists in e.g. bibliographic databases. We\nfollow a natural paradigm of applying simple operations on cells yielding on a\nmind-map structure that adapts over time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 9 May 2008 08:10:09 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Schommer",
                "Christoph",
                ""
            ]
        ]
    },
    {
        "id": "0805.1487",
        "submitter": "Spyros Sioutas SS",
        "authors": "Lagogiannis George, Lorentzos Nikos, Sioutas Spyros, Theodoridis\n  Evaggelos",
        "title": "A Time Efficient Indexing Scheme for Complex Spatiotemporal Retrieval",
        "comments": "6 pages, 7 figures, submitted to Sigmod Record",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper is concerned with the time efficient processing of spatiotemporal\npredicates, i.e. spatial predicates associated with an exact temporal\nconstraint. A set of such predicates forms a buffer query or a Spatio-temporal\nPattern (STP) Query with time. In the more general case of an STP query, the\ntemporal dimension is introduced via the relative order of the spatial\npredicates (STP queries with order). Therefore, the efficient processing of a\nspatiotemporal predicate is crucial for the efficient implementation of more\ncomplex queries of practical interest. We propose an extension of a known\napproach, suitable for processing spatial predicates, which has been used for\nthe efficient manipulation of STP queries with order. The extended method is\nsupported by efficient indexing structures. We also provide experimental\nresults that show the efficiency of the technique.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 10 May 2008 17:18:32 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "George",
                "Lagogiannis",
                ""
            ],
            [
                "Nikos",
                "Lorentzos",
                ""
            ],
            [
                "Spyros",
                "Sioutas",
                ""
            ],
            [
                "Evaggelos",
                "Theodoridis",
                ""
            ]
        ]
    },
    {
        "id": "0805.1489",
        "submitter": "Soumen Roy",
        "authors": "Vladimir Filkov, Zachary M. Saul, Soumen Roy, Raissa M. D'Souza and\n  Premkumar T. Devanbu",
        "title": "Modeling and verifying a broad array of network properties",
        "comments": "To appear in Europhysics Letters",
        "journal-ref": "Europhysics Letters, 86 (2009) 28003",
        "doi": "10.1209/0295-5075/86/28003",
        "report-no": null,
        "categories": "cond-mat.stat-mech cs.SE q-bio.QM stat.AP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by widely observed examples in nature, society and software, where\ngroups of already related nodes arrive together and attach to an existing\nnetwork, we consider network growth via sequential attachment of linked node\ngroups, or graphlets. We analyze the simplest case, attachment of the three\nnode V-graphlet, where, with probability alpha, we attach a peripheral node of\nthe graphlet, and with probability (1-alpha), we attach the central node. Our\nanalytical results and simulations show that tuning alpha produces a wide range\nin degree distribution and degree assortativity, achieving assortativity values\nthat capture a diverse set of many real-world systems. We introduce a\nfifteen-dimensional attribute vector derived from seven well-known network\nproperties, which enables comprehensive comparison between any two networks.\nPrincipal Component Analysis (PCA) of this attribute vector space shows a\nsignificantly larger coverage potential of real-world network properties by a\nsimple extension of the above model when compared against a classic model of\nnetwork growth.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 12 May 2008 10:09:37 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 26 Mar 2009 15:53:12 GMT"
            }
        ],
        "update_date": "2009-04-30",
        "authors_parsed": [
            [
                "Filkov",
                "Vladimir",
                ""
            ],
            [
                "Saul",
                "Zachary M.",
                ""
            ],
            [
                "Roy",
                "Soumen",
                ""
            ],
            [
                "D'Souza",
                "Raissa M.",
                ""
            ],
            [
                "Devanbu",
                "Premkumar T.",
                ""
            ]
        ]
    },
    {
        "id": "0805.1593",
        "submitter": "Bernd G\\\"unther",
        "authors": "Bernd G\\\"unther",
        "title": "On the Probability Distribution of Superimposed Random Codes",
        "comments": null,
        "journal-ref": "IEEE Trans. Inf. Theory, 54(7):3206--3210, 2008",
        "doi": "10.1109/TIT.2008.924658",
        "report-no": null,
        "categories": "cs.DB cs.DM cs.IT math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A systematic study of the probability distribution of superimposed random\ncodes is presented through the use of generating functions. Special attention\nis paid to the cases of either uniformly distributed but not necessarily\nindependent or non uniform but independent bit structures. Recommendations for\noptimal coding strategies are derived.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 12 May 2008 08:52:16 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 19 Jun 2008 05:30:49 GMT"
            }
        ],
        "update_date": "2008-06-19",
        "authors_parsed": [
            [
                "G\u00fcnther",
                "Bernd",
                ""
            ]
        ]
    },
    {
        "id": "0805.1696",
        "submitter": "Manuel Cebrian",
        "authors": "Manuel Cebrian, Manuel Alfonseca and Alfonso Ortega",
        "title": "Grammatical Evolution with Restarts for Fast Fractal Generation",
        "comments": "26 pages, 13 figures, Extended version of the paper presented at\n  ANNIE'04",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In a previous work, the authors proposed a Grammatical Evolution algorithm to\nautomatically generate Lindenmayer Systems which represent fractal curves with\na pre-determined fractal dimension. This paper gives strong statistical\nevidence that the probability distributions of the execution time of that\nalgorithm exhibits a heavy tail with an hyperbolic probability decay for long\nexecutions, which explains the erratic performance of different executions of\nthe algorithm. Three different restart strategies have been incorporated in the\nalgorithm to mitigate the problems associated to heavy tail distributions: the\nfirst assumes full knowledge of the execution time probability distribution,\nthe second and third assume no knowledge. These strategies exploit the fact\nthat the probability of finding a solution in short executions is\nnon-negligible and yield a severe reduction, both in the expected execution\ntime (up to one order of magnitude) and in its variance, which is reduced from\nan infinite to a finite value.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 12 May 2008 17:55:59 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 15 Oct 2010 15:30:53 GMT"
            }
        ],
        "update_date": "2010-10-18",
        "authors_parsed": [
            [
                "Cebrian",
                "Manuel",
                ""
            ],
            [
                "Alfonseca",
                "Manuel",
                ""
            ],
            [
                "Ortega",
                "Alfonso",
                ""
            ]
        ]
    },
    {
        "id": "0805.1740",
        "submitter": "Grenville Croll",
        "authors": "Yirsaw Ayalew, Markus Clermont, Roland T. Mittermeir",
        "title": "Detecting Errors in Spreadsheets",
        "comments": "12 Pages, 5 Figures; ISBN: 1 86166 158 4",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2000 51-63",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper presents two complementary strategies for identifying errors in\nspreadsheet programs. The strategies presented are grounded on the assumption\nthat spreadsheets are software, albeit of a different nature than conventional\nprocedural software. Correspondingly, strategies for identifying errors have to\ntake into account the inherent properties of spreadsheets as much as they have\nto recognize that the conceptual models of 'spreadsheet programmers' differ\nfrom the conceptual models of conventional programmers. Nevertheless, nobody\ncan and will write a spreadsheet, without having such a conceptual model in\nmind, be it of numeric nature or be it of geometrical nature focused on some\nlayout.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 12 May 2008 20:31:41 GMT"
            }
        ],
        "update_date": "2011-02-19",
        "authors_parsed": [
            [
                "Ayalew",
                "Yirsaw",
                ""
            ],
            [
                "Clermont",
                "Markus",
                ""
            ],
            [
                "Mittermeir",
                "Roland T.",
                ""
            ]
        ]
    },
    {
        "id": "0805.1806",
        "submitter": "Mark van der Zwaag",
        "authors": "J.A. Bergstra, S. Nolst Trenite, M.B. van der Zwaag",
        "title": "Tuplix Calculus Specifications of Financial Transfer Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "PRG0807",
        "categories": "cs.CE cs.LO",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the application of Tuplix Calculus in modular financial budget\ndesign. We formalize organizational structure using financial transfer\nnetworks. We consider the notion of flux of money over a network, and a way to\nenforce the matching of influx and outflux for parts of a network. We exploit\nso-called signed attribute notation to make internal streams visible through\nencapsulations. Finally, we propose a Tuplix Calculus construct for the\ndefinition of data functions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 13 May 2008 09:05:41 GMT"
            }
        ],
        "update_date": "2008-05-14",
        "authors_parsed": [
            [
                "Bergstra",
                "J. A.",
                ""
            ],
            [
                "Trenite",
                "S. Nolst",
                ""
            ],
            [
                "van der Zwaag",
                "M. B.",
                ""
            ]
        ]
    },
    {
        "id": "0805.1827",
        "submitter": "Mireille Bossy",
        "authors": "Mireille Bossy (INRIA Sophia Antipolis / INRIA Lorraine / IECN),\n  Fran\\c{c}oise Baude (INRIA Sophia Antipolis), Viet Dung Doan (INRIA Sophia\n  Antipolis), Abhijeet Gaikwad (INRIA Sophia Antipolis), Ian Stokes-Rees (INRIA\n  Sophia Antipolis)",
        "title": "Parallel Pricing Algorithms for Multi--Dimensional Bermudan/American\n  Options using Monte Carlo methods",
        "comments": null,
        "journal-ref": "N&deg; RR-6530 (2008)",
        "doi": "10.1016/j.matcom.2010.08.005",
        "report-no": "RR-6530",
        "categories": "cs.DC cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present two parallel Monte Carlo based algorithms for\npricing multi--dimensional Bermudan/American options. First approach relies on\ncomputation of the optimal exercise boundary while the second relies on\nclassification of continuation and exercise values. We also evaluate the\nperformance of both the algorithms in a desktop grid environment. We show the\neffectiveness of the proposed approaches in a heterogeneous computing\nenvironment, and identify scalability constraints due to the algorithmic\nstructure.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 13 May 2008 12:34:04 GMT"
            }
        ],
        "update_date": "2014-02-18",
        "authors_parsed": [
            [
                "Bossy",
                "Mireille",
                "",
                "INRIA Sophia Antipolis / INRIA Lorraine / IECN"
            ],
            [
                "Baude",
                "Fran\u00e7oise",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Doan",
                "Viet Dung",
                "",
                "INRIA Sophia\n  Antipolis"
            ],
            [
                "Gaikwad",
                "Abhijeet",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Stokes-Rees",
                "Ian",
                "",
                "INRIA\n  Sophia Antipolis"
            ]
        ]
    },
    {
        "id": "0805.1854",
        "submitter": "Alexandre Noma",
        "authors": "Alexandre Noma, Ana B. V. Graciano, Luis Augusto Consularo, Roberto M.\n  Cesar-Jr, Isabelle Bloch",
        "title": "A New Algorithm for Interactive Structural Image Segmentation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a novel algorithm for the problem of structural image\nsegmentation through an interactive model-based approach. Interaction is\nexpressed in the model creation, which is done according to user traces drawn\nover a given input image. Both model and input are then represented by means of\nattributed relational graphs derived on the fly. Appearance features are taken\ninto account as object attributes and structural properties are expressed as\nrelational attributes. To cope with possible topological differences between\nboth graphs, a new structure called the deformation graph is introduced. The\nsegmentation process corresponds to finding a labelling of the input graph that\nminimizes the deformations introduced in the model when it is updated with\ninput information. This approach has shown to be faster than other segmentation\nmethods, with competitive output quality. Therefore, the method solves the\nproblem of multiple label segmentation in an efficient way. Encouraging results\non both natural and target-specific color images, as well as examples showing\nthe reusability of the model, are presented and discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 13 May 2008 13:39:19 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 16 May 2008 13:45:56 GMT"
            }
        ],
        "update_date": "2008-05-16",
        "authors_parsed": [
            [
                "Noma",
                "Alexandre",
                ""
            ],
            [
                "Graciano",
                "Ana B. V.",
                ""
            ],
            [
                "Consularo",
                "Luis Augusto",
                ""
            ],
            [
                "Cesar-Jr",
                "Roberto M.",
                ""
            ],
            [
                "Bloch",
                "Isabelle",
                ""
            ]
        ]
    },
    {
        "id": "0805.1968",
        "submitter": "Jian Tan",
        "authors": "Predrag R. Jelenkovic, Xiaozhu Kang, Jian Tan",
        "title": "Heavy-Tailed Limits for Medium Size Jobs and Comparison Scheduling",
        "comments": "26 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the conditional sojourn time distributions of processor sharing\n(PS), foreground background processor sharing (FBPS) and shortest remaining\nprocessing time first (SRPT) scheduling disciplines on an event where the job\nsize of a customer arriving in stationarity is smaller than exactly k>=0 out of\nthe preceding m>=k arrivals. Then, conditioning on the preceding event, the\nsojourn time distribution of this newly arriving customer behaves\nasymptotically the same as if the customer were served in isolation with a\nserver of rate (1-\\rho)/(k+1) for PS/FBPS, and (1-\\rho) for SRPT, respectively,\nwhere \\rho is the traffic intensity. Hence, the introduced notion of\nconditional limits allows us to distinguish the asymptotic performance of the\nstudied schedulers by showing that SRPT exhibits considerably better asymptotic\nbehavior for relatively smaller jobs than PS/FBPS.\n  Inspired by the preceding results, we propose an approximation to the SRPT\ndiscipline based on a novel adaptive job grouping mechanism that uses relative\nsize comparison of a newly arriving job to the preceding m arrivals.\nSpecifically, if the newly arriving job is smaller than k and larger than m-k\nof the previous m jobs, it is routed into class k. Then, the classes of smaller\njobs are served with higher priorities using the static priority scheduling.\nThe good performance of this mechanism, even for a small number of classes m+1,\nis demonstrated using the asymptotic queueing analysis under the heavy-tailed\njob requirements. We also discuss refinements of the comparison grouping\nmechanism that improve the accuracy of job classification at the expense of a\nsmall additional complexity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 14 May 2008 05:00:12 GMT"
            }
        ],
        "update_date": "2008-05-15",
        "authors_parsed": [
            [
                "Jelenkovic",
                "Predrag R.",
                ""
            ],
            [
                "Kang",
                "Xiaozhu",
                ""
            ],
            [
                "Tan",
                "Jian",
                ""
            ]
        ]
    },
    {
        "id": "0805.2311",
        "submitter": "David Sevilla",
        "authors": "John McKay and David Sevilla",
        "title": "Aplicacion de la descomposicion racional univariada a monstrous\n  moonshine (in Spanish)",
        "comments": "6 pages, In Spanish",
        "journal-ref": "Proceedings of the 2004 Encuentro de Algebra Computacional y\n  Aplicaciones (EACA), p. 289-294. ISBN 84-688-6988-04",
        "doi": null,
        "report-no": null,
        "categories": "math.NT cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper shows how to use Computational Algebra techniques, namely the\ndecomposition of rational functions in one variable, to explore a certain set\nof modular functions, called replicable functions, that arise in Monstrous\nMoonshine. In particular, we have computed all the rational relations with\ncoefficients in Z between pairs of replicable functions.\n  -----\n  En este articulo mostramos como usar tecnicas de Algebra Computacional,\nconcretamente la descomposcion de funciones racionales univariadas, para\nestudiar un cierto conjunto de funciones modulares, llamadas funciones\nreplicables, que aparecen en Monstrous Moonshine. En concreto, hemos calculado\ntodas las relaciones racionales con coeficientes en Z entre pares de funciones\nreplicables.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 15 May 2008 14:02:45 GMT"
            }
        ],
        "update_date": "2009-04-19",
        "authors_parsed": [
            [
                "McKay",
                "John",
                ""
            ],
            [
                "Sevilla",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0805.2324",
        "submitter": "Zhang Yu",
        "authors": "Zhang Yu, Shi Zhong-ke, Wang Run-quan",
        "title": "A multilateral filtering method applied to airplane runway image",
        "comments": "8 pages, 5 figures, 4 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  By considering the features of the airport runway image filtering, an\nimproved bilateral filtering method was proposed which can remove noise with\nedge preserving. Firstly the steerable filtering decomposition is used to\ncalculate the sub-band parameters of 4 orients, and the texture feature matrix\nis then obtained from the sub-band local median energy. The texture similar,\nthe spatial closer and the color similar functions are used to filter the\nimage.The effect of the weighting function parameters is qualitatively analyzed\nalso. In contrast with the standard bilateral filter and the simulation results\nfor the real airport runway image show that the multilateral filtering is more\neffective than the standard bilateral filtering.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 15 May 2008 13:15:08 GMT"
            }
        ],
        "update_date": "2008-05-16",
        "authors_parsed": [
            [
                "Yu",
                "Zhang",
                ""
            ],
            [
                "Zhong-ke",
                "Shi",
                ""
            ],
            [
                "Run-quan",
                "Wang",
                ""
            ]
        ]
    },
    {
        "id": "0805.2331",
        "submitter": "David Sevilla",
        "authors": "Jaime Gutierrez, Rosario Rubio, David Sevilla",
        "title": "Computing the fixing group of a rational function",
        "comments": "8 pages",
        "journal-ref": "Proceedings of the 5th International workshop on Computer Algebra\n  in Scientific Computing (CASC), p. 159-164, Instituet fuer Informatik,\n  Technische Universitaet Muenchen, 2002. ISBN 3-9808546-0-4",
        "doi": null,
        "report-no": null,
        "categories": "cs.SC math.AC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let G=Aut_K (K(x)) be the Galois group of the transcendental degree one pure\nfield extension K(x)/K. In this paper we describe polynomial time algorithms\nfor computing the field Fix(H) fixed by a subgroup H < G and for computing the\nfixing group G_f of a rational function f in K(x).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 15 May 2008 14:58:18 GMT"
            }
        ],
        "update_date": "2009-04-19",
        "authors_parsed": [
            [
                "Gutierrez",
                "Jaime",
                ""
            ],
            [
                "Rubio",
                "Rosario",
                ""
            ],
            [
                "Sevilla",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0805.2338",
        "submitter": "David Sevilla",
        "authors": "Jaime Gutierrez, Rosario Rubio, David Sevilla",
        "title": "Unirational fields of transcendence degree one and functional\n  decomposition",
        "comments": "17 pages (single column)",
        "journal-ref": "Proceedings of the 2001 International Symposium on Symbolic and\n  Algebraic Computation (ISSAC), p. 167-175, ACM, New York, 2001. ISBN\n  1-58113-417-7",
        "doi": null,
        "report-no": null,
        "categories": "cs.SC math.AC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present an algorithm to compute all unirational fields of\ntranscendence degree one containing a given finite set of multivariate rational\nfunctions. In particular, we provide an algorithm to decompose a multivariate\nrational function f of the form f=g(h), where g is a univariate rational\nfunction and h a multivariate one.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 15 May 2008 15:19:44 GMT"
            }
        ],
        "update_date": "2009-04-19",
        "authors_parsed": [
            [
                "Gutierrez",
                "Jaime",
                ""
            ],
            [
                "Rubio",
                "Rosario",
                ""
            ],
            [
                "Sevilla",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0805.2438",
        "submitter": "Russell O'Connor",
        "authors": "Russell O'Connor",
        "title": "Certified Exact Transcendental Real Number Computation in Coq",
        "comments": "This paper is to be part of the proceedings of the 21st International\n  Conference on Theorem Proving in Higher Order Logics (TPHOLs 2008)",
        "journal-ref": "Ait Mohamed, C. Munoz, and S. Tahar (Eds.): TPHOLs 2008, LNCS\n  5170, pp. 246-261, 2008",
        "doi": "10.1007/978-3-540-71067-7_21",
        "report-no": null,
        "categories": "cs.LO cs.MS cs.NA",
        "license": "http://creativecommons.org/licenses/publicdomain/",
        "abstract": "  Reasoning about real number expressions in a proof assistant is challenging.\nSeveral problems in theorem proving can be solved by using exact real number\ncomputation. I have implemented a library for reasoning and computing with\ncomplete metric spaces in the Coq proof assistant and used this library to\nbuild a constructive real number implementation including elementary real\nnumber functions and proofs of correctness. Using this library, I have created\na tactic that automatically proves strict inequalities over closed elementary\nreal number expressions by computation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 16 May 2008 18:02:24 GMT"
            }
        ],
        "update_date": "2010-08-04",
        "authors_parsed": [
            [
                "O'Connor",
                "Russell",
                ""
            ]
        ]
    },
    {
        "id": "0805.2671",
        "submitter": "Spyros Sioutas SS",
        "authors": "Spyros Sioutas",
        "title": "Finger Indexed Sets: New Approaches",
        "comments": "13 pages, 1 figure, Submitted to Journal of Universal Computer\n  Science (J.UCS)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the particular case we have insertions/deletions at the tail of a given\nset S of $n$ one-dimensional elements, we present a simpler and more concrete\nalgorithm than that presented in [Anderson, 2007] achieving the same (but also\namortized) upper bound of $O(\\sqrt{logd/loglogd})$ for finger searching\nqueries, where $d$ is the number of sorted keys between the finger element and\nthe target element we are looking for. Furthermore, in general case we have\ninsertions/deletions anywhere we present a new randomized algorithm achieving\nthe same expected time bounds. Even the new solutions achieve the optimal\nbounds in amortized or expected case, the advantage of simplicity is of great\nimportance due to practical merits we gain.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 17 May 2008 14:05:12 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Sioutas",
                "Spyros",
                ""
            ]
        ]
    },
    {
        "id": "0805.2690",
        "submitter": "Mikhail Konnik",
        "authors": "M.V. Konnik, E.A. Manykin, S.N. Starikov",
        "title": "Increasing Linear Dynamic Range of Commercial Digital Photocamera Used\n  in Imaging Systems with Optical Coding",
        "comments": "unnecessary figures were removed; typos corrected",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Methods of increasing linear optical dynamic range of commercial photocamera\nfor optical-digital imaging systems are described. Use of such methods allows\nto use commercial photocameras for optical measurements. Experimental results\nare reported.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 17 May 2008 17:15:26 GMT"
            }
        ],
        "update_date": "2008-05-20",
        "authors_parsed": [
            [
                "Konnik",
                "M. V.",
                ""
            ],
            [
                "Manykin",
                "E. A.",
                ""
            ],
            [
                "Starikov",
                "S. N.",
                ""
            ]
        ]
    },
    {
        "id": "0805.2749",
        "submitter": "Victor Yodaiken",
        "authors": "Victor Yodaiken",
        "title": "State and history in operating systems",
        "comments": "A method based on sequence dependent functions is used to specify and\n  understand the operation of process switch in UNIX like systems",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.DC",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  A method of using recursive functions to describe state change is applied to\nprocess switching in UNIX-like operating systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 18 May 2008 19:44:21 GMT"
            }
        ],
        "update_date": "2008-05-20",
        "authors_parsed": [
            [
                "Yodaiken",
                "Victor",
                ""
            ]
        ]
    },
    {
        "id": "0805.2949",
        "submitter": "Fotis Georgatos Drs",
        "authors": "Fotis Georgatos, John Kouvakis, John Kouretis",
        "title": "Performability Aspects of the Atlas Vo; Using Lmbench Suite",
        "comments": "14 pages, 7 figures; Particular attention should be given to graph\n  int64mul; A related presentation on the subject matter is available by the\n  authors upon request",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.CE cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The ATLAS Virtual Organization is grid's largest Virtual Organization which\nis currently in full production stage. Hereby a case is being made that a user\nworking within that VO is going to face a wide spectrum of different systems,\nwhose heterogeneity is enough to count as \"orders of magnitude\" according to a\nnumber of metrics; including integer/float operations, memory throughput\n(STREAM) and communication latencies. Furthermore, the spread of performance\ndoes not appear to follow any known distribution pattern, which is demonstrated\nin graphs produced during May 2007 measurements. It is implied that the current\npractice where either \"all-WNs-are-equal\" or, the alternative of SPEC-based\nrating used by LCG/EGEE is an oversimplification which is inappropriate and\nexpensive from an operational point of view, therefore new techniques are\nneeded for optimal grid resources allocation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 19 May 2008 19:45:31 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Georgatos",
                "Fotis",
                ""
            ],
            [
                "Kouvakis",
                "John",
                ""
            ],
            [
                "Kouretis",
                "John",
                ""
            ]
        ]
    },
    {
        "id": "0805.2949",
        "submitter": "Fotis Georgatos Drs",
        "authors": "Fotis Georgatos, John Kouvakis, John Kouretis",
        "title": "Performability Aspects of the Atlas Vo; Using Lmbench Suite",
        "comments": "14 pages, 7 figures; Particular attention should be given to graph\n  int64mul; A related presentation on the subject matter is available by the\n  authors upon request",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.CE cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The ATLAS Virtual Organization is grid's largest Virtual Organization which\nis currently in full production stage. Hereby a case is being made that a user\nworking within that VO is going to face a wide spectrum of different systems,\nwhose heterogeneity is enough to count as \"orders of magnitude\" according to a\nnumber of metrics; including integer/float operations, memory throughput\n(STREAM) and communication latencies. Furthermore, the spread of performance\ndoes not appear to follow any known distribution pattern, which is demonstrated\nin graphs produced during May 2007 measurements. It is implied that the current\npractice where either \"all-WNs-are-equal\" or, the alternative of SPEC-based\nrating used by LCG/EGEE is an oversimplification which is inappropriate and\nexpensive from an operational point of view, therefore new techniques are\nneeded for optimal grid resources allocation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 19 May 2008 19:45:31 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Georgatos",
                "Fotis",
                ""
            ],
            [
                "Kouvakis",
                "John",
                ""
            ],
            [
                "Kouretis",
                "John",
                ""
            ]
        ]
    },
    {
        "id": "0805.3217",
        "submitter": "Fran\\c{c}ois Lecellier",
        "authors": "Fran\\c{c}ois Lecellier, St\\'ephanie Jehan-Besson, Jalal Fadili, Gilles\n  Aubert, Marinette Revenu",
        "title": "Statistical region-based active contours with exponential family\n  observations",
        "comments": "4 pages, ICASSP 2006",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we focus on statistical region-based active contour models\nwhere image features (e.g. intensity) are random variables whose distribution\nbelongs to some parametric family (e.g. exponential) rather than confining\nourselves to the special Gaussian case. Using shape derivation tools, our\neffort focuses on constructing a general expression for the derivative of the\nenergy (with respect to a domain) and derive the corresponding evolution speed.\nA general result is stated within the framework of multi-parameter exponential\nfamily. More particularly, when using Maximum Likelihood estimators, the\nevolution speed has a closed-form expression that depends simply on the\nprobability density function, while complicating additive terms appear when\nusing other estimators, e.g. moments method. Experimental results on both\nsynthesized and real images demonstrate the applicability of our approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 21 May 2008 07:54:07 GMT"
            }
        ],
        "update_date": "2008-05-22",
        "authors_parsed": [
            [
                "Lecellier",
                "Fran\u00e7ois",
                ""
            ],
            [
                "Jehan-Besson",
                "St\u00e9phanie",
                ""
            ],
            [
                "Fadili",
                "Jalal",
                ""
            ],
            [
                "Aubert",
                "Gilles",
                ""
            ],
            [
                "Revenu",
                "Marinette",
                ""
            ]
        ]
    },
    {
        "id": "0805.3218",
        "submitter": "Fran\\c{c}ois Lecellier",
        "authors": "Fran\\c{c}ois Lecellier, St\\'ephanie Jehan-Besson, Jalal Fadili, Gilles\n  Aubert, Marinette Revenu, Eric Saloux",
        "title": "Region-based active contour with noise and shape priors",
        "comments": "4 pages, ICIP 2006",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose to combine formally noise and shape priors in\nregion-based active contours. On the one hand, we use the general framework of\nexponential family as a prior model for noise. On the other hand, translation\nand scale invariant Legendre moments are considered to incorporate the shape\nprior (e.g. fidelity to a reference shape). The combination of the two prior\nterms in the active contour functional yields the final evolution equation\nwhose evolution speed is rigorously derived using shape derivative tools.\nExperimental results on both synthetic images and real life cardiac echography\ndata clearly demonstrate the robustness to initialization and noise,\nflexibility and large potential applicability of our segmentation algorithm.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 21 May 2008 08:06:01 GMT"
            }
        ],
        "update_date": "2008-05-22",
        "authors_parsed": [
            [
                "Lecellier",
                "Fran\u00e7ois",
                ""
            ],
            [
                "Jehan-Besson",
                "St\u00e9phanie",
                ""
            ],
            [
                "Fadili",
                "Jalal",
                ""
            ],
            [
                "Aubert",
                "Gilles",
                ""
            ],
            [
                "Revenu",
                "Marinette",
                ""
            ],
            [
                "Saloux",
                "Eric",
                ""
            ]
        ]
    },
    {
        "id": "0805.3237",
        "submitter": "Sebastien Collette",
        "authors": "S. Collette and L. Cucu and J. Goossens",
        "title": "Integrating Job Parallelism in Real-Time Scheduling Theory",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the global scheduling of sporadic, implicit deadline,\nreal-time task systems on multiprocessor platforms. We provide a task model\nwhich integrates job parallelism. We prove that the time-complexity of the\nfeasibility problem of these systems is linear relatively to the number of\n(sporadic) tasks for a fixed number of processors. We propose a scheduling\nalgorithm theoretically optimal (i.e., preemptions and migrations neglected).\nMoreover, we provide an exact feasibility utilization bound. Lastly, we propose\na technique to limit the number of migrations and preemptions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 21 May 2008 09:38:15 GMT"
            }
        ],
        "update_date": "2008-05-22",
        "authors_parsed": [
            [
                "Collette",
                "S.",
                ""
            ],
            [
                "Cucu",
                "L.",
                ""
            ],
            [
                "Goossens",
                "J.",
                ""
            ]
        ]
    },
    {
        "id": "0805.3339",
        "submitter": "Daniel Lemire",
        "authors": "Kamel Aouiche, Daniel Lemire, Owen Kaser",
        "title": "Tri de la table de faits et compression des index bitmaps avec\n  alignement sur les mots",
        "comments": "to appear at BDA'08",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Bitmap indexes are frequently used to index multidimensional data. They rely\nmostly on sequential input/output. Bitmaps can be compressed to reduce\ninput/output costs and minimize CPU usage. The most efficient compression\ntechniques are based on run-length encoding (RLE), such as Word-Aligned Hybrid\n(WAH) compression. This type of compression accelerates logical operations\n(AND, OR) over the bitmaps. However, run-length encoding is sensitive to the\norder of the facts. Thus, we propose to sort the fact tables. We review\nlexicographic, Gray-code, and block-wise sorting. We found that a lexicographic\nsort improves compression--sometimes generating indexes twice as small--and\nmake indexes several times faster. While sorting takes time, this is partially\noffset by the fact that it is faster to index a sorted table. Column order is\nsignificant: it is generally preferable to put the columns having more distinct\nvalues at the beginning. A block-wise sort is much less efficient than a full\nsort. Moreover, we found that Gray-code sorting is not better than\nlexicographic sorting when using word-aligned compression.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 21 May 2008 19:50:46 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 8 Jul 2008 23:46:51 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 15 Aug 2008 00:08:42 GMT"
            }
        ],
        "update_date": "2008-08-15",
        "authors_parsed": [
            [
                "Aouiche",
                "Kamel",
                ""
            ],
            [
                "Lemire",
                "Daniel",
                ""
            ],
            [
                "Kaser",
                "Owen",
                ""
            ]
        ]
    },
    {
        "id": "0805.3897",
        "submitter": "Harmen L.A. van der Spek",
        "authors": "H.L.A. van der Spek, E.M. Bakker, H.A.G. Wijshoff",
        "title": "SPARK00: A Benchmark Package for the Compiler Evaluation of\n  Irregular/Sparse Codes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a set of benchmarks that specifically targets a major cause of\nperformance degradation in high performance computing platforms: irregular\naccess patterns. These benchmarks are meant to be used to asses the performance\nof optimizing compilers on codes with a varying degree of irregular access. The\nirregularity caused by the use of pointers and indirection arrays are a major\nchallenge for optimizing compilers. Codes containing such patterns are\nnotoriously hard to optimize but they have a huge impact on the performance of\nmodern architectures, which are under-utilized when encountering irregular\nmemory accesses. In this paper, a set of benchmarks is described that\nexplicitly measures the performance of kernels containing a variety of\ndifferent access patterns found in real world applications. By offering a\nvarying degree of complexity, we provide a platform for measuring the\neffectiveness of transformations. The difference in complexity stems from a\ndifference in traversal patterns, the use of multiple indirections and control\nflow statements. The kernels used cover a variety of different access patterns,\nnamely pointer traversals, indirection arrays, dynamic loop bounds and run-time\ndependent if-conditions. The kernels are small enough to be fully understood\nwhich makes this benchmark set very suitable for the evaluation of\nrestructuring transformations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 26 May 2008 08:58:16 GMT"
            }
        ],
        "update_date": "2008-05-27",
        "authors_parsed": [
            [
                "van der Spek",
                "H. L. A.",
                ""
            ],
            [
                "Bakker",
                "E. M.",
                ""
            ],
            [
                "Wijshoff",
                "H. A. G.",
                ""
            ]
        ]
    },
    {
        "id": "0805.3964",
        "submitter": "Fabricio Martins Lopes",
        "authors": "Fabricio Martins Lopes, David Correa Martins-Jr and Roberto M.\n  Cesar-Jr",
        "title": "DimReduction - Interactive Graphic Environment for Dimensionality\n  Reduction",
        "comments": "13 pages, 4 figures, site http://code.google.com/p/dimreduction/",
        "journal-ref": "BMC Bioinformatics 2008, 9:451",
        "doi": "10.1186/1471-2105-9-451",
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  Feature selection is a pattern recognition approach to choose important\nvariables according to some criteria to distinguish or explain certain\nphenomena. There are many genomic and proteomic applications which rely on\nfeature selection to answer questions such as: selecting signature genes which\nare informative about some biological state, e.g. normal tissues and several\ntypes of cancer; or defining a network of prediction or inference among\nelements such as genes, proteins, external stimuli and other elements of\ninterest. In these applications, a recurrent problem is the lack of samples to\nperform an adequate estimate of the joint probabilities between element states.\nA myriad of feature selection algorithms and criterion functions are proposed,\nalthough it is difficult to point the best solution in general. The intent of\nthis work is to provide an open-source multiplataform graphical environment to\napply, test and compare many feature selection approaches suitable to be used\nin bioinformatics problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 26 May 2008 14:16:06 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 15 May 2011 18:57:54 GMT"
            }
        ],
        "update_date": "2011-06-13",
        "authors_parsed": [
            [
                "Lopes",
                "Fabricio Martins",
                ""
            ],
            [
                "Martins-Jr",
                "David Correa",
                ""
            ],
            [
                "Cesar-Jr",
                "Roberto M.",
                ""
            ]
        ]
    },
    {
        "id": "0805.4029",
        "submitter": "Avik Chaudhuri",
        "authors": "Avik Chaudhuri",
        "title": "Event Synchronization by Lightweight Message Passing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Concurrent ML's events and event combinators facilitate modular concurrent\nprogramming with first-class synchronization abstractions. A standard\nimplementation of these abstractions relies on fairly complex manipulations of\nfirst-class continuations in the underlying language. In this paper, we present\na lightweight implementation of these abstractions in Concurrent Haskell, a\nlanguage that already provides first-order message passing. At the heart of our\nimplementation is a new distributed synchronization protocol. In contrast with\nseveral previous translations of event abstractions in concurrent languages, we\nremain faithful to the standard semantics for events and event combinators; for\nexample, we retain the symmetry of $\\mathtt{choose}$ for expressing selective\ncommunication.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 27 May 2008 01:31:05 GMT"
            }
        ],
        "update_date": "2008-05-28",
        "authors_parsed": [
            [
                "Chaudhuri",
                "Avik",
                ""
            ]
        ]
    },
    {
        "id": "0805.4107",
        "submitter": "Nicolas Bonnel",
        "authors": "Nicolas Bonnel (VALORIA), Gilbas M\\'enier (VALORIA),\n  Pierre-Fran\\c{c}ois Marteau (VALORIA)",
        "title": "Spiral Walk on Triangular Meshes : Adaptive Replication in Data P2P\n  Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a decentralized replication strategy for peer-to-peer file\nexchange based on exhaustive exploration of the neighborhood of any node in the\nnetwork. The replication scheme lets the replicas evenly populate the network\nmesh, while regulating the total number of replicas at the same time. This is\nachieved by self adaptation to entering or leaving of nodes. Exhaustive\nexploration is achieved by a spiral walk algorithm that generates a number of\nmessages linearly proportional to the number of visited nodes. It requires a\ndedicated topology (a triangular mesh on a closed surface). We introduce\nprotocols for node connection and departure that maintain the triangular mesh\nat low computational and bandwidth cost. Search efficiency is increased using a\nmechanism based on dynamically allocated super peers. We conclude with a\ndiscussion on experimental validation results.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 27 May 2008 12:49:55 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Bonnel",
                "Nicolas",
                "",
                "VALORIA"
            ],
            [
                "M\u00e9nier",
                "Gilbas",
                "",
                "VALORIA"
            ],
            [
                "Marteau",
                "Pierre-Fran\u00e7ois",
                "",
                "VALORIA"
            ]
        ]
    },
    {
        "id": "0805.4134",
        "submitter": "Spyros Sioutas SS",
        "authors": "V.Chrissikopoulos, G.Papaloukopoulos, E.Sakkopoulos, S. Sioutas",
        "title": "Design and Implementation Aspects of a novel Java P2P Simulator with GUI",
        "comments": "8 Pages, 7 figures, This article was accepted for presentation in the\n  IEEE Panhellenic Conference in Informatics (PCI 2008),\n  http://www.aegean.gr/PCI2008",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.DB cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Peer-to-peer networks consist of thousands or millions of nodes that might\njoin and leave arbitrarily. The evaluation of new protocols in real\nenvironments is many times practically impossible, especially at design and\ntesting stages. The purpose of this paper is to describe the implementation\naspects of a new Java based P2P simulator that has been developed to support\nscalability in the evaluation of such P2P dynamic environments. Evolving the\nfunctionality presented by previous solutions, we provide a friendly graphical\nuser interface through which the high-level theoretic researcher/designer of a\nP2P system can easily construct an overlay with the desirable number of nodes\nand evaluate its operations using a number of key distributions. Furthermore,\nthe simulator has built-in ability to produce statistics about the distributed\nstructure. Emphasis was given to the parametrical configuration of the\nsimulator. As a result the developed tool can be utilized in the simulation and\nevaluation procedures of a variety of different protocols, with only few\nchanges in the Java code.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 27 May 2008 14:36:20 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Chrissikopoulos",
                "V.",
                ""
            ],
            [
                "Papaloukopoulos",
                "G.",
                ""
            ],
            [
                "Sakkopoulos",
                "E.",
                ""
            ],
            [
                "Sioutas",
                "S.",
                ""
            ]
        ]
    },
    {
        "id": "0805.4211",
        "submitter": "Grenville Croll",
        "authors": "Soheil Saadat",
        "title": "Managing Critical Spreadsheets in a Compliant Environment",
        "comments": "4 Pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2007 21-24\n  ISBN 978-905617-58-6",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.HC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The use of uncontrolled financial spreadsheets can expose organizations to\nunacceptable business and compliance risks, including errors in the financial\nreporting process, spreadsheet misuse and fraud, or even significant\noperational errors. These risks have been well documented and thoroughly\nresearched. With the advent of regulatory mandates such as SOX 404 and FDICIA\nin the U.S., and MiFID, Basel II and Combined Code in the UK and Europe,\nleading tax and audit firms are now recommending that organizations automate\ntheir internal controls over critical spreadsheets and other end-user computing\napplications, including Microsoft Access databases. At a minimum, auditors\nmandate version control, change control and access control for operational\nspreadsheets, with more advanced controls for critical financial spreadsheets.\nThis paper summarises the key issues regarding the establishment and\nmaintenance of control of Business Critical spreadsheets.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 27 May 2008 20:28:10 GMT"
            }
        ],
        "update_date": "2008-05-29",
        "authors_parsed": [
            [
                "Saadat",
                "Soheil",
                ""
            ]
        ]
    },
    {
        "id": "0805.4218",
        "submitter": "Grenville Croll",
        "authors": "Brian Knight, David Chadwick, Kamalesen Rajalingham",
        "title": "A Structured Methodology for Spreadsheet Modelling",
        "comments": "8 Pages, 7 Figures",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2000 43-50\n  ISBN:1 86166 158 4",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we discuss the problem of the software engineering of a class\nof business spreadsheet models. A methodology for structured software\ndevelopment is proposed, which is based on structured analysis of data,\nrepresented as Jackson diagrams. It is shown that this analysis allows a\nstraightforward modularisation, and that individual modules may be represented\nwith indentation in the block-structured form of structured programs. The\nbenefits of structured format are discussed, in terms of comprehensibility,\nease of maintenance, and reduction in errors. The capability of the methodology\nto provide a modular overview in the model is described, and examples are\ngiven. The potential for a reverse-engineering tool, to transform existing\nspreadsheet models is discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 27 May 2008 20:56:35 GMT"
            }
        ],
        "update_date": "2008-05-29",
        "authors_parsed": [
            [
                "Knight",
                "Brian",
                ""
            ],
            [
                "Chadwick",
                "David",
                ""
            ],
            [
                "Rajalingham",
                "Kamalesen",
                ""
            ]
        ]
    },
    {
        "id": "0805.4219",
        "submitter": "Grenville Croll",
        "authors": "Andrew Hawker",
        "title": "Building Financial Accuracy into Spreadsheets",
        "comments": "6 Pages",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2000 35-40\n  ISBN:1 86166 158 4",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Students learning how to apply spreadsheets to accounting problems are not\nalways well served by the built-in financial functions. Problems can arise\nbecause of differences between UK and US practice, through anomalies in the\nfunctions themselves, and because the promptings of Wizards' engender an\nattitude of filling in the blanks on the screen, and hoping for the best. Some\nexamples of these problems are described, and suggestions are presented for\nways of improving the situation. Principally, it is suggested that spreadsheet\nprompts and 'Help' screens should offer integrated guidance, covering some\naspects of financial practice, as well as matters of spreadsheet technique.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 27 May 2008 21:11:48 GMT"
            }
        ],
        "update_date": "2008-05-29",
        "authors_parsed": [
            [
                "Hawker",
                "Andrew",
                ""
            ]
        ]
    },
    {
        "id": "0805.4224",
        "submitter": "Grenville Croll",
        "authors": "Kamalasen Rajalingham, David R. Chadwick, Brian Knight",
        "title": "Classification of Spreadsheet Errors",
        "comments": "9 Pages, 6 Figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes a framework for a systematic classification of\nspreadsheet errors. This classification or taxonomy of errors is aimed at\nfacilitating analysis and comprehension of the different types of spreadsheet\nerrors. The taxonomy is an outcome of an investigation of the widespread\nproblem of spreadsheet errors and an analysis of specific types of these\nerrors. This paper contains a description of the various elements and\ncategories of the classification and is supported by appropriate examples.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 27 May 2008 21:27:42 GMT"
            }
        ],
        "update_date": "2008-05-29",
        "authors_parsed": [
            [
                "Rajalingham",
                "Kamalasen",
                ""
            ],
            [
                "Chadwick",
                "David R.",
                ""
            ],
            [
                "Knight",
                "Brian",
                ""
            ]
        ]
    },
    {
        "id": "0805.4236",
        "submitter": "Grenville Croll",
        "authors": "Raymond J. Butler",
        "title": "Risk Assessment For Spreadsheet Developments: Choosing Which Models to\n  Audit",
        "comments": "11 Pages, 1 Figure",
        "journal-ref": "Proc. European Spreadsheet Risks Int. Grp. (EuSpRIG) 2000 65-74\n  ISBN:1 86166 158 4",
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.CY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Errors in spreadsheet applications and models are alarmingly common (some\nauthorities, with justification cite spreadsheets containing errors as the norm\nrather than the exception). Faced with this body of evidence, the auditor can\nbe faced with a huge task - the temptation may be to launch code inspections\nfor every spreadsheet in an organisation. This can be very expensive and\ntime-consuming. This paper describes risk assessment based on the \"SpACE\" audit\nmethodology used by H M Customs & Excise's tax inspectors. This allows the\nauditor to target resources on the spreadsheets posing the highest risk of\nerror, and justify the deployment of those resources to managers and clients.\nSince the opposite of audit risk is audit assurance the paper also offers an\noverview of some elements of good practice in the use of spreadsheets in\nbusiness.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 27 May 2008 23:26:32 GMT"
            }
        ],
        "update_date": "2008-05-29",
        "authors_parsed": [
            [
                "Butler",
                "Raymond J.",
                ""
            ]
        ]
    },
    {
        "id": "0805.4543",
        "submitter": "Timur R. Seifullin",
        "authors": "Timur R. Seifullin",
        "title": "Determination of the basis of the space of all root functionals of a\n  system of polynomial equations and of the basis of its ideal by the operation\n  of the extension of bounded root functionals",
        "comments": "Paper translated from Russian. Summary in English",
        "journal-ref": "Dopov. Nats. Akad. Nauk Ukr. Mat. Prirodozn. Tekh. Nauki 2003, no.\n  8, 29--36. MR2046291 (2005a:13055)",
        "doi": null,
        "report-no": null,
        "categories": "math.AG cs.SC math.AC",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  It is proposed the algorithm that find a basis of the ideal and a basis of\nthe space of all root functionals by using the extension operation for bounded\nroot functionals, when the number of polynomials is equal to the number of\nvariables, if it is known that the ideal of polynomials is 0-dimensional. The\nasyptotic complexity of this algorithm is d^{O(n)} operations, where n is the\nnumber of polynomials and the number of variables, d is the maximal degree of\npolynomials. The extension operation has connection with the multivariate\nBezoutian construction.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 29 May 2008 18:15:22 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 30 May 2008 10:57:31 GMT"
            }
        ],
        "update_date": "2008-06-01",
        "authors_parsed": [
            [
                "Seifullin",
                "Timur R.",
                ""
            ]
        ]
    },
    {
        "id": "0805.4680",
        "submitter": "Marc Shapiro",
        "authors": "Lamia Benmouffok (INRIA Rocquencourt, LIP6), Jean-Michel Busca (INRIA\n  Rocquencourt, LIP6), Joan Manuel Marqu\\`es (LIP6, UOC), Marc Shapiro (INRIA\n  Rocquencourt, LIP6), Pierre Sutra (INRIA Rocquencourt, LIP6), Georgios\n  Tsoukalas (NTUA)",
        "title": "Telex: Principled System Support for Write-Sharing in Collaborative\n  Applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "RR-6546",
        "categories": "cs.OS cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Telex system is designed for sharing mutable data in a distributed\nenvironment, particularly for collaborative applications. Users operate on\ntheir local, persistent replica of shared documents; they can work disconnected\nand suffer no network latency. The Telex approach to detect and correct\nconflicts is application independent, based on an action-constraint graph (ACG)\nthat summarises the concurrency semantics of applications. The ACG is stored\nefficiently in a multilog structure that eliminates contention and is optimised\nfor locality. Telex supports multiple applications and multi-document updates.\nThe Telex system clearly separates system logic (which includes replication,\nviews, undo, security, consistency, conflicts, and commitment) from application\nlogic. An example application is a shared calendar for managing multi-user\nmeetings; the system detects meeting conflicts and resolves them consistently.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 30 May 2008 07:01:51 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 2 Jun 2008 08:40:53 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 10 Jun 2008 07:13:01 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Benmouffok",
                "Lamia",
                "",
                "INRIA Rocquencourt, LIP6"
            ],
            [
                "Busca",
                "Jean-Michel",
                "",
                "INRIA\n  Rocquencourt, LIP6"
            ],
            [
                "Marqu\u00e8s",
                "Joan Manuel",
                "",
                "LIP6, UOC"
            ],
            [
                "Shapiro",
                "Marc",
                "",
                "INRIA\n  Rocquencourt, LIP6"
            ],
            [
                "Sutra",
                "Pierre",
                "",
                "INRIA Rocquencourt, LIP6"
            ],
            [
                "Tsoukalas",
                "Georgios",
                "",
                "NTUA"
            ]
        ]
    },
    {
        "id": "0805.4745",
        "submitter": "Juan de Lara",
        "authors": "Juan de Lara and Esther Guerra",
        "title": "Pattern-based Model-to-Model Transformation: Long Version",
        "comments": "extended version of the paper from the ICGT'2008 conference\n  (Leicester)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.DM cs.LO",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new, high-level approach for the specification of model-to-model\ntransformations based on declarative patterns. These are (atomic or composite)\nconstraints on triple graphs declaring the allowed or forbidden relationships\nbetween source and target models. In this way, a transformation is defined by\nspecifying a set of triple graph constraints that should be satisfied by the\nresult of the transformation.\n  The description of the transformation is then compiled into lower-level\noperational mechanisms to perform forward or backward transformations, as well\nas to establish mappings between two existent models. In this paper we study\none of such mechanisms based on the generation of operational triple graph\ngrammar rules. Moreover, we exploit deduction techniques at the specification\nlevel to generate more specialized constraints (preserving the specification\nsemantics) reflecting pattern dependencies, from which additional rules can be\nderived.\n  This is an extended version of the paper submitted to ICGT'08, with\nadditional definitions and proofs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 30 May 2008 12:48:16 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "de Lara",
                "Juan",
                ""
            ],
            [
                "Guerra",
                "Esther",
                ""
            ]
        ]
    },
    {
        "id": "0806.0075",
        "submitter": "Sherif Sakr",
        "authors": "Sherif Sakr",
        "title": "An Experimental Investigation of XML Compression Tools",
        "comments": "http://xmlcompbench.sourceforge.net/",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents an extensive experimental study of the state-of-the-art\nof XML compression tools. The study reports the behavior of nine XML\ncompressors using a large corpus of XML documents which covers the different\nnatures and scales of XML documents. In addition to assessing and comparing the\nperformance characteristics of the evaluated XML compression tools, the study\ntries to assess the effectiveness and practicality of using these tools in the\nreal world. Finally, we provide some guidelines and recommen- dations which are\nuseful for helping developers and users for making an effective decision for\nselecting the most suitable XML compression tool for their needs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 31 May 2008 14:49:00 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Sakr",
                "Sherif",
                ""
            ]
        ]
    },
    {
        "id": "0806.0132",
        "submitter": "Feng Xia",
        "authors": "Feng Xia, Yu-Chu Tian, Youxian Sun, Jinxiang Dong",
        "title": "Control-theoretic dynamic voltage scaling for embedded controllers",
        "comments": "Accepted for publication in IET Computers and Digital Techniques.\n  doi:10.1049/iet-cdt:20070112",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For microprocessors used in real-time embedded systems, minimizing power\nconsumption is difficult due to the timing constraints. Dynamic voltage scaling\n(DVS) has been incorporated into modern microprocessors as a promising\ntechnique for exploring the trade-off between energy consumption and system\nperformance. However, it remains a challenge to realize the potential of DVS in\nunpredictable environments where the system workload cannot be accurately\nknown. Addressing system-level power-aware design for DVS-enabled embedded\ncontrollers, this paper establishes an analytical model for the DVS system that\nencompasses multiple real-time control tasks. From this model, a feedback\ncontrol based approach to power management is developed to reduce dynamic power\nconsumption while achieving good application performance. With this approach,\nthe unpredictability and variability of task execution times can be attacked.\nThanks to the use of feedback control theory, predictable performance of the\nDVS system is achieved, which is favorable to real-time applications. Extensive\nsimulations are conducted to evaluate the performance of the proposed approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 1 Jun 2008 08:30:10 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Xia",
                "Feng",
                ""
            ],
            [
                "Tian",
                "Yu-Chu",
                ""
            ],
            [
                "Sun",
                "Youxian",
                ""
            ],
            [
                "Dong",
                "Jinxiang",
                ""
            ]
        ]
    },
    {
        "id": "0806.0250",
        "submitter": "Arjen Hommersom",
        "authors": "Arjen Hommersom, Peter J.F. Lucas, and Patrick van Bommel",
        "title": "Checking the Quality of Clinical Guidelines using Automated Reasoning\n  Tools",
        "comments": "To appear in Theory and Practice of Logic Programming",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.LO cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Requirements about the quality of clinical guidelines can be represented by\nschemata borrowed from the theory of abductive diagnosis, using temporal logic\nto model the time-oriented aspects expressed in a guideline. Previously, we\nhave shown that these requirements can be verified using interactive theorem\nproving techniques. In this paper, we investigate how this approach can be\nmapped to the facilities of a resolution-based theorem prover, Otter, and a\ncomplementary program that searches for finite models of first-order\nstatements, Mace. It is shown that the reasoning required for checking the\nquality of a guideline can be mapped to such fully automated theorem-proving\nfacilities. The medical quality of an actual guideline concerning diabetes\nmellitus 2 is investigated in this way.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 2 Jun 2008 11:02:40 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Hommersom",
                "Arjen",
                ""
            ],
            [
                "Lucas",
                "Peter J. F.",
                ""
            ],
            [
                "van Bommel",
                "Patrick",
                ""
            ]
        ]
    },
    {
        "id": "0806.0314",
        "submitter": "Nicholas Manoukis",
        "authors": "N. C. Manoukis and E. C. Anderson",
        "title": "GuiLiner: A Configurable and Extensible Graphical User Interface for\n  Scientific Analysis and Simulation Software",
        "comments": "4 pages; for the current version of this software, please visit\n  http://guiliner.sourceforge.net/",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.HC cs.SE",
        "license": "http://creativecommons.org/licenses/publicdomain/",
        "abstract": "  The computer programs most users interact with daily are driven by a\ngraphical user interface (GUI). However, many scientific applications are used\nwith a command line interface (CLI) for the ease of development and increased\nflexibility this mode provides. Scientific application developers would benefit\nfrom being able to provide a GUI easily for their CLI programs, thus retaining\nthe advantages of both modes of interaction. GuiLiner is a generic, extensible\nand flexible front-end designed to ``host'' a wide variety of data analysis or\nsimulation programs. Scientific application developers who produce a correctly\nformatted XML file describing their program's options and some of its\ndocumentation can immediately use GuiLiner to produce a carefully implemented\nGUI for their analysis or simulation programs.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 2 Jun 2008 15:57:55 GMT"
            }
        ],
        "update_date": "2008-06-03",
        "authors_parsed": [
            [
                "Manoukis",
                "N. C.",
                ""
            ],
            [
                "Anderson",
                "E. C.",
                ""
            ]
        ]
    },
    {
        "id": "0806.0478",
        "submitter": "Akira Terui",
        "authors": "Akira Terui",
        "title": "Subresultants in Recursive Polynomial Remainder Sequence",
        "comments": "13 pages. Presented at CASC 2003 (Passau, Germany, September 20-26,\n  2003)",
        "journal-ref": "Proceedings of The 6th International Workshop on Computer Algebra\n  in Scientific Computing: CASC 2003, Institute for Informatics, Technische\n  Universitat Munchen, Garching, Germanay, 2003, 363-375",
        "doi": null,
        "report-no": null,
        "categories": "math.AC cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce concepts of \"recursive polynomial remainder sequence (PRS)\" and\n\"recursive subresultant,\" and investigate their properties. In calculating PRS,\nif there exists the GCD (greatest common divisor) of initial polynomials, we\ncalculate \"recursively\" with new PRS for the GCD and its derivative, until a\nconstant is derived. We call such a PRS a recursive PRS. We define recursive\nsubresultants to be determinants representing the coefficients in recursive PRS\nby coefficients of initial polynomials. Finally, we discuss usage of recursive\nsubresultants in approximate algebraic computation, which motivates the present\nwork.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Jun 2008 10:09:36 GMT"
            }
        ],
        "update_date": "2010-07-13",
        "authors_parsed": [
            [
                "Terui",
                "Akira",
                ""
            ]
        ]
    },
    {
        "id": "0806.0488",
        "submitter": "Akira Terui",
        "authors": "Akira Terui",
        "title": "Recursive Polynomial Remainder Sequence and the Nested Subresultants",
        "comments": "12 pages. Presented at CASC 2005 (Kalamata, Greece, Septermber 12-16,\n  2005)",
        "journal-ref": "Computer Algebra in Scientific Computing (Proc. CASC 2005),\n  Lecture Notes in Computer Science 3718, Springer, 2005, 445-456",
        "doi": "10.1007/11555964_38",
        "report-no": null,
        "categories": "math.AC cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We give two new expressions of subresultants, nested subresultant and reduced\nnested subresultant, for the recursive polynomial remainder sequence (PRS)\nwhich has been introduced by the author. The reduced nested subresultant\nreduces the size of the subresultant matrix drastically compared with the\nrecursive subresultant proposed by the authors before, hence it is much more\nuseful for investigation of the recursive PRS. Finally, we discuss usage of the\nreduced nested subresultant in approximate algebraic computation, which\nmotivates the present work.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Jun 2008 10:24:55 GMT"
            }
        ],
        "update_date": "2010-07-13",
        "authors_parsed": [
            [
                "Terui",
                "Akira",
                ""
            ]
        ]
    },
    {
        "id": "0806.0495",
        "submitter": "Akira Terui",
        "authors": "Akira Terui",
        "title": "Recursive Polynomial Remainder Sequence and its Subresultants",
        "comments": "30 pages. Preliminary versions of this paper have been presented at\n  CASC 2003 (arXiv:0806.0478 [math.AC]) and CASC 2005 (arXiv:0806.0488\n  [math.AC])",
        "journal-ref": "Journal of Algebra, Vol. 320, No. 2, pp. 633-659, 2008",
        "doi": "10.1016/j.jalgebra.2007.12.023",
        "report-no": null,
        "categories": "math.AC cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce concepts of \"recursive polynomial remainder sequence (PRS)\" and\n\"recursive subresultant,\" along with investigation of their properties. A\nrecursive PRS is defined as, if there exists the GCD (greatest common divisor)\nof initial polynomials, a sequence of PRSs calculated \"recursively\" for the GCD\nand its derivative until a constant is derived, and recursive subresultants are\ndefined by determinants representing the coefficients in recursive PRS as\nfunctions of coefficients of initial polynomials. We give three different\nconstructions of subresultant matrices for recursive subresultants; while the\nfirst one is built-up just with previously defined matrices thus the size of\nthe matrix increases fast as the recursion deepens, the last one reduces the\nsize of the matrix drastically by the Gaussian elimination on the second one\nwhich has a \"nested\" expression, i.e. a Sylvester matrix whose elements are\nthemselves determinants.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Jun 2008 10:52:48 GMT"
            }
        ],
        "update_date": "2010-07-13",
        "authors_parsed": [
            [
                "Terui",
                "Akira",
                ""
            ]
        ]
    },
    {
        "id": "0806.0689",
        "submitter": "Hongjun Jia",
        "authors": "Hongjun Jia, Li Zhang",
        "title": "Directional Cross Diamond Search Algorithm for Fast Block Motion\n  Estimation",
        "comments": "23 pages, 9 figures, technical report, related paper under submission",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In block-matching motion estimation (BMME), the search patterns have a\nsignificant impact on the algorithm's performance, both the search speed and\nthe search quality. The search pattern should be designed to fit the motion\nvector probability (MVP) distribution characteristics of the real-world\nsequences. In this paper, we build a directional model of MVP distribution to\ndescribe the directional-center-biased characteristic of the MVP distribution\nand the directional characteristics of the conditional MVP distribution more\nexactly based on the detailed statistical data of motion vectors of eighteen\npopular sequences. Three directional search patterns are firstly designed by\nutilizing the directional characteristics and they are the smallest search\npatterns among the popular ones. A new algorithm is proposed using the\nhorizontal cross search pattern as the initial step and the horizontal/vertical\ndiamond search pattern as the subsequent step for the fast BMME, which is\ncalled the directional cross diamond search (DCDS) algorithm. The DCDS\nalgorithm can obtain the motion vector with fewer search points than CDS, DS or\nHEXBS while maintaining the similar or even better search quality. The gain on\nspeedup of DCDS over CDS or DS can be up to 54.9%. The simulation results show\nthat DCDS is efficient, effective and robust, and it can always give the faster\nsearch speed on different sequences than other fast block-matching algorithm in\ncommon use.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Jun 2008 05:05:19 GMT"
            }
        ],
        "update_date": "2008-06-05",
        "authors_parsed": [
            [
                "Jia",
                "Hongjun",
                ""
            ],
            [
                "Zhang",
                "Li",
                ""
            ]
        ]
    },
    {
        "id": "0806.0870",
        "submitter": "Laurent Younes",
        "authors": "Darryl D. Holm, Alain Trouve and Laurent Younes",
        "title": "The Euler-Poincare theory of Metamorphosis",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV nlin.CD",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the pattern matching approach to imaging science, the process of\n``metamorphosis'' is template matching with dynamical templates. Here, we\nrecast the metamorphosis equations of into the Euler-Poincare variational\nframework of and show that the metamorphosis equations contain the equations\nfor a perfect complex fluid \\cite{Ho2002}. This result connects the ideas\nunderlying the process of metamorphosis in image matching to the physical\nconcept of order parameter in the theory of complex fluids. After developing\nthe general theory, we reinterpret various examples, including point set, image\nand density metamorphosis. We finally discuss the issue of matching measures\nwith metamorphosis, for which we provide existence theorems for the initial and\nboundary value problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Jun 2008 22:58:41 GMT"
            }
        ],
        "update_date": "2008-06-14",
        "authors_parsed": [
            [
                "Holm",
                "Darryl D.",
                ""
            ],
            [
                "Trouve",
                "Alain",
                ""
            ],
            [
                "Younes",
                "Laurent",
                ""
            ]
        ]
    },
    {
        "id": "0806.0899",
        "submitter": "Victor Patrangenaru",
        "authors": "V. Patrangenaru, X. Liu, S. Sugathadasa",
        "title": "A Nonparametric Approach to 3D Shape Analysis from Digital Camera Images\n  - I. in Memory of W.P. Dayawansa",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "stat.ME cs.CV math.ST stat.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, for the first time, one develops a nonparametric methodology\nfor an analysis of shapes of configurations of landmarks on real 3D objects\nfrom regular camera photographs, thus making 3D shape analysis very accessible.\nA fundamental result in computer vision by Faugeras (1992), Hartley, Gupta and\nChang (1992) is that generically, a finite 3D configuration of points can be\nretrieved up to a projective transformation, from corresponding configurations\nin a pair of camera images. Consequently, the projective shape of a 3D\nconfiguration can be retrieved from two of its planar views. Given the inherent\nregistration errors, the 3D projective shape can be estimated from a sample of\nphotos of the scene containing that configuration. Projective shapes are here\nregarded as points on projective shape manifolds. Using large sample and\nnonparametric bootstrap methodology for extrinsic means on manifolds, one gives\nconfidence regions and tests for the mean projective shape of a 3D\nconfiguration from its 2D camera images.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jun 2008 04:26:49 GMT"
            }
        ],
        "update_date": "2008-06-14",
        "authors_parsed": [
            [
                "Patrangenaru",
                "V.",
                ""
            ],
            [
                "Liu",
                "X.",
                ""
            ],
            [
                "Sugathadasa",
                "S.",
                ""
            ]
        ]
    },
    {
        "id": "0806.1006",
        "submitter": "Giuseppe Longo",
        "authors": "M. Brescia, S. Cavuoti, G. d'Angelo, R. D'Abrusco, N. Deniskina, M.\n  Garofalo, O. Laurino, G. Longo, A. Nocella, B. Skordovski",
        "title": "The VO-Neural project: recent developments and some applications",
        "comments": "Contributed, Data Centre Alliance Workshops: GRID and the Virtual\n  Observatory, April 9-11 Munich, to appear in Mem. SAIt",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "astro-ph cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  VO-Neural is the natural evolution of the Astroneural project which was\nstarted in 1994 with the aim to implement a suite of neural tools for data\nmining in astronomical massive data sets. At a difference with its ancestor,\nwhich was implemented under Matlab, VO-Neural is written in C++, object\noriented, and it is specifically tailored to work in distributed computing\narchitectures. We discuss the current status of implementation of VO-Neural,\npresent an application to the classification of Active Galactic Nuclei, and\noutline the ongoing work to improve the functionalities of the package.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jun 2008 16:44:23 GMT"
            }
        ],
        "update_date": "2008-06-14",
        "authors_parsed": [
            [
                "Brescia",
                "M.",
                ""
            ],
            [
                "Cavuoti",
                "S.",
                ""
            ],
            [
                "d'Angelo",
                "G.",
                ""
            ],
            [
                "D'Abrusco",
                "R.",
                ""
            ],
            [
                "Deniskina",
                "N.",
                ""
            ],
            [
                "Garofalo",
                "M.",
                ""
            ],
            [
                "Laurino",
                "O.",
                ""
            ],
            [
                "Longo",
                "G.",
                ""
            ],
            [
                "Nocella",
                "A.",
                ""
            ],
            [
                "Skordovski",
                "B.",
                ""
            ]
        ]
    },
    {
        "id": "0806.1071",
        "submitter": "Graham Cormode",
        "authors": "Graham Cormode and Minos Garofalakis",
        "title": "Histograms and Wavelets on Probabilistic Data",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is a growing realization that uncertain information is a first-class\ncitizen in modern database management. As such, we need techniques to correctly\nand efficiently process uncertain data in database systems. In particular, data\nreduction techniques that can produce concise, accurate synopses of large\nprobabilistic relations are crucial. Similar to their deterministic relation\ncounterparts, such compact probabilistic data synopses can form the foundation\nfor human understanding and interactive data exploration, probabilistic query\nplanning and optimization, and fast approximate query processing in\nprobabilistic database systems.\n  In this paper, we introduce definitions and algorithms for building\nhistogram- and wavelet-based synopses on probabilistic data. The core problem\nis to choose a set of histogram bucket boundaries or wavelet coefficients to\noptimize the accuracy of the approximate representation of a collection of\nprobabilistic tuples under a given error metric. For a variety of different\nerror metrics, we devise efficient algorithms that construct optimal or near\noptimal B-term histogram and wavelet synopses. This requires careful analysis\nof the structure of the probability distributions, and novel extensions of\nknown dynamic-programming-based techniques for the deterministic domain. Our\nexperiments show that this approach clearly outperforms simple ideas, such as\nbuilding summaries for samples drawn from the data distribution, while taking\nequal or less time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 5 Jun 2008 23:19:56 GMT"
            }
        ],
        "update_date": "2008-06-09",
        "authors_parsed": [
            [
                "Cormode",
                "Graham",
                ""
            ],
            [
                "Garofalakis",
                "Minos",
                ""
            ]
        ]
    },
    {
        "id": "0806.1139",
        "submitter": "Miguel Andres",
        "authors": "Miguel E. Andres, Pedro D'Argenio, Peter van Rossum",
        "title": "Significant Diagnostic Counterexamples in Probabilistic Model Checking",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a novel technique for counterexample generation in\nprobabilistic model checking of Markov Chains and Markov Decision Processes.\n(Finite) paths in counterexamples are grouped together in witnesses that are\nlikely to provide similar debugging information to the user. We list five\nproperties that witnesses should satisfy in order to be useful as debugging\naid: similarity, accuracy, originality, significance, and finiteness. Our\nwitnesses contain paths that behave similar outside strongly connected\ncomponents.\n  This papers shows how to compute these witnesses by reducing the problem of\ngenerating counterexamples for general properties over Markov Decision\nProcesses, in several steps, to the easy problem of generating counterexamples\nfor reachability properties over acyclic Markov Chains.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Jun 2008 13:09:49 GMT"
            }
        ],
        "update_date": "2008-06-09",
        "authors_parsed": [
            [
                "Andres",
                "Miguel E.",
                ""
            ],
            [
                "D'Argenio",
                "Pedro",
                ""
            ],
            [
                "van Rossum",
                "Peter",
                ""
            ]
        ]
    },
    {
        "id": "0806.1144",
        "submitter": "Giuseppe Longo",
        "authors": "N. Deniskina, M. Brescia, S. Cavuoti, G. d'Angelo, O. Laurino, G.\n  Longo",
        "title": "GRID-Launcher v.1.0",
        "comments": "Contributed, Data Centre Alliance Workshops: GRID and the Virtual\n  Observatory, April 9-11 Munich, to appear in Mem. SAIt",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "astro-ph cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  GRID-launcher-1.0 was built within the VO-Tech framework, as a software\ninterface between the UK-ASTROGRID and a generic GRID infrastructures in order\nto allow any ASTROGRID user to launch on the GRID computing intensive tasks\nfrom the ASTROGRID Workbench or Desktop. Even though of general application, so\nfar the Grid-Launcher has been tested on a few selected softwares\n(VONeural-MLP, VONeural-SVM, Sextractor and SWARP) and on the SCOPE-GRID.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Jun 2008 12:35:17 GMT"
            }
        ],
        "update_date": "2008-06-14",
        "authors_parsed": [
            [
                "Deniskina",
                "N.",
                ""
            ],
            [
                "Brescia",
                "M.",
                ""
            ],
            [
                "Cavuoti",
                "S.",
                ""
            ],
            [
                "d'Angelo",
                "G.",
                ""
            ],
            [
                "Laurino",
                "O.",
                ""
            ],
            [
                "Longo",
                "G.",
                ""
            ]
        ]
    },
    {
        "id": "0806.1381",
        "submitter": "Feng Xia",
        "authors": "Feng Xia, Guosong Tian, Youxian Sun",
        "title": "Feedback Scheduling: An Event-Driven Paradigm",
        "comments": "8 pages, 10 figures",
        "journal-ref": "ACM SIGPLAN Notices, vol.42, no.12, pp. 7-14, Dec. 2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Embedded computing systems today increasingly feature resource constraints\nand workload variability, which lead to uncertainty in resource availability.\nThis raises great challenges to software design and programming in multitasking\nenvironments. In this paper, the emerging methodology of feedback scheduling is\nintroduced to address these challenges. As a closed-loop approach to resource\nmanagement, feedback scheduling promises to enhance the flexibility and\nresource efficiency of various software programs through dynamically\ndistributing available resources among concurrent tasks based on feedback\ninformation about the actual usage of the resources. With emphasis on the\nbehavioral design of feedback schedulers, we describe a general framework of\nfeedback scheduling in the context of real-time control applications. A simple\nyet illustrative feedback scheduling algorithm is given. From a programming\nperspective, we describe how to modify the implementation of control tasks to\nfacilitate the application of feedback scheduling. An event-driven paradigm\nthat combines time-triggered and event-triggered approaches is proposed for\nprogramming of the feedback scheduler. Simulation results argue that the\nproposed event-driven paradigm yields better performance than time-triggered\nparadigm in dynamic environments where the workload varies irregularly and\nunpredictably.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 9 Jun 2008 07:23:28 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Xia",
                "Feng",
                ""
            ],
            [
                "Tian",
                "Guosong",
                ""
            ],
            [
                "Sun",
                "Youxian",
                ""
            ]
        ]
    },
    {
        "id": "0806.1446",
        "submitter": "Guoshen Yu",
        "authors": "Guoshen Yu and Jean-Jacques Slotine",
        "title": "Fast Wavelet-Based Visual Classification",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate a biologically motivated approach to fast visual\nclassification, directly inspired by the recent work of Serre et al.\nSpecifically, trading-off biological accuracy for computational efficiency, we\nexplore using wavelet and grouplet-like transforms to parallel the tuning of\nvisual cortex V1 and V2 cells, alternated with max operations to achieve scale\nand translation invariance. A feature selection procedure is applied during\nlearning to accelerate recognition. We introduce a simple attention-like\nfeedback mechanism, significantly improving recognition and robustness in\nmultiple-object scenes. In experiments, the proposed algorithm achieves or\nexceeds state-of-the-art success rate on object recognition, texture and\nsatellite image classification, language identification and sound\nclassification.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 8 Jun 2008 10:15:04 GMT"
            }
        ],
        "update_date": "2008-06-10",
        "authors_parsed": [
            [
                "Yu",
                "Guoshen",
                ""
            ],
            [
                "Slotine",
                "Jean-Jacques",
                ""
            ]
        ]
    },
    {
        "id": "0806.1749",
        "submitter": "Jacek Chrz{\\ka}szcz",
        "authors": "Daria Walukiewicz-Chrzaszcz and Jacek Chrzaszcz",
        "title": "Consistency and Completeness of Rewriting in the Calculus of\n  Constructions",
        "comments": "20 pages",
        "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 3 (September\n  15, 2008) lmcs:1141",
        "doi": "10.2168/LMCS-4(3:8)2008",
        "report-no": null,
        "categories": "cs.LO cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Adding rewriting to a proof assistant based on the Curry-Howard isomorphism,\nsuch as Coq, may greatly improve usability of the tool. Unfortunately adding an\narbitrary set of rewrite rules may render the underlying formal system\nundecidable and inconsistent. While ways to ensure termination and confluence,\nand hence decidability of type-checking, have already been studied to some\nextent, logical consistency has got little attention so far. In this paper we\nshow that consistency is a consequence of canonicity, which in turn follows\nfrom the assumption that all functions defined by rewrite rules are complete.\nWe provide a sound and terminating, but necessarily incomplete algorithm to\nverify this property. The algorithm accepts all definitions that follow\ndependent pattern matching schemes presented by Coquand and studied by McBride\nin his PhD thesis. It also accepts many definitions by rewriting, containing\nrules which depart from standard pattern matching.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Jun 2008 20:27:28 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 15 Sep 2008 19:30:14 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 15 Sep 2008 20:54:24 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Walukiewicz-Chrzaszcz",
                "Daria",
                ""
            ],
            [
                "Chrzaszcz",
                "Jacek",
                ""
            ]
        ]
    },
    {
        "id": "0806.1768",
        "submitter": "Ted Herman",
        "authors": "Ted Herman, Morten Mjelde",
        "title": "Local Read-Write Operations in Sensor Networks",
        "comments": "19 pages, 16 figures (using pstricks)",
        "journal-ref": null,
        "doi": null,
        "report-no": "TR08-02",
        "categories": "cs.OS cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Designing protocols and formulating convenient programming units of\nabstraction for sensor networks is challenging due to communication errors and\nplatform constraints. This paper investigates properties and implementation\nreliability for a \\emph{local read-write} abstraction. Local read-write is\ninspired by the class of read-modify-write operations defined for shared-memory\nmultiprocessor architectures. The class of read-modify-write operations is\nimportant in solving consensus and related synchronization problems for\nconcurrency control. Local read-write is shown to be an atomic abstraction for\nsynchronizing neighborhood states in sensor networks. The paper compares local\nread-write to similar lightweight operations in wireless sensor networks, such\nas read-all, write-all, and a transaction-based abstraction: for some\noptimistic scenarios, local read-write is a more efficient neighborhood\noperation. A partial implementation is described, which shows that three\noutcomes characterize operation response: success, failure, and cancel. A\nfailure response indicates possible inconsistency for the operation result,\nwhich is the result of a timeout event at the operation's initiator. The paper\npresents experimental results on operation performance with different timeout\nvalues and situations of no contention, with some tests also on various\nneighborhood sizes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Jun 2008 00:03:00 GMT"
            }
        ],
        "update_date": "2008-06-12",
        "authors_parsed": [
            [
                "Herman",
                "Ted",
                ""
            ],
            [
                "Mjelde",
                "Morten",
                ""
            ]
        ]
    },
    {
        "id": "0806.1796",
        "submitter": "Arnaud Martin",
        "authors": "Arnaud Martin (E3I2), Hicham Laanaya (E3I2), Andreas Arnold-Bos (E3I2)",
        "title": "Evaluation for Uncertain Image Classification and Segmentation",
        "comments": null,
        "journal-ref": "Pattern Recognition 39, 11 (2006) 1987-1995",
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Each year, numerous segmentation and classification algorithms are invented\nor reused to solve problems where machine vision is needed. Generally, the\nefficiency of these algorithms is compared against the results given by one or\nmany human experts. However, in many situations, the location of the real\nboundaries of the objects as well as their classes are not known with certainty\nby the human experts. Furthermore, only one aspect of the segmentation and\nclassification problem is generally evaluated. In this paper we present a new\nevaluation method for classification and segmentation of image, where we take\ninto account both the classification and segmentation results as well as the\nlevel of certainty given by the experts. As a concrete example of our method,\nwe evaluate an automatic seabed characterization algorithm based on sonar\nimages.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Jun 2008 07:02:45 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Martin",
                "Arnaud",
                "",
                "E3I2"
            ],
            [
                "Laanaya",
                "Hicham",
                "",
                "E3I2"
            ],
            [
                "Arnold-Bos",
                "Andreas",
                "",
                "E3I2"
            ]
        ]
    },
    {
        "id": "0806.1798",
        "submitter": "Arnaud Martin",
        "authors": "Arnaud Martin (E3I2), Christophe Osswald (E3I2)",
        "title": "Human expert fusion for image classification",
        "comments": null,
        "journal-ref": "Information & Security. An International Journal 20 (2006) 122-141",
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In image classification, merging the opinion of several human experts is very\nimportant for different tasks such as the evaluation or the training. Indeed,\nthe ground truth is rarely known before the scene imaging. We propose here\ndifferent models in order to fuse the informations given by two or more\nexperts. The considered unit for the classification, a small tile of the image,\ncan contain one or more kind of the considered classes given by the experts. A\nsecond problem that we have to take into account, is the amount of certainty of\nthe expert has for each pixel of the tile. In order to solve these problems we\ndefine five models in the context of the Dempster-Shafer Theory and in the\ncontext of the Dezert-Smarandache Theory and we study the possible decisions\nwith these models.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Jun 2008 07:09:15 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Martin",
                "Arnaud",
                "",
                "E3I2"
            ],
            [
                "Osswald",
                "Christophe",
                "",
                "E3I2"
            ]
        ]
    },
    {
        "id": "0806.1816",
        "submitter": "Michael Mrissa",
        "authors": "M. Mrissa, Ph. Thiran, J-M. Jacquet, D. Benslimane and Z. Maamar",
        "title": "Cardinality heterogeneities in Web service composition: Issues and\n  solutions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Data exchanges between Web services engaged in a composition raise several\nheterogeneities. In this paper, we address the problem of data cardinality\nheterogeneity in a composition. Firstly, we build a theoretical framework to\ndescribe different aspects of Web services that relate to data cardinality, and\nsecondly, we solve this problem by developing a solution for cardinality\nmediation based on constraint logic programming.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Jun 2008 09:05:21 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Mrissa",
                "M.",
                ""
            ],
            [
                "Thiran",
                "Ph.",
                ""
            ],
            [
                "Jacquet",
                "J-M.",
                ""
            ],
            [
                "Benslimane",
                "D.",
                ""
            ],
            [
                "Maamar",
                "Z.",
                ""
            ]
        ]
    },
    {
        "id": "0806.1984",
        "submitter": "Irina Kogan A",
        "authors": "S. Feng, I. A. Kogan, H. Krim",
        "title": "Classification of curves in 2D and 3D via affine integral signatures",
        "comments": "30 pages, 16 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a robust classification algorithm for curves in 2D and 3D, under\nthe special and full groups of affine transformations. To each plane or spatial\ncurve we assign a plane signature curve. Curves, equivalent under an affine\ntransformation, have the same signature. The signatures introduced in this\npaper are based on integral invariants, which behave much better on noisy\nimages than classically known differential invariants. The comparison with\nother types of invariants is given in the introduction. Though the integral\ninvariants for planar curves were known before, the affine integral invariants\nfor spatial curves are proposed here for the first time. Using the inductive\nvariation of the moving frame method we compute affine invariants in terms of\nEuclidean invariants. We present two types of signatures, the global signature\nand the local signature. Both signatures are independent of parameterization\n(curve sampling). The global signature depends on the choice of the initial\npoint and does not allow us to compare fragments of curves, and is therefore\nsensitive to occlusions. The local signature, although is slightly more\nsensitive to noise, is independent of the choice of the initial point and is\nnot sensitive to occlusions in an image. It helps establish local equivalence\nof curves. The robustness of these invariants and signatures in their\napplication to the problem of classification of noisy spatial curves extracted\nfrom a 3D object is analyzed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 12 Jun 2008 01:12:25 GMT"
            }
        ],
        "update_date": "2008-06-13",
        "authors_parsed": [
            [
                "Feng",
                "S.",
                ""
            ],
            [
                "Kogan",
                "I. A.",
                ""
            ],
            [
                "Krim",
                "H.",
                ""
            ]
        ]
    },
    {
        "id": "0806.2006",
        "submitter": "Arnaud Martin",
        "authors": "Arnaud Martin (E3I2)",
        "title": "Fusion de classifieurs pour la classification d'images sonar",
        "comments": null,
        "journal-ref": "Revue Nationale des Technologies de l'Information E, 5 (2005)\n  259-268",
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present some high level information fusion approaches for\nnumeric and symbolic data. We study the interest of such method particularly\nfor classifier fusion. A comparative study is made in a context of sea bed\ncharacterization from sonar images. The classi- fication of kind of sediment is\na difficult problem because of the data complexity. We compare high level\ninformation fusion and give the obtained performance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 12 Jun 2008 06:42:07 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 6 Jan 2012 20:39:14 GMT"
            }
        ],
        "update_date": "2012-01-09",
        "authors_parsed": [
            [
                "Martin",
                "Arnaud",
                "",
                "E3I2"
            ]
        ]
    },
    {
        "id": "0806.2007",
        "submitter": "Arnaud Martin",
        "authors": "Arnaud Martin (E3I2), Christophe Osswald (E3I2)",
        "title": "Experts Fusion and Multilayer Perceptron Based on Belief Learning for\n  Sonar Image Classification",
        "comments": "International Conference on Information & Communication Technologies:\n  from Theory to Applications (ICTTA), Damascus : Syrie (2008)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The sonar images provide a rapid view of the seabed in order to characterize\nit. However, in such as uncertain environment, real seabed is unknown and the\nonly information we can obtain, is the interpretation of different human\nexperts, sometimes in conflict. In this paper, we propose to manage this\nconflict in order to provide a robust reality for the learning step of\nclassification algorithms. The classification is conducted by a multilayer\nperceptron, taking into account the uncertainty of the reality in the learning\nstage. The results of this seabed characterization are presented on real sonar\nimages.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 12 Jun 2008 06:44:55 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Martin",
                "Arnaud",
                "",
                "E3I2"
            ],
            [
                "Osswald",
                "Christophe",
                "",
                "E3I2"
            ]
        ]
    },
    {
        "id": "0806.2008",
        "submitter": "Arnaud Martin",
        "authors": "Arnaud Martin (E3I2), Christophe Osswald (E3I2)",
        "title": "Generalized proportional conflict redistribution rule applied to Sonar\n  imagery and Radar targets classification",
        "comments": null,
        "journal-ref": "Advances and Applications of DSmT for Information Fusion,\n  Florentin Smarandache & Jean Dezert (Ed.) (2006) 289-304",
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this chapter, we present two applications in information fusion in order\nto evaluate the generalized proportional conflict redistribution rule presented\nin the chapter \\cite{Martin06a}. Most of the time the combination rules are\nevaluated only on simple examples. We study here different combination rules\nand compare them in terms of decision on real data. Indeed, in real\napplications, we need a reliable decision and it is the final results that\nmatter. Two applications are presented here: a fusion of human experts opinions\non the kind of underwater sediments depict on sonar image and a classifier\nfusion for radar targets recognition.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 12 Jun 2008 06:47:26 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Martin",
                "Arnaud",
                "",
                "E3I2"
            ],
            [
                "Osswald",
                "Christophe",
                "",
                "E3I2"
            ]
        ]
    },
    {
        "id": "0806.2035",
        "submitter": "Francesc Rossell\\'o",
        "authors": "Gabriel Cardona, Merce Llabres, Francesc Rossello, Gabriel Valiente",
        "title": "Nodal distances for rooted phylogenetic trees",
        "comments": "26 pages, Supplementary Material available at\n  http://bioinfo.uib.es/~recerca/phylotrees/nodal/",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-bio.PE cs.CE cs.DM",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  Dissimilarity measures for (possibly weighted) phylogenetic trees based on\nthe comparison of their vectors of path lengths between pairs of taxa, have\nbeen present in the systematics literature since the early seventies. But, as\nfar as rooted phylogenetic trees goes, these vectors can only separate\nnon-weighted binary trees, and therefore these dissimilarity measures are\nmetrics only on this class. In this paper we overcome this problem, by\nsplitting in a suitable way each path length between two taxa into two lengths.\nWe prove that the resulting splitted path lengths matrices single out arbitrary\nrooted phylogenetic trees with nested taxa and arcs weighted in the set of\npositive real numbers. This allows the definition of metrics on this general\nclass by comparing these matrices by means of metrics in spaces of real-valued\n$n\\times n$ matrices. We conclude this paper by establishing some basic facts\nabout the metrics for non-weighted phylogenetic trees defined in this way using\n$L^p$ metrics on these spaces of matrices.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 12 Jun 2008 10:17:02 GMT"
            }
        ],
        "update_date": "2008-07-06",
        "authors_parsed": [
            [
                "Cardona",
                "Gabriel",
                ""
            ],
            [
                "Llabres",
                "Merce",
                ""
            ],
            [
                "Rossello",
                "Francesc",
                ""
            ],
            [
                "Valiente",
                "Gabriel",
                ""
            ]
        ]
    },
    {
        "id": "0806.2312",
        "submitter": "Balint Joo",
        "authors": "Balint Joo (for the USQCD Collaboration)",
        "title": "Continuing Progress on a Lattice QCD Software Infrastructure",
        "comments": "5 Pages, to appear in the Proceedings of SciDAC 2008 conference,\n  (Seattle, July 13-17, 2008), Conference Poster Presentation Proceedings",
        "journal-ref": "J.Phys.Conf.Ser.125:012066,2008",
        "doi": "10.1088/1742-6596/125/1/012066",
        "report-no": "JLAB-IT-08-02",
        "categories": "hep-lat cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We report on the progress of the software effort in the QCD Application Area\nof SciDAC. In particular, we discuss how the software developed under SciDAC\nenabled the aggressive exploitation of leadership computers, and we report on\nprogress in the area of QCD software for multi-core architectures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 13 Jun 2008 19:22:00 GMT"
            }
        ],
        "update_date": "2019-08-13",
        "authors_parsed": [
            [
                "Joo",
                "Balint",
                "",
                "for the USQCD Collaboration"
            ]
        ]
    },
    {
        "id": "0806.2448",
        "submitter": "Nobuko Yoshida Dr",
        "authors": "Nobuko Yoshida, Kohei Honda and Martin Berger",
        "title": "Logical Reasoning for Higher-Order Functions with Local State",
        "comments": "68 pages",
        "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 4 (October\n  20, 2008) lmcs:830",
        "doi": "10.2168/LMCS-4(4:2)2008",
        "report-no": null,
        "categories": "cs.LO cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce an extension of Hoare logic for call-by-value higher-order\nfunctions with ML-like local reference generation. Local references may be\ngenerated dynamically and exported outside their scope, may store higher-order\nfunctions and may be used to construct complex mutable data structures. This\nprimitive is captured logically using a predicate asserting reachability of a\nreference name from a possibly higher-order datum and quantifiers over hidden\nreferences. We explore the logic's descriptive and reasoning power with\nnon-trivial programming examples combining higher-order procedures and\ndynamically generated local state. Axioms for reachability and local invariant\nplay a central role for reasoning about the examples.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 15 Jun 2008 14:43:25 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 20 Oct 2008 08:47:19 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Yoshida",
                "Nobuko",
                ""
            ],
            [
                "Honda",
                "Kohei",
                ""
            ],
            [
                "Berger",
                "Martin",
                ""
            ]
        ]
    },
    {
        "id": "0806.2680",
        "submitter": "Joerg Endrullis",
        "authors": "Joerg Endrullis, Clemens Grabmayer, Dimitri Hendriks",
        "title": "Data-Oblivious Stream Productivity",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We are concerned with demonstrating productivity of specifications of\ninfinite streams of data, based on orthogonal rewrite rules. In general, this\nproperty is undecidable, but for restricted formats computable sufficient\nconditions can be obtained. The usual analysis disregards the identity of data,\nthus leading to approaches that we call data-oblivious. We present a method\nthat is provably optimal among all such data-oblivious approaches. This means\nthat in order to improve on the algorithm in this paper one has to proceed in a\ndata-aware fashion.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 16 Jun 2008 22:02:56 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 26 Jun 2008 16:06:16 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 27 Jun 2008 14:18:25 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 4 Jul 2008 15:05:51 GMT"
            },
            {
                "version": "v5",
                "created": "Sat, 19 Jul 2008 22:45:16 GMT"
            }
        ],
        "update_date": "2008-07-20",
        "authors_parsed": [
            [
                "Endrullis",
                "Joerg",
                ""
            ],
            [
                "Grabmayer",
                "Clemens",
                ""
            ],
            [
                "Hendriks",
                "Dimitri",
                ""
            ]
        ]
    },
    {
        "id": "0806.2735",
        "submitter": "Jonathan Grattage",
        "authors": "Jonathan Grattage",
        "title": "An overview of QML with a concrete implementation in Haskell",
        "comments": "9 pages, final conference version (Quantum Physics and Logic 2008)",
        "journal-ref": "ENTCS: Proceedings of QPL V - DCV IV, 157-165, Reykjavik, Iceland,\n  2008",
        "doi": null,
        "report-no": null,
        "categories": "quant-ph cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper gives an introduction to and overview of the functional quantum\nprogramming language QML. The syntax of this language is defined and explained,\nalong with a new QML definition of the quantum teleport algorithm. The\ncategorical operational semantics of QML is also briefly introduced, in the\nform of annotated quantum circuits. This definition leads to a denotational\nsemantics, given in terms of superoperators. Finally, an implementation in\nHaskell of the semantics for QML is presented as a compiler. The compiler takes\nQML programs as input, which are parsed into a Haskell datatype. The output\nfrom the compiler is either a quantum circuit (operational), an isometry (pure\ndenotational) or a superoperator (impure denotational). Orthogonality\njudgements and problems with coproducts in QML are also discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 17 Jun 2008 10:02:04 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 21 Jul 2008 15:56:47 GMT"
            }
        ],
        "update_date": "2008-07-21",
        "authors_parsed": [
            [
                "Grattage",
                "Jonathan",
                ""
            ]
        ]
    },
    {
        "id": "0806.2890",
        "submitter": "Julian McAuley",
        "authors": "Tiberio S. Caetano, Julian J. McAuley, Li Cheng, Quoc V. Le and Alex\n  J. Smola",
        "title": "Learning Graph Matching",
        "comments": "10 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.LG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As a fundamental problem in pattern recognition, graph matching has\napplications in a variety of fields, from computer vision to computational\nbiology. In graph matching, patterns are modeled as graphs and pattern\nrecognition amounts to finding a correspondence between the nodes of different\ngraphs. Many formulations of this problem can be cast in general as a quadratic\nassignment problem, where a linear term in the objective function encodes node\ncompatibility and a quadratic term encodes edge compatibility. The main\nresearch focus in this theme is about designing efficient algorithms for\napproximately solving the quadratic assignment problem, since it is NP-hard. In\nthis paper we turn our attention to a different question: how to estimate\ncompatibility functions such that the solution of the resulting graph matching\nproblem best matches the expected solution that a human would manually provide.\nWe present a method for learning graph matching: the training examples are\npairs of graphs and the `labels' are matches between them. Our experimental\nresults reveal that learning can substantially improve the performance of\nstandard graph matching algorithms. In particular, we find that simple linear\nassignment with such a learning scheme outperforms Graduated Assignment with\nbistochastic normalisation, a state-of-the-art quadratic assignment relaxation\nalgorithm.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 17 Jun 2008 23:28:08 GMT"
            }
        ],
        "update_date": "2008-06-19",
        "authors_parsed": [
            [
                "Caetano",
                "Tiberio S.",
                ""
            ],
            [
                "McAuley",
                "Julian J.",
                ""
            ],
            [
                "Cheng",
                "Li",
                ""
            ],
            [
                "Le",
                "Quoc V.",
                ""
            ],
            [
                "Smola",
                "Alex J.",
                ""
            ]
        ]
    },
    {
        "id": "0806.3115",
        "submitter": "Daniel Hazel",
        "authors": "Dan Hazel (Technology One)",
        "title": "Using rational numbers to key nested sets",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": "DocSetID-311997",
        "categories": "cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This report details the generation and use of tree node ordering keys in a\nsingle relational database table. The keys for each node are calculated from\nthe keys of its parent, in such a way that the sort order places every node in\nthe tree before all of its descendants and after all siblings having a lower\nindex. The calculation from parent keys to child keys is simple, and reversible\nin the sense that the keys of every ancestor of a node can be calculated from\nthat node's keys without having to consult the database.\n  Proofs of the above properties of the key encoding process and of its\ncorrespondence to a finite continued fraction form are provided.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 19 Jun 2008 02:06:14 GMT"
            }
        ],
        "update_date": "2008-06-20",
        "authors_parsed": [
            [
                "Hazel",
                "Dan",
                "",
                "Technology One"
            ]
        ]
    },
    {
        "id": "0806.3121",
        "submitter": "Julien Langou",
        "authors": "George Bosilca, Remi Delmas, Jack Dongarra, and Julien Langou",
        "title": "Algorithmic Based Fault Tolerance Applied to High Performance Computing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a new approach to fault tolerance for High Performance Computing\nsystem. Our approach is based on a careful adaptation of the Algorithmic Based\nFault Tolerance technique (Huang and Abraham, 1984) to the need of parallel\ndistributed computation. We obtain a strongly scalable mechanism for fault\ntolerance. We can also detect and correct errors (bit-flip) on the fly of a\ncomputation. To assess the viability of our approach, we have developed a fault\ntolerant matrix-matrix multiplication subroutine and we propose some models to\npredict its running time. Our parallel fault-tolerant matrix-matrix\nmultiplication scores 1.4 TFLOPS on 484 processors (cluster jacquard.nersc.gov)\nand returns a correct result while one process failure has happened. This\nrepresents 65% of the machine peak efficiency and less than 12% overhead with\nrespect to the fastest failure-free implementation. We predict (and have\nobserved) that, as we increase the processor count, the overhead of the fault\ntolerance drops significantly.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 19 Jun 2008 02:06:57 GMT"
            }
        ],
        "update_date": "2008-06-20",
        "authors_parsed": [
            [
                "Bosilca",
                "George",
                ""
            ],
            [
                "Delmas",
                "Remi",
                ""
            ],
            [
                "Dongarra",
                "Jack",
                ""
            ],
            [
                "Langou",
                "Julien",
                ""
            ]
        ]
    },
    {
        "id": "0806.3646",
        "submitter": "George Eskander MSc",
        "authors": "George S. Eskander, Amir Atiya, Kil To Chong, Hyongsuk Kim, Sung Goo\n  Yoo",
        "title": "Round Trip Time Prediction Using the Symbolic Function Network Approach",
        "comments": null,
        "journal-ref": "ISITC, pp. 3-7, 2007 International Symposium on Information\n  Technology Convergence (ISITC 2007), 2007",
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we develop a novel approach to model the Internet round trip\ntime using a recently proposed symbolic type neural network model called\nsymbolic function network. The developed predictor is shown to have good\ngeneralization performance and simple representation compared to the multilayer\nperceptron based predictors.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Jun 2008 10:04:14 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 9 Aug 2008 21:45:53 GMT"
            }
        ],
        "update_date": "2008-08-11",
        "authors_parsed": [
            [
                "Eskander",
                "George S.",
                ""
            ],
            [
                "Atiya",
                "Amir",
                ""
            ],
            [
                "Chong",
                "Kil To",
                ""
            ],
            [
                "Kim",
                "Hyongsuk",
                ""
            ],
            [
                "Yoo",
                "Sung Goo",
                ""
            ]
        ]
    },
    {
        "id": "0806.3710",
        "submitter": "Stevan Harnad",
        "authors": "A. Blondin Masse, G. Chicoisne, Y. Gargouri, S. Harnad, O. Picard, O.\n  Marcotte",
        "title": "How Is Meaning Grounded in Dictionary Definitions?",
        "comments": "8 pages, 3 figures, TextGraphs-3 Workshop at the 22nd International\n  Conference on Computational Linguistics, Coling 2008, Manchester, 18-22\n  August, 2008",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CL cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Meaning cannot be based on dictionary definitions all the way down: at some\npoint the circularity of definitions must be broken in some way, by grounding\nthe meanings of certain words in sensorimotor categories learned from\nexperience or shaped by evolution. This is the \"symbol grounding problem.\" We\nintroduce the concept of a reachable set -- a larger vocabulary whose meanings\ncan be learned from a smaller vocabulary through definition alone, as long as\nthe meanings of the smaller vocabulary are themselves already grounded. We\nprovide simple algorithms to compute reachable sets for any given dictionary.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Jun 2008 15:53:05 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 15 Jul 2008 01:59:09 GMT"
            }
        ],
        "update_date": "2008-07-15",
        "authors_parsed": [
            [
                "Masse",
                "A. Blondin",
                ""
            ],
            [
                "Chicoisne",
                "G.",
                ""
            ],
            [
                "Gargouri",
                "Y.",
                ""
            ],
            [
                "Harnad",
                "S.",
                ""
            ],
            [
                "Picard",
                "O.",
                ""
            ],
            [
                "Marcotte",
                "O.",
                ""
            ]
        ]
    },
    {
        "id": "0806.3849",
        "submitter": "Etienne Lozes",
        "authors": "Daniel Hirschkoff, Etienne Lozes, Davide Sangiorgi",
        "title": "Separability in the Ambient Logic",
        "comments": "logical methods in computer science, 44 pages",
        "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 3 (September\n  4, 2008) lmcs:682",
        "doi": "10.2168/LMCS-4(3:4)2008",
        "report-no": null,
        "categories": "cs.LO cs.MA cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The \\it{Ambient Logic} (AL) has been proposed for expressing properties of\nprocess mobility in the calculus of Mobile Ambients (MA), and as a basis for\nquery languages on semistructured data. We study some basic questions\nconcerning the discriminating power of AL, focusing on the equivalence on\nprocesses induced by the logic $(=_L>)$. As underlying calculi besides MA we\nconsider a subcalculus in which an image-finiteness condition holds and that we\nprove to be Turing complete. Synchronous variants of these calculi are studied\nas well. In these calculi, we provide two operational characterisations of\n$_=L$: a coinductive one (as a form of bisimilarity) and an inductive one\n(based on structual properties of processes). After showing $_=L$ to be stricly\nfiner than barbed congruence, we establish axiomatisations of $_=L$ on the\nsubcalculus of MA (both the asynchronous and the synchronous version), enabling\nus to relate $_=L$ to structural congruence. We also present some\n(un)decidability results that are related to the above separation properties\nfor AL: the undecidability of $_=L$ on MA and its decidability on the\nsubcalculus.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jun 2008 10:00:00 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 4 Sep 2008 07:13:38 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Hirschkoff",
                "Daniel",
                ""
            ],
            [
                "Lozes",
                "Etienne",
                ""
            ],
            [
                "Sangiorgi",
                "Davide",
                ""
            ]
        ]
    },
    {
        "id": "0806.3885",
        "submitter": "Vincent Tariel",
        "authors": "Vincent Tariel",
        "title": "Conceptualization of seeded region growing by pixels aggregation. Part\n  1: the framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Adams and Bishop have proposed in 1994 a novel region growing algorithm\ncalled seeded region growing by pixels aggregation (SRGPA). This paper\nintroduces a framework to implement an algorithm using SRGPA. This framework is\nbuilt around two concepts: localization and organization of applied action.\nThis conceptualization gives a quick implementation of algorithms, a direct\ntranslation between the mathematical idea and the numerical implementation, and\nan improvement of algorithms efficiency.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jun 2008 13:43:06 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Tariel",
                "Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0806.3887",
        "submitter": "Vincent Tariel",
        "authors": "Vincent Tariel",
        "title": "Conceptualization of seeded region growing by pixels aggregation. Part\n  2: how to localize a final partition invariant about the seeded region\n  initialisation order",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the previous paper, we have conceptualized the localization and the\norganization of seeded region growing by pixels aggregation (SRGPA) but we do\nnot give the issue when there is a collision between two distinct regions\nduring the growing process. In this paper, we propose two implementations to\nmanage two classical growing processes: one without a boundary region region to\ndivide the other regions and another with. Unfortunately, as noticed by Mehnert\nand Jakway (1997), this partition depends on the seeded region initialisation\norder (SRIO). We propose a growing process, invariant about SRIO such as the\nboundary region is the set of ambiguous pixels.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jun 2008 13:34:15 GMT"
            }
        ],
        "update_date": "2008-06-25",
        "authors_parsed": [
            [
                "Tariel",
                "Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0806.3928",
        "submitter": "Vincent Tariel",
        "authors": "Vincent Tariel",
        "title": "Conceptualization of seeded region growing by pixels aggregation. Part\n  3: a wide range of algorithms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the two previous papers of this serie, we have created a library, called\nPopulation, dedicated to seeded region growing by pixels aggregation and we\nhave proposed different growing processes to get a partition with or without a\nboundary region to divide the other regions or to get a partition invariant\nabout the seeded region initialisation order. Using this work, we implement\nsome algorithms belonging to the field of SRGPA using this library and these\ngrowing processes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jun 2008 17:02:47 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Tariel",
                "Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0806.3939",
        "submitter": "Vincent Tariel",
        "authors": "Vincent Tariel",
        "title": "Conceptualization of seeded region growing by pixels aggregation. Part\n  4: Simple, generic and robust extraction of grains in granular materials\n  obtained by X-ray tomography",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a simple, generic and robust method to extract the grains\nfrom experimental tridimensionnal images of granular materials obtained by\nX-ray tomography. This extraction has two steps: segmentation and splitting.\nFor the segmentation step, if there is a sufficient contrast between the\ndifferent components, a classical threshold procedure followed by a succession\nof morphological filters can be applied. If not, and if the boundary needs to\nbe localized precisely, a watershed transformation controlled by labels is\napplied. The basement of this transformation is to localize a label included in\nthe component and another label in the component complementary. A \"soft\"\nthreshold following by an opening is applied on the initial image to localize a\nlabel in a component. For any segmentation procedure, the visualisation shows a\nproblem: some groups of two grains, close one to each other, become connected.\nSo if a classical cluster procedure is applied on the segmented binary image,\nthese numerical connected grains are considered as a single grain. To overcome\nthis problem, we applied a procedure introduced by L. Vincent in 1993. This\ngrains extraction is tested for various complexes porous media and granular\nmaterial, to predict various properties (diffusion, electrical conductivity,\ndeformation field) in a good agreement with experiment data.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Jun 2008 17:40:25 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 23 Jul 2008 15:09:43 GMT"
            }
        ],
        "update_date": "2008-07-23",
        "authors_parsed": [
            [
                "Tariel",
                "Vincent",
                ""
            ]
        ]
    },
    {
        "id": "0806.4127",
        "submitter": "Marc Dohm",
        "authors": "Marc Dohm (JAD, INRIA Sophia Antipolis), Severinas Zube",
        "title": "The implicit equation of a canal surface",
        "comments": "26 pages, to be published in Journal of Symbolic Computation",
        "journal-ref": null,
        "doi": "10.1016/j.jsc.2008.06.001",
        "report-no": null,
        "categories": "math.AG cs.SC math.AC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A canal surface is an envelope of a one parameter family of spheres. In this\npaper we present an efficient algorithm for computing the implicit equation of\na canal surface generated by a rational family of spheres. By using Laguerre\nand Lie geometries, we relate the equation of the canal surface to the equation\nof a dual variety of a certain curve in 5-dimensional projective space. We\ndefine the \\mu-basis for arbitrary dimension and give a simple algorithm for\nits computation. This is then applied to the dual variety, which allows us to\ndeduce the implicit equations of the the dual variety, the canal surface and\nany offset to the canal surface.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Jun 2008 15:22:16 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Dohm",
                "Marc",
                "",
                "JAD, INRIA Sophia Antipolis"
            ],
            [
                "Zube",
                "Severinas",
                ""
            ]
        ]
    },
    {
        "id": "0806.4627",
        "submitter": "Thomas Hornung",
        "authors": "Michael Schmidt, Thomas Hornung, Georg Lausen, Christoph Pinkel",
        "title": "SP2Bench: A SPARQL Performance Benchmark",
        "comments": "Conference paper to appear in Proc. ICDE'09",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, the SPARQL query language for RDF has reached the W3C\nrecommendation status. In response to this emerging standard, the database\ncommunity is currently exploring efficient storage techniques for RDF data and\nevaluation strategies for SPARQL queries. A meaningful analysis and comparison\nof these approaches necessitates a comprehensive and universal benchmark\nplatform. To this end, we have developed SP^2Bench, a publicly available,\nlanguage-specific SPARQL performance benchmark. SP^2Bench is settled in the\nDBLP scenario and comprises both a data generator for creating arbitrarily\nlarge DBLP-like documents and a set of carefully designed benchmark queries.\nThe generated documents mirror key characteristics and social-world\ndistributions encountered in the original DBLP data set, while the queries\nimplement meaningful requests on top of this data, covering a variety of SPARQL\noperator constellations and RDF access patterns. As a proof of concept, we\napply SP^2Bench to existing engines and discuss their strengths and weaknesses\nthat follow immediately from the benchmark results.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 30 Jun 2008 15:31:26 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 21 Oct 2008 14:44:17 GMT"
            }
        ],
        "update_date": "2008-10-21",
        "authors_parsed": [
            [
                "Schmidt",
                "Michael",
                ""
            ],
            [
                "Hornung",
                "Thomas",
                ""
            ],
            [
                "Lausen",
                "Georg",
                ""
            ],
            [
                "Pinkel",
                "Christoph",
                ""
            ]
        ]
    },
    {
        "id": "0806.4627",
        "submitter": "Thomas Hornung",
        "authors": "Michael Schmidt, Thomas Hornung, Georg Lausen, Christoph Pinkel",
        "title": "SP2Bench: A SPARQL Performance Benchmark",
        "comments": "Conference paper to appear in Proc. ICDE'09",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, the SPARQL query language for RDF has reached the W3C\nrecommendation status. In response to this emerging standard, the database\ncommunity is currently exploring efficient storage techniques for RDF data and\nevaluation strategies for SPARQL queries. A meaningful analysis and comparison\nof these approaches necessitates a comprehensive and universal benchmark\nplatform. To this end, we have developed SP^2Bench, a publicly available,\nlanguage-specific SPARQL performance benchmark. SP^2Bench is settled in the\nDBLP scenario and comprises both a data generator for creating arbitrarily\nlarge DBLP-like documents and a set of carefully designed benchmark queries.\nThe generated documents mirror key characteristics and social-world\ndistributions encountered in the original DBLP data set, while the queries\nimplement meaningful requests on top of this data, covering a variety of SPARQL\noperator constellations and RDF access patterns. As a proof of concept, we\napply SP^2Bench to existing engines and discuss their strengths and weaknesses\nthat follow immediately from the benchmark results.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 30 Jun 2008 15:31:26 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 21 Oct 2008 14:44:17 GMT"
            }
        ],
        "update_date": "2008-10-21",
        "authors_parsed": [
            [
                "Schmidt",
                "Michael",
                ""
            ],
            [
                "Hornung",
                "Thomas",
                ""
            ],
            [
                "Lausen",
                "Georg",
                ""
            ],
            [
                "Pinkel",
                "Christoph",
                ""
            ]
        ]
    },
    {
        "id": "0806.4703",
        "submitter": "Feng Li",
        "authors": "Feng Li and Shuigeng Zhou",
        "title": "Challenging More Updates: Towards Anonymous Re-publication of Fully\n  Dynamic Datasets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most existing anonymization work has been done on static datasets, which have\nno update and need only one-time publication. Recent studies consider\nanonymizing dynamic datasets with external updates: the datasets are updated\nwith record insertions and/or deletions. This paper addresses a new problem:\nanonymous re-publication of datasets with internal updates, where the attribute\nvalues of each record are dynamically updated. This is an important and\nchallenging problem for attribute values of records are updating frequently in\npractice and existing methods are unable to deal with such a situation.\n  We initiate a formal study of anonymous re-publication of dynamic datasets\nwith internal updates, and show the invalidation of existing methods. We\nintroduce theoretical definition and analysis of dynamic datasets, and present\na general privacy disclosure framework that is applicable to all anonymous\nre-publication problems. We propose a new counterfeited generalization\nprinciple alled m-Distinct to effectively anonymize datasets with both external\nupdates and internal updates. We also develop an algorithm to generalize\ndatasets to meet m-Distinct. The experiments conducted on real-world data\ndemonstrate the effectiveness of the proposed solution.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 28 Jun 2008 16:24:03 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 24 Jul 2008 08:24:57 GMT"
            }
        ],
        "update_date": "2008-07-24",
        "authors_parsed": [
            [
                "Li",
                "Feng",
                ""
            ],
            [
                "Zhou",
                "Shuigeng",
                ""
            ]
        ]
    },
    {
        "id": "0806.4746",
        "submitter": "Alexandr Savinov",
        "authors": "Alexandr Savinov",
        "title": "Concept-Oriented Programming",
        "comments": "46 pages, 8 figures, 11 listings",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Object-oriented programming (OOP) is aimed at describing the structure and\nbehaviour of objects by hiding the mechanism of their representation and access\nin primitive references. In this article we describe an approach, called\nconcept-oriented programming (COP), which focuses on modelling references\nassuming that they also possess application-specific structure and behaviour\naccounting for a great deal or even most of the overall program complexity.\nReferences in COP are completely legalized and get the same status as objects\nwhile the functions are distributed among both objects and references. In order\nto support this design we introduce a new programming construct, called\nconcept, which generalizes conventional classes and concept inclusion relation\ngeneralizing class inheritance. The main advantage of COP is that it allows\nprogrammers to describe two sides of any program: explicitly used functions of\nobjects and intermediate functionality of references having cross-cutting\nnature and executed implicitly behind the scenes during object access.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 29 Jun 2008 10:56:41 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 26 Sep 2010 11:21:01 GMT"
            }
        ],
        "update_date": "2010-09-28",
        "authors_parsed": [
            [
                "Savinov",
                "Alexandr",
                ""
            ]
        ]
    },
    {
        "id": "0806.4749",
        "submitter": "Alexandr Savinov",
        "authors": "Alexandr Savinov",
        "title": "Nested Ordered Sets and their Use for Data Modelling",
        "comments": "15 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present a new approach to data modelling, called the\nconcept-oriented model (CoM), and describe its main features and\ncharacteristics including data semantics and operations. The distinguishing\nfeature of this model is that it is based on the formalism of nested ordered\nsets where any element participates in two structures simultaneously:\nhierarchical (nested) and multi-dimensional (ordered). An element of the model\nis postulated to consist of two parts, called identity and entity, and the\nwhole approach can be naturally broken into two branches: identity modelling\nand entity modelling. We also propose a new query language with the main\nconstruct, called concept, defined as a pair of two classes: identity class and\nentity class. We describe how its operations of projection, de-projection and\nproduct can be used to solve typical data modelling tasks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 29 Jun 2008 11:38:06 GMT"
            }
        ],
        "update_date": "2008-07-01",
        "authors_parsed": [
            [
                "Savinov",
                "Alexandr",
                ""
            ]
        ]
    },
    {
        "id": "0806.4787",
        "submitter": "Herman Haverkort",
        "authors": "Herman Haverkort and Freek van Walderveen",
        "title": "Locality and Bounding-Box Quality of Two-Dimensional Space-Filling\n  Curves",
        "comments": "24 pages, full version of paper to appear in ESA. Difference with\n  first version: minor editing; Fig. 2(m) corrected",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CG cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Space-filling curves can be used to organise points in the plane into\nbounding-box hierarchies (such as R-trees). We develop measures of the\nbounding-box quality of space-filling curves that express how effective\ndifferent space-filling curves are for this purpose. We give general lower\nbounds on the bounding-box quality measures and on locality according to\nGotsman and Lindenbaum for a large class of space-filling curves. We describe a\ngeneric algorithm to approximate these and similar quality measures for any\ngiven curve. Using our algorithm we find good approximations of the locality\nand the bounding-box quality of several known and new space-filling curves.\nSurprisingly, some curves with relatively bad locality by Gotsman and\nLindenbaum's measure, have good bounding-box quality, while the curve with the\nbest-known locality has relatively bad bounding-box quality.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 29 Jun 2008 21:47:15 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 12 Jul 2008 12:02:51 GMT"
            }
        ],
        "update_date": "2008-07-12",
        "authors_parsed": [
            [
                "Haverkort",
                "Herman",
                ""
            ],
            [
                "van Walderveen",
                "Freek",
                ""
            ]
        ]
    },
    {
        "id": "0806.4920",
        "submitter": "Tuyet-Tram Dang-Ngoc",
        "authors": "Tuyet-Tram Dang-Ngoc (PRISM), Georges Gardarin (PRISM)",
        "title": "Conception et Evaluation de XQuery dans une architecture de m\\'ediation\n  \"Tout-XML\"",
        "comments": null,
        "journal-ref": "Revue ISI (Integration de syst\\`emes d'information) : Num\\'ero\n  sp\\'ecial sur les Bases de Donn\\'ees Semi-structur\\'ees 8, 5-6 (2003) 11-25",
        "doi": null,
        "report-no": null,
        "categories": "cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  XML has emerged as the leading language for representing and exchanging data\nnot only on the Web, but also in general in the enterprise. XQuery is emerging\nas the standard query language for XML. Thus, tools are required to mediate\nbetween XML queries and heterogeneous data sources to integrate data in XML.\nThis paper presents the XMedia mediator, a unique tool for integrating and\nquerying disparate heterogeneous information as unified XML views. It describes\nthe mediator architecture and focuses on the unique distributed query\nprocessing technology implemented in this component. Query evaluation is based\non an original XML algebra simply extending classical operators to process\ntuples of tree elements. Further, we present a set of performance evaluation on\na relational benchmark, which leads to discuss possible performance\nenhancements.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 30 Jun 2008 15:23:20 GMT"
            }
        ],
        "update_date": "2008-12-18",
        "authors_parsed": [
            [
                "Dang-Ngoc",
                "Tuyet-Tram",
                "",
                "PRISM"
            ],
            [
                "Gardarin",
                "Georges",
                "",
                "PRISM"
            ]
        ]
    },
    {
        "id": "0807.0087",
        "submitter": "Francesc Rossell\\'o",
        "authors": "Gabriel Cardona, Merce Llabres, Francesc Rossello, Gabriel Valiente",
        "title": "Path lengths in tree-child time consistent hybridization networks",
        "comments": "31 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-bio.PE cs.CE cs.DM q-bio.QM",
        "license": "http://creativecommons.org/licenses/publicdomain/",
        "abstract": "  Hybridization networks are representations of evolutionary histories that\nallow for the inclusion of reticulate events like recombinations,\nhybridizations, or lateral gene transfers. The recent growth in the number of\nhybridization network reconstruction algorithms has led to an increasing\ninterest in the definition of metrics for their comparison that can be used to\nassess the accuracy or robustness of these methods. In this paper we establish\nsome basic results that make it possible the generalization to tree-child time\nconsistent (TCTC) hybridization networks of some of the oldest known metrics\nfor phylogenetic trees: those based on the comparison of the vectors of path\nlengths between leaves. More specifically, we associate to each hybridization\nnetwork a suitably defined vector of `splitted' path lengths between its\nleaves, and we prove that if two TCTC hybridization networks have the same such\nvectors, then they must be isomorphic. Thus, comparing these vectors by means\nof a metric for real-valued vectors defines a metric for TCTC hybridization\nnetworks. We also consider the case of fully resolved hybridization networks,\nwhere we prove that simpler, `non-splitted' vectors can be used.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 Jul 2008 09:13:32 GMT"
            }
        ],
        "update_date": "2008-07-06",
        "authors_parsed": [
            [
                "Cardona",
                "Gabriel",
                ""
            ],
            [
                "Llabres",
                "Merce",
                ""
            ],
            [
                "Rossello",
                "Francesc",
                ""
            ],
            [
                "Valiente",
                "Gabriel",
                ""
            ]
        ]
    },
    {
        "id": "0807.0626",
        "submitter": "Christian Tanguy",
        "authors": "Christian Tanguy",
        "title": "Asymptotic Mean Time To Failure and Higher Moments for Large, Recursive\n  Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper deals with asymptotic expressions of the Mean Time To Failure\n(MTTF) and higher moments for large, recursive, and non-repairable systems in\nthe context of two-terminal reliability. Our aim is to extend the well-known\nresults of the series and parallel cases. We first consider several exactly\nsolvable configurations of identical components with exponential failure-time\ndistribution functions to illustrate different (logarithmic or power-law)\nbehaviors as the size of the system, indexed by an integer n, increases. The\ngeneral case is then addressed: it provides a simple interpretation of the\norigin of the power-law exponent and an efficient asymptotic expression for the\ntotal reliability of large, recursive systems. Finally, we assess the influence\nof the non-exponential character of the component reliability on the\nn-dependence of the MTTF.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 Jul 2008 19:44:36 GMT"
            }
        ],
        "update_date": "2008-07-04",
        "authors_parsed": [
            [
                "Tanguy",
                "Christian",
                ""
            ]
        ]
    },
    {
        "id": "0807.0629",
        "submitter": "Christian Tanguy",
        "authors": "Christian Tanguy",
        "title": "Exact two-terminal reliability of some directed networks",
        "comments": null,
        "journal-ref": "Proceedings of the 6th International Workshop on the Design of\n  Reliable Communication, La Rochelle : France (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The calculation of network reliability in a probabilistic context has long\nbeen an issue of practical and academic importance. Conventional approaches\n(determination of bounds, sums of disjoint products algorithms, Monte Carlo\nevaluations, studies of the reliability polynomials, etc.) only provide\napproximations when the network's size increases, even when nodes do not fail\nand all edges have the same reliability p. We consider here a directed, generic\ngraph of arbitrary size mimicking real-life long-haul communication networks,\nand give the exact, analytical solution for the two-terminal reliability. This\nsolution involves a product of transfer matrices, in which individual\nreliabilities of edges and nodes are taken into account. The special case of\nidentical edge and node reliabilities (p and rho, respectively) is addressed.\nWe consider a case study based on a commonly-used configuration, and assess the\ninfluence of the edges being directed (or not) on various measures of network\nperformance. While the two-terminal reliability, the failure frequency and the\nfailure rate of the connection are quite similar, the locations of complex\nzeros of the two-terminal reliability polynomials exhibit strong differences,\nand various structure transitions at specific values of rho. The present work\ncould be extended to provide a catalog of exactly solvable networks in terms of\nreliability, which could be useful as building blocks for new and improved\nbounds, as well as benchmarks, in the general case.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 Jul 2008 19:47:11 GMT"
            }
        ],
        "update_date": "2008-07-04",
        "authors_parsed": [
            [
                "Tanguy",
                "Christian",
                ""
            ]
        ]
    },
    {
        "id": "0807.0993",
        "submitter": "Damien Hardy",
        "authors": "Damien Hardy (IRISA), Isabelle Puaut (IRISA)",
        "title": "WCET analysis of multi-level set-associative instruction caches",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "RR-6574",
        "categories": "cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With the advent of increasingly complex hardware in real-time embedded\nsystems (processors with performance enhancing features such as pipelines,\ncache hierarchy, multiple cores), many processors now have a set-associative L2\ncache. Thus, there is a need for considering cache hierarchies when validating\nthe temporal behavior of real-time systems, in particular when estimating\ntasks' worst-case execution times (WCETs). To the best of our knowledge, there\nis only one approach for WCET estimation for systems with cache hierarchies\n[Mueller, 1997], which turns out to be unsafe for set-associative caches. In\nthis paper, we highlight the conditions under which the approach described in\n[Mueller, 1997] is unsafe. A safe static instruction cache analysis method is\nthen presented. Contrary to [Mueller, 1997] our method supports set-associative\nand fully associative caches. The proposed method is experimented on\nmedium-size and large programs. We show that the method is most of the time\ntight. We further show that in all cases WCET estimations are much tighter when\nconsidering the cache hierarchy than when considering only the L1 cache. An\nevaluation of the analysis time is conducted, demonstrating that analysing the\ncache hierarchy has a reasonable computation time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 7 Jul 2008 11:20:39 GMT"
            }
        ],
        "update_date": "2009-04-20",
        "authors_parsed": [
            [
                "Hardy",
                "Damien",
                "",
                "IRISA"
            ],
            [
                "Puaut",
                "Isabelle",
                "",
                "IRISA"
            ]
        ]
    },
    {
        "id": "0807.1160",
        "submitter": "Charles Shen",
        "authors": "Charles Shen, Henning Schulzrinne, Erich Nahum",
        "title": "Session Initiation Protocol (SIP) Server Overload Control: Design and\n  Evaluation",
        "comments": "In Proceedings of IPTComm 2008",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A Session Initiation Protocol (SIP) server may be overloaded by\nemergency-induced call volume, ``American Idol'' style flash crowd effects or\ndenial of service attacks. The SIP server overload problem is interesting\nespecially because the costs of serving or rejecting a SIP session can be\nsimilar. For this reason, the built-in SIP overload control mechanism based on\ngenerating rejection messages cannot prevent the server from entering\ncongestion collapse under heavy load. The SIP overload problem calls for a\npushback control solution in which the potentially overloaded receiving server\nmay notify its upstream sending servers to have them send only the amount of\nload within the receiving server's processing capacity. The pushback framework\ncan be achieved by either a rate-based feedback or a window-based feedback. The\ncenterpiece of the feedback mechanism is the algorithm used to generate load\nregulation information. We propose three new window-based feedback algorithms\nand evaluate them together with two existing rate-based feedback algorithms. We\ncompare the different algorithms in terms of the number of tuning parameters\nand performance under both steady and variable load. Furthermore, we identify\ntwo categories of fairness requirements for SIP overload control, namely,\nuser-centric and provider-centric fairness. With the introduction of a new\ndouble-feed SIP overload control architecture, we show how the algorithms can\nmeet those fairness criteria.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 8 Jul 2008 03:23:41 GMT"
            }
        ],
        "update_date": "2008-07-09",
        "authors_parsed": [
            [
                "Shen",
                "Charles",
                ""
            ],
            [
                "Schulzrinne",
                "Henning",
                ""
            ],
            [
                "Nahum",
                "Erich",
                ""
            ]
        ]
    },
    {
        "id": "0807.1162",
        "submitter": "Charles Shen",
        "authors": "Charles Shen and Henning Schulzrinne",
        "title": "Measurement and Evaluation of ENUM Server Performance",
        "comments": null,
        "journal-ref": "Proceedings of IEEE ICC 2007 p. 1967-1972",
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  ENUM is a DNS-based protocol standard for mapping E.164 telephone numbers to\nInternet Uniform Resource Identifiers (URIs). It places unique requirements on\nthe existing DNS infrastructure, such as data scalability, query throughput,\nresponse time, and database update rates. This paper measures and evaluates the\nperformance of existing name server implementation as ENUM servers. We compared\nPowerDNS (PDNS), BIND and Navitas. Results show that BIND is not suitable for\nENUM due to its poor scaling property. Both PDNS and Navitas can serve ENUM.\nHowever, Navitas turns out to be highly optimized and clearly outperforms PDNS\nin all aspects we have tested. We also instrumented the PDNS server to identify\nits performance bottleneck and investigated ways to improve it.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 8 Jul 2008 03:51:17 GMT"
            }
        ],
        "update_date": "2008-07-09",
        "authors_parsed": [
            [
                "Shen",
                "Charles",
                ""
            ],
            [
                "Schulzrinne",
                "Henning",
                ""
            ]
        ]
    },
    {
        "id": "0807.1211",
        "submitter": "James Cheney",
        "authors": "James Cheney",
        "title": "Flux: FunctionaL Updates for XML (extended report)",
        "comments": "Extended version of ICFP 2008 paper",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  XML database query languages have been studied extensively, but XML database\nupdates have received relatively little attention, and pose many challenges to\nlanguage design. We are developing an XML update language called Flux, which\nstands for FunctionaL Updates for XML, drawing upon ideas from functional\nprogramming languages. In prior work, we have introduced a core language for\nFlux with a clear operational semantics and a sound, decidable static type\nsystem based on regular expression types.\n  Our initial proposal had several limitations. First, it lacked support for\nrecursive types or update procedures. Second, although a high-level source\nlanguage can easily be translated to the core language, it is difficult to\npropagate meaningful type errors from the core language back to the source.\nThird, certain updates are well-formed yet contain path errors, or ``dead''\nsubexpressions which never do any useful work. It would be useful to detect\npath errors, since they often represent errors or optimization opportunities.\n  In this paper, we address all three limitations. Specifically, we present an\nimproved, sound type system that handles recursion. We also formalize a source\nupdate language and give a translation to the core language that preserves and\nreflects typability. We also develop a path-error analysis (a form of dead-code\nanalysis) for updates.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 8 Jul 2008 17:10:51 GMT"
            }
        ],
        "update_date": "2008-07-09",
        "authors_parsed": [
            [
                "Cheney",
                "James",
                ""
            ]
        ]
    },
    {
        "id": "0807.1228",
        "submitter": "Michele Garetto Dr.",
        "authors": "Michele Garetto and Emilio Leonardi",
        "title": "Restricted Mobility Improves Delay-Throughput Trade-offs in Mobile\n  Ad-Hoc Networks",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we analyze asymptotic delay-throughput trade-offs in mobile\nad-hoc networks comprising heterogeneous nodes with restricted mobility. We\nshow that node spatial heterogeneity has the ability to drastically improve\nupon existing scaling laws established under the assumption that nodes are\nidentical and uniformly visit the entire network area. In particular, we\nconsider the situation in which each node moves around its own home-point\naccording to a restricted mobility process which results into a spatial\nstationary distribution that decays as a power law of exponent delta with the\ndistance from the home-point. For such restricted mobility model, we propose a\nnovel class of scheduling and routing schemes, which significantly outperforms\nall delay-throughput results previously obtained in the case of identical\nnodes. In particular, for delta = 2 it is possible to achieve almost constant\ndelay and almost constant per-node throughput (except for a poly-logarithmic\nfactor) as the number of nodes increases, even without resorting to\nsophisticated coding or signal processing techniques.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 8 Jul 2008 13:04:05 GMT"
            }
        ],
        "update_date": "2008-07-09",
        "authors_parsed": [
            [
                "Garetto",
                "Michele",
                ""
            ],
            [
                "Leonardi",
                "Emilio",
                ""
            ]
        ]
    },
    {
        "id": "0807.1475",
        "submitter": "Maziar Nekovee",
        "authors": "Maziar Nekovee",
        "title": "Simulations of Large-scale WiFi-based Wireless Networks:\n  Interdisciplinary Challenges and Applications",
        "comments": "Future Generation Computer Systems, Article in Press",
        "journal-ref": null,
        "doi": "10.1016/j.future.2008.05.007",
        "report-no": null,
        "categories": "cs.CE cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Wireless Fidelity (WiFi) is the fastest growing wireless technology to date.\nIn addition to providing wire-free connectivity to the Internet WiFi technology\nalso enables mobile devices to connect directly to each other and form highly\ndynamic wireless adhoc networks. Such distributed networks can be used to\nperform cooperative communication tasks such ad data routing and information\ndissemination in the absence of a fixed infrastructure. Furthermore, adhoc\ngrids composed of wirelessly networked portable devices are emerging as a new\nparadigm in grid computing. In this paper we review computational and\nalgorithmic challenges of high-fidelity simulations of such WiFi-based wireless\ncommunication and computing networks, including scalable topology maintenance,\nmobility modelling, parallelisation and synchronisation. We explore\nsimilarities and differences between the simulations of these networks and\nsimulations of interacting many-particle systems, such as molecular dynamics\n(MD) simulations. We show how the cell linked-list algorithm which we have\nadapted from our MD simulations can be used to greatly improve the\ncomputational performance of wireless network simulators in the presence of\nmobility, and illustrate with an example from our simulation studies of worm\nattacks on mobile wireless adhoc networks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 Jul 2008 16:04:37 GMT"
            }
        ],
        "update_date": "2008-07-10",
        "authors_parsed": [
            [
                "Nekovee",
                "Maziar",
                ""
            ]
        ]
    },
    {
        "id": "0807.2043",
        "submitter": "Aikaterini Mitrokotsa",
        "authors": "Aikaterini Mitrokotsa and Christos Dimitrakakis and Christos\n  Douligeris",
        "title": "Intrusion Detection Using Cost-Sensitive Classification",
        "comments": "13 pages, 6 figures, presented at EC2ND 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.CV cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Intrusion Detection is an invaluable part of computer networks defense. An\nimportant consideration is the fact that raising false alarms carries a\nsignificantly lower cost than not detecting at- tacks. For this reason, we\nexamine how cost-sensitive classification methods can be used in Intrusion\nDetection systems. The performance of the approach is evaluated under different\nexperimental conditions, cost matrices and different classification models, in\nterms of expected cost, as well as detection and false alarm rates. We find\nthat even under unfavourable conditions, cost-sensitive classification can\nimprove performance significantly, if only slightly.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 13 Jul 2008 16:54:13 GMT"
            }
        ],
        "update_date": "2008-07-15",
        "authors_parsed": [
            [
                "Mitrokotsa",
                "Aikaterini",
                ""
            ],
            [
                "Dimitrakakis",
                "Christos",
                ""
            ],
            [
                "Douligeris",
                "Christos",
                ""
            ]
        ]
    },
    {
        "id": "0807.2047",
        "submitter": "Mahzad Kalantari",
        "authors": "Mahzad Kalantari, Franck Jung, JeanPierre Guedon, Nicolas Paparoditis",
        "title": "The Five Points Pose Problem : A New and Accurate Solution Adapted to\n  any Geometric Configuration",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of this paper is to estimate directly the rotation and translation\nbetween two stereoscopic images with the help of five homologous points. The\nmethodology presented does not mix the rotation and translation parameters,\nwhich is comparably an important advantage over the methods using the\nwell-known essential matrix. This results in correct behavior and accuracy for\nsituations otherwise known as quite unfavorable, such as planar scenes, or\npanoramic sets of images (with a null base length), while providing quite\ncomparable results for more \"standard\" cases. The resolution of the algebraic\npolynomials resulting from the modeling of the coplanarity constraint is made\nwith the help of powerful algebraic solver tools (the Groebner bases and the\nRational Univariate Representation).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 13 Jul 2008 18:37:06 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 14 Jul 2008 20:17:12 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 16 Jul 2008 15:20:03 GMT"
            }
        ],
        "update_date": "2008-07-16",
        "authors_parsed": [
            [
                "Kalantari",
                "Mahzad",
                ""
            ],
            [
                "Jung",
                "Franck",
                ""
            ],
            [
                "Guedon",
                "JeanPierre",
                ""
            ],
            [
                "Paparoditis",
                "Nicolas",
                ""
            ]
        ]
    },
    {
        "id": "0807.2108",
        "submitter": "Kalyana Babu Nakshatrala",
        "authors": "K.B.Nakshatrala, A. Prakash, K.D.Hjelmstad",
        "title": "On dual Schur domain decomposition method for linear first-order\n  transient problems",
        "comments": "22 Figures, 49 pages (double spacing using amsart)",
        "journal-ref": null,
        "doi": "10.1016/j.jcp.2009.07.016",
        "report-no": null,
        "categories": "cs.NA cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses some numerical and theoretical aspects of dual Schur\ndomain decomposition methods for linear first-order transient partial\ndifferential equations. In this work, we consider the trapezoidal family of\nschemes for integrating the ordinary differential equations (ODEs) for each\nsubdomain and present four different coupling methods, corresponding to\ndifferent algebraic constraints, for enforcing kinematic continuity on the\ninterface between the subdomains.\n  Method 1 (d-continuity) is based on the conventional approach using\ncontinuity of the primary variable and we show that this method is unstable for\na lot of commonly used time integrators including the mid-point rule. To\nalleviate this difficulty, we propose a new Method 2 (Modified d-continuity)\nand prove its stability for coupling all time integrators in the trapezoidal\nfamily (except the forward Euler). Method 3 (v-continuity) is based on\nenforcing the continuity of the time derivative of the primary variable.\nHowever, this constraint introduces a drift in the primary variable on the\ninterface. We present Method 4 (Baumgarte stabilized) which uses Baumgarte\nstabilization to limit this drift and we derive bounds for the stabilization\nparameter to ensure stability.\n  Our stability analysis is based on the ``energy'' method, and one of the main\ncontributions of this paper is the extension of the energy method (which was\npreviously introduced in the context of numerical methods for ODEs) to assess\nthe stability of numerical formulations for index-2 differential-algebraic\nequations (DAEs).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 Jul 2008 08:10:33 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 23 Jul 2009 21:20:22 GMT"
            }
        ],
        "update_date": "2015-05-13",
        "authors_parsed": [
            [
                "Nakshatrala",
                "K. B.",
                ""
            ],
            [
                "Prakash",
                "A.",
                ""
            ],
            [
                "Hjelmstad",
                "K. D.",
                ""
            ]
        ]
    },
    {
        "id": "0807.2282",
        "submitter": "Arfan Ghani Mr.",
        "authors": "Arfan Ghani, Martin McGinnity, Liam Maguire, Jim Harkin",
        "title": "Hardware/Software Co-Design for Spike Based Recognition",
        "comments": "6 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.AI cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The practical applications based on recurrent spiking neurons are limited due\nto their non-trivial learning algorithms. The temporal nature of spiking\nneurons is more favorable for hardware implementation where signals can be\nrepresented in binary form and communication can be done through the use of\nspikes. This work investigates the potential of recurrent spiking neurons\nimplementations on reconfigurable platforms and their applicability in temporal\nbased applications. A theoretical framework of reservoir computing is\ninvestigated for hardware/software implementation. In this framework, only\nreadout neurons are trained which overcomes the burden of training at the\nnetwork level. These recurrent neural networks are termed as microcircuits\nwhich are viewed as basic computational units in cortical computation. This\npaper investigates the potential of recurrent neural reservoirs and presents a\nnovel hardware/software strategy for their implementation on FPGAs. The design\nis implemented and the functionality is tested in the context of speech\nrecognition application.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 Jul 2008 23:44:47 GMT"
            }
        ],
        "update_date": "2008-07-16",
        "authors_parsed": [
            [
                "Ghani",
                "Arfan",
                ""
            ],
            [
                "McGinnity",
                "Martin",
                ""
            ],
            [
                "Maguire",
                "Liam",
                ""
            ],
            [
                "Harkin",
                "Jim",
                ""
            ]
        ]
    },
    {
        "id": "0807.2382",
        "submitter": "Michel Rueher",
        "authors": "Alexandre Goldsztejn (I3S), Yahia Lebbah (I3S), Claude Michel (I3S),\n  Michel Rueher (I3S)",
        "title": "Revisiting the upper bounding process in a safe Branch and Bound\n  algorithm",
        "comments": "Optimization, continuous domains, nonlinear constraint problems, safe\n  constraint based approaches; 14th International Conference on Principles and\n  Practice of Constraint Programming, Sydney : Australie (2008)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NA cs.MS math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Finding feasible points for which the proof succeeds is a critical issue in\nsafe Branch and Bound algorithms which handle continuous problems. In this\npaper, we introduce a new strategy to compute very accurate approximations of\nfeasible points. This strategy takes advantage of the Newton method for\nunder-constrained systems of equations and inequalities. More precisely, it\nexploits the optimal solution of a linear relaxation of the problem to compute\nefficiently a promising upper bound. First experiments on the Coconuts\nbenchmarks demonstrate that this approach is very effective.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 15 Jul 2008 14:18:23 GMT"
            }
        ],
        "update_date": "2008-07-16",
        "authors_parsed": [
            [
                "Goldsztejn",
                "Alexandre",
                "",
                "I3S"
            ],
            [
                "Lebbah",
                "Yahia",
                "",
                "I3S"
            ],
            [
                "Michel",
                "Claude",
                "",
                "I3S"
            ],
            [
                "Rueher",
                "Michel",
                "",
                "I3S"
            ]
        ]
    },
    {
        "id": "0807.2928",
        "submitter": "Guoshen Yu",
        "authors": "Guoshen Yu and Jean-Jacques Slotine",
        "title": "Visual Grouping by Neural Oscillators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.NE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Distributed synchronization is known to occur at several scales in the brain,\nand has been suggested as playing a key functional role in perceptual grouping.\nState-of-the-art visual grouping algorithms, however, seem to give\ncomparatively little attention to neural synchronization analogies. Based on\nthe framework of concurrent synchronization of dynamic systems, simple networks\nof neural oscillators coupled with diffusive connections are proposed to solve\nvisual grouping problems. Multi-layer algorithms and feedback mechanisms are\nalso studied. The same algorithm is shown to achieve promising results on\nseveral classical visual grouping problems, including point clustering, contour\nintegration and image segmentation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 18 Jul 2008 11:23:27 GMT"
            }
        ],
        "update_date": "2008-07-21",
        "authors_parsed": [
            [
                "Yu",
                "Guoshen",
                ""
            ],
            [
                "Slotine",
                "Jean-Jacques",
                ""
            ]
        ]
    },
    {
        "id": "0807.3096",
        "submitter": "Giuseppina Guatteri",
        "authors": "Giuseppina Guatteri",
        "title": "Stochastic Maximum Principle for a PDEs with noise and control on the\n  boundary",
        "comments": "15pgs",
        "journal-ref": "Systems Control Lett. 60 (2011), no. 3, 198/204",
        "doi": null,
        "report-no": null,
        "categories": "math.PR cs.SY math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we prove necessary conditions for optimality of a stochastic\ncontrol problem for a class of stochastic partial differential equations that\nis controlled through the boundary. This kind of problems can be interpreted as\na stochastic control problem for an evolution system in an Hilbert space. The\nregularity of the solution of the adjoint equation, that is a backward\nstochastic equation in infinite dimension, plays a crucial role in the\nformulation of the maximum principle.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 19 Jul 2008 14:15:52 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 29 Jan 2010 21:39:43 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 22 Feb 2011 14:45:46 GMT"
            }
        ],
        "update_date": "2016-12-05",
        "authors_parsed": [
            [
                "Guatteri",
                "Giuseppina",
                ""
            ]
        ]
    },
    {
        "id": "0807.3451",
        "submitter": "Etienne Payet",
        "authors": "Etienne Payet and Fred Mesnard",
        "title": "A Non-Termination Criterion for Binary Constraint Logic Programs",
        "comments": "32 pages. Long version of a paper accepted for publication in Theory\n  and Practice of Logic Programming (TPLP)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On the one hand, termination analysis of logic programs is now a fairly\nestablished research topic within the logic programming community. On the other\nhand, non-termination analysis seems to remain a much less attractive subject.\nIf we divide this line of research into two kinds of approaches: dynamic versus\nstatic analysis, this paper belongs to the latter. It proposes a criterion for\ndetecting non-terminating atomic queries with respect to binary CLP rules,\nwhich strictly generalizes our previous works on this subject. We give a\ngeneric operational definition and an implemented logical form of this\ncriterion. Then we show that the logical form is correct and complete with\nrespect to the operational definition.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 22 Jul 2008 13:51:33 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 12 Dec 2008 11:26:03 GMT"
            },
            {
                "version": "v3",
                "created": "Sat, 10 Jan 2009 11:09:46 GMT"
            }
        ],
        "update_date": "2009-01-10",
        "authors_parsed": [
            [
                "Payet",
                "Etienne",
                ""
            ],
            [
                "Mesnard",
                "Fred",
                ""
            ]
        ]
    },
    {
        "id": "0807.3879",
        "submitter": "Alessandra Di Pierro",
        "authors": "Alessandra Di Pierro, Chris Hankin and Herbert Wiklicky",
        "title": "Quantifying Timing Leaks and Cost Optimisation",
        "comments": "16 pages, 2 figures, 4 tables. A shorter version is included in the\n  proceedings of ICICS'08 - 10th International Conference on Information and\n  Communications Security, 20-22 October, 2008 Birmingham, UK",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a new notion of security against timing attacks where the attacker\nis able to simultaneously observe the execution time of a program and the\nprobability of the values of low variables. We then show how to measure the\nsecurity of a program with respect to this notion via a computable estimate of\nthe timing leakage and use this estimate for cost optimisation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 Jul 2008 13:17:19 GMT"
            }
        ],
        "update_date": "2008-07-25",
        "authors_parsed": [
            [
                "Di Pierro",
                "Alessandra",
                ""
            ],
            [
                "Hankin",
                "Chris",
                ""
            ],
            [
                "Wiklicky",
                "Herbert",
                ""
            ]
        ]
    },
    {
        "id": "0807.3933",
        "submitter": "Frederic Le Mouel",
        "authors": "Fr\\'ed\\'eric Le Mou\\\"el (INRIA Rh\\^one-Alpes / CITI), Noha Ibrahim\n  (INRIA Rh\\^one-Alpes / CITI), St\\'ephane Fr\\'enot (INRIA Rh\\^one-Alpes /\n  CITI)",
        "title": "Interface Matching and Combining Techniques for Services Integration",
        "comments": null,
        "journal-ref": "Dans 3er Congreso Nacional de Ciencias de la Computacion\n  (CNCC'2005) (2005)",
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The development of many highly dynamic environments, like pervasive\nenvironments, introduces the possibility to use geographically close-related\nservices. Dynamically integrating and unintegrating these services in running\napplications is a key challenge for this use. In this article, we classify\nservice integration issues according to interfaces exported by services and\ninternal combining techniques. We also propose a contextual integration\nservice, IntegServ, and an interface, Integrable, for developing services.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 Jul 2008 17:51:07 GMT"
            }
        ],
        "update_date": "2008-07-25",
        "authors_parsed": [
            [
                "Mou\u00ebl",
                "Fr\u00e9d\u00e9ric Le",
                "",
                "INRIA Rh\u00f4ne-Alpes / CITI"
            ],
            [
                "Ibrahim",
                "Noha",
                "",
                "INRIA Rh\u00f4ne-Alpes / CITI"
            ],
            [
                "Fr\u00e9not",
                "St\u00e9phane",
                "",
                "INRIA Rh\u00f4ne-Alpes /\n  CITI"
            ]
        ]
    },
    {
        "id": "0807.3979",
        "submitter": "Maria Chiara Meo",
        "authors": "Maurizio Gabbrielli, Maria Chiara Meo, Paolo Tacchella",
        "title": "Unfolding in CHR",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Program transformation is an appealing technique which allows to improve\nrun-time efficiency, space-consumption and more generally to optimize a given\nprogram. Essentially it consists of a sequence of syntactic program\nmanipulations which preserves some kind of semantic equivalence. One of the\nbasic operations which is used by most program transformation systems is\nunfolding which consists in the replacement of a procedure call by its\ndefinition. While there is a large body of literature on transformation and\nunfolding of sequential programs, very few papers have addressed this issue for\nconcurrent languages and, to the best of our knowledge, no other has considered\nunfolding of CHR programs.\n  This paper defines a correct unfolding system for CHR programs. We define an\nunfolding rule, show its correctness and discuss some conditions which can be\nused to delete an unfolded rule while preserving the program meaning. We prove\nthat confluence and termination properties are preserved by the above\ntransformations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 Jul 2008 15:21:46 GMT"
            }
        ],
        "update_date": "2008-07-28",
        "authors_parsed": [
            [
                "Gabbrielli",
                "Maurizio",
                ""
            ],
            [
                "Meo",
                "Maria Chiara",
                ""
            ],
            [
                "Tacchella",
                "Paolo",
                ""
            ]
        ]
    },
    {
        "id": "0807.4478",
        "submitter": "Carlos Miravet",
        "authors": "Carlos Miravet, Luis Pascual, Eloise Krouch, Juan Manuel del Cura",
        "title": "An Image-Based Sensor System for Autonomous Rendez-Vous with\n  Uncooperative Satellites",
        "comments": "12 pages, 13 figures. Presented in the 7th International ESA\n  Conference on Guidance, Navigation & Control Systems, Tralee, Ireland, 2008",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.AI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper are described the image processing algorithms developed by\nSENER, Ingenieria y Sistemas to cope with the problem of image-based,\nautonomous rendez-vous (RV) with an orbiting satellite. The methods developed\nhave a direct application in the OLEV (Orbital Life Extension Extension\nVehicle) mission. OLEV is a commercial mission under development by a\nconsortium formed by Swedish Space Corporation, Kayser-Threde and SENER, aimed\nto extend the operational life of geostationary telecommunication satellites by\nsupplying them control, navigation and guidance services. OLEV is planned to\nuse a set of cameras to determine the angular position and distance to the\nclient satellite during the complete phases of rendez-vous and docking, thus\nenabling the operation with satellites not equipped with any specific\nnavigational aid to provide support during the approach. The ability to operate\nwith un-equipped client satellites significantly expands the range of\napplicability of the system under development, compared to other competing\nvideo technologies already tested in previous spatial missions, such as the\nones described here below.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Jul 2008 15:46:02 GMT"
            }
        ],
        "update_date": "2008-07-29",
        "authors_parsed": [
            [
                "Miravet",
                "Carlos",
                ""
            ],
            [
                "Pascual",
                "Luis",
                ""
            ],
            [
                "Krouch",
                "Eloise",
                ""
            ],
            [
                "del Cura",
                "Juan Manuel",
                ""
            ]
        ]
    },
    {
        "id": "0807.4619",
        "submitter": "Ian Petersen",
        "authors": "A. J. Shaiju, I. R. Petersen, and M. R. James",
        "title": "Guaranteed Cost LQG Control of Uncertain Linear Quantum Stochastic\n  Systems",
        "comments": "15 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "quant-ph cs.SY math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we formulate and solve a guaranteed cost control problem for a\nclass of uncertain linear stochastic quantum systems. For these quantum\nsystems, a connection with an associated classical (non-quantum) system is\nfirst established. Using this connection, the desired guaranteed cost results\nare established. The theory presented is illustrated using an example from\nquantum optics.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 29 Jul 2008 10:02:43 GMT"
            }
        ],
        "update_date": "2011-07-29",
        "authors_parsed": [
            [
                "Shaiju",
                "A. J.",
                ""
            ],
            [
                "Petersen",
                "I. R.",
                ""
            ],
            [
                "James",
                "M. R.",
                ""
            ]
        ]
    },
    {
        "id": "0807.4701",
        "submitter": "Amelia Sparavigna",
        "authors": "A. Sparavigna, R. Marazzato",
        "title": "An image processing analysis of skin textures",
        "comments": null,
        "journal-ref": "Skin Research and Technology, Volume 16 Issue 2, Pages 161 - 167,\n  2010",
        "doi": "10.1111/j.1600-0846.2009.00413.x",
        "report-no": null,
        "categories": "cs.CV",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Colour and coarseness of skin are visually different. When image processing\nis involved in the skin analysis, it is important to quantitatively evaluate\nsuch differences using texture features. In this paper, we discuss a texture\nanalysis and measurements based on a statistical approach to the pattern\nrecognition. Grain size and anisotropy are evaluated with proper diagrams. The\npossibility to determine the presence of pattern defects is also discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 29 Jul 2008 16:28:44 GMT"
            }
        ],
        "update_date": "2010-05-11",
        "authors_parsed": [
            [
                "Sparavigna",
                "A.",
                ""
            ],
            [
                "Marazzato",
                "R.",
                ""
            ]
        ]
    },
    {
        "id": "0807.5120",
        "submitter": "Andreas Grau",
        "authors": "Stefan Dirnstorfer, Andreas J. Grau",
        "title": "Accelerated Option Pricing in Multiple Scenarios",
        "comments": "17 pages: Page 17, References corrected",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper covers a massive acceleration of Monte-Carlo based pricing method\nfor financial products and financial derivatives. The method is applicable in\nrisk management settings, where a financial product has to be priced under a\nnumber of potential future scenarios. Instead of starting a separate nested\nMonte Carlo simulation for each scenario under consideration, the new method\ncovers the utilization of very few representative nested simulations and\nestimating the product prices at each scenario by a smoothing method based on\nthe state-space. This smoothing technique can be e.g. non-parametric regression\nor kernel smoothing.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 31 Jul 2008 17:40:55 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 30 Sep 2008 10:24:51 GMT"
            }
        ],
        "update_date": "2008-09-30",
        "authors_parsed": [
            [
                "Dirnstorfer",
                "Stefan",
                ""
            ],
            [
                "Grau",
                "Andreas J.",
                ""
            ]
        ]
    },
    {
        "id": "0808.0540",
        "submitter": "Paul Tarau",
        "authors": "Paul Tarau",
        "title": "Executable Set Theory and Arithmetic Encodings in Prolog",
        "comments": "Unpublished draft",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.DM cs.DS cs.MS cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper is organized as a self-contained literate Prolog program that\nimplements elements of an executable finite set theory with focus on\ncombinatorial generation and arithmetic encodings. The complete Prolog code is\navailable at http://logic.csci.unt.edu/tarau/research/2008/pHFS.zip . First,\nranking and unranking functions for some \"mathematically elegant\" data types in\nthe universe of Hereditarily Finite Sets with Urelements are provided,\nresulting in arithmetic encodings for powersets, hypergraphs, ordinals and\nchoice functions. After implementing a digraph representation of Hereditarily\nFinite Sets we define {\\em decoration functions} that can recover well-founded\nsets from encodings of their associated acyclic digraphs. We conclude with an\nencoding of arbitrary digraphs and discuss a concept of duality induced by the\nset membership relation. In the process, we uncover the surprising possibility\nof internally sharing isomorphic objects, independently of their language level\ntypes and meanings.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 5 Aug 2008 04:59:56 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Tarau",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0808.0540",
        "submitter": "Paul Tarau",
        "authors": "Paul Tarau",
        "title": "Executable Set Theory and Arithmetic Encodings in Prolog",
        "comments": "Unpublished draft",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.DM cs.DS cs.MS cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper is organized as a self-contained literate Prolog program that\nimplements elements of an executable finite set theory with focus on\ncombinatorial generation and arithmetic encodings. The complete Prolog code is\navailable at http://logic.csci.unt.edu/tarau/research/2008/pHFS.zip . First,\nranking and unranking functions for some \"mathematically elegant\" data types in\nthe universe of Hereditarily Finite Sets with Urelements are provided,\nresulting in arithmetic encodings for powersets, hypergraphs, ordinals and\nchoice functions. After implementing a digraph representation of Hereditarily\nFinite Sets we define {\\em decoration functions} that can recover well-founded\nsets from encodings of their associated acyclic digraphs. We conclude with an\nencoding of arbitrary digraphs and discuss a concept of duality induced by the\nset membership relation. In the process, we uncover the surprising possibility\nof internally sharing isomorphic objects, independently of their language level\ntypes and meanings.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 5 Aug 2008 04:59:56 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Tarau",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0808.0554",
        "submitter": "Paul Tarau",
        "authors": "Paul Tarau",
        "title": "Ranking and Unranking of Hereditarily Finite Functions and Permutations",
        "comments": "unpublished draft",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Prolog's ability to return multiple answers on backtracking provides an\nelegant mechanism to derive reversible encodings of combinatorial objects as\nNatural Numbers i.e. {\\em ranking} and {\\em unranking} functions. Starting from\na generalization of Ackerman's encoding of Hereditarily Finite Sets with\nUrelements and a novel tupling/untupling operation, we derive encodings for\nFinite Functions and use them as building blocks for an executable theory of\n{\\em Hereditarily Finite Functions}. The more difficult problem of {\\em\nranking} and {\\em unranking} {\\em Hereditarily Finite Permutations} is then\ntackled using Lehmer codes and factoradics.\n  The paper is organized as a self-contained literate Prolog program available\nat \\url{http://logic.csci.unt.edu/tarau/research/2008/pHFF.zip}\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 5 Aug 2008 05:20:51 GMT"
            }
        ],
        "update_date": "2008-08-06",
        "authors_parsed": [
            [
                "Tarau",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0808.0555",
        "submitter": "Paul Tarau",
        "authors": "Paul Tarau",
        "title": "Pairing Functions, Boolean Evaluation and Binary Decision Diagrams in\n  Prolog",
        "comments": "also in the informal proceedings of CICLOPS 2008 workshop at:\n  http://clip.dia.fi.upm.es/Conferences/CICLOPS-2008/CICLOPS-2008-proceedings.pdf",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A \"pairing function\" J associates a unique natural number z to any two\nnatural numbers x,y such that for two \"unpairing functions\" K and L, the\nequalities K(J(x,y))=x, L(J(x,y))=y and J(K(z),L(z))=z hold. Using pairing\nfunctions on natural number representations of truth tables, we derive an\nencoding for Binary Decision Diagrams with the unique property that its boolean\nevaluation faithfully mimics its structural conversion to a a natural number\nthrough recursive application of a matching pairing function. We then use this\nresult to derive {\\em ranking} and {\\em unranking} functions for BDDs and\nreduced BDDs. The paper is organized as a self-contained literate Prolog\nprogram, available at http://logic.csci.unt.edu/tarau/research/2008/pBDD.zip\n  Keywords: logic programming and computational mathematics, pairing/unpairing\nfunctions, encodings of boolean functions, binary decision diagrams, natural\nnumber representations of truth tables\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 5 Aug 2008 05:33:09 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 4 Feb 2009 03:25:22 GMT"
            }
        ],
        "update_date": "2009-02-04",
        "authors_parsed": [
            [
                "Tarau",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0808.0556",
        "submitter": "Paul Tarau",
        "authors": "Paul Tarau",
        "title": "Logic Engines as Interactors",
        "comments": "unpublished draft",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.MA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We introduce a new programming language construct, Interactors, supporting\nthe agent-oriented view that programming is a dialog between simple,\nself-contained, autonomous building blocks.\n  We define Interactors as an abstraction of answer generation and refinement\nin Logic Engines resulting in expressive language extension and metaprogramming\npatterns, including emulation of Prolog's dynamic database.\n  A mapping between backtracking based answer generation in the callee and\n\"forward\" recursion in the caller enables interaction between different\nbranches of the callee's search process and provides simplified design patterns\nfor algorithms involving combinatorial generation and infinite answer streams.\n  Interactors extend language constructs like Ruby, Python and C#'s multiple\ncoroutining block returns through yield statements and they can emulate the\naction of monadic constructs and catamorphisms in functional languages.\n  Keywords: generalized iterators, logic engines, agent oriented programming\nlanguage constructs, interoperation with stateful objects, metaprogramming\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 5 Aug 2008 05:48:32 GMT"
            }
        ],
        "update_date": "2008-08-07",
        "authors_parsed": [
            [
                "Tarau",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0808.0586",
        "submitter": "Xavier Leroy",
        "authors": "Xavier Leroy (INRIA Rocquencourt), Herv\\'e Grall (INRIA Rennes, LINA)",
        "title": "Coinductive big-step operational semantics",
        "comments": null,
        "journal-ref": "Information and Computation (2007)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using a call-by-value functional language as an example, this article\nillustrates the use of coinductive definitions and proofs in big-step\noperational semantics, enabling it to describe diverging evaluations in\naddition to terminating evaluations. We formalize the connections between the\ncoinductive big-step semantics and the standard small-step semantics, proving\nthat both semantics are equivalent. We then study the use of coinductive\nbig-step semantics in proofs of type soundness and proofs of semantic\npreservation for compilers. A methodological originality of this paper is that\nall results have been proved using the Coq proof assistant. We explain the\nproof-theoretic presentation of coinductive definitions and proofs offered by\nCoq, and show that it facilitates the discovery and the presentation of the\nresults.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 5 Aug 2008 14:47:32 GMT"
            }
        ],
        "update_date": "2008-08-06",
        "authors_parsed": [
            [
                "Leroy",
                "Xavier",
                "",
                "INRIA Rocquencourt"
            ],
            [
                "Grall",
                "Herv\u00e9",
                "",
                "INRIA Rennes, LINA"
            ]
        ]
    },
    {
        "id": "0808.0753",
        "submitter": "Paul Tarau",
        "authors": "Paul Tarau",
        "title": "Ranking Catamorphisms and Unranking Anamorphisms on Hereditarily Finite\n  Datatypes",
        "comments": "unpublished draft",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.DM cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using specializations of unfold and fold on a generic tree data type we\nderive unranking and ranking functions providing natural number encodings for\nvarious Hereditarily Finite datatypes.\n  In this context, we interpret unranking operations as instances of a generic\nanamorphism and ranking operations as instances of the corresponding\ncatamorphism.\n  Starting with Ackerman's Encoding from Hereditarily Finite Sets to Natural\nNumbers we define pairings and tuple encodings that provide building blocks for\na theory of Hereditarily Finite Functions.\n  The more difficult problem of ranking and unranking Hereditarily Finite\nPermutations is then tackled using Lehmer codes and factoradics.\n  The self-contained source code of the paper, as generated from a literate\nHaskell program, is available at\n\\url{http://logic.csci.unt.edu/tarau/research/2008/fFUN.zip}.\n  Keywords: ranking/unranking, pairing/tupling functions, Ackermann encoding,\nhereditarily finite sets, hereditarily finite functions, permutations and\nfactoradics, computational mathematics, Haskell data representations\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 6 Aug 2008 00:54:05 GMT"
            }
        ],
        "update_date": "2008-08-07",
        "authors_parsed": [
            [
                "Tarau",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0808.0754",
        "submitter": "Paul Tarau",
        "authors": "Paul Tarau",
        "title": "A Functional Hitchhiker's Guide to Hereditarily Finite Sets, Ackermann\n  Encodings and Pairing Functions",
        "comments": "unpublished draft",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.DM cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper is organized as a self-contained literate Haskell program that\nimplements elements of an executable finite set theory with focus on\ncombinatorial generation and arithmetic encodings. The code, tested under GHC\n6.6.1, is available at http://logic.csci.unt.edu/tarau/research/2008/fSET.zip .\n  We introduce ranking and unranking functions generalizing Ackermann's\nencoding to the universe of Hereditarily Finite Sets with Urelements. Then we\nbuild a lazy enumerator for Hereditarily Finite Sets with Urelements that\nmatches the unranking function provided by the inverse of Ackermann's encoding\nand we describe functors between them resulting in arithmetic encodings for\npowersets, hypergraphs, ordinals and choice functions. After implementing a\ndigraph representation of Hereditarily Finite Sets we define {\\em decoration\nfunctions} that can recover well-founded sets from encodings of their\nassociated acyclic digraphs. We conclude with an encoding of arbitrary digraphs\nand discuss a concept of duality induced by the set membership relation.\n  Keywords: hereditarily finite sets, ranking and unranking functions,\nexecutable set theory, arithmetic encodings, Haskell data representations,\nfunctional programming and computational mathematics\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 6 Aug 2008 01:05:09 GMT"
            }
        ],
        "update_date": "2008-08-07",
        "authors_parsed": [
            [
                "Tarau",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0808.0920",
        "submitter": "Mahesh Arumugam",
        "authors": "Mahesh Arumugam",
        "title": "A Distributed and Deterministic TDMA Algorithm for\n  Write-All-With-Collision Model",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Several self-stabilizing time division multiple access (TDMA) algorithms are\nproposed for sensor networks. In addition to providing a collision-free\ncommunication service, such algorithms enable the transformation of programs\nwritten in abstract models considered in distributed computing literature into\na model consistent with sensor networks, i.e., write all with collision (WAC)\nmodel. Existing TDMA slot assignment algorithms have one or more of the\nfollowing properties: (i) compute slots using a randomized algorithm, (ii)\nassume that the topology is known upfront, and/or (iii) assign slots\nsequentially. If these algorithms are used to transform abstract programs into\nprograms in WAC model then the transformed programs are probabilistically\ncorrect, do not allow the addition of new nodes, and/or converge in a\nsequential fashion. In this paper, we propose a self-stabilizing deterministic\nTDMA algorithm where a sensor is aware of only its neighbors. We show that the\nslots are assigned to the sensors in a concurrent fashion and starting from\narbitrary initial states, the algorithm converges to states where\ncollision-free communication among the sensors is restored. Moreover, this\nalgorithm facilitates the transformation of abstract programs into programs in\nWAC model that are deterministically correct.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 6 Aug 2008 20:33:56 GMT"
            }
        ],
        "update_date": "2008-08-08",
        "authors_parsed": [
            [
                "Arumugam",
                "Mahesh",
                ""
            ]
        ]
    },
    {
        "id": "0808.1062",
        "submitter": "Qinglin Zhao",
        "authors": "Qinglin Zhao, Soung C. Liew",
        "title": "Optimization of Location Management for PCS Networks with CTRW Mobility\n  Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the design of the optimal locationupdate area (LA) of\nthe distance-based scheme for personal communication service (PCS) networks. We\nfocus on the optimization of two design parameters associated with the LA: 1)\ninitial position upon LA update; 2) distance threshold for triggering of LA\nupdate. Based on the popular continuous-time random walk (CTRW) mobility model,\nwe propose a novel analytical framework that uses a diffusion equation to\nminimize the location management cost. In this framework, a number of\nmeasurable physical parameters, such as length of road section, angle between\nroad sections, and road section crossing time, can be integrated into the\nsystem design. This framework allows us to easily evaluate the total cost under\ngeneral call arrival distributions and LA of different shapes. For the\nparticular case of circular LA and small Poisson call-arrival rate, we prove\nthe following: (1) When the drift is weak, the optimal initial position\napproaches the center of the LA; when the drift is strong, it approaches the\nboundary of the LA. (2) Comparing the optimal initial-position and\ncenter-initial-position solutions (which is assumed in most prior work), when\nthe drift is weak, the optimal distance threshold and the minimum total cost\nare roughly equal; when the drift is strong, the optimal distance threshold in\nthe later is about 1.260 times that in the former, and the minimum total cost\nin the later is about 1.587 times that in the former. That is, optimizing on\ninitial position, which previous work did not consider, has the potential of\nreducing the cost measure by 37%.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Aug 2008 16:32:51 GMT"
            }
        ],
        "update_date": "2008-08-08",
        "authors_parsed": [
            [
                "Zhao",
                "Qinglin",
                ""
            ],
            [
                "Liew",
                "Soung C.",
                ""
            ]
        ]
    },
    {
        "id": "0808.1378",
        "submitter": "George Eskander MSc",
        "authors": "George S. Eskander, and Amir F. Atiya",
        "title": "A Novel Symbolic Type Neural Network Model- Application to River Flow\n  Forecasting",
        "comments": "Published in ICENCO2007, Cairo, December 2007",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NE cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we introduce a new symbolic type neural tree network called\nsymbolic function network (SFN) that is based on using elementary functions to\nmodel systems in a symbolic form. The proposed formulation permits feature\nselection, functional selection, and flexible structure. We applied this model\non the River Flow forecasting problem. The results found to be superior in both\nfitness and sparsity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 9 Aug 2008 22:05:48 GMT"
            }
        ],
        "update_date": "2008-08-12",
        "authors_parsed": [
            [
                "Eskander",
                "George S.",
                ""
            ],
            [
                "Atiya",
                "Amir F.",
                ""
            ]
        ]
    },
    {
        "id": "0808.1431",
        "submitter": "Neil J. Gunther",
        "authors": "Neil J. Gunther",
        "title": "A General Theory of Computational Scalability Based on Rational\n  Functions",
        "comments": "14 pages, 5 figures; several typos corrected, 1 reference updated,\n  page number reduced with 10 pt font",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The universal scalability law of computational capacity is a rational\nfunction C_p = P(p)/Q(p) with P(p) a linear polynomial and Q(p) a second-degree\npolynomial in the number of physical processors p, that has been long used for\nstatistical modeling and prediction of computer system performance. We prove\nthat C_p is equivalent to the synchronous throughput bound for a\nmachine-repairman with state-dependent service rate. Simpler rational\nfunctions, such as Amdahl's law and Gustafson speedup, are corollaries of this\nqueue-theoretic bound. C_p is further shown to be both necessary and sufficient\nfor modeling all practical characteristics of computational scalability.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 11 Aug 2008 00:06:16 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 25 Aug 2008 16:20:42 GMT"
            }
        ],
        "update_date": "2008-08-25",
        "authors_parsed": [
            [
                "Gunther",
                "Neil J.",
                ""
            ]
        ]
    },
    {
        "id": "0808.2543",
        "submitter": "Carsten Schneider",
        "authors": "Carsten Schneider",
        "title": "A Refined Difference Field Theory for Symbolic Summation",
        "comments": "Uses elseart.cls and yjsco.sty",
        "journal-ref": "J. Symbolic Comput. 43(9), pp. 611-644. 2008",
        "doi": "10.1016/j.jsc.2008.01.001",
        "report-no": "SFB F013, J. Kepler University Linz. Technical report no. 2007-24",
        "categories": "cs.SC math-ph math.CO math.MP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we present a refined summation theory based on Karr's\ndifference field approach. The resulting algorithms find sum representations\nwith optimal nested depth. For instance, the algorithms have been applied\nsuccessively to evaluate Feynman integrals from Perturbative Quantum Field\nTheory.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 19 Aug 2008 07:46:04 GMT"
            }
        ],
        "update_date": "2008-09-02",
        "authors_parsed": [
            [
                "Schneider",
                "Carsten",
                ""
            ]
        ]
    },
    {
        "id": "0808.2596",
        "submitter": "Carsten Schneider",
        "authors": "Carsten Schneider",
        "title": "Parameterized Telescoping Proves Algebraic Independence of Sums",
        "comments": "To appear in Annals of Combinatorics",
        "journal-ref": null,
        "doi": null,
        "report-no": "SFB F013, J. Kepler University Linz. Technical report no. 2006-40,\n  2006",
        "categories": "cs.SC math.CO math.NT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Usually creative telescoping is used to derive recurrences for sums. In this\narticle we show that the non-existence of a creative telescoping solution, and\nmore generally, of a parameterized telescoping solution, proves algebraic\nindependence of certain types of sums. Combining this fact with\nsummation-theory shows transcendence of whole classes of sums. Moreover, this\nresult throws new light on the question why, e.g., Zeilberger's algorithm fails\nto find a recurrence with minimal order.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 19 Aug 2008 13:58:01 GMT"
            }
        ],
        "update_date": "2008-09-02",
        "authors_parsed": [
            [
                "Schneider",
                "Carsten",
                ""
            ]
        ]
    },
    {
        "id": "0808.2794",
        "submitter": "Julien Langou",
        "authors": "Marc Baboulin, Alfredo Buttari, Jack Dongarra, Jakub Kurzak, Julie\n  Langou, Julien Langou, Piotr Luszczek, and Stanimire Tomov",
        "title": "Accelerating Scientific Computations with Mixed Precision Algorithms",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.cpc.2008.11.005",
        "report-no": null,
        "categories": "cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  On modern architectures, the performance of 32-bit operations is often at\nleast twice as fast as the performance of 64-bit operations. By using a\ncombination of 32-bit and 64-bit floating point arithmetic, the performance of\nmany dense and sparse linear algebra algorithms can be significantly enhanced\nwhile maintaining the 64-bit accuracy of the resulting solution. The approach\npresented here can apply not only to conventional processors but also to other\ntechnologies such as Field Programmable Gate Arrays (FPGA), Graphical\nProcessing Units (GPU), and the STI Cell BE processor. Results on modern\nprocessor architectures and the STI Cell BE are presented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 20 Aug 2008 17:50:36 GMT"
            }
        ],
        "update_date": "2015-05-13",
        "authors_parsed": [
            [
                "Baboulin",
                "Marc",
                ""
            ],
            [
                "Buttari",
                "Alfredo",
                ""
            ],
            [
                "Dongarra",
                "Jack",
                ""
            ],
            [
                "Kurzak",
                "Jakub",
                ""
            ],
            [
                "Langou",
                "Julie",
                ""
            ],
            [
                "Langou",
                "Julien",
                ""
            ],
            [
                "Luszczek",
                "Piotr",
                ""
            ],
            [
                "Tomov",
                "Stanimire",
                ""
            ]
        ]
    },
    {
        "id": "0808.2953",
        "submitter": "Paul Tarau",
        "authors": "Paul Tarau",
        "title": "Declarative Combinatorics: Isomorphisms, Hylomorphisms and Hereditarily\n  Finite Data Types in Haskell",
        "comments": "unpublished draft, revision 3, added various new encodings, with\n  focus on primes and multisets, now 104 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is an exploration in a functional programming framework of {\\em\nisomorphisms} between elementary data types (natural numbers, sets, multisets,\nfinite functions, permutations binary decision diagrams, graphs, hypergraphs,\nparenthesis languages, dyadic rationals, primes, DNA sequences etc.) and their\nextension to hereditarily finite universes through {\\em hylomorphisms} derived\nfrom {\\em ranking/unranking} and {\\em pairing/unpairing} operations.\n  An embedded higher order {\\em combinator language} provides any-to-any\nencodings automatically.\n  Besides applications to experimental mathematics, a few examples of ``free\nalgorithms'' obtained by transferring operations between data types are shown.\nOther applications range from stream iterators on combinatorial objects to\nself-delimiting codes, succinct data representations and generation of random\ninstances.\n  The paper covers 59 data types and, through the use of the embedded\ncombinator language, provides 3540 distinct bijective transformations between\nthem.\n  The self-contained source code of the paper, as generated from a literate\nHaskell program, is available at\n\\url{http://logic.csci.unt.edu/tarau/research/2008/fISO.zip}.\n  {\\bf Keywords}: Haskell data representations, data type isomorphisms,\ndeclarative combinatorics, computational mathematics, Ackermann encoding,\nG\\\"{o}del numberings, arithmetization, ranking/unranking, hereditarily finite\nsets, functions and permutations, encodings of binary decision diagrams, dyadic\nrationals, DNA encodings\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 21 Aug 2008 16:47:38 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 20 Oct 2008 18:47:59 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 9 Dec 2008 01:28:15 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 19 Jan 2009 19:39:51 GMT"
            }
        ],
        "update_date": "2009-01-19",
        "authors_parsed": [
            [
                "Tarau",
                "Paul",
                ""
            ]
        ]
    },
    {
        "id": "0808.3038",
        "submitter": "David Sevilla",
        "authors": "Josef Schicho and David Sevilla",
        "title": "Tschirnhaus-Weierstrass curves",
        "comments": "v2: 10 pages, major revision due to errors in the main result. v1: 14\n  pages, submitted to Mathematics of Computation",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.AG cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We define the concept of Tschirnhaus-Weierstrass curve, named after the\nWeierstrass form of an elliptic curve and Tschirnhaus transformations. Every\npointed curve has a Tschirnhaus-Weierstrass form, and this representation is\nunique up to a scaling of variables. This is useful for computing isomorphisms\nbetween curves.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 22 Aug 2008 08:00:36 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 9 Jun 2011 14:40:45 GMT"
            }
        ],
        "update_date": "2011-06-10",
        "authors_parsed": [
            [
                "Schicho",
                "Josef",
                ""
            ],
            [
                "Sevilla",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0808.3100",
        "submitter": "Petr Ivankov",
        "authors": "Petr R. Ivankov",
        "title": "Optimizing Compiler for Engineering Problems",
        "comments": "6 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  New information technologies provide a lot of prospects for performance\nimprovement. One of them is \"Dynamic Source Code Generation and Compilation\".\nThis article shows how this way provides high performance for engineering\nproblems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 22 Aug 2008 15:27:34 GMT"
            }
        ],
        "update_date": "2008-08-25",
        "authors_parsed": [
            [
                "Ivankov",
                "Petr R.",
                ""
            ]
        ]
    },
    {
        "id": "0808.3307",
        "submitter": "Naokata Shikuma",
        "authors": "Naokata Shikuma, Atsushi Igarashi",
        "title": "Proving Noninterference by a Fully Complete Translation to the Simply\n  Typed lambda-calculus",
        "comments": "31 pages",
        "journal-ref": "Logical Methods in Computer Science, Volume 4, Issue 3 (September\n  20, 2008) lmcs:683",
        "doi": "10.2168/LMCS-4(3:10)2008",
        "report-no": null,
        "categories": "cs.PL cs.CR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Tse and Zdancewic have formalized the notion of noninterference for Abadi et\nal.'s DCC in terms of logical relations and given a proof of noninterference by\nreduction to parametricity of System F. Unfortunately, their proof contains\nerrors in a key lemma that their translation from DCC to System F preserves the\nlogical relations defined for both calculi. In fact, we have found a\ncounterexample for it. In this article, instead of DCC, we prove\nnoninterference for sealing calculus, a new variant of DCC, by reduction to the\nbasic lemma of a logical relation for the simply typed lambda-calculus, using a\nfully complete translation to the simply typed lambda-calculus. Full\ncompleteness plays an important role in showing preservation of the two logical\nrelations through the translation. Also, we investigate relationship among\nsealing calculus, DCC, and an extension of DCC by Tse and Zdancewic and show\nthat the first and the last of the three are equivalent.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 25 Aug 2008 06:56:05 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 20 Sep 2008 21:35:24 GMT"
            }
        ],
        "update_date": "2015-07-01",
        "authors_parsed": [
            [
                "Shikuma",
                "Naokata",
                ""
            ],
            [
                "Igarashi",
                "Atsushi",
                ""
            ]
        ]
    },
    {
        "id": "0808.3548",
        "submitter": "Ioan Raicu",
        "authors": "Yong Zhao, Ioan Raicu, Ian Foster, Mihael Hategan, Veronika Nefedova,\n  Mike Wilde",
        "title": "Realizing Fast, Scalable and Reliable Scientific Computations in Grid\n  Environments",
        "comments": "Book chapter in Grid Computing Research Progress, ISBN:\n  978-1-60456-404-4, Nova Publisher 2008",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The practical realization of managing and executing large scale scientific\ncomputations efficiently and reliably is quite challenging. Scientific\ncomputations often involve thousands or even millions of tasks operating on\nlarge quantities of data, such data are often diversely structured and stored\nin heterogeneous physical formats, and scientists must specify and run such\ncomputations over extended periods on collections of compute, storage and\nnetwork resources that are heterogeneous, distributed and may change\nconstantly. We present the integration of several advanced systems: Swift,\nKarajan, and Falkon, to address the challenges in running various large scale\nscientific applications in Grid environments. Swift is a parallel programming\ntool for rapid and reliable specification, execution, and management of\nlarge-scale science and engineering workflows. Swift consists of a simple\nscripting language called SwiftScript and a powerful runtime system that is\nbased on the CoG Karajan workflow engine and integrates the Falkon light-weight\ntask execution service that uses multi-level scheduling and a streamlined\ndispatcher. We showcase the scalability, performance and reliability of the\nintegrated system using application examples drawn from astronomy, cognitive\nneuroscience and molecular dynamics, which all comprise large number of\nfine-grained jobs. We show that Swift is able to represent dynamic workflows\nwhose structures can only be determined during runtime and reduce largely the\ncode size of various workflow representations using SwiftScript; schedule the\nexecution of hundreds of thousands of parallel computations via the Karajan\nengine; and achieve up to 90% reduction in execution time when compared to\ntraditional batch schedulers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 26 Aug 2008 16:15:42 GMT"
            }
        ],
        "update_date": "2008-08-27",
        "authors_parsed": [
            [
                "Zhao",
                "Yong",
                ""
            ],
            [
                "Raicu",
                "Ioan",
                ""
            ],
            [
                "Foster",
                "Ian",
                ""
            ],
            [
                "Hategan",
                "Mihael",
                ""
            ],
            [
                "Nefedova",
                "Veronika",
                ""
            ],
            [
                "Wilde",
                "Mike",
                ""
            ]
        ]
    },
    {
        "id": "0808.3747",
        "submitter": "Francesco De Pellegrini Dr.",
        "authors": "Eitan Altman and Francesco De Pellegrini",
        "title": "Forward Correction and Fountain codes in Delay Tolerant Networks",
        "comments": "10 pages - typos fixes",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Delay tolerant Ad-hoc Networks make use of mobility of relay nodes to\ncompensate for lack of permanent connectivity and thus enable communication\nbetween nodes that are out of range of each other. To decrease delivery delay,\nthe information that needs to be delivered is replicated in the network. Our\nobjective in this paper is to study replication mechanisms that include coding\nin order to improve the probability of successful delivery within a given time\nlimit. We propose an analytical approach that allows to quantify tradeoffs\nbetween resources and performance measures (energy and delay). We study the\neffect of coding on the performance of the network while optimizing parameters\nthat govern routing. Our results, based on fluid approximations, are compared\nto simulations which validate the model\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Aug 2008 17:33:40 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 1 Sep 2008 15:36:37 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 19 Jan 2009 11:25:38 GMT"
            }
        ],
        "update_date": "2009-01-19",
        "authors_parsed": [
            [
                "Altman",
                "Eitan",
                ""
            ],
            [
                "De Pellegrini",
                "Francesco",
                ""
            ]
        ]
    },
    {
        "id": "0808.3937",
        "submitter": "Markus Fidler",
        "authors": "Michael Bredel, Markus Fidler",
        "title": "Understanding Fairness and its Impact on Quality of Service in IEEE\n  802.11",
        "comments": null,
        "journal-ref": "IEEE INFOCOM 2009",
        "doi": "10.1109/INFCOM.2009.5062022",
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Distributed Coordination Function (DCF) aims at fair and efficient medium\naccess in IEEE 802.11. In face of its success, it is remarkable that there is\nlittle consensus on the actual degree of fairness achieved, particularly\nbearing its impact on quality of service in mind. In this paper we provide an\naccurate model for the fairness of the DCF. Given M greedy stations we assume\nfairness if a tagged station contributes a share of 1/M to the overall number\nof packets transmitted. We derive the probability distribution of fairness\ndeviations and support our analytical results by an extensive set of\nmeasurements. We find a closed-form expression for the improvement of long-term\nover short-term fairness. Regarding the random countdown values we quantify the\nsignificance of their distribution whereas we discover that fairness is largely\ninsensitive to the distribution parameters. Based on our findings we view the\nDCF as emulating an ideal fair queuing system to quantify the deviations from a\nfair rate allocation. We deduce a stochastic service curve model for the DCF to\npredict packet delays in IEEE 802.11. We show how a station can estimate its\nfair bandwidth share from passive measurements of its traffic arrivals and\ndepartures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 28 Aug 2008 15:35:10 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Bredel",
                "Michael",
                ""
            ],
            [
                "Fidler",
                "Markus",
                ""
            ]
        ]
    },
    {
        "id": "0809.0063",
        "submitter": "Jean-Guillaume Dumas",
        "authors": "Jean-Guillaume Dumas (LJK), Laurent Fousse (LJK), Bruno Salvy (INRIA\n  Rocquencourt)",
        "title": "Simultaneous Modular Reduction and Kronecker Substitution for Small\n  Finite Fields",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1016/j.jsc.2010.08.015",
        "report-no": null,
        "categories": "cs.SC math.NT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present algorithms to perform modular polynomial multiplication or modular\ndot product efficiently in a single machine word. We pack polynomials into\nintegers and perform several modular operations with machine integer or\nfloating point arithmetic. The modular polynomials are converted into integers\nusing Kronecker substitution (evaluation at a sufficiently large integer). With\nsome control on the sizes and degrees, arithmetic operations on the polynomials\ncan be performed directly with machine integers or floating point numbers and\nthe number of conversions can be reduced. We also present efficient ways to\nrecover the modular values of the coefficients. This leads to practical gains\nof quite large constant factors for polynomial multiplication, prime field\nlinear algebra and small extension field arithmetic.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 30 Aug 2008 14:28:23 GMT"
            }
        ],
        "update_date": "2013-06-19",
        "authors_parsed": [
            [
                "Dumas",
                "Jean-Guillaume",
                "",
                "LJK"
            ],
            [
                "Fousse",
                "Laurent",
                "",
                "LJK"
            ],
            [
                "Salvy",
                "Bruno",
                "",
                "INRIA\n  Rocquencourt"
            ]
        ]
    },
    {
        "id": "0809.0545",
        "submitter": "Ian Petersen",
        "authors": "S. Z. Sayed Hassen, M. Heurs, E. H. Huntington, I. R. Petersen",
        "title": "Frequency Locking of an Optical Cavity using LQG Integral Control",
        "comments": "18 pages, 9 figures",
        "journal-ref": "Journal of Physics B: Atomic, Molecular and Optical Physics, vol.\n  42, 175501, 2009",
        "doi": "10.1088/0953-4075/42/17/175501",
        "report-no": null,
        "categories": "quant-ph cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the application of integral Linear Quadratic Gaussian\n(LQG) optimal control theory to a problem of cavity locking in quantum optics.\nThe cavity locking problem involves controlling the error between the laser\nfrequency and the resonant frequency of the cavity. A model for the cavity\nsystem, which comprises a piezo-electric actuator and an optical cavity is\nexperimentally determined using a subspace identification method. An LQG\ncontroller which includes integral action is synthesized to stabilize the\nfrequency of the cavity to the laser frequency and to reject low frequency\nnoise. The controller is successfully implemented in the laboratory using a\ndSpace DSP board.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 3 Sep 2008 04:46:39 GMT"
            }
        ],
        "update_date": "2015-05-13",
        "authors_parsed": [
            [
                "Hassen",
                "S. Z. Sayed",
                ""
            ],
            [
                "Heurs",
                "M.",
                ""
            ],
            [
                "Huntington",
                "E. H.",
                ""
            ],
            [
                "Petersen",
                "I. R.",
                ""
            ]
        ]
    },
    {
        "id": "0809.0840",
        "submitter": "Sergei Chekanov V.",
        "authors": "S.Chekanov",
        "title": "HEP data analysis using jHepWork and Java",
        "comments": "5 pages, Proceedings of the HERA-LHC workshops (2007-2008), DESY-CERN",
        "journal-ref": null,
        "doi": null,
        "report-no": "ANL-HEP-CP-08-53",
        "categories": "cs.CE hep-ex hep-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A role of Java in high-energy physics and recent progress in development of a\nplatform-independent data-analysis framework, jHepWork, is discussed. The\nframework produces professional graphics and has many libraries for data\nmanipulation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 4 Sep 2008 15:33:23 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 9 Feb 2009 19:17:26 GMT"
            }
        ],
        "update_date": "2009-02-09",
        "authors_parsed": [
            [
                "Chekanov",
                "S.",
                ""
            ]
        ]
    },
    {
        "id": "0809.1132",
        "submitter": "Vandy Berten",
        "authors": "Vandy Berten, Chi-Ju Chang, Tei-Wei Kuo",
        "title": "Managing Varying Worst Case Execution Times on DVS Platforms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Energy efficient real-time task scheduling attracted a lot of attention in\nthe past decade. Most of the time, deterministic execution lengths for tasks\nwere considered, but this model fits less and less with the reality, especially\nwith the increasing number of multimedia applications. It's why a lot of\nresearch is starting to consider stochastic models, where execution times are\nonly known stochastically. However, authors consider that they have a pretty\nmuch precise knowledge about the properties of the system, especially regarding\nto the worst case execution time (or worst case execution cycles, WCEC).\n  In this work, we try to relax this hypothesis, and assume that the WCEC can\nvary. We propose miscellaneous methods to react to such a situation, and give\nmany simulation results attesting that with a small effort, we can provide very\ngood results, allowing to keep a low deadline miss rate as well as an energy\nconsumption similar to clairvoyant algorithms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 6 Sep 2008 04:38:34 GMT"
            }
        ],
        "update_date": "2008-09-09",
        "authors_parsed": [
            [
                "Berten",
                "Vandy",
                ""
            ],
            [
                "Chang",
                "Chi-Ju",
                ""
            ],
            [
                "Kuo",
                "Tei-Wei",
                ""
            ]
        ]
    },
    {
        "id": "0809.1177",
        "submitter": "Andrzej Karbowski",
        "authors": "Andrzej Karbowski",
        "title": "Amdahl's and Gustafson-Barsis laws revisited",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.GT cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper presents a simple derivation of the Gustafson-Barsis law from the\nAmdahl's law. In the computer literature these two laws describing the speedup\nlimits of parallel applications are derived separately. It is shown, that\ntreating the time of the execution of the sequential part of the application as\na constant, in few lines the Gustafson-Barsis law can be obtained from the\nAmdahl's law and that the popular claim, that Gustafson-Barsis law overthrows\nAmdahl's law is a mistake.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 6 Sep 2008 15:06:53 GMT"
            }
        ],
        "update_date": "2008-09-09",
        "authors_parsed": [
            [
                "Karbowski",
                "Andrzej",
                ""
            ]
        ]
    },
    {
        "id": "0809.1437",
        "submitter": "Fotis Georgatos Drs",
        "authors": "Fotis Georgatos",
        "title": "How applicable is Python as first computer language for teaching\n  programming in a pre-university educational environment, from a teacher's\n  point of view?",
        "comments": "135 pages, 20 tables, 10 figures (incl. evolution of computer\n  languages)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.CY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This project report attempts to evaluate the educational properties of the\nPython computer language, in practice. This is done by examining computer\nlanguage evolution history, related scientific background work, the existing\neducational research on computer languages and Python's experimental\napplication in higher secondary education in Greece, during first half of year\n2002. This Thesis Report was delivered in advance of a thesis defense for a\nMasters/Doctorandus (MSc/Drs) title with the Amstel Institute/Universiteit van\nAmsterdam, during the same year.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 9 Sep 2008 14:39:57 GMT"
            }
        ],
        "update_date": "2008-09-10",
        "authors_parsed": [
            [
                "Georgatos",
                "Fotis",
                ""
            ]
        ]
    },
    {
        "id": "0809.1476",
        "submitter": "Qin Xiaolin",
        "authors": "Yong Feng, Jingzhong Zhang, Xiaolin Qin, Xun Yuan",
        "title": "Obtaining Exact Interpolation Multivariate Polynomial by Approximation",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.CG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In some fields such as Mathematics Mechanization, automated reasoning and\nTrustworthy Computing etc., exact results are needed. Symbolic computations are\nused to obtain the exact results. Symbolic computations are of high complexity.\nIn order to improve the situation, exactly interpolating methods are often\nproposed for the exact results and approximate interpolating methods for the\napproximate ones. In this paper, we study how to obtain exact interpolation\npolynomial with rational coefficients by approximate interpolating methods.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 9 Sep 2008 02:33:30 GMT"
            }
        ],
        "update_date": "2008-09-10",
        "authors_parsed": [
            [
                "Feng",
                "Yong",
                ""
            ],
            [
                "Zhang",
                "Jingzhong",
                ""
            ],
            [
                "Qin",
                "Xiaolin",
                ""
            ],
            [
                "Yuan",
                "Xun",
                ""
            ]
        ]
    },
    {
        "id": "0809.2083",
        "submitter": "Nicole Berline",
        "authors": "Velleda Baldoni, Nicole Berline (CMLS-EcolePolytechnique), Jesus De\n  Loera, Matthias K\\\"oppe, Mich\\`ele Vergne (CMLS-EcolePolytechnique)",
        "title": "How to Integrate a Polynomial over a Simplex",
        "comments": "Tables added with new experimental results. References added",
        "journal-ref": "Mathematics of Computation 80, 273 (2011) 297-325",
        "doi": "10.1090/S0025-5718-2010-02378-6",
        "report-no": null,
        "categories": "math.MG cs.CC cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper settles the computational complexity of the problem of integrating\na polynomial function f over a rational simplex. We prove that the problem is\nNP-hard for arbitrary polynomials via a generalization of a theorem of Motzkin\nand Straus. On the other hand, if the polynomial depends only on a fixed number\nof variables, while its degree and the dimension of the simplex are allowed to\nvary, we prove that integration can be done in polynomial time. As a\nconsequence, for polynomials of fixed total degree, there is a polynomial time\nalgorithm as well. We conclude the article with extensions to other polytopes,\ndiscussion of other available methods and experimental results.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 11 Sep 2008 19:00:12 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 1 Oct 2008 16:59:52 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 13 Feb 2009 14:05:09 GMT"
            }
        ],
        "update_date": "2013-06-27",
        "authors_parsed": [
            [
                "Baldoni",
                "Velleda",
                "",
                "CMLS-EcolePolytechnique"
            ],
            [
                "Berline",
                "Nicole",
                "",
                "CMLS-EcolePolytechnique"
            ],
            [
                "De Loera",
                "Jesus",
                "",
                "CMLS-EcolePolytechnique"
            ],
            [
                "K\u00f6ppe",
                "Matthias",
                "",
                "CMLS-EcolePolytechnique"
            ],
            [
                "Vergne",
                "Mich\u00e8le",
                "",
                "CMLS-EcolePolytechnique"
            ]
        ]
    },
    {
        "id": "0809.2532",
        "submitter": "Neil J. Gunther",
        "authors": "Tanel Poder and Neil J. Gunther",
        "title": "Multidimensional Visualization of Oracle Performance Using Barry007",
        "comments": "To appear in the Proc. CMG International Conference, Las Vegas,\n  Nevada, December 2008",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.DB",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most generic performance tools display only system-level performance data\nusing 2-dimensional plots or diagrams and this limits the informational detail\nthat can be displayed. Moreover, a modern relational database system, like\nOracle, can concurrently serve thousands of client processes with different\nworkload characteristics, so that generic performance-data displays inevitably\nhide important information. Drawing on our previous work, this paper\ndemonstrates the application of Barry007 multidimensional visualization to the\nanalysis of Oracle end-user, session-level, performance data, showing both\ncollective trends and individual performance anomalies.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 15 Sep 2008 14:11:34 GMT"
            }
        ],
        "update_date": "2008-09-16",
        "authors_parsed": [
            [
                "Poder",
                "Tanel",
                ""
            ],
            [
                "Gunther",
                "Neil J.",
                ""
            ]
        ]
    },
    {
        "id": "0809.2541",
        "submitter": "Neil J. Gunther",
        "authors": "Jim Holtman and Neil J. Gunther",
        "title": "Getting in the Zone for Successful Scalability",
        "comments": "14 pages, 15 figures. To appear in Proc. CMG International\n  Conference, Las Vegas, Nevada, December 2008",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The universal scalability law (USL) is an analytic model used to quantify\napplication scaling. It is universal because it subsumes Amdahl's law and\nGustafson linearized scaling as special cases. Using simulation, we show: (i)\nthat the USL is equivalent to synchronous queueing in a load-dependent machine\nrepairman model and (ii) how USL, Amdahl's law, and Gustafson scaling can be\nregarded as boundaries defining three scalability zones. Typical throughput\nmeasurements lie across all three zones. Simulation scenarios provide deeper\ninsight into queueing effects and thus provide a clearer indication of which\napplication features should be tuned to get into the optimal performance zone.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 15 Sep 2008 14:51:04 GMT"
            }
        ],
        "update_date": "2008-09-16",
        "authors_parsed": [
            [
                "Holtman",
                "Jim",
                ""
            ],
            [
                "Gunther",
                "Neil J.",
                ""
            ]
        ]
    },
    {
        "id": "0809.2696",
        "submitter": "Christoph Schommer",
        "authors": "Christoph Schommer",
        "title": "An Unified Definition of Data Mining",
        "comments": "7 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.CY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Since many years, theoretical concepts of Data Mining have been developed and\nimproved. Data Mining has become applied to many academic and industrial\nsituations, and recently, soundings of public opinion about privacy have been\ncarried out. However, a consistent and standardized definition is still\nmissing, and the initial explanation given by Frawley et al. has pragmatically\noften changed over the years. Furthermore, alternative terms like Knowledge\nDiscovery have been conjured and forged, and a necessity of a Data Warehouse\nhas been endeavoured to persuade the users. In this work, we pick up current\ndefinitions and introduce an unified definition that covers existing attempted\nexplanations. For this, we appeal to the natural original of chemical states of\naggregation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 16 Sep 2008 13:13:17 GMT"
            }
        ],
        "update_date": "2008-09-17",
        "authors_parsed": [
            [
                "Schommer",
                "Christoph",
                ""
            ]
        ]
    },
    {
        "id": "0809.2978",
        "submitter": "Jon Wilkening",
        "authors": "Jon Wilkening and Jia Yu",
        "title": "A local construction of the Smith normal form of a matrix polynomial",
        "comments": "26 pages, 6 figures; introduction expanded, 10 references added, two\n  additional tests performed",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an algorithm for computing a Smith form with multipliers of a\nregular matrix polynomial over a field. This algorithm differs from previous\nones in that it computes a local Smith form for each irreducible factor in the\ndeterminant separately and then combines them into a global Smith form, whereas\nother algorithms apply a sequence of unimodular row and column operations to\nthe original matrix. The performance of the algorithm in exact arithmetic is\nreported for several test cases.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 17 Sep 2008 18:58:42 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 9 Jun 2010 19:23:17 GMT"
            }
        ],
        "update_date": "2015-03-13",
        "authors_parsed": [
            [
                "Wilkening",
                "Jon",
                ""
            ],
            [
                "Yu",
                "Jia",
                ""
            ]
        ]
    },
    {
        "id": "0809.4082",
        "submitter": "Vandy Berten",
        "authors": "Vandy Berten and Jo\\\"el Goossens",
        "title": "Multiprocessor Global Scheduling on Frame-Based DVFS Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this ongoing work, we are interested in multiprocessor energy efficient\nsystems, where task durations are not known in advance, but are know\nstochastically. More precisely, we consider global scheduling algorithms for\nframe-based multiprocessor stochastic DVFS (Dynamic Voltage and Frequency\nScaling) systems. Moreover, we consider processors with a discrete set of\navailable frequencies.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 24 Sep 2008 06:10:39 GMT"
            }
        ],
        "update_date": "2008-09-25",
        "authors_parsed": [
            [
                "Berten",
                "Vandy",
                ""
            ],
            [
                "Goossens",
                "Jo\u00ebl",
                ""
            ]
        ]
    },
    {
        "id": "0809.4635",
        "submitter": "Jan Bergstra",
        "authors": "Jan A. Bergstra and Mark B. van der Zwaag",
        "title": "Mechanistic Behavior of Single-Pass Instruction Sequences",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.LO",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Earlier work on program and thread algebra detailed the functional,\nobservable behavior of programs under execution. In this article we add the\nmodeling of unobservable, mechanistic processing, in particular processing due\nto jump instructions. We model mechanistic processing preceding some further\nbehavior as a delay of that behavior; we borrow a unary delay operator from\ndiscrete time process algebra. We define a mechanistic improvement ordering on\nthreads and observe that some threads do not have an optimal implementation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 26 Sep 2008 13:57:36 GMT"
            }
        ],
        "update_date": "2008-09-29",
        "authors_parsed": [
            [
                "Bergstra",
                "Jan A.",
                ""
            ],
            [
                "van der Zwaag",
                "Mark B.",
                ""
            ]
        ]
    },
    {
        "id": "0809.4983",
        "submitter": "Frederic Butin",
        "authors": "Fr\\'ed\\'eric Butin (ICJ)",
        "title": "Poisson Homology in Degree 0 for some Rings of Symplectic Invariants",
        "comments": "24 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math-ph cs.SC math.MP math.RA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $\\go{g}$ be a finite-dimensional semi-simple Lie algebra, $\\go{h}$ a\nCartan subalgebra of $\\go{g}$, and $W$ its Weyl group. The group $W$ acts\ndiagonally on $V:=\\go{h}\\oplus\\go{h}^*$, as well as on $\\mathbb{C}[V]$. The\npurpose of this article is to study the Poisson homology of the algebra of\ninvariants $\\mathbb{C}[V]^W$ endowed with the standard symplectic bracket. To\nbegin with, we give general results about the Poisson homology space in degree\n0, denoted by $HP_0(\\mathbb{C}[V]^W)$, in the case where $\\go{g}$ is of type\n$B_n-C_n$ or $D_n$, results which support Alev's conjecture. Then we are\nfocusing the interest on the particular cases of ranks 2 and 3, by computing\nthe Poisson homology space in degree 0 in the cases where $\\go{g}$ is of type\n$B_2$ ($\\go{so}_5$), $D_2$ ($\\go{so}_4$), then $B_3$ ($\\go{so}_7$), and\n$D_3=A_3$ ($\\go{so}_6\\simeq\\go{sl}_4$). In order to do this, we make use of a\nfunctional equation introduced by Y. Berest, P. Etingof and V. Ginzburg. We\nrecover, by a different method, the result established by J. Alev and L.\nFoissy, according to which the dimension of $HP_0(\\mathbb{C}[V]^W)$ equals 2\nfor $B_2$. Then we calculate the dimension of this space and we show that it is\nequal to 1 for $D_2$. We also calculate it for the rank 3 cases, we show that\nit is equal to 3 for $B_3-C_3$ and 1 for $D_3=A_3$.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 29 Sep 2008 15:16:33 GMT"
            }
        ],
        "update_date": "2008-09-30",
        "authors_parsed": [
            [
                "Butin",
                "Fr\u00e9d\u00e9ric",
                "",
                "ICJ"
            ]
        ]
    },
    {
        "id": "0809.5238",
        "submitter": "Vincent N\\'elis",
        "authors": "Vincent N\\'elis and Jo\\\"el Goossens",
        "title": "Mode Change Protocol for Multi-Mode Real-Time Systems upon Identical\n  Multiprocessors",
        "comments": "4 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a synchronous protocol without periodicity for\nscheduling multi-mode real-time systems upon identical multiprocessor\nplatforms. Our proposal can be considered to be a multiprocessor extension of\nthe uniprocessor protocol called \"Minimal Single Offset protocol\".\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 30 Sep 2008 15:55:04 GMT"
            }
        ],
        "update_date": "2008-10-01",
        "authors_parsed": [
            [
                "N\u00e9lis",
                "Vincent",
                ""
            ],
            [
                "Goossens",
                "Jo\u00ebl",
                ""
            ]
        ]
    },
    {
        "id": "0810.0135",
        "submitter": "Murat Alanyali",
        "authors": "Murat Alanyali, Maxim Dashouk",
        "title": "Occupancy distributions of homogeneous queueing systems under\n  opportunistic scheduling",
        "comments": "Submitted for possible publication",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze opportunistic schemes for transmission scheduling from one of $n$\nhomogeneous queues whose channel states fluctuate independently. Considered\nschemes consist of the LCQ policy, which transmits from a longest connected\nqueue in the entire system, and its low-complexity variants that transmit from\na longest queue within a randomly chosen subset of connected queues. A\nMarkovian model is studied where mean packet transmission time is $n^{-1}$ and\npacket arrival rate is $\\lambda<1$ per queue. Transient and equilibrium\ndistributions of queue occupancies are obtained in the limit as the system size\n$n$ tends to infinity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 1 Oct 2008 15:02:16 GMT"
            }
        ],
        "update_date": "2008-10-02",
        "authors_parsed": [
            [
                "Alanyali",
                "Murat",
                ""
            ],
            [
                "Dashouk",
                "Maxim",
                ""
            ]
        ]
    },
    {
        "id": "0810.0372",
        "submitter": "Luis Veiga",
        "authors": "Filipe Cabecinhas and Nuno Lopes and Renato Crisostomo and Luis Veiga",
        "title": "Optimizing Binary Code Produced by Valgrind (Project Report on Virtual\n  Execution Environments Course - AVExe)",
        "comments": "Technical report from INESC-ID Lisboa describing optimizations to\n  code generation of the Valgring execution environment. Work developed in the\n  context of a Virtual Execution Environments course (AVExe) at IST/Technical\n  university of Lisbon",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Valgrind is a widely used framework for dynamic binary instrumentation and\nits mostly known by its memcheck tool. Valgrind's code generation module is far\nfrom producing optimal code. In addition it has many backends for different CPU\narchitectures, which difficults code optimization in an architecture\nindependent way. Our work focused on identifying sub-optimal code produced by\nValgrind and optimizing it.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 2 Oct 2008 09:41:52 GMT"
            }
        ],
        "update_date": "2008-10-03",
        "authors_parsed": [
            [
                "Cabecinhas",
                "Filipe",
                ""
            ],
            [
                "Lopes",
                "Nuno",
                ""
            ],
            [
                "Crisostomo",
                "Renato",
                ""
            ],
            [
                "Veiga",
                "Luis",
                ""
            ]
        ]
    },
    {
        "id": "0810.0394",
        "submitter": "Benedek Kov\\'acs",
        "authors": "Peter Fulop, Benedek Kovacs, Sandor Imre",
        "title": "Mobility Management Framework",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates mobility management strategies from the point of view\nof their need of signalling and processing resources on the backbone network\nand load on the air interface. A method is proposed to model the serving\nnetwork and mobile node mobility in order to be able to compare the different\ntypes of mobility management algorithms. To obtain a good description of the\nnetwork we calculate descriptive parameters from given topologies. Most\nmobility approaches derived from existing protocols are analyzed and their\nperformances are numerically compared in various network and mobility\nscenarios. We developed a mobility management framework that is able to give\ngeneral designing guidelines for the next generation mobility managements on\ngiven network, technology and mobility properties. With our model an operator\ncan design the network and tune the parameters to obtain the optimal\nimplementation of course revising existing systems is also possible. We present\na vertical handover decision method as a special application of our model\nframework.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 2 Oct 2008 11:44:47 GMT"
            }
        ],
        "update_date": "2008-10-03",
        "authors_parsed": [
            [
                "Fulop",
                "Peter",
                ""
            ],
            [
                "Kovacs",
                "Benedek",
                ""
            ],
            [
                "Imre",
                "Sandor",
                ""
            ]
        ]
    },
    {
        "id": "0810.1316",
        "submitter": "Victor Yodaiken",
        "authors": "Victor Yodaiken",
        "title": "The meaning of concurrent programs",
        "comments": "Technical report on using recursive functions for the low level\n  semantics of concurrent systems",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DM cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The semantics of assignment and mutual exclusion in concurrent and\nmulti-core/multi-processor systems is presented with attention to low level\narchitectural features in an attempt to make the presentation realistic.\nRecursive functions on event sequences are used to define state dependent\nfunctions and variables in ordinary (non-formal-method) algebra.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 7 Oct 2008 23:03:29 GMT"
            }
        ],
        "update_date": "2008-10-09",
        "authors_parsed": [
            [
                "Yodaiken",
                "Victor",
                ""
            ]
        ]
    },
    {
        "id": "0810.1481",
        "submitter": "Marko A. Rodriguez",
        "authors": "Marko A. Rodriguez, Joe Geldart",
        "title": "An Evidential Path Logic for Multi-Relational Networks",
        "comments": null,
        "journal-ref": "Proceedings of the Association for the Advancement of Artificial\n  Intelligence Spring Symposium: Technosocial Predictive Analytics, volume\n  SS-09-09, pages 114-119, ISBN:978-1-57735-416-1, AAAI Press, Stanford\n  University, March 2009.",
        "doi": null,
        "report-no": "LA-UR-08-06397",
        "categories": "cs.LO cs.SC",
        "license": "http://creativecommons.org/licenses/publicdomain/",
        "abstract": "  Multi-relational networks are used extensively to structure knowledge.\nPerhaps the most popular instance, due to the widespread adoption of the\nSemantic Web, is the Resource Description Framework (RDF). One of the primary\npurposes of a knowledge network is to reason; that is, to alter the topology of\nthe network according to an algorithm that uses the existing topological\nstructure as its input. There exist many such reasoning algorithms. With\nrespect to the Semantic Web, the bivalent, monotonic reasoners of the RDF\nSchema (RDFS) and the Web Ontology Language (OWL) are the most prevalent.\nHowever, nothing prevents other forms of reasoning from existing in the\nSemantic Web. This article presents a non-bivalent, non-monotonic, evidential\nlogic and reasoner that is an algebraic ring over a multi-relational network\nequipped with two binary operations that can be composed to execute various\nforms of inference. Given its multi-relational grounding, it is possible to use\nthe presented evidential framework as another method for structuring knowledge\nand reasoning in the Semantic Web. The benefits of this framework are that it\nworks with arbitrary, partial, and contradictory knowledge while, at the same\ntime, it supports a tractable approximate reasoning process.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 8 Oct 2008 17:49:15 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 30 Dec 2008 18:17:46 GMT"
            }
        ],
        "update_date": "2009-05-13",
        "authors_parsed": [
            [
                "Rodriguez",
                "Marko A.",
                ""
            ],
            [
                "Geldart",
                "Joe",
                ""
            ]
        ]
    },
    {
        "id": "0810.1571",
        "submitter": "Rena Bakhshi",
        "authors": "Rena Bakhshi and Daniela Gavidia and Wan Fokkink and Maarten van Steen",
        "title": "An Analytical Model of Information Dissemination for a Gossip-based\n  Protocol",
        "comments": "20 pages, 8 figures, technical report",
        "journal-ref": null,
        "doi": "10.1016/j.comnet.2009.03.017",
        "report-no": null,
        "categories": "cs.DC cs.DM cs.IT cs.PF math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop an analytical model of information dissemination for a gossiping\nprotocol that combines both pull and push approaches. With this model we\nanalyse how fast an item is replicated through a network, and how fast the item\nspreads in the network, and how fast the item covers the network. We also\ndetermine the optimal size of the exchange buffer, to obtain fast replication.\nOur results are confirmed by large-scale simulation experiments.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 9 Oct 2008 04:42:27 GMT"
            }
        ],
        "update_date": "2010-03-11",
        "authors_parsed": [
            [
                "Bakhshi",
                "Rena",
                ""
            ],
            [
                "Gavidia",
                "Daniela",
                ""
            ],
            [
                "Fokkink",
                "Wan",
                ""
            ],
            [
                "van Steen",
                "Maarten",
                ""
            ]
        ]
    },
    {
        "id": "0810.1574",
        "submitter": "Michael Singer",
        "authors": "Ruyong Feng, Michael F. Singer, Min Wu",
        "title": "Liouvillian Solutions of Difference-Differential Equations",
        "comments": "53 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC math.CA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For a field k$with an automorphism \\sigma and a derivation \\delta, we\nintroduce the notion of liouvillian solutions of linear difference-differential\nsystems {\\sigma(Y) = AY, \\delta(Y) = BY} over k and characterize the existence\nof liouvillian solutions in terms of the Galois group of the systems. We will\ngive an algorithm to decide whether such a system has liouvillian solutions\nwhen k = C(x,t), \\sigma(x) = x+1, \\delta = d/dt$ and the size of the system is\na prime.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 9 Oct 2008 05:16:48 GMT"
            }
        ],
        "update_date": "2008-10-10",
        "authors_parsed": [
            [
                "Feng",
                "Ruyong",
                ""
            ],
            [
                "Singer",
                "Michael F.",
                ""
            ],
            [
                "Wu",
                "Min",
                ""
            ]
        ]
    },
    {
        "id": "0810.1997",
        "submitter": "Heping Gao",
        "authors": "Heping Gao, Meera Sitharam",
        "title": "Characterizing 1-Dof Henneberg-I graphs with efficient configuration\n  spaces",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CG cs.RO cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We define and study exact, efficient representations of realization spaces of\na natural class of underconstrained 2D Euclidean Distance Constraint\nSystems(EDCS) or Frameworks based on 1-dof Henneberg-I graphs. Each\nrepresentation corresponds to a choice of parameters and yields a different\nparametrized configuration space. Our notion of efficiency is based on the\nalgebraic complexities of sampling the configuration space and of obtaining a\nrealization from the sample (parametrized) configuration. Significantly, we\ngive purely combinatorial characterizations that capture (i) the class of\ngraphs that have efficient configuration spaces and (ii) the possible choices\nof representation parameters that yield efficient configuration spaces for a\ngiven graph. Our results automatically yield an efficient algorithm for\nsampling realizations, without missing extreme or boundary realizations. In\naddition, our results formally show that our definition of efficient\nconfiguration space is robust and that our characterizations are tight. We\nchoose the class of 1-dof Henneberg-I graphs in order to take the next step in\na systematic and graded program of combinatorial characterizations of efficient\nconfiguration spaces. In particular, the results presented here are the first\ncharacterizations that go beyond graphs that have connected and convex\nconfiguration spaces.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 12 Oct 2008 20:17:21 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 6 Nov 2008 03:48:59 GMT"
            }
        ],
        "update_date": "2008-11-06",
        "authors_parsed": [
            [
                "Gao",
                "Heping",
                ""
            ],
            [
                "Sitharam",
                "Meera",
                ""
            ]
        ]
    },
    {
        "id": "0810.3203",
        "submitter": "David Harvey",
        "authors": "David Harvey",
        "title": "A cache-friendly truncated FFT",
        "comments": "14 pages, 11 figures, uses algorithm2e package",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe a cache-friendly version of van der Hoeven's truncated FFT and\ninverse truncated FFT, focusing on the case of `large' coefficients, such as\nthose arising in the Schonhage--Strassen algorithm for multiplication in Z[x].\nWe describe two implementations and examine their performance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 17 Oct 2008 17:36:27 GMT"
            }
        ],
        "update_date": "2008-10-20",
        "authors_parsed": [
            [
                "Harvey",
                "David",
                ""
            ]
        ]
    },
    {
        "id": "0810.3468",
        "submitter": "Muthiah Annamalai",
        "authors": "Muthiah Annamalai, Leela Velusamy",
        "title": "A Call-Graph Profiler for GNU Octave",
        "comments": "6 pages, 2 figures, 1 table. Fix typos",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.PL cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We report the design and implementation of a call-graph profiler for GNU\nOctave, a numerical computing platform. GNU Octave simplifies matrix\ncomputation for use in modeling or simulation. Our work provides a call-graph\nprofiler, which is an improvement on the flat profiler. We elaborate design\nconstraints of building a profiler for numerical computation, and benchmark the\nprofiler by comparing it to the rudimentary timer start-stop (tic-toc)\nmeasurements, for a similar set of programs. The profiler code provides clean\ninterfaces to internals of GNU Octave, for other (newer) profiling tools on GNU\nOctave.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 20 Oct 2008 08:29:21 GMT"
            }
        ],
        "update_date": "2008-10-21",
        "authors_parsed": [
            [
                "Annamalai",
                "Muthiah",
                ""
            ],
            [
                "Velusamy",
                "Leela",
                ""
            ]
        ]
    },
    {
        "id": "0810.3641",
        "submitter": "Gerard Henry Edmond Duchamp",
        "authors": "G\\'erard Henry Edmond Duchamp (LIPN), Silvia Goodenough (LIPN), Karol\n  A. Penson (LPTMC)",
        "title": "Rational Hadamard products via Quantum Diagonal Operators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC math-ph math.CO math.MP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We use the remark that, through Bargmann-Fock representation, diagonal\noperators of the Heisenberg-Weyl algebra are scalars for the Hadamard product\nto give some properties (like the stability of periodic fonctions) of the\nHadamard product by a rational fraction. In particular, we provide through this\nway explicit formulas for the multiplication table of the Hadamard product in\nthe algebra of rational functions in $\\C[[z]]$.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 20 Oct 2008 19:16:29 GMT"
            }
        ],
        "update_date": "2008-10-21",
        "authors_parsed": [
            [
                "Duchamp",
                "G\u00e9rard Henry Edmond",
                "",
                "LIPN"
            ],
            [
                "Goodenough",
                "Silvia",
                "",
                "LIPN"
            ],
            [
                "Penson",
                "Karol A.",
                "",
                "LPTMC"
            ]
        ]
    },
    {
        "id": "0810.4727",
        "submitter": "Xinjia Chen",
        "authors": "Xinjia Chen",
        "title": "Robust Estimation of Mean Values",
        "comments": "12 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.ST cs.SY math.PR stat.CO stat.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we develop a computational approach for estimating the mean\nvalue of a quantity in the presence of uncertainty. We demonstrate that, under\nsome mild assumptions, the upper and lower bounds of the mean value are\nefficiently computable via a sample reuse technique, of which the computational\ncomplexity is shown to posses a Poisson distribution.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 26 Oct 2008 23:22:15 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 11 Nov 2008 02:49:24 GMT"
            }
        ],
        "update_date": "2013-11-05",
        "authors_parsed": [
            [
                "Chen",
                "Xinjia",
                ""
            ]
        ]
    },
    {
        "id": "0810.5647",
        "submitter": "Gilles Villard",
        "authors": "Gilles Villard (LIP)",
        "title": "Kaltofen's division-free determinant algorithm differentiated for matrix\n  adjoint computation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.CC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Kaltofen has proposed a new approach in 1992 for computing matrix\ndeterminants without divisions. The algorithm is based on a baby steps/giant\nsteps construction of Krylov subspaces, and computes the determinant as the\nconstant term of a characteristic polynomial. For matrices over an abstract\nring, by the results of Baur and Strassen, the determinant algorithm, actually\na straight-line program, leads to an algorithm with the same complexity for\ncomputing the adjoint of a matrix. However, the latter adjoint algorithm is\nobtained by the reverse mode of automatic differentiation, hence somehow is not\n\"explicit\". We present an alternative (still closely related) algorithm for the\nadjoint thatcan be implemented directly, we mean without resorting to an\nautomatic transformation. The algorithm is deduced by applying program\ndifferentiation techniques \"by hand\" to Kaltofen's method, and is completely\ndecribed. As subproblem, we study the differentiation of programs that compute\nminimum polynomials of lineraly generated sequences, and we use a lazy\npolynomial evaluation mechanism for reducing the cost of Strassen's avoidance\nof divisions in our case.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 31 Oct 2008 09:43:48 GMT"
            }
        ],
        "update_date": "2008-11-03",
        "authors_parsed": [
            [
                "Villard",
                "Gilles",
                "",
                "LIP"
            ]
        ]
    },
    {
        "id": "0810.5685",
        "submitter": "Daniel Roche",
        "authors": "Mark Giesbrecht and Daniel S. Roche",
        "title": "Interpolation of Shifted-Lacunary Polynomials",
        "comments": "22 pages, to appear in Computational Complexity",
        "journal-ref": "Computational Complexity, Vol. 19, No 3., pp. 333-354, 2010",
        "doi": "10.1007/s00037-010-0294-0",
        "report-no": null,
        "categories": "cs.SC cs.DS cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a \"black box\" function to evaluate an unknown rational polynomial f in\nQ[x] at points modulo a prime p, we exhibit algorithms to compute the\nrepresentation of the polynomial in the sparsest shifted power basis. That is,\nwe determine the sparsity t, the shift s (a rational), the exponents 0 <= e1 <\ne2 < ... < et, and the coefficients c1,...,ct in Q\\{0} such that f(x) =\nc1(x-s)^e1+c2(x-s)^e2+...+ct(x-s)^et. The computed sparsity t is absolutely\nminimal over any shifted power basis. The novelty of our algorithm is that the\ncomplexity is polynomial in the (sparse) representation size, and in particular\nis logarithmic in deg(f). Our method combines previous celebrated results on\nsparse interpolation and computing sparsest shifts, and provides a way to\nhandle polynomials with extremely high degree which are, in some sense, sparse\nin information.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 31 Oct 2008 13:35:08 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 4 Nov 2008 00:39:28 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 10 Nov 2008 18:33:23 GMT"
            },
            {
                "version": "v4",
                "created": "Thu, 11 Dec 2008 04:05:14 GMT"
            },
            {
                "version": "v5",
                "created": "Fri, 4 Dec 2009 06:10:58 GMT"
            },
            {
                "version": "v6",
                "created": "Mon, 23 Aug 2010 16:20:07 GMT"
            }
        ],
        "update_date": "2010-12-06",
        "authors_parsed": [
            [
                "Giesbrecht",
                "Mark",
                ""
            ],
            [
                "Roche",
                "Daniel S.",
                ""
            ]
        ]
    },
    {
        "id": "0810.5685",
        "submitter": "Daniel Roche",
        "authors": "Mark Giesbrecht and Daniel S. Roche",
        "title": "Interpolation of Shifted-Lacunary Polynomials",
        "comments": "22 pages, to appear in Computational Complexity",
        "journal-ref": "Computational Complexity, Vol. 19, No 3., pp. 333-354, 2010",
        "doi": "10.1007/s00037-010-0294-0",
        "report-no": null,
        "categories": "cs.SC cs.DS cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a \"black box\" function to evaluate an unknown rational polynomial f in\nQ[x] at points modulo a prime p, we exhibit algorithms to compute the\nrepresentation of the polynomial in the sparsest shifted power basis. That is,\nwe determine the sparsity t, the shift s (a rational), the exponents 0 <= e1 <\ne2 < ... < et, and the coefficients c1,...,ct in Q\\{0} such that f(x) =\nc1(x-s)^e1+c2(x-s)^e2+...+ct(x-s)^et. The computed sparsity t is absolutely\nminimal over any shifted power basis. The novelty of our algorithm is that the\ncomplexity is polynomial in the (sparse) representation size, and in particular\nis logarithmic in deg(f). Our method combines previous celebrated results on\nsparse interpolation and computing sparsest shifts, and provides a way to\nhandle polynomials with extremely high degree which are, in some sense, sparse\nin information.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 31 Oct 2008 13:35:08 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 4 Nov 2008 00:39:28 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 10 Nov 2008 18:33:23 GMT"
            },
            {
                "version": "v4",
                "created": "Thu, 11 Dec 2008 04:05:14 GMT"
            },
            {
                "version": "v5",
                "created": "Fri, 4 Dec 2009 06:10:58 GMT"
            },
            {
                "version": "v6",
                "created": "Mon, 23 Aug 2010 16:20:07 GMT"
            }
        ],
        "update_date": "2010-12-06",
        "authors_parsed": [
            [
                "Giesbrecht",
                "Mark",
                ""
            ],
            [
                "Roche",
                "Daniel S.",
                ""
            ]
        ]
    },
    {
        "id": "0811.1061",
        "submitter": "Heinz Kredel",
        "authors": "Raphael Jolly and Heinz Kredel",
        "title": "How to turn a scripting language into a domain specific language for\n  computer algebra",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We have developed two computer algebra systems, meditor [Jolly:2007] and JAS\n[Kredel:2006]. These CAS systems are available as Java libraries. For the\nuse-case of interactively entering and manipulating mathematical expressions,\nthere is a need of a scripting front-end for our libraries. Most other CAS\ninvent and implement their own scripting interface for this purpose. We,\nhowever, do not want to reinvent the wheel and propose to use a contemporary\nscripting language with access to Java code. In this paper we discuss the\nrequirements for a scripting language in computer algebra and check whether the\nlanguages Python, Ruby, Groovy and Scala meet these requirements. We conclude\nthat, with minor problems, any of these languages is suitable for our purpose.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Nov 2008 23:07:36 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 27 Nov 2008 16:34:04 GMT"
            }
        ],
        "update_date": "2008-11-27",
        "authors_parsed": [
            [
                "Jolly",
                "Raphael",
                ""
            ],
            [
                "Kredel",
                "Heinz",
                ""
            ]
        ]
    },
    {
        "id": "0811.1081",
        "submitter": "Mircea Andrecut Dr",
        "authors": "M. Andrecut",
        "title": "Parallel GPU Implementation of Iterative PCA Algorithms",
        "comments": "45 pages, 1 figure, source code included",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "q-bio.QM cs.MS physics.comp-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Principal component analysis (PCA) is a key statistical technique for\nmultivariate data analysis. For large data sets the common approach to PCA\ncomputation is based on the standard NIPALS-PCA algorithm, which unfortunately\nsuffers from loss of orthogonality, and therefore its applicability is usually\nlimited to the estimation of the first few components. Here we present an\nalgorithm based on Gram-Schmidt orthogonalization (called GS-PCA), which\neliminates this shortcoming of NIPALS-PCA. Also, we discuss the GPU (Graphics\nProcessing Unit) parallel implementation of both NIPALS-PCA and GS-PCA\nalgorithms. The numerical results show that the GPU parallel optimized\nversions, based on CUBLAS (NVIDIA) are substantially faster (up to 12 times)\nthan the CPU optimized versions based on CBLAS (GNU Scientific Library).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Nov 2008 04:34:01 GMT"
            }
        ],
        "update_date": "2008-11-10",
        "authors_parsed": [
            [
                "Andrecut",
                "M.",
                ""
            ]
        ]
    },
    {
        "id": "0811.1151",
        "submitter": "Benoit Delahaye",
        "authors": "Beno\\^it Delahaye (IRISA), Beno\\^it Caillaud (IRISA, Irisa, Irisa)",
        "title": "A Model for Probabilistic Reasoning on Assume/Guarantee Contracts",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "RR-6719",
        "categories": "cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present a probabilistic adaptation of an Assume/Guarantee\ncontract formalism. For the sake of generality, we assume that the extended\nstate machines used in the contracts and implementations define sets of runs on\na given set of variables, that compose by intersection over the common\nvariables. In order to enable probabilistic reasoning, we consider that the\ncontracts dictate how certain input variables will behave, being either\nnon-deterministic, or probabilistic; the introduction of probabilistic\nvariables leading us to tune the notions of implementation, refinement and\ncomposition. As shown in the report, this probabilistic adaptation of the\nAssume/Guarantee contract theory preserves compositionality and therefore\nallows modular reliability analysis, either with a top-down or a bottom-up\napproach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 Nov 2008 14:33:10 GMT"
            }
        ],
        "update_date": "2009-04-20",
        "authors_parsed": [
            [
                "Delahaye",
                "Beno\u00eet",
                "",
                "IRISA"
            ],
            [
                "Caillaud",
                "Beno\u00eet",
                "",
                "IRISA, Irisa, Irisa"
            ]
        ]
    },
    {
        "id": "0811.1714",
        "submitter": "Martin Albrecht",
        "authors": "Martin Albrecht, Gregory Bard, William Hart",
        "title": "Efficient Multiplication of Dense Matrices over GF(2)",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1145/1644001.1644010",
        "report-no": null,
        "categories": "cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe an efficient implementation of a hierarchy of algorithms for\nmultiplication of dense matrices over the field with two elements (GF(2)). In\nparticular we present our implementation -- in the M4RI library -- of\nStrassen-Winograd matrix multiplication and the \"Method of the Four Russians\"\nmultiplication (M4RM) and compare it against other available implementations.\nGood performance is demonstrated on on AMD's Opteron and particulary good\nperformance on Intel's Core 2 Duo. The open-source M4RI library is available\nstand-alone as well as part of the Sage mathematics software.\n  In machine terms, addition in GF(2) is logical-XOR, and multiplication is\nlogical-AND, thus a machine word of 64-bits allows one to operate on 64\nelements of GF(2) in parallel: at most one CPU cycle for 64 parallel additions\nor multiplications. As such, element-wise operations over GF(2) are relatively\ncheap. In fact, in this paper, we conclude that the actual bottlenecks are\nmemory reads and writes and issues of data locality. We present our empirical\nfindings in relation to minimizing these and give an analysis thereof.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 11 Nov 2008 14:23:49 GMT"
            }
        ],
        "update_date": "2012-03-27",
        "authors_parsed": [
            [
                "Albrecht",
                "Martin",
                ""
            ],
            [
                "Bard",
                "Gregory",
                ""
            ],
            [
                "Hart",
                "William",
                ""
            ]
        ]
    },
    {
        "id": "0811.2596",
        "submitter": "Mohamed Hamdy Morsy Osman",
        "authors": "Mohamed H.S. Morsy, Mohamad Y.S. Sowailem and Hossam M.H. Shalaby",
        "title": "An Enhanced Mathematical Model for Performance Evaluation of Optical\n  Burst Switched Networks",
        "comments": "This paper has been withdrawn by the authors",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper has been withdrawn by the authors.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 16 Nov 2008 19:10:43 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 6 Dec 2009 01:34:28 GMT"
            }
        ],
        "update_date": "2009-12-07",
        "authors_parsed": [
            [
                "Morsy",
                "Mohamed H. S.",
                ""
            ],
            [
                "Sowailem",
                "Mohamad Y. S.",
                ""
            ],
            [
                "Shalaby",
                "Hossam M. H.",
                ""
            ]
        ]
    },
    {
        "id": "0811.3165",
        "submitter": "Nitin Saxena",
        "authors": "G\\'abor Ivanyos, Marek Karpinski, Lajos R\\'onyai and Nitin Saxena",
        "title": "Trading GRH for algebra: algorithms for factoring polynomials and\n  related structures",
        "comments": "35 pages, preliminary version",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CC cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we develop techniques that eliminate the need of the\nGeneralized Riemann Hypothesis (GRH) from various (almost all) known results\nabout deterministic polynomial factoring over finite fields. Our main result\nshows that given a polynomial f(x) of degree n over a finite field k, we can\nfind in deterministic poly(n^{\\log n},\\log |k|) time \"either\" a nontrivial\nfactor of f(x) \"or\" a nontrivial automorphism of k[x]/(f(x)) of order n. This\nmain tool leads to various new GRH-free results, most striking of which are:\n  (1) Given a noncommutative algebra over a finite field, we can find a zero\ndivisor in deterministic subexponential time.\n  (2) Given a positive integer r such that either 8|r or r has at least two\ndistinct odd prime factors. There is a deterministic polynomial time algorithm\nto find a nontrivial factor of the r-th cyclotomic polynomial over a finite\nfield.\n  In this paper, following the seminal work of Lenstra (1991) on constructing\nisomorphisms between finite fields, we further generalize classical Galois\ntheory constructs like cyclotomic extensions, Kummer extensions, Teichmuller\nsubgroups, to the case of commutative semisimple algebras with automorphisms.\nThese generalized constructs help eliminate the dependence on GRH.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 Nov 2008 17:57:25 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 8 Feb 2009 17:14:49 GMT"
            }
        ],
        "update_date": "2009-02-08",
        "authors_parsed": [
            [
                "Ivanyos",
                "G\u00e1bor",
                ""
            ],
            [
                "Karpinski",
                "Marek",
                ""
            ],
            [
                "R\u00f3nyai",
                "Lajos",
                ""
            ],
            [
                "Saxena",
                "Nitin",
                ""
            ]
        ]
    },
    {
        "id": "0811.3272",
        "submitter": "Ali Sydney",
        "authors": "Ali Sydney, Caterina Scoglio, Mina Youssef, and Phillip Schumm",
        "title": "Characterizing the Robustness of Complex Networks",
        "comments": "This paper serves as a replacement to its predecessor",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF physics.data-an",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  With increasingly ambitious initiatives such as GENI and FIND that seek to\ndesign the future Internet, it becomes imperative to define the characteristics\nof robust topologies, and build future networks optimized for robustness. This\npaper investigates the characteristics of network topologies that maintain a\nhigh level of throughput in spite of multiple attacks. To this end, we select\nnetwork topologies belonging to the main network models and some real world\nnetworks. We consider three types of attacks: removal of random nodes, high\ndegree nodes, and high betweenness nodes. We use elasticity as our robustness\nmeasure and, through our analysis, illustrate that different topologies can\nhave different degrees of robustness. In particular, elasticity can fall as low\nas 0.8% of the upper bound based on the attack employed. This result\nsubstantiates the need for optimized network topology design. Furthermore, we\nimplement a tradeoff function that combines elasticity under the three attack\nstrategies and considers the cost of the network. Our extensive simulations\nshow that, for a given network density, regular and semi-regular topologies can\nhave higher degrees of robustness than heterogeneous topologies, and that link\nredundancy is a sufficient but not necessary condition for robustness.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 20 Nov 2008 07:35:39 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 8 Jun 2009 18:55:35 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 25 Sep 2009 20:31:20 GMT"
            }
        ],
        "update_date": "2009-09-25",
        "authors_parsed": [
            [
                "Sydney",
                "Ali",
                ""
            ],
            [
                "Scoglio",
                "Caterina",
                ""
            ],
            [
                "Youssef",
                "Mina",
                ""
            ],
            [
                "Schumm",
                "Phillip",
                ""
            ]
        ]
    },
    {
        "id": "0811.3387",
        "submitter": "Matthias W\\\"ahlisch",
        "authors": "Matthias W\\\"ahlisch, Thomas C. Schmidt and Georg Wittenburg",
        "title": "Broadcasting in Prefix Space: P2P Data Dissemination with Predictable\n  Performance",
        "comments": "final version for ICIW'09",
        "journal-ref": "Matthias W\\\"ahlisch, Thomas C. Schmidt, and Georg Wittenburg,\n  \"Broadcasting in Prefix Space: P2P Data Dissemination with Predictable\n  Performance,\" in Proc. of the Fourth ICIW: IEEE ComSoc Press, 2009, pp. 74-83",
        "doi": "10.1109/ICIW.2009.19",
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A broadcast mode may augment peer-to-peer overlay networks with an efficient,\nscalable data replication function, but may also give rise to a virtual link\nlayer in VPN-type solutions. We introduce a simple broadcasting mechanism that\noperates in the prefix space of distributed hash tables without signaling. This\npaper concentrates on the performance analysis of the prefix flooding scheme.\nStarting from simple models of recursive $k$-ary trees, we analytically derive\ndistributions of hop counts and the replication load. Extensive simulation\nresults are presented further on, based on an implementation within the OverSim\nframework. Comparisons are drawn to Scribe, taken as a general reference model\nfor group communication according to the shared, rendezvous-point-centered\ndistribution paradigm. The prefix flooding scheme thereby confirmed its widely\npredictable performance and consistently outperformed Scribe in all metrics.\nReverse path selection in overlays is identified as a major cause of\nperformance degradation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 20 Nov 2008 17:56:17 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 25 May 2009 15:16:39 GMT"
            }
        ],
        "update_date": "2009-05-25",
        "authors_parsed": [
            [
                "W\u00e4hlisch",
                "Matthias",
                ""
            ],
            [
                "Schmidt",
                "Thomas C.",
                ""
            ],
            [
                "Wittenburg",
                "Georg",
                ""
            ]
        ]
    },
    {
        "id": "0811.3521",
        "submitter": "Angelo Brillout",
        "authors": "Angelo Brillout, Daniel Kroening, and Thomas Wahl",
        "title": "Craig Interpolation for Quantifier-Free Presburger Arithmetic",
        "comments": "15 pages, 1 algorithm, 1 figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Craig interpolation has become a versatile algorithmic tool for improving\nsoftware verification. Interpolants can, for instance, accelerate the\nconvergence of fixpoint computations for infinite-state systems. They also help\nimprove the refinement of iteratively computed lazy abstractions. Efficient\ninterpolation procedures have been presented only for a few theories. In this\npaper, we introduce a complete interpolation method for the full range of\nquantifier-free Presburger arithmetic formulas. We propose a novel convex\nvariable projection for integer inequalities and a technique to combine them\nwith equalities. The derivation of the interpolant has complexity low-degree\npolynomial in the size of the refutation proof and is typically fast in\npractice.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Nov 2008 11:44:22 GMT"
            }
        ],
        "update_date": "2008-11-24",
        "authors_parsed": [
            [
                "Brillout",
                "Angelo",
                ""
            ],
            [
                "Kroening",
                "Daniel",
                ""
            ],
            [
                "Wahl",
                "Thomas",
                ""
            ]
        ]
    },
    {
        "id": "0811.3712",
        "submitter": "Kui Wu",
        "authors": "Kui Wu, Yuming Jiang, Guoqiang Hu",
        "title": "Performance Modeling and Evaluation for Information-Driven Networks",
        "comments": "14 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Information-driven networks include a large category of networking systems,\nwhere network nodes are aware of information delivered and thus can not only\nforward data packets but may also perform information processing. In many\nsituations, the quality of service (QoS) in information-driven networks is\nprovisioned with the redundancy in information. Traditional performance models\ngenerally adopt evaluation measures suitable for packet-oriented service\nguarantee, such as packet delay, throughput, and packet loss rate. These\nperformance measures, however, do not align well with the actual need of\ninformation-driven networks. New performance measures and models for\ninformation-driven networks, despite their importance, have been mainly blank,\nlargely because information processing is clearly application dependent and\ncannot be easily captured within a generic framework. To fill the vacancy, we\npresent a new performance evaluation framework particularly tailored for\ninformation-driven networks, based on the recent development of stochastic\nnetwork calculus. We analyze the QoS with respect to information delivery and\nstudy the scheduling problem with the new performance metrics. Our analytical\nframework can be used to calculate the network capacity in information delivery\nand in the meantime to help transmission scheduling for a large body of systems\nwhere QoS is stochastically guaranteed with the redundancy in information.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 22 Nov 2008 21:29:47 GMT"
            }
        ],
        "update_date": "2008-11-25",
        "authors_parsed": [
            [
                "Wu",
                "Kui",
                ""
            ],
            [
                "Jiang",
                "Yuming",
                ""
            ],
            [
                "Hu",
                "Guoqiang",
                ""
            ]
        ]
    },
    {
        "id": "0811.4061",
        "submitter": "Valery Pipin",
        "authors": "Valery V. Pipin",
        "title": "Benchmarking the solar dynamo with Maxima",
        "comments": "8 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Recently, Jouve et al(A&A, 2008) published the paper that presents the\nnumerical benchmark for the solar dynamo models. Here, I would like to show a\nway how to get it with help of computer algebra system Maxima. This way was\nused in our paper (Pipin & Seehafer, A&A 2008, in print) to test some new ideas\nin the large-scale stellar dynamos. In the present paper I complement the\ndynamo benchmark with the standard test that address the problem of the\nfree-decay modes in the sphere which is submerged in vacuum.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Nov 2008 11:35:35 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 26 Nov 2008 04:13:36 GMT"
            }
        ],
        "update_date": "2008-11-26",
        "authors_parsed": [
            [
                "Pipin",
                "Valery V.",
                ""
            ]
        ]
    },
    {
        "id": "0811.4720",
        "submitter": "Florent Jacquemard",
        "authors": "Adel Bouhoula and Florent Jacquemard",
        "title": "Automated Induction for Complex Data Structures",
        "comments": "This is the long version of a paper which appeared in IJCAR 2008. 38\n  pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a procedure for automated implicit inductive theorem proving for\nequational specifications made of rewrite rules with conditions and\nconstraints. The constraints are interpreted over constructor terms\n(representing data values), and may express syntactic equality, disequality,\nordering and also membership in a fixed tree language. Constrained equational\naxioms between constructor terms are supported and can be used in order to\nspecify complex data structures like sets, sorted lists, trees, powerlists...\n  Our procedure is based on tree grammars with constraints, a formalism which\ncan describe exactly the initial model of the given specification (when it is\nsufficiently complete and terminating). They are used in the inductive proofs\nfirst as an induction scheme for the generation of subgoals at induction steps,\nsecond for checking validity and redundancy criteria by reduction to an\nemptiness problem, and third for defining and solving membership constraints.\n  We show that the procedure is sound and refutationally complete. It\ngeneralizes former test set induction techniques and yields natural proofs for\nseveral non-trivial examples presented in the paper, these examples are\ndifficult to specify and carry on automatically with related induction\nprocedures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 28 Nov 2008 13:58:46 GMT"
            }
        ],
        "update_date": "2008-12-01",
        "authors_parsed": [
            [
                "Bouhoula",
                "Adel",
                ""
            ],
            [
                "Jacquemard",
                "Florent",
                ""
            ]
        ]
    },
    {
        "id": "0812.0067",
        "submitter": "Bernard Mourrain",
        "authors": "Bernard Mourrain (INRIA Sophia Antipolis), Philippe Tr\\'ebuchet (LIP6)",
        "title": "Stable normal forms for polynomial system solving",
        "comments": null,
        "journal-ref": "Theoretical Computer Science 409, 2 (2008) 229-240",
        "doi": "10.1016/j.tcs.2008.09.004",
        "report-no": null,
        "categories": "cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes and analyzes a method for computing border bases of a\nzero-dimensional ideal $I$. The criterion used in the computation involves\nspecific commutation polynomials and leads to an algorithm and an\nimplementation extending the one provided in [MT'05]. This general border basis\nalgorithm weakens the monomial ordering requirement for \\grob bases\ncomputations. It is up to date the most general setting for representing\nquotient algebras, embedding into a single formalism Gr\\\"obner bases, Macaulay\nbases and new representation that do not fit into the previous categories. With\nthis formalism we show how the syzygies of the border basis are generated by\ncommutation relations. We also show that our construction of normal form is\nstable under small perturbations of the ideal, if the number of solutions\nremains constant. This new feature for a symbolic algorithm has a huge impact\non the practical efficiency as it is illustrated by the experiments on\nclassical benchmark polynomial systems, at the end of the paper.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 29 Nov 2008 11:26:50 GMT"
            }
        ],
        "update_date": "2008-12-02",
        "authors_parsed": [
            [
                "Mourrain",
                "Bernard",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Tr\u00e9buchet",
                "Philippe",
                "",
                "LIP6"
            ]
        ]
    },
    {
        "id": "0812.0088",
        "submitter": "Bernard Mourrain",
        "authors": "Itnuit Janovitz-Freireich, Agnes Szanto, Bernard Mourrain (INRIA\n  Sophia Antipolis), Lajos Ronyai",
        "title": "Moment matrices, trace matrices and the radical of ideals",
        "comments": null,
        "journal-ref": "nternational Conference on Symbolic and Algebraic Computation\n  (2008) 125-132",
        "doi": null,
        "report-no": null,
        "categories": "cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $f_1,...,f_s \\in \\mathbb{K}[x_1,...,x_m]$ be a system of polynomials\ngenerating a zero-dimensional ideal $\\I$, where $\\mathbb{K}$ is an arbitrary\nalgebraically closed field. Assume that the factor algebra\n$\\A=\\mathbb{K}[x_1,...,x_m]/\\I$ is Gorenstein and that we have a bound\n$\\delta>0$ such that a basis for $\\A$ can be computed from multiples of\n$f_1,...,f_s$ of degrees at most $\\delta$. We propose a method using Sylvester\nor Macaulay type resultant matrices of $f_1,...,f_s$ and $J$, where $J$ is a\npolynomial of degree $\\delta$ generalizing the Jacobian, to compute moment\nmatrices, and in particular matrices of traces for $\\A$. These matrices of\ntraces in turn allow us to compute a system of multiplication matrices\n$\\{M_{x_i}|i=1,...,m\\}$ of the radical $\\sqrt{\\I}$, following the approach in\nthe previous work by Janovitz-Freireich, R\\'{o}nyai and Sz\\'ant\\'o.\nAdditionally, we give bounds for $\\delta$ for the case when $\\I$ has finitely\nmany projective roots in $\\mathbb{P}^m_\\CC$.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 29 Nov 2008 16:14:07 GMT"
            }
        ],
        "update_date": "2009-01-23",
        "authors_parsed": [
            [
                "Janovitz-Freireich",
                "Itnuit",
                "",
                "INRIA\n  Sophia Antipolis"
            ],
            [
                "Szanto",
                "Agnes",
                "",
                "INRIA\n  Sophia Antipolis"
            ],
            [
                "Mourrain",
                "Bernard",
                "",
                "INRIA\n  Sophia Antipolis"
            ],
            [
                "Ronyai",
                "Lajos",
                ""
            ]
        ]
    },
    {
        "id": "0812.0192",
        "submitter": "Mohamed Hamdy Morsy Osman",
        "authors": "Mohamed H. S. Morsy, Mohammad Y. S. Sowailem and Hossam M. H. Shalaby",
        "title": "A Simple Performance Analysis of a Core Node in an Optical Burst\n  Switched Network",
        "comments": "This paper has been withdrawn",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper has been withdrawn\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 1 Dec 2008 00:55:14 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 6 Dec 2009 01:14:22 GMT"
            }
        ],
        "update_date": "2009-12-07",
        "authors_parsed": [
            [
                "Morsy",
                "Mohamed H. S.",
                ""
            ],
            [
                "Sowailem",
                "Mohammad Y. S.",
                ""
            ],
            [
                "Shalaby",
                "Hossam M. H.",
                ""
            ]
        ]
    },
    {
        "id": "0812.0601",
        "submitter": "Shaowei Lin",
        "authors": "Shaowei Lin, Bernd Sturmfels",
        "title": "Polynomial relations among principal minors of a 4x4-matrix",
        "comments": "16 pages, v2: Remark 14",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.AG cs.SC math.AC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The image of the principal minor map for n x n-matrices is shown to be\nclosed. In the 19th century, Nansen and Muir studied the implicitization\nproblem of finding all relations among principal minors when n=4. We complete\ntheir partial results by constructing explicit polynomials of degree 12 that\nscheme-theoretically define this affine variety and also its projective closure\nin $\\PP^{15}$. The latter is the main component in the singular locus of the 2\nx 2 x 2 x 2-hyperdeterminant.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 2 Dec 2008 21:34:51 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 18 Dec 2008 17:12:26 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 22 Jun 2009 20:31:19 GMT"
            }
        ],
        "update_date": "2009-06-22",
        "authors_parsed": [
            [
                "Lin",
                "Shaowei",
                ""
            ],
            [
                "Sturmfels",
                "Bernd",
                ""
            ]
        ]
    },
    {
        "id": "0812.0904",
        "submitter": "Jun Kyoung Lee",
        "authors": "Jun Kyoung Lee, Janghoon Yang, Dong Ku Kim",
        "title": "An Approximation of the Outage Probability for Multi-hop AF Fixed Gain\n  Relay",
        "comments": "3 pages, 3 figures, Submitted to IEEE Communication Letters",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this letter, we present a closed-form approximation of the outage\nprobability for the multi-hop amplify-and-forward (AF) relaying systems with\nfixed gain in Rayleigh fading channel. The approximation is derived from the\noutage event for each hop. The simulation results show the tightness of the\nproposed approximation in low and high signal-to-noise ratio (SNR) region.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 4 Dec 2008 11:45:48 GMT"
            }
        ],
        "update_date": "2008-12-05",
        "authors_parsed": [
            [
                "Lee",
                "Jun Kyoung",
                ""
            ],
            [
                "Yang",
                "Janghoon",
                ""
            ],
            [
                "Kim",
                "Dong Ku",
                ""
            ]
        ]
    },
    {
        "id": "0812.2563",
        "submitter": "Bernard Mourrain",
        "authors": "Monique Laurent (CWI), Bernard Mourrain (INRIA Sophia Antipolis)",
        "title": "A Sparse Flat Extension Theorem for Moment Matrices",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC math.AC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this note we prove a generalization of the flat extension theorem of Curto\nand Fialkow for truncated moment matrices. It applies to moment matrices\nindexed by an arbitrary set of monomials and its border, assuming that this set\nis connected to 1. When formulated in a basis-free setting, this gives an\nequivalent result for truncated Hankel operators.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 15 Dec 2008 07:49:22 GMT"
            }
        ],
        "update_date": "2009-01-19",
        "authors_parsed": [
            [
                "Laurent",
                "Monique",
                "",
                "CWI"
            ],
            [
                "Mourrain",
                "Bernard",
                "",
                "INRIA Sophia Antipolis"
            ]
        ]
    },
    {
        "id": "0812.2769",
        "submitter": "Dan Gordon",
        "authors": "Dan Gordon (Univ. of Haifa), Rachel Gordon (Technion-Israel Inst. of\n  Technology)",
        "title": "Geometric scaling: a simple preconditioner for certain linear systems\n  with discontinuous coefficients",
        "comments": "22 pages, 13 tables, 14 figures, 22 references. Submitted for\n  publication",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Linear systems with large differences between coefficients (\"discontinuous\ncoefficients\") arise in many cases in which partial differential\nequations(PDEs) model physical phenomena involving heterogeneous media. The\nstandard approach to solving such problems is to use domain decomposition\ntechniques, with domain boundaries conforming to the boundaries between the\ndifferent media. This approach can be difficult to implement when the geometry\nof the domain boundaries is complicated or the grid is unstructured. This work\nexamines the simple preconditioning technique of scaling the equations by\ndividing each equation by the Lp-norm of its coefficients. This preconditioning\nis called geometric scaling (GS). It has long been known that diagonal scaling\ncan be useful in improving convergence, but there is no study on the general\nusefulness of this approach for discontinuous coefficients. GS was tested on\nseveral nonsymmetric linear systems with discontinuous coefficients derived\nfrom convection-diffusion elliptic PDEs with small to moderate convection\nterms. It is shown that GS improved the convergence properties of restarted\nGMRES and Bi-CGSTAB, with and without the ILUT preconditioner. GS was also\nshown to improve the distribution of the eigenvalues by reducing their\nconcentration around the origin very significantly.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 15 Dec 2008 11:35:17 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 4 May 2009 15:31:09 GMT"
            }
        ],
        "update_date": "2009-05-04",
        "authors_parsed": [
            [
                "Gordon",
                "Dan",
                "",
                "Univ. of Haifa"
            ],
            [
                "Gordon",
                "Rachel",
                "",
                "Technion-Israel Inst. of\n  Technology"
            ]
        ]
    },
    {
        "id": "0812.4706",
        "submitter": "Laurent Buse",
        "authors": "Laurent Bus\\'e (INRIA Sophia Antipolis), Guillaume Ch\\`eze (IMT)",
        "title": "On the total order of reducibility of a pencil of algebraic plane curves",
        "comments": null,
        "journal-ref": "Journal of Algebra 341, 1 (2011) 256-278",
        "doi": "10.1016/j.jalgebra.2011.06.006",
        "report-no": null,
        "categories": "math.AC cs.SC math.AG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, the problem of bounding the number of reducible curves in a\npencil of algebraic plane curves is addressed. Unlike most of the previous\nrelated works, each reducible curve of the pencil is here counted with its\nappropriate multiplicity. It is proved that this number of reducible curves,\ncounted with multiplicity, is bounded by d^2-1 where d is the degree of the\npencil. Then, a sharper bound is given by taking into account the Newton's\npolygon of the pencil.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 26 Dec 2008 20:56:46 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 17 Aug 2011 11:54:19 GMT"
            }
        ],
        "update_date": "2011-08-18",
        "authors_parsed": [
            [
                "Bus\u00e9",
                "Laurent",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Ch\u00e8ze",
                "Guillaume",
                "",
                "IMT"
            ]
        ]
    },
    {
        "id": "0812.4974",
        "submitter": "Charles Fulton",
        "authors": "Cecilia Knoll, Charles Fulton",
        "title": "Using a computer algebra system to simplify expressions for\n  Titchmarsh-Weyl m-functions associated with the Hydrogen Atom on the half\n  line",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.SP cs.SC math.CO",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we give simplified formulas for certain polynomials which arise\nin some new Titchmarsh-Weyl m-functions for the radial part of the separated\nHydrogen atom on the half line and two independent programs for generating them\nusing the symbolic manipulator Mathematica.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 29 Dec 2008 21:10:17 GMT"
            }
        ],
        "update_date": "2008-12-31",
        "authors_parsed": [
            [
                "Knoll",
                "Cecilia",
                ""
            ],
            [
                "Fulton",
                "Charles",
                ""
            ]
        ]
    },
    {
        "id": "0901.0148",
        "submitter": "Michal Zerola",
        "authors": "Michal Zerola, Jerome Lauret, Roman Bartak and Michal Sumbera",
        "title": "Using constraint programming to resolve the multi-source/multi-site data\n  movement paradigm on the Grid",
        "comments": "10 pages; ACAT 2008 workshop",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In order to achieve both fast and coordinated data transfer to collaborative\nsites as well as to create a distribution of data over multiple sites,\nefficient data movement is one of the most essential aspects in distributed\nenvironment. With such capabilities at hand, truly distributed task scheduling\nwith minimal latencies would be reachable by internationally distributed\ncollaborations (such as ones in HENP) seeking for scavenging or maximizing on\ngeographically spread computational resources. But it is often not all clear\n(a) how to move data when available from multiple sources or (b) how to move\ndata to multiple compute resources to achieve an optimal usage of available\nresources. We present a method of creating a Constraint Programming (CP) model\nconsisting of sites, links and their attributes such as bandwidth for grid\nnetwork data transfer also considering user tasks as part of the objective\nfunction for an optimal solution. We will explore and explain trade-off between\nschedule generation time and divergence from the optimal solution and show how\nto improve and render viable the solution's finding time by using search tree\ntime limit, approximations, restrictions such as symmetry breaking or grouping\nsimilar tasks together, or generating sequence of optimal schedules by\nsplitting the input problem. Results of data transfer simulation for each case\nwill also include a well known Peer-2-Peer model, and time taken to generate a\nschedule as well as time needed for a schedule execution will be compared to a\nCP optimal solution. We will additionally present a possible implementation\naimed to bring a distributed datasets (multiple sources) to a given site in a\nminimal time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 31 Dec 2008 21:25:32 GMT"
            }
        ],
        "update_date": "2009-04-14",
        "authors_parsed": [
            [
                "Zerola",
                "Michal",
                ""
            ],
            [
                "Lauret",
                "Jerome",
                ""
            ],
            [
                "Bartak",
                "Roman",
                ""
            ],
            [
                "Sumbera",
                "Michal",
                ""
            ]
        ]
    },
    {
        "id": "0901.0633",
        "submitter": "Vicen\\c{c} G\\'omez Cerd\\`a",
        "authors": "B. Kappen, V. Gomez, M. Opper",
        "title": "Optimal control as a graphical model inference problem",
        "comments": "26 pages, 12 Figures; Machine Learning Journal (2012)",
        "journal-ref": null,
        "doi": "10.1007/s10994-012-5278-7",
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We reformulate a class of non-linear stochastic optimal control problems\nintroduced by Todorov (2007) as a Kullback-Leibler (KL) minimization problem.\nAs a result, the optimal control computation reduces to an inference\ncomputation and approximate inference methods can be applied to efficiently\ncompute approximate optimal controls. We show how this KL control theory\ncontains the path integral control method as a special case. We provide an\nexample of a block stacking task and a multi-agent cooperative game where we\ndemonstrate how approximate inference can be successfully applied to instances\nthat are too complex for exact computation. We discuss the relation of the KL\ncontrol approach to other inference approaches to control.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 6 Jan 2009 11:54:29 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 7 Jan 2009 08:17:04 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 18 Jan 2012 15:26:08 GMT"
            }
        ],
        "update_date": "2012-01-19",
        "authors_parsed": [
            [
                "Kappen",
                "B.",
                ""
            ],
            [
                "Gomez",
                "V.",
                ""
            ],
            [
                "Opper",
                "M.",
                ""
            ]
        ]
    },
    {
        "id": "0901.1413",
        "submitter": "Tomas Boothby",
        "authors": "Tomas J. Boothby, Robert W. Bradshaw",
        "title": "Bitslicing and the Method of Four Russians Over Larger Finite Fields",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a method of computing with matrices over very small finite fields\nof size larger than 2. Specifically, we show how the Method of Four Russians\ncan be efficiently adapted to these larger fields, and introduce a row-wise\nmatrix compression scheme that both reduces memory requirements and allows one\nto vectorize element operations. We also present timings which confirm the\nefficiency of these methods and exceed the speed of the fastest implementations\nthe authors are aware of.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 11 Jan 2009 04:43:51 GMT"
            }
        ],
        "update_date": "2009-01-13",
        "authors_parsed": [
            [
                "Boothby",
                "Tomas J.",
                ""
            ],
            [
                "Bradshaw",
                "Robert W.",
                ""
            ]
        ]
    },
    {
        "id": "0901.1696",
        "submitter": "Julien Langou",
        "authors": "Fred G. Gustavson, Jerzy Wasniewski, Jack J. Dongarra and Julien\n  Langou",
        "title": "Rectangular Full Packed Format for Cholesky's Algorithm: Factorization,\n  Solution and Inversion",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe a new data format for storing triangular, symmetric, and\nHermitian matrices called RFPF (Rectangular Full Packed Format). The standard\ntwo dimensional arrays of Fortran and C (also known as full format) that are\nused to represent triangular and symmetric matrices waste nearly half of the\nstorage space but provide high performance via the use of Level 3 BLAS.\nStandard packed format arrays fully utilize storage (array space) but provide\nlow performance as there is no Level 3 packed BLAS. We combine the good\nfeatures of packed and full storage using RFPF to obtain high performance via\nusing Level 3 BLAS as RFPF is a standard full format representation. Also, RFPF\nrequires exactly the same minimal storage as packed format. Each LAPACK full\nand/or packed triangular, symmetric, and Hermitian routine becomes a single new\nRFPF routine based on eight possible data layouts of RFPF. This new RFPF\nroutine usually consists of two calls to the corresponding LAPACK full format\nroutine and two calls to Level 3 BLAS routines. This means {\\it no} new\nsoftware is required. As examples, we present LAPACK routines for Cholesky\nfactorization, Cholesky solution and Cholesky inverse computation in RFPF to\nillustrate this new work and to describe its performance on several commonly\nused computer platforms. Performance of LAPACK full routines using RFPF versus\nLAPACK full routines using standard format for both serial and SMP parallel\nprocessing is about the same while using half the storage. Performance gains\nare roughly one to a factor of 43 for serial and one to a factor of 97 for SMP\nparallel times faster using vendor LAPACK full routines with RFPF than with\nusing vendor and/or reference packed routines.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 13 Jan 2009 01:08:27 GMT"
            }
        ],
        "update_date": "2009-01-14",
        "authors_parsed": [
            [
                "Gustavson",
                "Fred G.",
                ""
            ],
            [
                "Wasniewski",
                "Jerzy",
                ""
            ],
            [
                "Dongarra",
                "Jack J.",
                ""
            ],
            [
                "Langou",
                "Julien",
                ""
            ]
        ]
    },
    {
        "id": "0901.1782",
        "submitter": "Pietro Michiardi Dr.",
        "authors": "Claudio Casetti, Carla-Fabiana Chiasserini, Marco Fiore, Chi-Anh La,\n  Pietro Michiardi",
        "title": "A Holistic Approach to Information Distribution in Ad Hoc Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We investigate the problem of spreading information contents in a wireless ad\nhoc network with mechanisms embracing the peer-to-peer paradigm. In our vision,\ninformation dissemination should satisfy the following requirements: (i) it\nconforms to a predefined distribution and (ii) it is evenly and fairly carried\nby all nodes in their turn. In this paper, we observe the dissemination effects\nwhen the information moves across nodes according to two well-known mobility\nmodels, namely random walk and random direction. Our approach is fully\ndistributed and comes at a very low cost in terms of protocol overhead; in\naddition, simulation results show that the proposed solution can achieve the\naforementioned goals under different network scenarios, provided that a\nsufficient number of information replicas are injected into the network. This\nobservation calls for a further step: in the realistic case where the user\ncontent demand varies over time, we need a content replication/drop strategy to\nadapt the number of information replicas to the changes in the information\nquery rate. We therefore devise a distributed, lightweight scheme that performs\nefficiently in a variety of scenarios.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 13 Jan 2009 13:12:05 GMT"
            }
        ],
        "update_date": "2009-01-14",
        "authors_parsed": [
            [
                "Casetti",
                "Claudio",
                ""
            ],
            [
                "Chiasserini",
                "Carla-Fabiana",
                ""
            ],
            [
                "Fiore",
                "Marco",
                ""
            ],
            [
                "La",
                "Chi-Anh",
                ""
            ],
            [
                "Michiardi",
                "Pietro",
                ""
            ]
        ]
    },
    {
        "id": "0901.1821",
        "submitter": "Didier Henrion",
        "authors": "Didier Henrion (LAAS, CTU/FEE)",
        "title": "Semidefinite representation of convex hulls of rational varieties",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "Rapport LAAS No. 09001",
        "categories": "math.OC cs.SY math.AG",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using elementary duality properties of positive semidefinite moment matrices\nand polynomial sum-of-squares decompositions, we prove that the convex hull of\nrationally parameterized algebraic varieties is semidefinite representable\n(that is, it can be represented as a projection of an affine section of the\ncone of positive semidefinite matrices) in the case of (a) curves; (b)\nhypersurfaces parameterized by quadratics; and (c) hypersurfaces parameterized\nby bivariate quartics; all in an ambient space of arbitrary dimension.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 13 Jan 2009 16:26:53 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 31 May 2010 09:18:16 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 28 Jan 2011 13:54:52 GMT"
            }
        ],
        "update_date": "2011-01-31",
        "authors_parsed": [
            [
                "Henrion",
                "Didier",
                "",
                "LAAS, CTU/FEE"
            ]
        ]
    },
    {
        "id": "0901.1848",
        "submitter": "Daniel Roche",
        "authors": "Mark Giesbrecht and Daniel S. Roche",
        "title": "Detecting lacunary perfect powers and computing their roots",
        "comments": "to appear in Journal of Symbolic Computation (JSC), 2011",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider solutions to the equation f = h^r for polynomials f and h and\ninteger r > 1. Given a polynomial f in the lacunary (also called sparse or\nsuper-sparse) representation, we first show how to determine if f can be\nwritten as h^r and, if so, to find such an r. This is a Monte Carlo randomized\nalgorithm whose cost is polynomial in the number of non-zero terms of f and in\nlog(deg f), i.e., polynomial in the size of the lacunary representation, and it\nworks over GF(q)[x] (for large characteristic) as well as Q[x]. We also give\ntwo deterministic algorithms to compute the perfect root h given f and r. The\nfirst is output-sensitive (based on the sparsity of h) and works only over\nQ[x]. A sparsity-sensitive Newton iteration forms the basis for the second\napproach to computing h, which is extremely efficient and works over both\nGF(q)[x] (for large characteristic) and Q[x], but depends on a number-theoretic\nconjecture. Work of Erdos, Schinzel, Zannier, and others suggests that both of\nthese algorithms are unconditionally polynomial-time in the lacunary size of\nthe input polynomial f. Finally, we demonstrate the efficiency of the\nrandomized detection algorithm and the latter perfect root computation\nalgorithm with an implementation in the C++ library NTL.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 13 Jan 2009 18:44:43 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 3 Dec 2010 20:27:03 GMT"
            }
        ],
        "update_date": "2015-03-13",
        "authors_parsed": [
            [
                "Giesbrecht",
                "Mark",
                ""
            ],
            [
                "Roche",
                "Daniel S.",
                ""
            ]
        ]
    },
    {
        "id": "0901.2612",
        "submitter": "Gerard Henry Edmond Duchamp",
        "authors": "G\\'erard Henry Edmond Duchamp (LIPN), H. Cheballah (LIPN)",
        "title": "Some Open Problems in Combinatorial Physics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC math.CO quant-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We point out four problems which have arisen during the recent research in\nthe domain of Combinatorial Physics.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 17 Jan 2009 07:11:12 GMT"
            }
        ],
        "update_date": "2009-01-20",
        "authors_parsed": [
            [
                "Duchamp",
                "G\u00e9rard Henry Edmond",
                "",
                "LIPN"
            ],
            [
                "Cheballah",
                "H.",
                "",
                "LIPN"
            ]
        ]
    },
    {
        "id": "0901.2665",
        "submitter": "Eric Polizzi",
        "authors": "Eric Polizzi",
        "title": "A Density Matrix-based Algorithm for Solving Eigenvalue Problems",
        "comments": "7 pages, 3 figures",
        "journal-ref": null,
        "doi": "10.1103/PhysRevB.79.115112",
        "report-no": null,
        "categories": "cs.CE cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new numerical algorithm for solving the symmetric eigenvalue problem is\npresented. The technique deviates fundamentally from the traditional Krylov\nsubspace iteration based techniques (Arnoldi and Lanczos algorithms) or other\nDavidson-Jacobi techniques, and takes its inspiration from the contour\nintegration and density matrix representation in quantum mechanics. It will be\nshown that this new algorithm - named FEAST - exhibits high efficiency,\nrobustness, accuracy and scalability on parallel architectures. Examples from\nelectronic structure calculations of Carbon nanotubes (CNT) are presented, and\nnumerical performances and capabilities are discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 17 Jan 2009 23:36:23 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Polizzi",
                "Eric",
                ""
            ]
        ]
    },
    {
        "id": "0901.2778",
        "submitter": "Bernard Mourrain",
        "authors": "Itnuit Janovitz-Freireich, Bernard Mourrain (INRIA Sophia Antipolis),\n  Lajos Ronayi, Agnes Szanto",
        "title": "On the Computation of Matrices of Traces and Radicals of Ideals",
        "comments": null,
        "journal-ref": "Journal of Symbolic Computation 47, 1 (2012) 102-122",
        "doi": "10.1016/j.jsc.2011.08.020",
        "report-no": null,
        "categories": "cs.SC math.AC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Let $f_1,...,f_s \\in \\mathbb{K}[x_1,...,x_m]$ be a system of polynomials\ngenerating a zero-dimensional ideal $\\I$, where $\\mathbb{K}$ is an arbitrary\nalgebraically closed field. We study the computation of \"matrices of traces\"\nfor the factor algebra $\\A := \\CC[x_1, ..., x_m]/ \\I$, i.e. matrices with\nentries which are trace functions of the roots of $\\I$. Such matrices of traces\nin turn allow us to compute a system of multiplication matrices\n$\\{M_{x_i}|i=1,...,m\\}$ of the radical $\\sqrt{\\I}$. We first propose a method\nusing Macaulay type resultant matrices of $f_1,...,f_s$ and a polynomial $J$ to\ncompute moment matrices, and in particular matrices of traces for $\\A$. Here\n$J$ is a polynomial generalizing the Jacobian. We prove bounds on the degrees\nneeded for the Macaulay matrix in the case when $\\I$ has finitely many\nprojective roots in $\\mathbb{P}^m_\\CC$. We also extend previous results which\nwork only for the case where $\\A$ is Gorenstein to the non-Gorenstein case. The\nsecond proposed method uses Bezoutian matrices to compute matrices of traces of\n$\\A$. Here we need the assumption that $s=m$ and $f_1,...,f_m$ define an affine\ncomplete intersection. This second method also works if we have higher\ndimensional components at infinity. A new explicit description of the\ngenerators of $\\sqrt{\\I}$ are given in terms of Bezoutians.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 19 Jan 2009 07:57:18 GMT"
            }
        ],
        "update_date": "2011-12-02",
        "authors_parsed": [
            [
                "Janovitz-Freireich",
                "Itnuit",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Mourrain",
                "Bernard",
                "",
                "INRIA Sophia Antipolis"
            ],
            [
                "Ronayi",
                "Lajos",
                ""
            ],
            [
                "Szanto",
                "Agnes",
                ""
            ]
        ]
    },
    {
        "id": "0901.4323",
        "submitter": "Joris van der Hoeven",
        "authors": "Joris van der Hoeven, Gr\\'egoire Lecerf",
        "title": "On the bit-complexity of sparse polynomial multiplication",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we present fast algorithms for the product of two multivariate\npolynomials in sparse representation. The bit complexity of our algorithms are\nstudied in detail for various types of coefficients, and we derive new\ncomplexity results for the power series multiplication in many variables. Our\nalgorithms are implemented and freely available within the Mathemagix software.\nWe show that their theoretical costs are well-reflected in practice.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 26 Jan 2009 21:48:35 GMT"
            }
        ],
        "update_date": "2009-01-28",
        "authors_parsed": [
            [
                "van der Hoeven",
                "Joris",
                ""
            ],
            [
                "Lecerf",
                "Gr\u00e9goire",
                ""
            ]
        ]
    },
    {
        "id": "0901.4417",
        "submitter": "Marcel Wild",
        "authors": "Marcel Wild",
        "title": "ALLSAT compressed with wildcards: All, or all maximum independent sets",
        "comments": "The best way to efficiently generate all maximum anticliques of a\n  bipartite graph, a problem left open in v3, is settled in the present version\n  v4",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.DM cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An odd cycle cover is a vertex set whose removal makes a graph bipartite. We\nshow that if a $k$-element odd cycle cover of a graph with w vertices is known\nthen all $N$ maximum anticliques (= independent sets) can be generated in time\n$O(2^k w^3 + N w^2))$. Generating ${\\it all}\\ N'$ anticliques (maximum or not)\nis easier and works for arbitrary graphs in time $O(N'w^2)$. In fact the use of\nwildcards allows to compactly generate the anticliques in clusters.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 28 Jan 2009 09:09:36 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 12 Feb 2010 16:08:40 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 27 May 2019 15:24:08 GMT"
            },
            {
                "version": "v4",
                "created": "Wed, 5 Jun 2019 13:15:22 GMT"
            }
        ],
        "update_date": "2019-06-06",
        "authors_parsed": [
            [
                "Wild",
                "Marcel",
                ""
            ]
        ]
    },
    {
        "id": "0902.0558",
        "submitter": "Marc Portoles Comeras",
        "authors": "Marc Portoles-Comeras, Albert Cabellos-Aparicio, Josep\n  Mangues-Bafalluy, Jordi Domingo-Pascual",
        "title": "Analysis of bandwidth measurement methodologies over WLAN systems",
        "comments": "14 pages, 17 figures,",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  WLAN devices have become a fundamental component of nowadays network\ndeployments. However, even though traditional networking applications run\nmostly unchanged over wireless links, the actual interaction between these\napplications and the dynamics of wireless transmissions is not yet fully\nunderstood. An important example of such applications are bandwidth estimation\ntools. This area has become a mature research topic with well-developed\nresults. Unfortunately recent studies have shown that the application of these\nresults to WLAN links is not straightforward. The main reasons for this is that\nthe assumptions taken to develop bandwidth measurements tools do not hold any\nlonger in the presence of wireless links (e.g. non-FIFO scheduling). This paper\nbuilds from these observations and its main goal is to analyze the interaction\nbetween probe packets and WLAN transmissions in bandwidth estimation processes.\nThe paper proposes an analytical model that better accounts for the\nparticularities of WLAN links. The model is validated through extensive\nexperimentation and simulation and reveals that (1) the distribution of the\ndelay to transmit probing packets is not the same for the whole probing\nsequence, this biases the measurements process and (2) existing tools and\ntechniques point at the achievable throughput rather than the available\nbandwidth or the capacity, as previously assumed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Feb 2009 17:23:55 GMT"
            }
        ],
        "update_date": "2009-02-04",
        "authors_parsed": [
            [
                "Portoles-Comeras",
                "Marc",
                ""
            ],
            [
                "Cabellos-Aparicio",
                "Albert",
                ""
            ],
            [
                "Mangues-Bafalluy",
                "Josep",
                ""
            ],
            [
                "Domingo-Pascual",
                "Jordi",
                ""
            ]
        ]
    },
    {
        "id": "0902.0782",
        "submitter": "Katia  Jaffres-Runser",
        "authors": "Katia Jaffr\\`es-Runser, Cristina Comaniciu, Jean-Marie Gorce",
        "title": "A Multiobjective Optimization Framework for Routing in Wireless Ad Hoc\n  Networks",
        "comments": null,
        "journal-ref": "IEEE International Symposium on Conference Modeling and\n  Optimization in Mobile, Ad Hoc, and Wireless Networks (WiOpt) 2010",
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Wireless ad hoc networks are seldom characterized by one single performance\nmetric, yet the current literature lacks a flexible framework to assist in\ncharacterizing the design tradeoffs in such networks. In this work, we address\nthis problem by proposing a new modeling framework for routing in ad hoc\nnetworks, which used in conjunction with metaheuristic multiobjective search\nalgorithms, will result in a better understanding of network behavior and\nperformance when multiple criteria are relevant. Our approach is to take a\nholistic view of the network that captures the cross-interactions among\ninterference management techniques implemented at various layers of the\nprotocol stack. The resulting framework is a complex multiobjective\noptimization problem that can be efficiently solved through existing\nmultiobjective search techniques. In this contribution, we present the Pareto\noptimal sets for an example sensor network when delay, robustness and energy\nare considered. The aim of this paper is to present the framework and hence for\nconciseness purposes, the multiobjective optimization search is not developed\nherein.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Feb 2009 19:48:42 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 6 Jan 2010 11:15:29 GMT"
            }
        ],
        "update_date": "2010-08-17",
        "authors_parsed": [
            [
                "Jaffr\u00e8s-Runser",
                "Katia",
                ""
            ],
            [
                "Comaniciu",
                "Cristina",
                ""
            ],
            [
                "Gorce",
                "Jean-Marie",
                ""
            ]
        ]
    },
    {
        "id": "0902.1035",
        "submitter": "Sid Touati",
        "authors": "Sid Touati (PRISM)",
        "title": "Towards a Statistical Methodology to Evaluate Program Speedups and their\n  Optimisation Techniques",
        "comments": "12 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The community of program optimisation and analysis, code performance\nevaluation, parallelisation and optimising compilation has published since many\ndecades hundreds of research and engineering articles in major conferences and\njournals. These articles study efficient algorithms, strategies and techniques\nto accelerate programs execution times, or optimise other performance metrics\n(MIPS, code size, energy/power, MFLOPS, etc.). Many speedups are published, but\nnobody is able to reproduce them exactly. The non-reproducibility of our\nresearch results is a dark point of the art, and we cannot be qualified as {\\it\ncomputer scientists} if we do not provide rigorous experimental methodology.\nThis article provides a first effort towards a correct statistical protocol for\nanalysing and measuring speedups. As we will see, some common mistakes are done\nby the community inside published articles, explaining part of the\nnon-reproducibility of the results. Our current article is not sufficient by\nits own to deliver a complete experimental methodology, further efforts must be\ndone by the community to decide about a common protocol for our future\nexperiences. Anyway, our community should take care about the aspect of\nreproducibility of the results in the future.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Feb 2009 09:30:53 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 12 Feb 2009 15:50:41 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 17 Feb 2009 08:23:45 GMT"
            },
            {
                "version": "v4",
                "created": "Thu, 19 Feb 2009 13:34:55 GMT"
            },
            {
                "version": "v5",
                "created": "Fri, 27 Feb 2009 10:08:32 GMT"
            },
            {
                "version": "v6",
                "created": "Thu, 5 Mar 2009 05:36:54 GMT"
            },
            {
                "version": "v7",
                "created": "Thu, 2 Apr 2009 09:50:06 GMT"
            },
            {
                "version": "v8",
                "created": "Mon, 6 Jul 2009 11:46:38 GMT"
            }
        ],
        "update_date": "2009-07-06",
        "authors_parsed": [
            [
                "Touati",
                "Sid",
                "",
                "PRISM"
            ]
        ]
    },
    {
        "id": "0902.1040",
        "submitter": "Pierre Courrieu",
        "authors": "Pierre Courrieu (LPC)",
        "title": "Fast solving of Weighted Pairing Least-Squares systems",
        "comments": null,
        "journal-ref": "Journal of Computational and Applied Mathematics 231, 1 (2009)\n  39-48",
        "doi": "10.1016/j.cam.2009.01.016",
        "report-no": null,
        "categories": "cs.MS cs.NE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a generalization of the \"weighted least-squares\" (WLS),\nnamed \"weighted pairing least-squares\" (WPLS), which uses a rectangular weight\nmatrix and is suitable for data alignment problems. Two fast solving methods,\nsuitable for solving full rank systems as well as rank deficient systems, are\nstudied. Computational experiments clearly show that the best method, in terms\nof speed, accuracy, and numerical stability, is based on a special {1, 2,\n3}-inverse, whose computation reduces to a very simple generalization of the\nusual \"Cholesky factorization-backward substitution\" method for solving linear\nsystems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Feb 2009 09:51:09 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 29 May 2009 09:02:51 GMT"
            }
        ],
        "update_date": "2009-05-29",
        "authors_parsed": [
            [
                "Courrieu",
                "Pierre",
                "",
                "LPC"
            ]
        ]
    },
    {
        "id": "0902.1169",
        "submitter": "Gagan Raj Gupta",
        "authors": "Gagan Raj Gupta, Sujay Sanghavi and Ness B. Shroff",
        "title": "Node Weighted Scheduling",
        "comments": "To appear in Sigmetrics 2009",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a new class of online policies for scheduling in\ninput-buffered crossbar switches. Our policies are throughput optimal for a\nlarge class of arrival processes which satisfy strong-law of large numbers.\nGiven an initial configuration and no further arrivals, our policies drain all\npackets in the system in the minimal amount of time (providing an online\nalternative to the batch approach based on Birkhoff-VonNeumann decompositions).\nWe show that it is possible for policies in our class to be throughput optimal\neven if they are not constrained to be maximal in every time slot.\n  Most algorithms for switch scheduling take an edge based approach; in\ncontrast, we focus on scheduling (a large enough set of) the most congested\nports. This alternate approach allows for lower-complexity algorithms, and also\nrequires a non-standard technique to prove throughput-optimality. One algorithm\nin our class, Maximum Vertex-weighted Matching (MVM) has worst-case complexity\nsimilar to Max-size Matching, and in simulations shows slightly better delay\nperformance than Max-(edge)weighted-Matching (MWM).\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 6 Feb 2009 20:34:09 GMT"
            }
        ],
        "update_date": "2009-02-09",
        "authors_parsed": [
            [
                "Gupta",
                "Gagan Raj",
                ""
            ],
            [
                "Sanghavi",
                "Sujay",
                ""
            ],
            [
                "Shroff",
                "Ness B.",
                ""
            ]
        ]
    },
    {
        "id": "0902.1394",
        "submitter": "Francesca Lo Piccolo",
        "authors": "Giuseppe Bianchi, Nicola Blefari Melazzi, Lorenzo Bracciale, Francesca\n  Lo Piccolo, Stefano Salsano",
        "title": "Fundamental delay bounds in peer-to-peer chunk-based real-time streaming\n  systems",
        "comments": "8 pages, 5 figures",
        "journal-ref": "Proceedings of 21st International Teletraffic Congress (ITC 21),\n  2009",
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.MM",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  This paper addresses the following foundational question: what is the maximum\ntheoretical delay performance achievable by an overlay peer-to-peer streaming\nsystem where the streamed content is subdivided into chunks? As shown in this\npaper, when posed for chunk-based systems, and as a consequence of the\nstore-and-forward way in which chunks are delivered across the network, this\nquestion has a fundamentally different answer with respect to the case of\nsystems where the streamed content is distributed through one or more flows\n(sub-streams). To circumvent the complexity emerging when directly dealing with\ndelay, we express performance in term of a convenient metric, called \"stream\ndiffusion metric\". We show that it is directly related to the end-to-end\nminimum delay achievable in a P2P streaming network. In a homogeneous scenario,\nwe derive a performance bound for such metric, and we show how this bound\nrelates to two fundamental parameters: the upload bandwidth available at each\nnode, and the number of neighbors a node may deliver chunks to. In this bound,\nk-step Fibonacci sequences do emerge, and appear to set the fundamental laws\nthat characterize the optimal operation of chunk-based systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 9 Feb 2009 10:05:48 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 1 Feb 2010 17:56:48 GMT"
            }
        ],
        "update_date": "2010-02-01",
        "authors_parsed": [
            [
                "Bianchi",
                "Giuseppe",
                ""
            ],
            [
                "Melazzi",
                "Nicola Blefari",
                ""
            ],
            [
                "Bracciale",
                "Lorenzo",
                ""
            ],
            [
                "Piccolo",
                "Francesca Lo",
                ""
            ],
            [
                "Salsano",
                "Stefano",
                ""
            ]
        ]
    },
    {
        "id": "0902.1610",
        "submitter": "Stefano Zacchiroli",
        "authors": "Roberto Di Cosmo (PPS), Stefano Zacchiroli (PPS), Paulo Trezentos",
        "title": "Package upgrades in FOSS distributions: details and challenges",
        "comments": null,
        "journal-ref": "International Workshop On Hot Topics In Software Upgrades\n  Proceedings of the 1st International Workshop on Hot Topics in Software\n  Upgrades, Nashville, Tennessee : \\'Etats-Unis d'Am\\'erique (2008)",
        "doi": "10.1145/1490283.1490292",
        "report-no": null,
        "categories": "cs.SE cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The upgrade problems faced by Free and Open Source Software distributions\nhave characteristics not easily found elsewhere. We describe the structure of\npackages and their role in the upgrade process. We show that state of the art\npackage managers have shortcomings inhibiting their ability to cope with\nfrequent upgrade failures. We survey current countermeasures to such failures,\nargue that they are not satisfactory, and sketch alternative solutions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 10 Feb 2009 09:14:59 GMT"
            }
        ],
        "update_date": "2009-02-11",
        "authors_parsed": [
            [
                "Di Cosmo",
                "Roberto",
                "",
                "PPS"
            ],
            [
                "Zacchiroli",
                "Stefano",
                "",
                "PPS"
            ],
            [
                "Trezentos",
                "Paulo",
                ""
            ]
        ]
    },
    {
        "id": "0902.1884",
        "submitter": "Georg Hager",
        "authors": "Markus Wittmann and Georg Hager",
        "title": "A Proof of Concept for Optimizing Task Parallelism by Locality Queues",
        "comments": "8 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Task parallelism as employed by the OpenMP task construct, although ideal for\ntackling irregular problems or typical producer/consumer schemes, bears some\npotential for performance bottlenecks if locality of data access is important,\nwhich is typically the case for memory-bound code on ccNUMA systems. We present\na programming technique which ameliorates adverse effects of dynamic task\ndistribution by sorting tasks into locality queues, each of which is preferably\nprocessed by threads that belong to the same locality domain. Dynamic\nscheduling is fully preserved inside each domain, and is preferred over\npossible load imbalance even if non-local access is required. The effectiveness\nof the approach is demonstrated using a blocked six-point stencil solver as a\ntoy model.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Feb 2009 13:51:27 GMT"
            }
        ],
        "update_date": "2009-02-12",
        "authors_parsed": [
            [
                "Wittmann",
                "Markus",
                ""
            ],
            [
                "Hager",
                "Georg",
                ""
            ]
        ]
    },
    {
        "id": "0902.2446",
        "submitter": "Paolo Frasca",
        "authors": "Paolo Frasca, Paolo Mason, Benedetto Piccoli",
        "title": "Detection of Gaussian signals via hexagonal sensor networks",
        "comments": "16 pages, 4 figures. Accepted. v1-current: corrected typos, added\n  clarifications, updated and added references, extended intro and final\n  remarks",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers a special case of the problem of identifying a static\nscalar signal, depending on the location, using a planar network of sensors in\na distributed fashion. Motivated by the application to monitoring wild-fires\nspreading and pollutants dispersion, we assume the signal to be Gaussian in\nspace. Using a network of sensors positioned to form a regular hexagonal\ntessellation, we prove that each node can estimate the parameters of the\nGaussian from local measurements. Moreover, we study the sensitivity of these\nestimates to additive errors affecting the measurements. Finally, we show how a\nconsensus algorithm can be designed to fuse the local estimates into a shared\nglobal estimate, effectively compensating the measurement errors.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 14 Feb 2009 09:32:57 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 31 May 2009 11:07:48 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 6 Jul 2009 21:01:02 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 17 Jul 2009 13:46:52 GMT"
            }
        ],
        "update_date": "2011-07-25",
        "authors_parsed": [
            [
                "Frasca",
                "Paolo",
                ""
            ],
            [
                "Mason",
                "Paolo",
                ""
            ],
            [
                "Piccoli",
                "Benedetto",
                ""
            ]
        ]
    },
    {
        "id": "0902.2736",
        "submitter": "Publications Loria",
        "authors": "Florian Horn",
        "title": "Random Fruits on the Zielonka Tree",
        "comments": null,
        "journal-ref": "26th International Symposium on Theoretical Aspects of Computer\n  Science - STACS 2009 (2009) 541-552",
        "doi": null,
        "report-no": null,
        "categories": "cs.GT cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Stochastic games are a natural model for the synthesis of controllers\nconfronted to adversarial and/or random actions. In particular,\n$\\omega$-regular games of infinite length can represent reactive systems which\nare not expected to reach a correct state, but rather to handle a continuous\nstream of events. One critical resource in such applications is the memory used\nby the controller. In this paper, we study the amount of memory that can be\nsaved through the use of randomisation in strategies, and present matching\nupper and lower bounds for stochastic Muller games.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 16 Feb 2009 14:26:36 GMT"
            }
        ],
        "update_date": "2009-02-17",
        "authors_parsed": [
            [
                "Horn",
                "Florian",
                ""
            ]
        ]
    },
    {
        "id": "0902.3065",
        "submitter": "Giuliano Casale",
        "authors": "Giuliano Casale",
        "title": "The Multi-Branched Method of Moments for Queueing Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a new exact solution algorithm for closed multiclass product-form\nqueueing networks that is several orders of magnitude faster and less memory\nconsuming than established methods for multiclass models, such as the Mean\nValue Analysis (MVA) algorithm. The technique is an important generalization of\nthe recently proposed Method of Moments (MoM) which, differently from MVA,\nrecursively computes higher-order moments of queue-lengths instead of mean\nvalues.\n  The main contribution of this paper is to prove that the information used in\nthe MoM recursion can be increased by considering multiple recursive branches\nthat evaluate models with different number of queues. This reformulation allows\nto formulate a simpler matrix difference equation which leads to large\ncomputational savings with respect to the original MoM recursion. Computational\nanalysis shows several cases where the proposed algorithm is between 1,000 and\n10,000 times faster and less memory consuming than the original MoM, thus\nextending the range of multiclass models where exact solutions are feasible.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Feb 2009 07:48:34 GMT"
            }
        ],
        "update_date": "2009-02-19",
        "authors_parsed": [
            [
                "Casale",
                "Giuliano",
                ""
            ]
        ]
    },
    {
        "id": "0902.3088",
        "submitter": "Daniel Fulger",
        "authors": "Daniel Fulger and Guido Germano",
        "title": "Automatic generation of non-uniform random variates for arbitrary\n  pointwise computable probability densities by tiling",
        "comments": "20 pages, 3 figures, submitted to a peer-reviewed journal",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a rejection method based on recursive covering of the probability\ndensity function with equal tiles. The concept works for any probability\ndensity function that is pointwise computable or representable by tabular data.\nBy the implicit construction of piecewise constant majorizing and minorizing\nfunctions that are arbitrarily close to the density function the production of\nrandom variates is arbitrarily independent of the computation of the density\nfunction and extremely fast. The method works unattended for probability\ndensities with discontinuities (jumps and poles). The setup time is short,\nmarginally independent of the shape of the probability density and linear in\ntable size. Recently formulated requirements to a general and automatic\nnon-uniform random number generator are topped. We give benchmarks together\nwith a similar rejection method and with a transformation method.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Feb 2009 10:28:45 GMT"
            }
        ],
        "update_date": "2009-02-19",
        "authors_parsed": [
            [
                "Fulger",
                "Daniel",
                ""
            ],
            [
                "Germano",
                "Guido",
                ""
            ]
        ]
    },
    {
        "id": "0902.3207",
        "submitter": "Daniel Fulger",
        "authors": "Daniel Fulger, Enrico Scalas and Guido Germano",
        "title": "Random numbers from the tails of probability distributions using the\n  transformation method",
        "comments": "17 pages, 7 figures, submitted to a peer-reviewed journal",
        "journal-ref": "Fractional Calculus and Applied Analysis 16 (2), 332-353, 2013",
        "doi": "10.2478/s13540-013-0021-z",
        "report-no": null,
        "categories": "cs.MS cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The speed of many one-line transformation methods for the production of, for\nexample, Levy alpha-stable random numbers, which generalize Gaussian ones, and\nMittag-Leffler random numbers, which generalize exponential ones, is very high\nand satisfactory for most purposes. However, for the class of decreasing\nprobability densities fast rejection implementations like the Ziggurat by\nMarsaglia and Tsang promise a significant speed-up if it is possible to\ncomplement them with a method that samples the tails of the infinite support.\nThis requires the fast generation of random numbers greater or smaller than a\ncertain value. We present a method to achieve this, and also to generate random\nnumbers within any arbitrary interval. We demonstrate the method showing the\nproperties of the transform maps of the above mentioned distributions as\nexamples of stable and geometric stable random numbers used for the stochastic\nsolution of the space-time fractional diffusion equation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Feb 2009 17:53:36 GMT"
            }
        ],
        "update_date": "2015-09-01",
        "authors_parsed": [
            [
                "Fulger",
                "Daniel",
                ""
            ],
            [
                "Scalas",
                "Enrico",
                ""
            ],
            [
                "Germano",
                "Guido",
                ""
            ]
        ]
    },
    {
        "id": "0902.3208",
        "submitter": "Ilya Safro",
        "authors": "Dorit Ron, Ilya Safro, Achi Brandt",
        "title": "A Fast Multigrid Algorithm for Energy Minimization Under Planar Density\n  Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.MS cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The two-dimensional layout optimization problem reinforced by the efficient\nspace utilization demand has a wide spectrum of practical applications.\nFormulating the problem as a nonlinear minimization problem under planar\nequality and/or inequality density constraints, we present a linear time\nmultigrid algorithm for solving correction to this problem. The method is\ndemonstrated on various graph drawing (visualization) instances.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Feb 2009 17:59:47 GMT"
            }
        ],
        "update_date": "2009-02-19",
        "authors_parsed": [
            [
                "Ron",
                "Dorit",
                ""
            ],
            [
                "Safro",
                "Ilya",
                ""
            ],
            [
                "Brandt",
                "Achi",
                ""
            ]
        ]
    },
    {
        "id": "0902.4481",
        "submitter": "Jian Tan",
        "authors": "Predrag R. Jelenkovic and Jian Tan",
        "title": "Stability of Finite Population ALOHA with Variable Packets",
        "comments": "14 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": "EE2009-02-20",
        "categories": "cs.PF cs.IT math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  ALOHA is one of the most basic Medium Access Control (MAC) protocols and\nrepresents a foundation for other more sophisticated distributed and\nasynchronous MAC protocols, e.g., CSMA. In this paper, unlike in the\ntraditional work that focused on mean value analysis, we study the\ndistributional properties of packet transmission delays over an ALOHA channel.\nWe discover a new phenomenon showing that a basic finite population ALOHA model\nwith variable size (exponential) packets is characterized by power law\ntransmission delays, possibly even resulting in zero throughput. These results\nare in contrast to the classical work that shows exponential delays and\npositive throughput for finite population ALOHA with fixed packets.\nFurthermore, we characterize a new stability condition that is entirely derived\nfrom the tail behavior of the packet and backoff distributions that may not be\ndetermined by mean values. The power law effects and the possible instability\nmight be diminished, or perhaps eliminated, by reducing the variability of\npackets. However, we show that even a slotted (synchronized) ALOHA with packets\nof constant size can exhibit power law delays when the number of active users\nis random. From an engineering perspective, our results imply that the\nvariability of packet sizes and number of active users need to be taken into\nconsideration when designing robust MAC protocols, especially for ad-hoc/sensor\nnetworks where other factors, such as link failures and mobility, might further\ncompound the problem.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Feb 2009 22:59:58 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 26 Feb 2009 22:34:03 GMT"
            }
        ],
        "update_date": "2009-02-27",
        "authors_parsed": [
            [
                "Jelenkovic",
                "Predrag R.",
                ""
            ],
            [
                "Tan",
                "Jian",
                ""
            ]
        ]
    },
    {
        "id": "0902.4514",
        "submitter": "Pavel Chebotarev",
        "authors": "Pavel Chebotarev",
        "title": "Analytical Expression of the Expected Values of Capital at Voting in the\n  Stochastic Environment",
        "comments": "13 pages, 5 figures, translated from Russian by M.A. Kasner",
        "journal-ref": "Automation and Remote Control, 2006, Vol. 67, No. 2, pp. 480-492.\n  Original Russian text published in Avtomatika i Telemekhanika, 2006, No. 3,\n  pp. 152-165",
        "doi": "10.1134/S000511790603012X",
        "report-no": null,
        "categories": "math.OC cs.MA cs.SI cs.SY math.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In the simplest version of the model of group decision making in the\nstochastic environment, the participants are segregated into egoists and a\ngroup of collectivists. A \"proposal of the environment\" is a stochastically\ngenerated vector of algebraic increments of participants' capitals. The social\ndynamics is determined by the sequence of proposals accepted by a majority\nvoting (with a threshold) of the participants. In this paper, we obtain\nanalytical expressions for the expected values of capitals for all the\nparticipants, including collectivists and egoists. In addition, distinctions\nbetween some principles of group voting are discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 26 Feb 2009 06:25:07 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 26 Feb 2009 22:51:44 GMT"
            }
        ],
        "update_date": "2011-06-06",
        "authors_parsed": [
            [
                "Chebotarev",
                "Pavel",
                ""
            ]
        ]
    },
    {
        "id": "0902.4527",
        "submitter": "Nikolaos Livathinos",
        "authors": "Nikolaos S. Livathinos",
        "title": "EXtensible Animator for Mobile Simulations: EXAMS",
        "comments": "9 pages with 7 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  One of the most widely used simulation environments for mobile wireless\nnetworks is the Network Simulator 2 (NS-2). However NS-2 stores its outcome in\na text file, so there is a need for a visualization tool to animate the\nsimulation of the wireless network. The purpose of this tool is to help the\nresearcher examine in detail how the wireless protocol works both on a network\nand a node basis. It is clear that much of this information is protocol\ndependent and cannot be depicted properly by a general purpose animation\nprocess. Existing animation tools do not provide this level of information\nneither permit the specific protocol to control the animation at all. EXAMS is\nan NS-2 visualization tool for mobile simulations which makes possible the\nportrayal of NS-2 internal information like transmission properties and node\ndata structures. This is mainly possible due to EXAMS extensible architecture\nwhich separates the animation process into a general and a protocol specific\npart. The latter can be developed independently by the protocol designer and\nloaded on demand. These and other useful characteristics of the EXAMS tool can\nbe an invaluable help for a researcher in order to investigate and debug a\nmobile networking protocol.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 26 Feb 2009 08:03:30 GMT"
            }
        ],
        "update_date": "2009-02-27",
        "authors_parsed": [
            [
                "Livathinos",
                "Nikolaos S.",
                ""
            ]
        ]
    },
    {
        "id": "0902.4822",
        "submitter": "Xavier Grehant",
        "authors": "Xavier Grehant and Sverre Jarp",
        "title": "Lightweight Task Analysis for Cache-Aware Scheduling on Heterogeneous\n  Clusters",
        "comments": "The paper was originally published in: ISBN #: 1-60132-084-1 (a\n  two-volume set) Proceedings of the 2008 International Conference on Parallel\n  and Distributed Processing Techniques and Applications (PDPTA'08) Editors:\n  Hamid R. Arabnia and Youngsong Mun",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a novel characterization of how a program stresses cache. This\ncharacterization permits fast performance prediction in order to simulate and\nassist task scheduling on heterogeneous clusters. It is based on the estimation\nof stack distance probability distributions. The analysis requires the\nobservation of a very small subset of memory accesses, and yields a reasonable\nto very accurate prediction in constant time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Feb 2009 12:47:22 GMT"
            }
        ],
        "update_date": "2009-03-02",
        "authors_parsed": [
            [
                "Grehant",
                "Xavier",
                ""
            ],
            [
                "Jarp",
                "Sverre",
                ""
            ]
        ]
    },
    {
        "id": "0902.4881",
        "submitter": "Pierre Cornilleau",
        "authors": "Pierre Cornilleau, Sergio Guerrero",
        "title": "Controllability and observabiliy of an artificial advection-diffusion\n  problem",
        "comments": "20 pages, accepted for publication in MCSS. DOI:\n  10.1007/s00498-012-0076-0",
        "journal-ref": "Mathematics of Control, Signals, and Systems July 2012, Volume 24,\n  Issue 3, pp 265-294",
        "doi": "10.1007/s00498-012-0076-0",
        "report-no": null,
        "categories": "math.OC cs.SY math.AP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the controllability of an artificial\nadvection-diffusion system through the boundary. Suitable Carleman estimates\ngive us the observability on the adjoint system in the one dimensional case. We\nalso study some basic properties of our problem such as backward uniqueness and\nwe get an intuitive result on the control cost for vanishing viscosity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Feb 2009 17:57:23 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 3 May 2011 10:23:12 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 8 Jun 2011 12:05:35 GMT"
            },
            {
                "version": "v4",
                "created": "Thu, 2 Feb 2012 09:28:33 GMT"
            },
            {
                "version": "v5",
                "created": "Sat, 3 Mar 2012 10:28:33 GMT"
            }
        ],
        "update_date": "2016-04-05",
        "authors_parsed": [
            [
                "Cornilleau",
                "Pierre",
                ""
            ],
            [
                "Guerrero",
                "Sergio",
                ""
            ]
        ]
    },
    {
        "id": "0903.0034",
        "submitter": "Vladimir Braverman",
        "authors": "Vladimir Braverman, Rafail Ostrovsky",
        "title": "Measuring Independence of Datasets",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.DB cs.IR cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A data stream model represents setting where approximating pairwise, or\n$k$-wise, independence with sublinear memory is of considerable importance. In\nthe streaming model the joint distribution is given by a stream of $k$-tuples,\nwith the goal of testing correlations among the components measured over the\nentire stream. In the streaming model, Indyk and McGregor (SODA 08) recently\ngave exciting new results for measuring pairwise independence. The Indyk and\nMcGregor methods provide $\\log{n}$-approximation under statistical distance\nbetween the joint and product distributions in the streaming model. Indyk and\nMcGregor leave, as their main open question, the problem of improving their\n$\\log n$-approximation for the statistical distance metric.\n  In this paper we solve the main open problem posed by of Indyk and McGregor\nfor the statistical distance for pairwise independence and extend this result\nto any constant $k$. In particular, we present an algorithm that computes an\n$(\\epsilon, \\delta)$-approximation of the statistical distance between the\njoint and product distributions defined by a stream of $k$-tuples. Our\nalgorithm requires $O(({1\\over \\epsilon}\\log({nm\\over \\delta}))^{(30+k)^k})$\nmemory and a single pass over the data stream.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 1 Mar 2009 01:29:54 GMT"
            }
        ],
        "update_date": "2009-03-03",
        "authors_parsed": [
            [
                "Braverman",
                "Vladimir",
                ""
            ],
            [
                "Ostrovsky",
                "Rafail",
                ""
            ]
        ]
    },
    {
        "id": "0903.0035",
        "submitter": "Hari Pyla",
        "authors": "Hari K. Pyla, Bharath Ramesh, Calvin J. Ribbens and Srinidhi\n  Varadarajan",
        "title": "ScALPEL: A Scalable Adaptive Lightweight Performance Evaluation Library\n  for application performance monitoring",
        "comments": "10 pages, 4 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  As supercomputers continue to grow in scale and capabilities, it is becoming\nincreasingly difficult to isolate processor and system level causes of\nperformance degradation. Over the last several years, a significant number of\nperformance analysis and monitoring tools have been built/proposed. However,\nthese tools suffer from several important shortcomings, particularly in\ndistributed environments. In this paper we present ScALPEL, a Scalable Adaptive\nLightweight Performance Evaluation Library for application performance\nmonitoring at the functional level. Our approach provides several distinct\nadvantages. First, ScALPEL is portable across a wide variety of architectures,\nand its ability to selectively monitor functions presents low run-time\noverhead, enabling its use for large-scale production applications. Second, it\nis run-time configurable, enabling both dynamic selection of functions to\nprofile as well as events of interest on a per function basis. Third, our\napproach is transparent in that it requires no source code modifications.\nFinally, ScALPEL is implemented as a pluggable unit by reusing existing\nperformance monitoring frameworks such as Perfmon and PAPI and extending them\nto support both sequential and MPI applications.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 28 Feb 2009 04:11:39 GMT"
            }
        ],
        "update_date": "2009-03-03",
        "authors_parsed": [
            [
                "Pyla",
                "Hari K.",
                ""
            ],
            [
                "Ramesh",
                "Bharath",
                ""
            ],
            [
                "Ribbens",
                "Calvin J.",
                ""
            ],
            [
                "Varadarajan",
                "Srinidhi",
                ""
            ]
        ]
    },
    {
        "id": "0903.0096",
        "submitter": "Manoj Panda",
        "authors": "Manoj K. Panda and Anurag Kumar",
        "title": "Modeling Multi-Cell IEEE 802.11 WLANs with Application to Channel\n  Assignment",
        "comments": "Technical Report, Indian Institute of Science, Bangalore, 17 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a simple and accurate analytical model for multi-cell\ninfrastructure IEEE 802.11 WLANs. Our model applies if the cell radius, $R$, is\nmuch smaller than the carrier sensing range, $R_{cs}$. We argue that, the\ncondition $R_{cs} >> R$ is likely to hold in a dense deployment of Access\nPoints (APs) where, for every client or station (STA), there is an AP very\nclose to the STA such that the STA can associate with the AP at a high physical\nrate. We develop a scalable cell level model for such WLANs with saturated AP\nand STA queues as well as for TCP-controlled long file downloads. The accuracy\nof our model is demonstrated by comparison with ns-2 simulations. We also\ndemonstrate how our analytical model could be applied in conjunction with a\nLearning Automata (LA) algorithm for optimal channel assignment. Based on the\ninsights provided by our analytical model, we propose a simple decentralized\nalgorithm which provides static channel assignments that are Nash equilibria in\npure strategies for the objective of maximizing normalized network throughput.\nOur channel assignment algorithm requires neither any explicit knowledge of the\ntopology nor any message passing, and provides assignments in only as many\nsteps as there are channels. In contrast to prior work, our approach to channel\nassignment is based on the throughput metric.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 1 Mar 2009 02:29:37 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 14 Mar 2009 17:31:07 GMT"
            }
        ],
        "update_date": "2009-03-14",
        "authors_parsed": [
            [
                "Panda",
                "Manoj K.",
                ""
            ],
            [
                "Kumar",
                "Anurag",
                ""
            ]
        ]
    },
    {
        "id": "0903.0520",
        "submitter": "Francesco Pasquale",
        "authors": "Andrea E.F. Clementi, Francesco Pasquale, and Riccardo Silvestri",
        "title": "MANETS: High mobility can make up for low transmission power",
        "comments": "18 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DM cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a Mobile Ad-hoc NETworks (MANET) formed by \"n\" nodes that move\nindependently at random over a finite square region of the plane. Nodes\nexchange data if they are at distance at most \"r\" within each other, where r>0\nis the node transmission radius. The \"flooding time\" is the number of time\nsteps required to broadcast a message from a source node to every node of the\nnetwork. Flooding time is an important measure of the speed of information\nspreading in dynamic networks.\n  We derive a nearly-tight upper bound on the flooding time which is a\ndecreasing function of the maximal \"velocity\" of the nodes. It turns out that,\nwhen the node velocity is sufficiently high, even if the node transmission\nradius \"r\" is far below the \"connectivity threshold\", the flooding time does\nnot asymptotically depend on \"r\". This implies that flooding can be very fast\neven though every \"snapshot\" (i.e. the static random geometric graph at any\nfixed time) of the MANET is fully disconnected. Data reach all nodes quickly\ndespite these ones use very low transmission power.\n  Our result is the first analytical evidence of the fact that high, random\nnode mobility strongly speed-up information spreading and, at the same time,\nlet nodes save energy.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Mar 2009 12:50:27 GMT"
            }
        ],
        "update_date": "2009-03-04",
        "authors_parsed": [
            [
                "Clementi",
                "Andrea E. F.",
                ""
            ],
            [
                "Pasquale",
                "Francesco",
                ""
            ],
            [
                "Silvestri",
                "Riccardo",
                ""
            ]
        ]
    },
    {
        "id": "0903.1337",
        "submitter": "Paolo Frasca",
        "authors": "Ruggero Carli, Fabio Fagnani, Paolo Frasca, Sandro Zampieri",
        "title": "Efficient quantization for average consensus",
        "comments": "Based on material from the third author's PhD thesis, and on a 2007\n  conference paper",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents an algorithm which solves exponentially fast the average\nconsensus problem on strongly connected network of digital links. The algorithm\nis based on an efficient zooming-in/zooming-out quantization scheme.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 7 Mar 2009 11:16:35 GMT"
            }
        ],
        "update_date": "2011-07-25",
        "authors_parsed": [
            [
                "Carli",
                "Ruggero",
                ""
            ],
            [
                "Fagnani",
                "Fabio",
                ""
            ],
            [
                "Frasca",
                "Paolo",
                ""
            ],
            [
                "Zampieri",
                "Sandro",
                ""
            ]
        ]
    },
    {
        "id": "0903.2119",
        "submitter": "Martin Ziegler",
        "authors": "Matthias Fischer, Claudius J\\\"ahn and Martin Ziegler",
        "title": "Adaptive Mesh Approach for Predicting Algorithm Behavior with\n  Application to Visibility Culling in Computer Graphics",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.GR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a concise approximate description, and a method for efficiently\nobtaining this description, via adaptive random sampling of the performance\n(running time, memory consumption, or any other profileable numerical quantity)\nof a given algorithm on some low-dimensional rectangular grid of inputs. The\nformal correctness is proven under reasonable assumptions on the algorithm\nunder consideration; and the approach's practical benefit is demonstrated by\npredicting for which observer positions and viewing directions an occlusion\nculling algorithm yields a net performance benefit or loss compared to a simple\nbrute force renderer.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 12 Mar 2009 10:16:13 GMT"
            }
        ],
        "update_date": "2009-03-13",
        "authors_parsed": [
            [
                "Fischer",
                "Matthias",
                ""
            ],
            [
                "J\u00e4hn",
                "Claudius",
                ""
            ],
            [
                "Ziegler",
                "Martin",
                ""
            ]
        ]
    },
    {
        "id": "0903.2352",
        "submitter": "Nicolas Gast",
        "authors": "Nicolas Gast (INRIA Rh\\^one-Alpes / LIG laboratoire d'Informatique de\n  Grenoble), Bruno Gaujal (INRIA Rh\\^one-Alpes / LIG laboratoire d'Informatique\n  de Grenoble)",
        "title": "A Mean Field Approach for Optimization in Particles Systems and\n  Applications",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "RR-6877",
        "categories": "math.PR cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper investigates the limit behavior of Markov Decision Processes\n(MDPs) made of independent particles evolving in a common environment, when the\nnumber of particles goes to infinity. In the finite horizon case or with a\ndiscounted cost and an infinite horizon, we show that when the number of\nparticles becomes large, the optimal cost of the system converges almost surely\nto the optimal cost of a discrete deterministic system (the ``optimal mean\nfield''). Convergence also holds for optimal policies. We further provide\ninsights on the speed of convergence by proving several central limits theorems\nfor the cost and the state of the Markov decision process with explicit\nformulas for the variance of the limit Gaussian laws. Then, our framework is\napplied to a brokering problem in grid computing. The optimal policy for the\nlimit deterministic system is computed explicitly. Several simulations with\ngrowing numbers of processors are reported. They compare the performance of the\noptimal policy of the limit system used in the finite case with classical\npolicies (such as Join the Shortest Queue) by measuring its asymptotic gain as\nwell as the threshold above which it starts outperforming classical policies.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 13 Mar 2009 10:42:34 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 9 Jun 2009 14:26:02 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 10 Jun 2009 14:40:59 GMT"
            }
        ],
        "update_date": "2009-06-10",
        "authors_parsed": [
            [
                "Gast",
                "Nicolas",
                "",
                "INRIA Rh\u00f4ne-Alpes / LIG laboratoire d'Informatique de\n  Grenoble"
            ],
            [
                "Gaujal",
                "Bruno",
                "",
                "INRIA Rh\u00f4ne-Alpes / LIG laboratoire d'Informatique\n  de Grenoble"
            ]
        ]
    },
    {
        "id": "0903.2361",
        "submitter": "Ivan Yu. Tyukin",
        "authors": "Ivan Y. Tyukin, Erik Steur, Henk Nijmeijer, and Cees van Leeuwen",
        "title": "Adaptive Observers and Parameter Estimation for a Class of Systems\n  Nonlinear in the Parameters",
        "comments": "Preliminary version is presented at the 17-th IFAC World Congress,\n  6-11 Seoul, 2008",
        "journal-ref": null,
        "doi": "10.1016/j.automatica.2013.05.008",
        "report-no": null,
        "categories": "math.OC cs.SY math.DS q-bio.QM",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of asymptotic reconstruction of the state and\nparameter values in systems of ordinary differential equations. A solution to\nthis problem is proposed for a class of systems of which the unknowns are\nallowed to be nonlinearly parameterized functions of state and time.\nReconstruction of state and parameter values is based on the concepts of weakly\nattracting sets and non-uniform convergence and is subjected to persistency of\nexcitation conditions. In absence of nonlinear parametrization the resulting\nobservers reduce to standard estimation schemes. In this respect, the proposed\nmethod constitutes a generalization of the conventional canonical adaptive\nobserver design.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 13 Mar 2009 12:01:17 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 16 Mar 2009 12:55:15 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 13 Jul 2011 06:41:53 GMT"
            },
            {
                "version": "v4",
                "created": "Tue, 10 Jul 2012 08:40:28 GMT"
            },
            {
                "version": "v5",
                "created": "Thu, 16 May 2013 16:28:14 GMT"
            },
            {
                "version": "v6",
                "created": "Mon, 20 May 2013 16:55:02 GMT"
            },
            {
                "version": "v7",
                "created": "Fri, 21 Jun 2013 13:47:22 GMT"
            }
        ],
        "update_date": "2015-03-13",
        "authors_parsed": [
            [
                "Tyukin",
                "Ivan Y.",
                ""
            ],
            [
                "Steur",
                "Erik",
                ""
            ],
            [
                "Nijmeijer",
                "Henk",
                ""
            ],
            [
                "van Leeuwen",
                "Cees",
                ""
            ]
        ]
    },
    {
        "id": "0903.2525",
        "submitter": "Rajkumar Buyya",
        "authors": "Rodrigo N. Calheiros, Rajiv Ranjan, Cesar A. F. De Rose, and Rajkumar\n  Buyya",
        "title": "CloudSim: A Novel Framework for Modeling and Simulation of Cloud\n  Computing Infrastructures and Services",
        "comments": "9 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": "echnical Report, GRIDS-TR-2009-1, Grid Computing and Distributed\n  Systems Laboratory, The University of Melbourne, Australia, March 13, 2009",
        "categories": "cs.DC cs.OS cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Cloud computing focuses on delivery of reliable, secure, fault-tolerant,\nsustainable, and scalable infrastructures for hosting Internet-based\napplication services. These applications have different composition,\nconfiguration, and deployment requirements. Quantifying the performance of\nscheduling and allocation policy on a Cloud infrastructure (hardware, software,\nservices) for different application and service models under varying load,\nenergy performance (power consumption, heat dissipation), and system size is an\nextremely challenging problem to tackle. To simplify this process, in this\npaper we propose CloudSim: a new generalized and extensible simulation\nframework that enables seamless modelling, simulation, and experimentation of\nemerging Cloud computing infrastructures and management services. The\nsimulation framework has the following novel features: (i) support for\nmodelling and instantiation of large scale Cloud computing infrastructure,\nincluding data centers on a single physical computing node and java virtual\nmachine; (ii) a self-contained platform for modelling data centers, service\nbrokers, scheduling, and allocations policies; (iii) availability of\nvirtualization engine, which aids in creation and management of multiple,\nindependent, and co-hosted virtualized services on a data center node; and (iv)\nflexibility to switch between space-shared and time-shared allocation of\nprocessing cores to virtualized services.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 14 Mar 2009 04:28:55 GMT"
            }
        ],
        "update_date": "2009-03-17",
        "authors_parsed": [
            [
                "Calheiros",
                "Rodrigo N.",
                ""
            ],
            [
                "Ranjan",
                "Rajiv",
                ""
            ],
            [
                "De Rose",
                "Cesar A. F.",
                ""
            ],
            [
                "Buyya",
                "Rajkumar",
                ""
            ]
        ]
    },
    {
        "id": "0903.3106",
        "submitter": "Sebastien Tixeuil",
        "authors": "Toshimitsu Masuzawa, S\\'ebastien Tixeuil (LIP6)",
        "title": "Stabilizing Maximal Independent Set in Unidirectional Networks is Hard",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "RR-6880",
        "categories": "cs.DS cs.CC cs.DC cs.NI cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A distributed algorithm is self-stabilizing if after faults and attacks hit\nthe system and place it in some arbitrary global state, the system recovers\nfrom this catastrophic situation without external intervention in finite time.\nIn this paper, we consider the problem of constructing self-stabilizingly a\n\\emph{maximal independent set} in uniform unidirectional networks of arbitrary\nshape. On the negative side, we present evidence that in uniform networks,\n\\emph{deterministic} self-stabilization of this problem is \\emph{impossible}.\nAlso, the \\emph{silence} property (\\emph{i.e.} having communication fixed from\nsome point in every execution) is impossible to guarantee, either for\ndeterministic or for probabilistic variants of protocols. On the positive side,\nwe present a deterministic protocol for networks with arbitrary unidirectional\nnetworks with unique identifiers that exhibits polynomial space and time\ncomplexity in asynchronous scheduling. We complement the study with\nprobabilistic protocols for the uniform case: the first probabilistic protocol\nrequires infinite memory but copes with asynchronous scheduling, while the\nsecond probabilistic protocol has polynomial space complexity but can only\nhandle synchronous scheduling. Both probabilistic solutions have expected\npolynomial time complexity.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 18 Mar 2009 08:42:02 GMT"
            }
        ],
        "update_date": "2009-04-20",
        "authors_parsed": [
            [
                "Masuzawa",
                "Toshimitsu",
                "",
                "LIP6"
            ],
            [
                "Tixeuil",
                "S\u00e9bastien",
                "",
                "LIP6"
            ]
        ]
    },
    {
        "id": "0903.3900",
        "submitter": "Osman Ugus",
        "authors": "Osman Ugus, Dirk Westhoff, Ralf Laue, Abdulhadi Shoufan, and Sorin A.\n  Huss",
        "title": "Optimized Implementation of Elliptic Curve Based Additive Homomorphic\n  Encryption for Wireless Sensor Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When deploying wireless sensor networks (WSNs) in public environments it may\nbecome necessary to secure their data storage and transmission against possible\nattacks such as node-compromise and eavesdropping. The nodes feature only small\ncomputational and energy resources, thus requiring efficient algorithms. As a\nsolution for this problem the TinyPEDS approach was proposed in [7], which\nutilizes the Elliptic Curve ElGamal (EC-ElGamal) cryptosystem for additive\nhomomorphic encryption allowing concealed data aggregation. This work presents\nan optimized implementation of EC-ElGamal on a MicaZ mote, which is a typical\nsensor node platform with 8-bit processor for WSNs. Compared to the best\nprevious result, our implementation is at least 44% faster for fixed-point\nmultiplication. Because most parts of the algorithm are similar to standard\nElliptic Curve algorithms, the results may be reused in other realizations on\nconstrained devices as well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Mar 2009 16:43:01 GMT"
            }
        ],
        "update_date": "2009-03-24",
        "authors_parsed": [
            [
                "Ugus",
                "Osman",
                ""
            ],
            [
                "Westhoff",
                "Dirk",
                ""
            ],
            [
                "Laue",
                "Ralf",
                ""
            ],
            [
                "Shoufan",
                "Abdulhadi",
                ""
            ],
            [
                "Huss",
                "Sorin A.",
                ""
            ]
        ]
    },
    {
        "id": "0903.4053",
        "submitter": "Fortis Alexandra Ms.",
        "authors": "Laura Stefan",
        "title": "The generating of Fractal Images Using MathCAD Program",
        "comments": "10 pages, exposed on 2nd \"European Conference on Computer Science and\n  Applications\" - XA2008, Timisoara, Romania",
        "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series 6 (2008), 211 - 220",
        "doi": null,
        "report-no": null,
        "categories": "cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents the graphic representation in the z-plane of the first\nthree iterations of the algorithm that generates the Sierpinski Gasket. It\nanalyzes the influence of the f(z) map when we represent fractal images.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 24 Mar 2009 10:10:37 GMT"
            }
        ],
        "update_date": "2009-03-25",
        "authors_parsed": [
            [
                "Stefan",
                "Laura",
                ""
            ]
        ]
    },
    {
        "id": "0903.4307",
        "submitter": "Florentina Pintea",
        "authors": "Simona Apostol",
        "title": "FISLAB - the Fuzzy Inference Tool-box for SCILAB",
        "comments": null,
        "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series V (2007), 105-114",
        "doi": null,
        "report-no": null,
        "categories": "cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present study represents \"The Fislab package of programs meant to develop\nthe fuzzy regulators in the Scilab environment\" in which we present some\ngeneral issues, usage requirements and the working mode of the Fislab\nenvironment. In the second part of the article some features of the Scilab\nfunctions from the Fislab package are described.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Mar 2009 11:42:46 GMT"
            }
        ],
        "update_date": "2009-03-26",
        "authors_parsed": [
            [
                "Apostol",
                "Simona",
                ""
            ]
        ]
    },
    {
        "id": "0903.4313",
        "submitter": "Florentina Pintea",
        "authors": "Simona Apostol",
        "title": "The development of a fuzzy regulator with an entry and an output in\n  Fislab",
        "comments": "6 pages, exposed on 4th International Conferences \"Actualities and\n  Perspectives on Hardware and Software\" - APHS2007, Timisoara, Romania",
        "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series V (2007), 115-120",
        "doi": null,
        "report-no": null,
        "categories": "cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The present article is a sequel of the article \"Fislab the Fuzzy Inference\nTool-Box for Scilab\" and it represents the practical application of:\"The\ndevelopment of the Fuzzy regulator with an input and an output in Fislab\". The\narticle contains, besides this application, some functions to be used in the\nprogram, namely Scilab functions for the fuzzification of the firm information,\nfunctions for the operation of de-fuzzification and functions for the\nimplementation of.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Mar 2009 12:10:32 GMT"
            }
        ],
        "update_date": "2009-03-26",
        "authors_parsed": [
            [
                "Apostol",
                "Simona",
                ""
            ]
        ]
    },
    {
        "id": "0903.4898",
        "submitter": "Predrag Jelenkovic",
        "authors": "Predrag R. Jelenkovic and Ana Radovanovic",
        "title": "Asymptotic Optimality of the Static Frequency Caching in the Presence of\n  Correlated Requests",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well known that the static caching algorithm that keeps the most\nfrequently requested documents in the cache is optimal in case when documents\nare of the same size and requests are independent and equally distributed.\nHowever, it is hard to develop explicit and provably optimal caching algorithms\nwhen requests are statistically correlated. In this paper, we show that keeping\nthe most frequently requested documents in the cache is still optimal for large\ncache sizes even if the requests are strongly correlated.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 27 Mar 2009 20:05:17 GMT"
            }
        ],
        "update_date": "2009-03-31",
        "authors_parsed": [
            [
                "Jelenkovic",
                "Predrag R.",
                ""
            ],
            [
                "Radovanovic",
                "Ana",
                ""
            ]
        ]
    },
    {
        "id": "0904.1150",
        "submitter": "Xiujie Huang",
        "authors": "Xiujie Huang, Aleksandar Kavcic and Xiao Ma",
        "title": "Upper Bounds on the Capacities of Noncontrollable Finite-State Channels\n  with/without Feedback",
        "comments": "15 pages, Two columns, 6 figures; appears in IEEE Transaction on\n  Information Theory",
        "journal-ref": null,
        "doi": "10.1109/TIT.2012.2201341",
        "report-no": "CLN: 9-255",
        "categories": "cs.IT cs.SY math.IT math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Noncontrollable finite-state channels (FSCs) are FSCs in which the channel\ninputs have no influence on the channel states, i.e., the channel states evolve\nfreely. Since single-letter formulae for the channel capacities are rarely\navailable for general noncontrollable FSCs, computable bounds are usually\nutilized to numerically bound the capacities. In this paper, we take the\ndelayed channel state as part of the channel input and then define the {\\em\ndirected information rate} from the new channel input (including the source and\nthe delayed channel state) sequence to the channel output sequence. With this\ntechnique, we derive a series of upper bounds on the capacities of\nnoncontrollable FSCs with/without feedback. These upper bounds can be achieved\nby conditional Markov sources and computed by solving an average reward per\nstage stochastic control problem (ARSCP) with a compact state space and a\ncompact action space. By showing that the ARSCP has a uniformly continuous\nreward function, we transform the original ARSCP into a finite-state and\nfinite-action ARSCP that can be solved by a value iteration method. Under a\nmild assumption, the value iteration algorithm is convergent and delivers a\nnear-optimal stationary policy and a numerical upper bound.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 7 Apr 2009 14:20:41 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 8 Apr 2009 14:50:52 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 29 Apr 2009 06:54:34 GMT"
            },
            {
                "version": "v4",
                "created": "Sun, 1 Aug 2010 02:29:47 GMT"
            },
            {
                "version": "v5",
                "created": "Sun, 22 May 2011 13:52:31 GMT"
            },
            {
                "version": "v6",
                "created": "Sun, 29 Jul 2012 00:53:07 GMT"
            }
        ],
        "update_date": "2016-11-18",
        "authors_parsed": [
            [
                "Huang",
                "Xiujie",
                ""
            ],
            [
                "Kavcic",
                "Aleksandar",
                ""
            ],
            [
                "Ma",
                "Xiao",
                ""
            ]
        ]
    },
    {
        "id": "0904.4152",
        "submitter": "Danny van Dyk",
        "authors": "Danny van Dyk, Markus Geveler, Sven Mallach, Dirk Ribbrock, Dominik\n  Goeddeke, Carsten Gutwenger",
        "title": "HONEI: A collection of libraries for numerical computations targeting\n  multiple processor architectures",
        "comments": "19 pages, 7 figures",
        "journal-ref": "Computer Physics Communications 180(12), pp. 2534-2543, December\n  2009",
        "doi": "10.1016/j.cpc.2009.04.018",
        "report-no": "DO-TH 09/05",
        "categories": "cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present HONEI, an open-source collection of libraries offering a hardware\noriented approach to numerical calculations. HONEI abstracts the hardware, and\napplications written on top of HONEI can be executed on a wide range of\ncomputer architectures such as CPUs, GPUs and the Cell processor. We\ndemonstrate the flexibility and performance of our approach with two test\napplications, a Finite Element multigrid solver for the Poisson problem and a\nrobust and fast simulation of shallow water waves. By linking against HONEI's\nlibraries, we achieve a twofold speedup over straight forward C++ code using\nHONEI's SSE backend, and additional 3-4 and 4-16 times faster execution on the\nCell and a GPU. A second important aspect of our approach is that the full\nperformance capabilities of the hardware under consideration can be exploited\nby adding optimised application-specific operations to the HONEI libraries.\nHONEI provides all necessary infrastructure for development and evaluation of\nsuch kernels, significantly simplifying their development.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 27 Apr 2009 13:00:36 GMT"
            }
        ],
        "update_date": "2010-06-28",
        "authors_parsed": [
            [
                "van Dyk",
                "Danny",
                ""
            ],
            [
                "Geveler",
                "Markus",
                ""
            ],
            [
                "Mallach",
                "Sven",
                ""
            ],
            [
                "Ribbrock",
                "Dirk",
                ""
            ],
            [
                "Goeddeke",
                "Dominik",
                ""
            ],
            [
                "Gutwenger",
                "Carsten",
                ""
            ]
        ]
    },
    {
        "id": "0904.4358",
        "submitter": "Maben Rabi",
        "authors": "Maben Rabi, George V. Moustakides, John S. Baras",
        "title": "Adaptive sampling for linear state estimation",
        "comments": "Submitted to the SIAM journal on control and optimization. 32 pages,\n  7 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": "IR-EE-RT 2009:019",
        "categories": "math.OC cs.SY math.PR math.ST stat.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  When a sensor has continuous measurements but sends limited messages over a\ndata network to a supervisor which estimates the state, the available packet\nrate fixes the achievable quality of state estimation. When such rate limits\nturn stringent, the sensor's messaging policy should be designed anew. What are\nthe good causal messaging policies ? What should message packets contain ? What\nis the lowest possible distortion in a causal estimate at the supervisor ? Is\nDelta sampling better than periodic sampling ? We answer these questions under\nan idealized model of the network and the assumption of perfect measurements at\nthe sensor. For a scalar, linear diffusion process, we study the problem of\nchoosing the causal sampling times that will give the lowest aggregate squared\nerror distortion. We stick to finite-horizons and impose a hard upper bound on\nthe number of allowed samples. We cast the design as a problem of choosing an\noptimal sequence of stopping times. We reduce this to a nested sequence of\nproblems each asking for a single optimal stopping time. Under an unproven but\nnatural assumption about the least-square estimate at the supervisor, each of\nthese single stopping problems are of standard form. The optimal stopping times\nare random times when the estimation error exceeds designed envelopes. For the\ncase where the state is a Brownian motion, we give analytically: the shape of\nthe optimal sampling envelopes, the shape of the envelopes under optimal Delta\nsampling, and their performances. Surprisingly, we find that Delta sampling\nperforms badly. Hence, when the rate constraint is a hard limit on the number\nof samples over a finite horizon, we should should not use Delta sampling.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 28 Apr 2009 10:08:58 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 20 Jul 2011 10:58:47 GMT"
            }
        ],
        "update_date": "2011-07-21",
        "authors_parsed": [
            [
                "Rabi",
                "Maben",
                ""
            ],
            [
                "Moustakides",
                "George V.",
                ""
            ],
            [
                "Baras",
                "John S.",
                ""
            ]
        ]
    },
    {
        "id": "0905.0586",
        "submitter": "Mohamed Abouelhoda",
        "authors": "Mohamed Abouelhoda and Hisham Mohamed",
        "title": "WinBioinfTools: Bioinformatics Tools for Windows High Performance\n  Computing Server 2008",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.CE q-bio.QM",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  Open source bioinformatics tools running under MS Windows are rare to find,\nand those running under Windows HPC cluster are almost non-existing. This is\ndespite the fact that the Windows is the most popular operating system used\namong life scientists. Therefore, we introduce in this initiative\nWinBioinfTools, a toolkit containing a number of bioinformatics tools running\nunder Windows High Performance Computing Server 2008. It is an open source code\npackage, where users and developers can share and add to. We currently start\nwith three programs from the area of sequence analysis: 1) CoCoNUT for pairwise\ngenome comparison, 2) parallel BLAST for biological database search, and 3)\nparallel global pairwise sequence alignment. In this report, we focus on\ntechnical aspects concerning how some components of these tools were ported\nfrom Linux/Unix environment to run under Windows. We also show the advantages\nof using the Windows HPC Cluster 2008. We demonstrate by experiments the\nperformance gain achieved when using a computer cluster against a single\nmachine. Furthermore, we show the results of comparing the performance of\nWinBioinfTools on the Windows and Linux Cluster.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 5 May 2009 12:04:12 GMT"
            }
        ],
        "update_date": "2009-05-06",
        "authors_parsed": [
            [
                "Abouelhoda",
                "Mohamed",
                ""
            ],
            [
                "Mohamed",
                "Hisham",
                ""
            ]
        ]
    },
    {
        "id": "0905.1187",
        "submitter": "Markus Haltmeier",
        "authors": "Markus Grasmair, Markus Haltmeier, and Otmar Scherzer",
        "title": "The Residual Method for Regularizing Ill-Posed Problems",
        "comments": "29 pages, one figure",
        "journal-ref": "Appl. Math. Comput. 218(6): pp. 2693-2710 (2011)",
        "doi": "10.1016/j.amc.2011.08.009",
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Although the \\emph{residual method}, or \\emph{constrained regularization}, is\nfrequently used in applications, a detailed study of its properties is still\nmissing. This sharply contrasts the progress of the theory of Tikhonov\nregularization, where a series of new results for regularization in Banach\nspaces has been published in the recent years. The present paper intends to\nbridge the gap between the existing theories as far as possible. We develop a\nstability and convergence theory for the residual method in general topological\nspaces. In addition, we prove convergence rates in terms of (generalized)\nBregman distances, which can also be applied to non-convex regularization\nfunctionals. We provide three examples that show the applicability of our\ntheory. The first example is the regularized solution of linear operator\nequations on $L^p$-spaces, where we show that the results of Tikhonov\nregularization generalize unchanged to the residual method. As a second\nexample, we consider the problem of density estimation from a finite number of\nsampling points, using the Wasserstein distance as a fidelity term and an\nentropy measure as regularization term. It is shown that the densities obtained\nin this way depend continuously on the location of the sampled points and that\nthe underlying density can be recovered as the number of sampling points tends\nto infinity. Finally, we apply our theory to compressed sensing. Here, we show\nthe well-posedness of the method and derive convergence rates both for convex\nand non-convex regularization under rather weak conditions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 May 2009 13:38:17 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 7 Dec 2010 17:13:23 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 21 Jul 2011 14:06:36 GMT"
            }
        ],
        "update_date": "2012-12-06",
        "authors_parsed": [
            [
                "Grasmair",
                "Markus",
                ""
            ],
            [
                "Haltmeier",
                "Markus",
                ""
            ],
            [
                "Scherzer",
                "Otmar",
                ""
            ]
        ]
    },
    {
        "id": "0905.4430",
        "submitter": "Florentina Pintea",
        "authors": "Valerian Antohe",
        "title": "Limits of Educational Soft \"GeoGebra\" in a Critical Constructive Review",
        "comments": "8 pages, exposed on 5th International Conference \"Actualities and\n  Perspectives on Hardware and Software\" - APHS2009, Timisoara, Romania",
        "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series VII(2009), 47-54",
        "doi": null,
        "report-no": null,
        "categories": "cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mathematical educational soft explore, investigating in a dynamical way, some\nalgebraically, geometrically problems, the expected results being used to\ninvolve a lot of mathematical results. One such software soft is GeoGebra. The\nsoftware is free and multi-platform dynamic mathematics software for learning\nand teaching, awards in Europe and the USA. This paper describes some critical\nbut constructive investigation using the platform for graph functions and\ndynamic geometry.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 May 2009 14:11:37 GMT"
            }
        ],
        "update_date": "2009-05-28",
        "authors_parsed": [
            [
                "Antohe",
                "Valerian",
                ""
            ]
        ]
    },
    {
        "id": "0905.4598",
        "submitter": "Florentina Pintea",
        "authors": "Claudiu Chirilov",
        "title": "Iterative Methods for Systems' Solving - a C# approach",
        "comments": "8 pages,exposed on 5th International Conference \"Actualities and\n  Perspectives on Hardware and Software\" - APHS2009, Timisoara, Romania",
        "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series VII(2009), 71-78",
        "doi": null,
        "report-no": null,
        "categories": "cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work wishes to support various mathematical issues concerning the\niterative methods with the help of new programming languages. We consider a way\nto show how problems in math have an answer by using different academic\nresources and different thoughts. Here we treat methods like Gauss-Seidel's,\nCramer's and Gauss-Jordan's.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 28 May 2009 10:10:06 GMT"
            }
        ],
        "update_date": "2009-05-29",
        "authors_parsed": [
            [
                "Chirilov",
                "Claudiu",
                ""
            ]
        ]
    },
    {
        "id": "0905.4757",
        "submitter": "Michael Neely",
        "authors": "Michael J. Neely",
        "title": "Stochastic Optimization for Markov Modulated Networks with Application\n  to Delay Constrained Wireless Scheduling",
        "comments": "This version adds an author and includes simulation results. It also\n  corrects an error in the earlier version of this arxiv tech report,\n  concerning use of a norm defined in terms of expectations (see footnote 4 of\n  the current version for a discussion of the error). The current paper uses a\n  standard norm without expectations",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a wireless system with a small number of delay constrained users\nand a larger number of users without delay constraints. We develop a scheduling\nalgorithm that reacts to time varying channels and maximizes throughput utility\n(to within a desired proximity), stabilizes all queues, and satisfies the delay\nconstraints. The problem is solved by reducing the constrained optimization to\na set of weighted stochastic shortest path problems, which act as natural\ngeneralizations of max-weight policies to Markov decision networks. We also\npresent approximation results for the corresponding shortest path problems, and\ndiscuss the additional complexity and delay incurred as compared to systems\nwithout delay constraints. The solution technique is general and applies to\nother constrained stochastic decision problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 28 May 2009 21:55:30 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 28 Jul 2011 03:43:01 GMT"
            }
        ],
        "update_date": "2011-07-29",
        "authors_parsed": [
            [
                "Neely",
                "Michael J.",
                ""
            ]
        ]
    },
    {
        "id": "0906.0268",
        "submitter": "Vincent N\\'elis",
        "authors": "Vincent Nelis and Joel Goossens",
        "title": "MORA: an Energy-Aware Slack Reclamation Scheme for Scheduling Sporadic\n  Real-Time Tasks upon Multiprocessor Platforms",
        "comments": "11 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we address the global and preemptive energy-aware scheduling\nproblem of sporadic constrained-deadline tasks on DVFS-identical multiprocessor\nplatforms. We propose an online slack reclamation scheme which profits from the\ndiscrepancy between the worst- and actual-case execution time of the tasks by\nslowing down the speed of the processors in order to save energy. Our algorithm\ncalled MORA takes into account the application-specific consumption profile of\nthe tasks. We demonstrate that MORA does not jeopardize the system\nschedulability and we show by performing simulations that it can save up to 32%\nof energy (in average) compared to execution without using any energy-aware\nalgorithm.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 1 Jun 2009 12:10:10 GMT"
            }
        ],
        "update_date": "2009-06-02",
        "authors_parsed": [
            [
                "Nelis",
                "Vincent",
                ""
            ],
            [
                "Goossens",
                "Joel",
                ""
            ]
        ]
    },
    {
        "id": "0906.1272",
        "submitter": "Pasha Zusmanovich",
        "authors": "Askar Dzhumadil'daev and Pasha Zusmanovich",
        "title": "The alternative operad is not Koszul",
        "comments": "v3: added corrigendum",
        "journal-ref": "Experiment. Math. 20 (2011), 138-144; Corrigendum: 21 (2012), 418",
        "doi": "10.1080/10586458.2011.544558 10.1080/10586458.2012.738538",
        "report-no": null,
        "categories": "math.RA cs.MS cs.SC math.CO",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Using computer calculations, we prove the statement in the title.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 6 Jun 2009 12:44:23 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 21 May 2011 20:34:27 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 13 Mar 2013 18:13:53 GMT"
            }
        ],
        "update_date": "2013-03-15",
        "authors_parsed": [
            [
                "Dzhumadil'daev",
                "Askar",
                ""
            ],
            [
                "Zusmanovich",
                "Pasha",
                ""
            ]
        ]
    },
    {
        "id": "0906.3065",
        "submitter": "Zhihai Zhang",
        "authors": "Zhihai Zhang, Tian Fang, Bican Xia",
        "title": "Real Solution Isolation with Multiplicity of Zero-Dimensional Triangular\n  Systems",
        "comments": "12 pages, no figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Existing algorithms for isolating real solutions of zero-dimensional\npolynomial systems do not compute the multiplicities of the solutions. In this\npaper, we define in a natural way the multiplicity of solutions of\nzero-dimensional triangular polynomial systems and prove that our definition is\nequivalent to the classical definition of local (intersection) multiplicity.\nThen we present an effective and complete algorithm for isolating real\nsolutions with multiplicities of zero-dimensional triangular polynomial systems\nusing our definition. The algorithm is based on interval arithmetic and\nsquare-free factorization of polynomials with real algebraic coefficients. The\ncomputational results on some examples from the literature are presented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 17 Jun 2009 03:29:37 GMT"
            }
        ],
        "update_date": "2009-06-18",
        "authors_parsed": [
            [
                "Zhang",
                "Zhihai",
                ""
            ],
            [
                "Fang",
                "Tian",
                ""
            ],
            [
                "Xia",
                "Bican",
                ""
            ]
        ]
    },
    {
        "id": "0906.4121",
        "submitter": "Mark Giesbrecht",
        "authors": "Mark Giesbrecht and Myung Sub Kim",
        "title": "On computing the Hermite form of a matrix of differential polynomials",
        "comments": null,
        "journal-ref": null,
        "doi": "10.1007/978-3-642-04103-7_12",
        "report-no": null,
        "categories": "cs.SC cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given an n x n matrix over the ring of differential polynomials\nF(t)[\\D;\\delta], we show how to compute the Hermite form H of A, and a\nunimodular matrix U such that UA=H. The algorithm requires a polynomial number\nof operations in terms of n, deg_D(A), and deg_t(A). When F is the field of\nrational numbers, it also requires time polynomial in the bit-length of the\ncoefficients.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 22 Jun 2009 20:29:09 GMT"
            }
        ],
        "update_date": "2015-05-13",
        "authors_parsed": [
            [
                "Giesbrecht",
                "Mark",
                ""
            ],
            [
                "Kim",
                "Myung Sub",
                ""
            ]
        ]
    },
    {
        "id": "0906.5339",
        "submitter": "Salah A. Aly",
        "authors": "Salah A. Aly",
        "title": "Asymmetric Quantum Cyclic Codes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.MS math.IT quant-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is recently conjectured in quantum information processing that phase-shift\nerrors occur with high probability than qubit-flip errors, hence the former is\nmore disturbing to quantum information than the later one. This leads us to\nconstruct asymmetric quantum error controlling codes to protect quantum\ninformation over asymmetric channels, $\\Pr Z \\geq \\Pr X$. In this paper we\npresent two generic methods to derive asymmetric quantum cyclic codes using the\ngenerator polynomials and defining sets of classical cyclic codes.\nConsequently, the methods allow us to construct several families of asymmetric\nquantum BCH, RS, and RM codes. Finally, the methods are used to construct\nfamilies of asymmetric subsystem codes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 29 Jun 2009 19:37:12 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 30 Nov 2009 03:03:56 GMT"
            }
        ],
        "update_date": "2009-11-30",
        "authors_parsed": [
            [
                "Aly",
                "Salah A.",
                ""
            ]
        ]
    },
    {
        "id": "0907.0748",
        "submitter": "Paolo Frasca",
        "authors": "Ruggero Carli, Fabio Fagnani, Paolo Frasca, Sandro Zampieri",
        "title": "Gossip consensus algorithms via quantized communication",
        "comments": "Accepted for publication",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the average consensus problem on a network of digital\nlinks, and proposes a set of algorithms based on pairwise ''gossip''\ncommunications and updates. We study the convergence properties of such\nalgorithms with the goal of answering two design questions, arising from the\nliterature: whether the agents should encode their communication by a\ndeterministic or a randomized quantizer, and whether they should use, and how,\nexact information regarding their own states in the update.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 4 Jul 2009 09:12:16 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 14 Sep 2009 15:15:01 GMT"
            }
        ],
        "update_date": "2011-07-25",
        "authors_parsed": [
            [
                "Carli",
                "Ruggero",
                ""
            ],
            [
                "Fagnani",
                "Fabio",
                ""
            ],
            [
                "Frasca",
                "Paolo",
                ""
            ],
            [
                "Zampieri",
                "Sandro",
                ""
            ]
        ]
    },
    {
        "id": "0907.0792",
        "submitter": "James Raynolds",
        "authors": "James E. Raynolds and Lenore M. Mullin",
        "title": "A generalized inner and outer product of arbitrary multi-dimensional\n  arrays using A Mathematics of Arrays (MoA)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An algorithm has been devised to compute the inner and outer product between\ntwo arbitrary multi-dimensional arrays A and B in a single piece of code. It\nwas derived using A Mathematics of Arrays (MoA) and the $\\psi$-calculus.\nExtensive tests of the new algorithm are presented for running in sequential as\nwell as OpenMP multiple processor modes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 4 Jul 2009 20:17:28 GMT"
            }
        ],
        "update_date": "2009-07-07",
        "authors_parsed": [
            [
                "Raynolds",
                "James E.",
                ""
            ],
            [
                "Mullin",
                "Lenore M.",
                ""
            ]
        ]
    },
    {
        "id": "0907.0796",
        "submitter": "James Raynolds",
        "authors": "Lenore M. Mullin and James E. Raynolds",
        "title": "Tensors and n-d Arrays:A Mathematics of Arrays (MoA), psi-Calculus and\n  the Composition of Tensor and Array Operations",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Kronecker product is a key algorithm and is ubiquitous across the\nphysical, biological, and computation social sciences. Thus considerations of\noptimal implementation are important. The need to have high performance and\ncomputational reproducibility is paramount. Moreover, due to the need to\ncompose multiple Kronecker products, issues related to data structures, layout\nand indexing algebra require a new look at an old problem. This paper discusses\nthe outer product/tensor product and a special case of the tensor product: the\nKronecker product, along with optimal implementation when composed, and mapped\nto complex processor/memory hierarchies. We discuss how the use of ``A\nMathematics of Arrays\" (MoA), and the psi-Calculus, (a calculus of indexing\nwith shapes), provides optimal, verifiable, reproducible, scalable, and\nportable implementations of both hardware and software.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 4 Jul 2009 20:38:39 GMT"
            }
        ],
        "update_date": "2009-07-07",
        "authors_parsed": [
            [
                "Mullin",
                "Lenore M.",
                ""
            ],
            [
                "Raynolds",
                "James E.",
                ""
            ]
        ]
    },
    {
        "id": "0907.2557",
        "submitter": "Johannes Bluemlein",
        "authors": "J. Bl\\\"umlein, D.J. Broadhurst, J.A.M. Vermaseren",
        "title": "The Multiple Zeta Value Data Mine",
        "comments": null,
        "journal-ref": "Comput.Phys.Commun.181:582-625,2010",
        "doi": "10.1016/j.cpc.2009.11.007",
        "report-no": "DESY 09-003, SFB/CPP-09-65",
        "categories": "math-ph cs.MS hep-ph hep-th math.AG math.MP math.NT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We provide a data mine of proven results for multiple zeta values (MZVs) of\nthe form $\\zeta(s_1,s_2,...,s_k)=\\sum_{n_1>n_2>...>n_k>0}^\\infty \\{1/(n_1^{s_1}\n>... n_k^{s_k})\\}$ with weight $w=\\sum_{i=1}^k s_i$ and depth $k$ and for Euler\nsums of the form $\\sum_{n_1>n_2>...>n_k>0}^\\infty t\\{(\\epsilon_1^{n_1}\n>...\\epsilon_1 ^{n_k})/ (n_1^{s_1} ... n_k^{s_k}) \\}$ with signs\n$\\epsilon_i=\\pm1$. Notably, we achieve explicit proven reductions of all MZVs\nwith weights $w\\le22$, and all Euler sums with weights $w\\le12$, to bases whose\ndimensions, bigraded by weight and depth, have sizes in precise agreement with\nthe Broadhurst--Kreimer and Broadhurst conjectures. Moreover, we lend further\nsupport to these conjectures by studying even greater weights ($w\\le30$), using\nmodular arithmetic. To obtain these results we derive a new type of relation\nfor Euler sums, the Generalized Doubling Relations. We elucidate the \"pushdown\"\nmechanism, whereby the ornate enumeration of primitive MZVs, by weight and\ndepth, is reconciled with the far simpler enumeration of primitive Euler sums.\nThere is some evidence that this pushdown mechanism finds its origin in\ndoubling relations. We hope that our data mine, obtained by exploiting the\nunique power of the computer algebra language {\\sc form}, will enable the study\nof many more such consequences of the double-shuffle algebra of MZVs, and their\nEuler cousins, which are already the subject of keen interest, to practitioners\nof quantum field theory, and to mathematicians alike.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 15 Jul 2009 11:14:37 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 14 Nov 2009 18:08:24 GMT"
            }
        ],
        "update_date": "2010-01-21",
        "authors_parsed": [
            [
                "Bl\u00fcmlein",
                "J.",
                ""
            ],
            [
                "Broadhurst",
                "D. J.",
                ""
            ],
            [
                "Vermaseren",
                "J. A. M.",
                ""
            ]
        ]
    },
    {
        "id": "0907.3654",
        "submitter": "J\\'er\\^ome Gauthier",
        "authors": "Jerome Gauthier, Laurent Duval and Jean-Christophe Pesquet",
        "title": "Optimization of Synthesis Oversampled Complex Filter Banks",
        "comments": null,
        "journal-ref": "IEEE Transactions on Signal Processing, October 2009, Volume 57,\n  Issue 10, p. 3827-3843",
        "doi": "10.1109/TSP.2009.2023947",
        "report-no": null,
        "categories": "cs.IT cs.SY eess.SY math.IT math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An important issue with oversampled FIR analysis filter banks (FBs) is to\ndetermine inverse synthesis FBs, when they exist. Given any complex oversampled\nFIR analysis FB, we first provide an algorithm to determine whether there\nexists an inverse FIR synthesis system. We also provide a method to ensure the\nHermitian symmetry property on the synthesis side, which is serviceable to\nprocessing real-valued signals. As an invertible analysis scheme corresponds to\na redundant decomposition, there is no unique inverse FB. Given a particular\nsolution, we parameterize the whole family of inverses through a null space\nprojection. The resulting reduced parameter set simplifies design procedures,\nsince the perfect reconstruction constrained optimization problem is recast as\nan unconstrained optimization problem. The design of optimized synthesis FBs\nbased on time or frequency localization criteria is then investigated, using a\nsimple yet efficient gradient algorithm.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 21 Jul 2009 13:41:45 GMT"
            }
        ],
        "update_date": "2023-01-19",
        "authors_parsed": [
            [
                "Gauthier",
                "Jerome",
                ""
            ],
            [
                "Duval",
                "Laurent",
                ""
            ],
            [
                "Pesquet",
                "Jean-Christophe",
                ""
            ]
        ]
    },
    {
        "id": "0907.4622",
        "submitter": "Rajkumar Buyya",
        "authors": "Christian Vecchiola, Xingchen Chu, and Rajkumar Buyya",
        "title": "Aneka: A Software Platform for .NET-based Cloud Computing",
        "comments": "30 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": "GRIDS-TR-2009-4, Grid Computing and Distributed Systems Laboratory,\n  The University of Melbourne, Australia, May 25, 2009",
        "categories": "cs.DC cs.CE cs.NI cs.OS cs.PL cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Aneka is a platform for deploying Clouds developing applications on top of\nit. It provides a runtime environment and a set of APIs that allow developers\nto build .NET applications that leverage their computation on either public or\nprivate clouds. One of the key features of Aneka is the ability of supporting\nmultiple programming models that are ways of expressing the execution logic of\napplications by using specific abstractions. This is accomplished by creating a\ncustomizable and extensible service oriented runtime environment represented by\na collection of software containers connected together. By leveraging on these\narchitecture advanced services including resource reservation, persistence,\nstorage management, security, and performance monitoring have been implemented.\nOn top of this infrastructure different programming models can be plugged to\nprovide support for different scenarios as demonstrated by the engineering,\nlife science, and industry applications.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 26 Jul 2009 02:19:42 GMT"
            }
        ],
        "update_date": "2009-07-28",
        "authors_parsed": [
            [
                "Vecchiola",
                "Christian",
                ""
            ],
            [
                "Chu",
                "Xingchen",
                ""
            ],
            [
                "Buyya",
                "Rajkumar",
                ""
            ]
        ]
    },
    {
        "id": "0908.1273",
        "submitter": "Mohammad Naghshvar",
        "authors": "Mohammad Naghshvar, Hairuo Zhuang, and Tara Javidi",
        "title": "A General Class of Throughput Optimal Routing Policies in Multi-hop\n  Wireless Networks",
        "comments": "31 pages (one column), 8 figures, (revision submitted to IEEE\n  Transactions on Information Theory)",
        "journal-ref": null,
        "doi": "10.1109/TIT.2011.2178152",
        "report-no": null,
        "categories": "math.OC cs.NI cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers the problem of throughput optimal routing/scheduling in\na multi-hop constrained queueing network with random connectivity whose special\ncase includes opportunistic multi-hop wireless networks and input-queued switch\nfabrics. The main challenge in the design of throughput optimal routing\npolicies is closely related to identifying appropriate and universal Lyapunov\nfunctions with negative expected drift. The few well-known throughput optimal\npolicies in the literature are constructed using simple quadratic or\nexponential Lyapunov functions of the queue backlogs and as such they seek to\nbalance the queue backlogs across network independent of the topology. By\nconsidering a class of continuous, differentiable, and piece-wise quadratic\nLyapunov functions, this paper provides a large class of throughput optimal\nrouting policies. The proposed class of Lyapunov functions allow for the\nrouting policy to control the traffic along short paths for a large portion of\nstate-space while ensuring a negative expected drift. This structure enables\nthe design of a large class of routing policies. In particular, and in addition\nto recovering the throughput optimality of the well known backpressure routing\npolicy, an opportunistic routing policy with congestion diversity is proved to\nbe throughput optimal.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 10 Aug 2009 06:54:38 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 11 Nov 2010 00:08:52 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 10 Mar 2011 19:49:09 GMT"
            }
        ],
        "update_date": "2015-03-13",
        "authors_parsed": [
            [
                "Naghshvar",
                "Mohammad",
                ""
            ],
            [
                "Zhuang",
                "Hairuo",
                ""
            ],
            [
                "Javidi",
                "Tara",
                ""
            ]
        ]
    },
    {
        "id": "0908.3091",
        "submitter": "Attila Egri-Nagy",
        "authors": "Attila Egri-Nagy and Chrystopher L. Nehaniv",
        "title": "Computational Understanding and Manipulation of Symmetries",
        "comments": "14 pages, 5 figures, v2 major revision of computational examples",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  For natural and artificial systems with some symmetry structure,\ncomputational understanding and manipulation can be achieved without learning\nby exploiting the algebraic structure. Here we describe this algebraic\ncoordinatization method and apply it to permutation puzzles. Coordinatization\nyields a structural understanding, not just solutions for the puzzles.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 21 Aug 2009 10:37:52 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 14 Oct 2014 05:09:35 GMT"
            }
        ],
        "update_date": "2014-10-15",
        "authors_parsed": [
            [
                "Egri-Nagy",
                "Attila",
                ""
            ],
            [
                "Nehaniv",
                "Chrystopher L.",
                ""
            ]
        ]
    },
    {
        "id": "0908.3519",
        "submitter": "Joel Goossens",
        "authors": "Liliana Cucu-Grosjean, Jo\\\"el Goossens",
        "title": "Predictability of Fixed-Job Priority Schedulers on Heterogeneous\n  Multiprocessor Real-Time Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The multiprocessor Fixed-Job Priority (FJP) scheduling of real-time systems\nis studied. An important property for the schedulability analysis, the\npredictability (regardless to the execution times), is studied for\nheterogeneous multiprocessor platforms. Our main contribution is to show that\nany FJP schedulers are predictable on unrelated platforms. A convenient\nconsequence is the fact that any FJP schedulers are predictable on uniform\nmultiprocessors.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Aug 2009 12:46:56 GMT"
            }
        ],
        "update_date": "2009-08-26",
        "authors_parsed": [
            [
                "Cucu-Grosjean",
                "Liliana",
                ""
            ],
            [
                "Goossens",
                "Jo\u00ebl",
                ""
            ]
        ]
    },
    {
        "id": "0908.3565",
        "submitter": "K R Guruprasad",
        "authors": "K.R. Guruprasad and Debasish Ghose",
        "title": "Coverage Optimization using Generalized Voronoi Partition",
        "comments": "16 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY math.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper a generalization of the Voronoi partition is used for optimal\ndeployment of autonomous agents carrying sensors with heterogeneous\ncapabilities, to maximize the sensor coverage. The generalized centroidal\nVoronoi configuration, in which the agents are located at the centroids of the\ncorresponding generalized Voronoi cells, is shown to be a local optimal\nconfiguration. Simulation results are presented to illustrate the presented\ndeployment strategy.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 25 Aug 2009 08:55:07 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 20 Jun 2011 20:37:54 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 12 Oct 2011 16:06:56 GMT"
            }
        ],
        "update_date": "2011-10-13",
        "authors_parsed": [
            [
                "Guruprasad",
                "K. R.",
                ""
            ],
            [
                "Ghose",
                "Debasish",
                ""
            ]
        ]
    },
    {
        "id": "0908.4427",
        "submitter": "Matthew Knepley",
        "authors": "Matthew G. Knepley, Dmitry A. Karpeev",
        "title": "Mesh Algorithms for PDE with Sieve I: Mesh Distribution",
        "comments": "36 pages, 22 figures",
        "journal-ref": "Scientific Programming, 17(3), 215-230, 2009",
        "doi": "10.3233/SPR-2009-0249",
        "report-no": null,
        "categories": "cs.CE cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We have developed a new programming framework, called Sieve, to support\nparallel numerical PDE algorithms operating over distributed meshes. We have\nalso developed a reference implementation of Sieve in C++ as a library of\ngeneric algorithms operating on distributed containers conforming to the Sieve\ninterface. Sieve makes instances of the incidence relation, or \\emph{arrows},\nthe conceptual first-class objects represented in the containers. Further,\ngeneric algorithms acting on this arrow container are systematically used to\nprovide natural geometric operations on the topology and also, through duality,\non the data. Finally, coverings and duality are used to encode not only\nindividual meshes, but all types of hierarchies underlying PDE data structures,\nincluding multigrid and mesh partitions.\n  In order to demonstrate the usefulness of the framework, we show how the mesh\npartition data can be represented and manipulated using the same fundamental\nmechanisms used to represent meshes. We present the complete description of an\nalgorithm to encode a mesh partition and then distribute a mesh, which is\nindependent of the mesh dimension, element shape, or embedding. Moreover, data\nassociated with the mesh can be similarly distributed with exactly the same\nalgorithm. The use of a high level of abstraction within the Sieve leads to\nseveral benefits in terms of code reuse, simplicity, and extensibility. We\ndiscuss these benefits and compare our approach to other existing mesh\nlibraries.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 30 Aug 2009 21:53:01 GMT"
            }
        ],
        "update_date": "2010-09-02",
        "authors_parsed": [
            [
                "Knepley",
                "Matthew G.",
                ""
            ],
            [
                "Karpeev",
                "Dmitry A.",
                ""
            ]
        ]
    },
    {
        "id": "0909.0777",
        "submitter": "Arian Maleki",
        "authors": "Arian Maleki, David L. Donoho",
        "title": "Optimally Tuned Iterative Reconstruction Algorithms for Compressed\n  Sensing",
        "comments": "12 pages, 14 figures",
        "journal-ref": null,
        "doi": "10.1109/JSTSP.2009.2039176",
        "report-no": null,
        "categories": "cs.NA cs.IT cs.MS math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We conducted an extensive computational experiment, lasting multiple\nCPU-years, to optimally select parameters for two important classes of\nalgorithms for finding sparse solutions of underdetermined systems of linear\nequations. We make the optimally tuned implementations available at {\\tt\nsparselab.stanford.edu}; they run `out of the box' with no user tuning: it is\nnot necessary to select thresholds or know the likely degree of sparsity. Our\nclass of algorithms includes iterative hard and soft thresholding with or\nwithout relaxation, as well as CoSaMP, subspace pursuit and some natural\nextensions. As a result, our optimally tuned algorithms dominate such\nproposals. Our notion of optimality is defined in terms of phase transitions,\ni.e. we maximize the number of nonzeros at which the algorithm can successfully\noperate. We show that the phase transition is a well-defined quantity with our\nsuite of random underdetermined linear systems. Our tuning gives the highest\ntransition possible within each class of algorithms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 Sep 2009 22:26:32 GMT"
            }
        ],
        "update_date": "2015-05-14",
        "authors_parsed": [
            [
                "Maleki",
                "Arian",
                ""
            ],
            [
                "Donoho",
                "David L.",
                ""
            ]
        ]
    },
    {
        "id": "0909.1763",
        "submitter": "Ragib Hasan",
        "authors": "Ragib Hasan (University of Illinois), Radu Sion (Stony Brook\n  University), Marianne Winslett (University of Illinois)",
        "title": "Remembrance: The Unbearable Sentience of Being Digital",
        "comments": "CIDR 2009",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DB cs.OS",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  We introduce a world vision in which data is endowed with memory. In this\ndata-centric systems paradigm, data items can be enabled to retain all or some\nof their previous values. We call this ability \"remembrance\" and posit that it\nempowers significant leaps in the security, availability, and general\noperational dimensions of systems. With the explosion in cheap, fast memories\nand storage, large-scale remembrance will soon become practical. Here, we\nintroduce and explore the advantages of such a paradigm and the challenges in\nmaking it a reality.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 Sep 2009 18:09:06 GMT"
            }
        ],
        "update_date": "2009-09-15",
        "authors_parsed": [
            [
                "Hasan",
                "Ragib",
                "",
                "University of Illinois"
            ],
            [
                "Sion",
                "Radu",
                "",
                "Stony Brook\n  University"
            ],
            [
                "Winslett",
                "Marianne",
                "",
                "University of Illinois"
            ]
        ]
    },
    {
        "id": "0909.3248",
        "submitter": "Juan Gerardo Alcazar Arribas",
        "authors": "Juan Gerardo Alcazar, Gema Maria Diaz-Toca",
        "title": "Topology of 2D and 3D Rational Curves",
        "comments": "26 pages, 19 figures",
        "journal-ref": null,
        "doi": "10.1016/j.cagd.2010.07.001",
        "report-no": null,
        "categories": "cs.SC cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we present algorithms for computing the topology of planar and\nspace rational curves defined by a parametrization. The algorithms given here\nwork directly with the parametrization of the curve, and do not require to\ncompute or use the implicit equation of the curve (in the case of planar\ncurves) or of any projection (in the case of space curves). Moreover, these\nalgorithms have been implemented in Maple; the examples considered and the\ntimings obtained show good performance skills.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 17 Sep 2009 14:58:09 GMT"
            }
        ],
        "update_date": "2015-02-17",
        "authors_parsed": [
            [
                "Alcazar",
                "Juan Gerardo",
                ""
            ],
            [
                "Diaz-Toca",
                "Gema Maria",
                ""
            ]
        ]
    },
    {
        "id": "0909.4474",
        "submitter": "Blaise Faugeras",
        "authors": "Jacques Blum (JAD), Cedric Boulbe (JAD), Blaise Faugeras (JAD)",
        "title": "Reconstruction of the equilibrium of the plasma in a Tokamak and\n  identification of the current density profile in real time",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.NA cs.SY math.AP math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The reconstruction of the equilibrium of a plasma in a Tokamak is a free\nboundary problem described by the Grad-Shafranov equation in axisymmetric\nconfiguration. The right-hand side of this equation is a nonlinear source,\nwhich represents the toroidal component of the plasma current density. This\npaper deals with the identification of this nonlinearity source from\nexperimental measurements in real time. The proposed method is based on a fixed\npoint algorithm, a finite element resolution, a reduced basis method and a\nleast-square optimization formulation. This is implemented in a software called\nEquinox with which several numerical experiments are conducted to explore the\nidentification problem. It is shown that the identification of the profile of\nthe averaged current density and of the safety factor as a function of the\npoloidal flux is very robust.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 Sep 2009 14:59:43 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 14 Dec 2010 08:14:20 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 17 Mar 2011 17:18:06 GMT"
            }
        ],
        "update_date": "2011-03-18",
        "authors_parsed": [
            [
                "Blum",
                "Jacques",
                "",
                "JAD"
            ],
            [
                "Boulbe",
                "Cedric",
                "",
                "JAD"
            ],
            [
                "Faugeras",
                "Blaise",
                "",
                "JAD"
            ]
        ]
    },
    {
        "id": "0909.4888",
        "submitter": "Mugurel Ionut Andreica",
        "authors": "Andrei-Horia Mogos, Mugurel Ionut Andreica",
        "title": "Approximating Mathematical Semantic Web Services Using Approximation\n  Formulas and Numerical Methods",
        "comments": "The International Workshop on Multi-Agent Systems Technology and\n  Semantics - MASTS 2009",
        "journal-ref": "Proc. of the 17th Intl. Conf. on Control Systems and Computer\n  Science (CSCS), vol. 2, pp. 533-538, Bucharest, Romania, 26-29 May, 2009",
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mathematical semantic web services are very useful in practice, but only a\nsmall number of research results are reported in this area. In this paper we\npresent a method of obtaining an approximation of a mathematical semantic web\nservice, from its semantic description, using existing mathematical semantic\nweb services, approximation formulas, and numerical methods techniques. We also\ngive a method for automatic comparison of two complexity functions. In\naddition, we present a method for classifying the numerical methods\nmathematical semantic web services from a library.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 26 Sep 2009 18:52:59 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Mogos",
                "Andrei-Horia",
                ""
            ],
            [
                "Andreica",
                "Mugurel Ionut",
                ""
            ]
        ]
    },
    {
        "id": "0909.4950",
        "submitter": "Mikael Vejdemo-Johansson",
        "authors": "Vladimir Dotsenko and Mikael Vejdemo-Johansson",
        "title": "Implementing Gr\\\"obner bases for operads",
        "comments": "18 pages, 6 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.MS math.QA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present an implementation of the algorithm for computing Groebner bases\nfor operads due to the first author and A. Khoroshkin. We discuss the actual\nalgorithms, the choices made for the implementation platform and the data\nrepresentation, and strengths and weaknesses of our approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 27 Sep 2009 16:54:34 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 26 Aug 2010 13:30:08 GMT"
            }
        ],
        "update_date": "2010-08-27",
        "authors_parsed": [
            [
                "Dotsenko",
                "Vladimir",
                ""
            ],
            [
                "Vejdemo-Johansson",
                "Mikael",
                ""
            ]
        ]
    },
    {
        "id": "0909.4955",
        "submitter": "Juan Gerardo Alcazar Arribas",
        "authors": "Juan Gerardo Alcazar",
        "title": "On the Different Shapes Arising in a Family of Rational Curves Depending\n  on a Parameter",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a family of rational curves depending on a real parameter, defined by\nits parametric equations, we provide an algorithm to compute a finite partition\nof the parameter space (${\\Bbb R}$, in general) so that the shape of the family\nstays invariant along each element of the partition. So, from this partition\nthe topology types in the family can be determined. The algorithm is based on a\ngeometric interpretation of previous work (\\cite{JGRS}) for the implicit case.\nHowever, in our case the algorithm works directly with the parametrization of\nthe family, and the implicit equation does not need to be computed. Timings\ncomparing the algorithm in the implicit and the parametric cases are given;\nthese timings show that the parametric algorithm developed here provides in\ngeneral better results than the known algorithm for the implicit case.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 27 Sep 2009 18:47:17 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 13 Nov 2009 10:14:05 GMT"
            }
        ],
        "update_date": "2009-11-13",
        "authors_parsed": [
            [
                "Alcazar",
                "Juan Gerardo",
                ""
            ]
        ]
    },
    {
        "id": "0909.4956",
        "submitter": "Juan Gerardo Alcazar Arribas",
        "authors": "Juan Gerardo Alcazar",
        "title": "Local Shape of Generalized Offsets to Algebraic Curves",
        "comments": "19 pages, 9 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the local behavior of an algebraic curve under a\ngeometric construction which is a variation of the usual offsetting\nconstruction, namely the {\\it generalized} offsetting process (\\cite {SS99}).\nMore precisely, here we discuss when and how this geometric construction may\ncause local changes in the shape of an algebraic curve, and we compare our\nresults with those obtained for the case of classical offsets (\\cite{JGS07}).\nFor these purposes, we use well-known notions of Differential Geometry, and\nalso the notion of {\\it local shape} introduced in \\cite{JGS07}.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 27 Sep 2009 19:38:00 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "Alcazar",
                "Juan Gerardo",
                ""
            ]
        ]
    },
    {
        "id": "0909.5064",
        "submitter": "Norbert B\\'atfai",
        "authors": "Norbert B\\'atfai",
        "title": "A Conceivable Origin of Machine Consciousness in the IDLE process",
        "comments": "4 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this short paper, we would like to call professional community's attention\nto a daring idea that is surely unhelpful, but is exciting for programmers and\nanyway conflicts with the trend of energy consumption in computer systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 28 Sep 2009 10:54:48 GMT"
            }
        ],
        "update_date": "2009-09-29",
        "authors_parsed": [
            [
                "B\u00e1tfai",
                "Norbert",
                ""
            ]
        ]
    },
    {
        "id": "0909.5413",
        "submitter": "Rio Yokota Dr.",
        "authors": "Rio Yokota, L. A. Barba, Matthew G. Knepley",
        "title": "PetRBF--A parallel O(N) algorithm for radial basis function\n  interpolation",
        "comments": "Submitted to Computer Methods in Applied Mechanics and Engineering",
        "journal-ref": "Computer Methods in Applied Mechanics and Engineering, 199(25-28),\n  pp. 1793-1804, 2010",
        "doi": "10.1016/j.cma.2010.02.008",
        "report-no": null,
        "categories": "cs.MS cs.DC cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We have developed a parallel algorithm for radial basis function (RBF)\ninterpolation that exhibits O(N) complexity,requires O(N) storage, and scales\nexcellently up to a thousand processes. The algorithm uses a GMRES iterative\nsolver with a restricted additive Schwarz method (RASM) as a preconditioner and\na fast matrix-vector algorithm. Previous fast RBF methods, --,achieving at most\nO(NlogN) complexity,--, were developed using multiquadric and polyharmonic\nbasis functions. In contrast, the present method uses Gaussians with a small\nvariance (a common choice in particle methods for fluid simulation, our main\ntarget application). The fast decay of the Gaussian basis function allows rapid\nconvergence of the iterative solver even when the subdomains in the RASM are\nvery small. The present method was implemented in parallel using the PETSc\nlibrary (developer version). Numerical experiments demonstrate its capability\nin problems of RBF interpolation with more than 50 million data points, timing\nat 106 seconds (19 iterations for an error tolerance of 10^-15 on 1024\nprocessors of a Blue Gene/L (700 MHz PowerPC processors). The parallel code is\nfreely available in the open-source model.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 29 Sep 2009 19:27:51 GMT"
            }
        ],
        "update_date": "2011-09-21",
        "authors_parsed": [
            [
                "Yokota",
                "Rio",
                ""
            ],
            [
                "Barba",
                "L. A.",
                ""
            ],
            [
                "Knepley",
                "Matthew G.",
                ""
            ]
        ]
    },
    {
        "id": "0910.0663",
        "submitter": "Fei Wei",
        "authors": "Fei Wei, Huazhong Yang",
        "title": "Transmission line inspires a new distributed algorithm to solve linear\n  system of circuit",
        "comments": "This work was finished in Nov 2007. Recently we are preparing it for\n  IEEE Trans. CAD. More info, see my web page at\n  http://weifei00.googlepages.com",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.DC cs.MS cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Transmission line, or wire, is always troublesome to integrated circuits\ndesigners, but it could be helpful to parallel computing researchers. This\npaper proposes the Virtual Transmission Method (VTM), which is a new\ndistributed and stationary iterative algorithm to solve the linear system\nextracted from circuit. It tears the circuit by virtual transmission lines to\nachieve distributed computing. For the symmetric positive definite (SPD) linear\nsystem, VTM is proved to be convergent. For the unsymmetrical linear system,\nnumerical experiments show that VTM is possible to achieve better convergence\nproperty than the traditional stationary algorithms. VTM could be accelerated\nby some preconditioning techniques, and the convergence speed of VTM is fast\nwhen its preconditioner is properly chosen.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 5 Oct 2009 03:08:43 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 25 Nov 2009 09:42:47 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 26 Apr 2010 09:17:58 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 28 Jun 2010 06:48:46 GMT"
            },
            {
                "version": "v5",
                "created": "Tue, 7 Sep 2010 23:43:49 GMT"
            },
            {
                "version": "v6",
                "created": "Sun, 31 Oct 2010 02:42:32 GMT"
            },
            {
                "version": "v7",
                "created": "Thu, 9 Dec 2010 08:33:24 GMT"
            }
        ],
        "update_date": "2010-12-10",
        "authors_parsed": [
            [
                "Wei",
                "Fei",
                ""
            ],
            [
                "Yang",
                "Huazhong",
                ""
            ]
        ]
    },
    {
        "id": "0910.1845",
        "submitter": "N Vunka Jungum",
        "authors": "Mandhapati P. Raju",
        "title": "Parallel Computation of Finite Element Navier-Stokes codes using MUMPS\n  Solver",
        "comments": "\"International Journal of Computer Science Issues, IJCSI, Volume 4,\n  Issue 2, pp20-24, September 2009\"",
        "journal-ref": "M. P. Raju,\" Parallel Computation of Finite Element Navier-Stokes\n  codes using MUMPS Solver\",International Journal of Computer Science Issues,\n  IJCSI, Volume 4, Issue 2, pp20-24, September 2009\"",
        "doi": null,
        "report-no": null,
        "categories": "cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The study deals with the parallelization of 2D and 3D finite element based\nNavier-Stokes codes using direct solvers. Development of sparse direct solvers\nusing multifrontal solvers has significantly reduced the computational time of\ndirect solution methods. Although limited by its stringent memory requirements,\nmultifrontal solvers can be computationally efficient. First the performance of\nMUltifrontal Massively Parallel Solver (MUMPS) is evaluated for both 2D and 3D\ncodes in terms of memory requirements and CPU times. The scalability of both\nNewton and modified Newton algorithms is tested.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 9 Oct 2009 20:11:36 GMT"
            }
        ],
        "update_date": "2009-10-13",
        "authors_parsed": [
            [
                "Raju",
                "Mandhapati P.",
                ""
            ]
        ]
    },
    {
        "id": "0910.1923",
        "submitter": "David Bremner",
        "authors": "David Bremner and Dan Chen",
        "title": "A Branch and Cut Algorithm for the Halfspace Depth Problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CG cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The concept of \\emph{data depth} in non-parametric multivariate descriptive\nstatistics is the generalization of the univariate rank method to multivariate\ndata. \\emph{Halfspace depth} is a measure of data depth. Given a set $S$ of\npoints and a point $p$, the halfspace depth (or rank) of $p$ is defined as the\nminimum number of points of $S$ contained in any closed halfspace with $p$ on\nits boundary. Computing halfspace depth is NP-hard, and it is equivalent to the\nMaximum Feasible Subsystem problem. In this paper a mixed integer program is\nformulated with the big-$M$ method for the halfspace depth problem. We suggest\na branch and cut algorithm for these integer programs. In this algorithm,\nChinneck's heuristic algorithm is used to find an upper bound and a related\ntechnique based on sensitivity analysis is used for branching. Irreducible\nInfeasible Subsystem (IIS) hitting set cuts are applied. We also suggest a\nbinary search algorithm which may be more numerically stable. The algorithms\nare implemented with the BCP framework from the \\textbf{COIN-OR} project.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 10 Oct 2009 14:42:28 GMT"
            }
        ],
        "update_date": "2009-10-13",
        "authors_parsed": [
            [
                "Bremner",
                "David",
                ""
            ],
            [
                "Chen",
                "Dan",
                ""
            ]
        ]
    },
    {
        "id": "0910.2187",
        "submitter": "Gunther Rei{\\ss}ig",
        "authors": "Gunther Rei{\\ss}ig",
        "title": "Computing abstractions of nonlinear systems",
        "comments": "This work has been accepted for publication in the IEEE Trans.\n  Automatic Control. v3: minor modifications; accepted version",
        "journal-ref": "IEEE Trans. Automat. Control 56, no 11, Nov 2011, pp. 2583-2598",
        "doi": "10.1109/TAC.2011.2118950",
        "report-no": null,
        "categories": "math.OC cs.SY math.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sufficiently accurate finite state models, also called symbolic models or\ndiscrete abstractions, allow one to apply fully automated methods, originally\ndeveloped for purely discrete systems, to formally reason about continuous and\nhybrid systems, and to design finite state controllers that provably enforce\npredefined specifications. We present a novel algorithm to compute such finite\nstate models for nonlinear discrete-time and sampled systems which depends on\nquantizing the state space using polyhedral cells, embedding these cells into\nsuitable supersets whose attainable sets are convex, and over-approximating\nattainable sets by intersections of supporting half-spaces. We prove a novel\nrecursive description of these half-spaces and propose an iterative procedure\nto compute them efficiently. We also provide new sufficient conditions for the\nconvexity of attainable sets which imply the existence of the aforementioned\nembeddings of quantizer cells. Our method yields highly accurate abstractions\nand applies to nonlinear systems under mild assumptions, which reduce to\nsufficient smoothness in the case of sampled systems. Its practicability in the\ndesign of discrete controllers for nonlinear continuous plants under state and\ncontrol constraints is demonstrated by an example.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 12 Oct 2009 16:13:09 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 1 Dec 2010 20:20:56 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 15 Feb 2011 12:29:55 GMT"
            }
        ],
        "update_date": "2011-11-03",
        "authors_parsed": [
            [
                "Rei\u00dfig",
                "Gunther",
                ""
            ]
        ]
    },
    {
        "id": "0910.2350",
        "submitter": "Daoyi Dong",
        "authors": "Daoyi Dong and Ian R Petersen",
        "title": "Quantum control theory and applications: A survey",
        "comments": "38 pages, invited survey paper from a control systems perspective,\n  some references are added, published version",
        "journal-ref": "IET Control Theory & Applications, vol. 4, no. 12, pp.2651-2671,\n  2010",
        "doi": "10.1049/iet-cta.2009.0508",
        "report-no": null,
        "categories": "quant-ph cs.SY math-ph math.MP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a survey on quantum control theory and applications from\na control systems perspective. Some of the basic concepts and main developments\n(including open-loop control and closed-loop control) in quantum control theory\nare reviewed. In the area of open-loop quantum control, the paper surveys the\nnotion of controllability for quantum systems and presents several control\ndesign strategies including optimal control, Lyapunov-based methodologies,\nvariable structure control and quantum incoherent control. In the area of\nclosed-loop quantum control, the paper reviews closed-loop learning control and\nseveral important issues related to quantum feedback control including quantum\nfiltering, feedback stabilization, LQG control and robust quantum control.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 13 Oct 2009 09:56:16 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 27 Dec 2010 11:53:04 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 10 Jan 2011 00:09:56 GMT"
            }
        ],
        "update_date": "2011-01-12",
        "authors_parsed": [
            [
                "Dong",
                "Daoyi",
                ""
            ],
            [
                "Petersen",
                "Ian R",
                ""
            ]
        ]
    },
    {
        "id": "0910.4052",
        "submitter": "Andrei    Yafimau",
        "authors": "Andrei I. Yafimau",
        "title": "Virtual-Threading: Advanced General Purpose Processors Architecture",
        "comments": "56 pages, 5 PNG figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AR cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The paper describes the new computers architecture, the main features of\nwhich has been claimed in the Russian Federation patent 2312388 and in the US\npatent application 11/991331. This architecture is intended to effective\nsupport of the General Purpose Parallel Computing (GPPC), the essence of which\nis extremely frequent switching of threads between states of activity and\nstates of viewed in the paper the algorithmic latency. To emphasize the same\nimpact of the architectural latency and the algorithmic latency upon GPPC, is\nintroduced the new notion of the generalized latency and is defined its\nquantitative measure - the Generalized Latency Tolerance (GLT). It is shown\nthat a well suited for GPPC implementation architecture should have high level\nof GLT and is described such architecture, which is called the Virtual-Threaded\nMachine. This architecture originates a processor virtualization in the\ndirection of activities virtualization, which is orthogonal to the well-known\ndirection of memory virtualization. The key elements of the architecture are 1)\nthe distributed fine grain representation of the architectural register file,\nwhich elements are hardware swapped through levels of a microarchitectural\nmemory, 2) the prioritized fine grain direct hardware multiprogramming, 3) the\naccess controlled virtual addressing and 4) the hardware driven semaphores. The\ncomposition of these features lets to introduce new styles of operating system\n(OS) programming, which is free of interruptions, and of applied programming\nwith a very rare using the OS services.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 21 Oct 2009 11:31:14 GMT"
            }
        ],
        "update_date": "2009-10-22",
        "authors_parsed": [
            [
                "Yafimau",
                "Andrei I.",
                ""
            ]
        ]
    },
    {
        "id": "0910.4336",
        "submitter": "G David Forney Jr.",
        "authors": "G. David Forney Jr",
        "title": "Minimal realizations of linear systems: The \"shortest basis\" approach",
        "comments": "20 pages. Final version, to appear in special issue of IEEE\n  Transactions on Information Theory on \"Facets of coding theory: From\n  algorithms to networks,\" dedicated to Ralf Koetter",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.SY math.IT math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Given a controllable discrete-time linear system C, a shortest basis for C is\na set of linearly independent generators for C with the least possible lengths.\nA basis B is a shortest basis if and only if it has the predictable span\nproperty (i.e., has the predictable delay and degree properties, and is\nnon-catastrophic), or alternatively if and only if it has the subsystem basis\nproperty (for any interval J, the generators in B whose span is in J is a basis\nfor the subsystem C_J). The dimensions of the minimal state spaces and minimal\ntransition spaces of C are simply the numbers of generators in a shortest basis\nB that are active at any given state or symbol time, respectively. A minimal\nlinear realization for C in controller canonical form follows directly from a\nshortest basis for C, and a minimal linear realization for C in observer\ncanonical form follows directly from a shortest basis for the orthogonal system\nC^\\perp. This approach seems conceptually simpler than that of classical\nminimal realization theory.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 22 Oct 2009 14:23:28 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 10 May 2010 18:15:57 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 24 Aug 2010 18:41:44 GMT"
            }
        ],
        "update_date": "2010-12-17",
        "authors_parsed": [
            [
                "Forney",
                "G. David",
                "Jr"
            ]
        ]
    },
    {
        "id": "0910.4738",
        "submitter": "Debasish Chatterjee",
        "authors": "Federico Ramponi, Debasish Chatterjee, Sean Summers, and John Lygeros",
        "title": "On the connections between PCTL and Dynamic Programming",
        "comments": "Submitted",
        "journal-ref": "HSCC Stockholm, 2010, pages 253-262",
        "doi": "10.1145/1755952.1755988",
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Probabilistic Computation Tree Logic (PCTL) is a well-known modal logic which\nhas become a standard for expressing temporal properties of finite-state Markov\nchains in the context of automated model checking. In this paper, we give a\ndefinition of PCTL for noncountable-space Markov chains, and we show that there\nis a substantial affinity between certain of its operators and problems of\nDynamic Programming. After proving some uniqueness properties of the solutions\nto the latter, we conclude the paper with two examples to show that some\nrecovery strategies in practical applications, which are naturally stated as\nreach-avoid problems, can be actually viewed as particular cases of PCTL\nformulas.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 25 Oct 2009 14:59:37 GMT"
            }
        ],
        "update_date": "2012-02-22",
        "authors_parsed": [
            [
                "Ramponi",
                "Federico",
                ""
            ],
            [
                "Chatterjee",
                "Debasish",
                ""
            ],
            [
                "Summers",
                "Sean",
                ""
            ],
            [
                "Lygeros",
                "John",
                ""
            ]
        ]
    },
    {
        "id": "0910.5046",
        "submitter": "Gene Cooperman",
        "authors": "Ana Maria Visan, Artem Polyakov, Praveen S. Solanki, Kapil Arya, Tyler\n  Denniston, Gene Cooperman",
        "title": "Temporal Debugging using URDB",
        "comments": "20 pages, 3 figures, 5 tables; software at urdb.sourceforge.net",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A new style of temporal debugging is proposed. The new URDB debugger can\nemploy such techniques as temporal search for finding an underlying fault that\nis causing a bug. This improves on the standard iterative debugging style,\nwhich iteratively re-executes a program under debugger control in the search\nfor the underlying fault. URDB acts as a meta-debugger, with current support\nfor four widely used debuggers: gdb, MATLAB, python, and perl. Support for a\nnew debugger can be added in a few hours. Among its points of novelty are: (i)\nthe first reversible debuggers for MATLAB, python, and perl; (ii) support for\ntoday's multi-core architectures; (iii) reversible debugging of multi-process\nand distributed computations; and (iv) temporal search on changes in program\nexpressions. URDB gains its reversibility and temporal abilities through the\nfast checkpoint-restart capability of DMTCP (Distributed MultiThreaded\nCheckPointing). The recently enhanced DMTCP also adds ptrace support, enabling\none to freeze, migrate, and replicate debugging sessions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 27 Oct 2009 17:31:02 GMT"
            }
        ],
        "update_date": "2009-10-28",
        "authors_parsed": [
            [
                "Visan",
                "Ana Maria",
                ""
            ],
            [
                "Polyakov",
                "Artem",
                ""
            ],
            [
                "Solanki",
                "Praveen S.",
                ""
            ],
            [
                "Arya",
                "Kapil",
                ""
            ],
            [
                "Denniston",
                "Tyler",
                ""
            ],
            [
                "Cooperman",
                "Gene",
                ""
            ]
        ]
    },
    {
        "id": "0910.5370",
        "submitter": "Daniel Shumow",
        "authors": "Daniel Shumow",
        "title": "Isogenies of Elliptic Curves: A Computational Approach",
        "comments": "Submitted as a Masters Thesis in the Mathematics department of the\n  University of Washington",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Isogenies, the mappings of elliptic curves, have become a useful tool in\ncryptology. These mathematical objects have been proposed for use in computing\npairings, constructing hash functions and random number generators, and\nanalyzing the reducibility of the elliptic curve discrete logarithm problem.\nWith such diverse uses, understanding these objects is important for anyone\ninterested in the field of elliptic curve cryptography. This paper, targeted at\nan audience with a knowledge of the basic theory of elliptic curves, provides\nan introduction to the necessary theoretical background for understanding what\nisogenies are and their basic properties. This theoretical background is used\nto explain some of the basic computational tasks associated with isogenies.\nHerein, algorithms for computing isogenies are collected and presented with\nproofs of correctness and complexity analyses. As opposed to the complex\nanalytic approach provided in most texts on the subject, the proofs in this\npaper are primarily algebraic in nature. This provides alternate explanations\nthat some with a more concrete or computational bias may find more clear.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 28 Oct 2009 01:48:42 GMT"
            }
        ],
        "update_date": "2009-10-29",
        "authors_parsed": [
            [
                "Shumow",
                "Daniel",
                ""
            ]
        ]
    },
    {
        "id": "0910.5577",
        "submitter": "Ilkka Norros",
        "authors": "Ilkka Norros, Hannu Reittu, Timo Eirola",
        "title": "On the stability of two-chunk file-sharing systems",
        "comments": "19 pages, 7 figures",
        "journal-ref": "Queueing Systems (2011) 67: 183",
        "doi": "10.1007/s11134-011-9209-2",
        "report-no": null,
        "categories": "cs.OS math.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider five different peer-to-peer file sharing systems with two chunks,\nwith the aim of finding chunk selection algorithms that have provably stable\nperformance with any input rate and assuming non-altruistic peers who leave the\nsystem immediately after downloading the second chunk. We show that many\nalgorithms that first looked promising lead to unstable or oscillating\nbehavior. However, we end up with a system with desirable properties. Most of\nour rigorous results concern the corresponding deterministic large system\nlimits, but in two simplest cases we provide proofs for the stochastic systems\nalso.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 29 Oct 2009 08:38:32 GMT"
            }
        ],
        "update_date": "2018-10-24",
        "authors_parsed": [
            [
                "Norros",
                "Ilkka",
                ""
            ],
            [
                "Reittu",
                "Hannu",
                ""
            ],
            [
                "Eirola",
                "Timo",
                ""
            ]
        ]
    },
    {
        "id": "0910.5673",
        "submitter": "Florian D\\\"orfler",
        "authors": "Florian Dorfler and Francesco Bullo",
        "title": "Synchronization and Transient Stability in Power Networks and\n  Non-Uniform Kuramoto Oscillators",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY math-ph math.DS math.MP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Motivated by recent interest for multi-agent systems and smart power grid\narchitectures, we discuss the synchronization problem for the network-reduced\nmodel of a power system with non-trivial transfer conductances. Our key insight\nis to exploit the relationship between the power network model and a\nfirst-order model of coupled oscillators. Assuming overdamped generators\n(possibly due to local excitation controllers), a singular perturbation\nanalysis shows the equivalence between the classic swing equations and a\nnon-uniform Kuramoto model. Here, non-uniform Kuramoto oscillators are\ncharacterized by multiple time constants, non-homogeneous coupling, and\nnon-uniform phase shifts. Extending methods from transient stability,\nsynchronization theory, and consensus protocols, we establish sufficient\nconditions for synchronization of non-uniform Kuramoto oscillators. These\nconditions reduce to and improve upon previously-available tests for the\nstandard Kuramoto model. Combining our singular perturbation and Kuramoto\nanalyses, we derive concise and purely algebraic conditions that relate\nsynchronization and transient stability of a power network to the underlying\nsystem parameters and initial conditions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 29 Oct 2009 16:32:56 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 29 Jan 2010 23:55:51 GMT"
            },
            {
                "version": "v3",
                "created": "Sat, 2 Oct 2010 19:11:11 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 27 Jun 2011 17:44:11 GMT"
            }
        ],
        "update_date": "2011-06-28",
        "authors_parsed": [
            [
                "Dorfler",
                "Florian",
                ""
            ],
            [
                "Bullo",
                "Francesco",
                ""
            ]
        ]
    },
    {
        "id": "0911.0231",
        "submitter": "Mohammad Karimadini",
        "authors": "Mohammad Karimadini and Hai Lin",
        "title": "Synchronized Task Decomposition for Cooperative Multi-agent Systems",
        "comments": "Submitted for publication, 2010",
        "journal-ref": null,
        "doi": null,
        "report-no": "NUS-ACT-10-Ver.3-001",
        "categories": "cs.MA cs.DC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is an amazing fact that remarkably complex behaviors could emerge from a\nlarge collection of very rudimentary dynamical agents through very simple local\ninteractions. However, it still remains elusive on how to design these local\ninteractions among agents so as to achieve certain desired collective\nbehaviors. This paper aims to tackle this challenge and proposes a\ndivide-and-conquer approach to guarantee specified global behaviors through\nlocal coordination and control design for multi-agent systems. The basic idea\nis to decompose the requested global specification into subtasks for each\nindividual agent. It should be noted that the decomposition is not arbitrary.\nThe global specification should be decomposed in such a way that the fulfilment\nof these subtasks by each individual agent will imply the satisfaction of the\nglobal specification as a team. First, it is shown by a counterexample that not\nall specifications can be decomposed in this sense. Then, a natural follow-up\nquestion is what the necessary and sufficient condition should be for the\nproposed decomposability of a global specification. The main part of the paper\nis set to answer this question. The case of two cooperative agents is\ninvestigated first, and a necessary and sufficient condition is presented and\nproven. Later on, the result is generalized to the case of arbitrary finite\nnumber of agents, and a hierarchical algorithm is proposed, which is shown to\nbe a sufficient condition. Finally, a cooperative control scenario for a team\nof three robots is developed to illustrate the task decomposition procedure.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 2 Nov 2009 04:23:25 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 28 Apr 2010 13:14:44 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 12 Jan 2011 02:58:06 GMT"
            }
        ],
        "update_date": "2011-01-13",
        "authors_parsed": [
            [
                "Karimadini",
                "Mohammad",
                ""
            ],
            [
                "Lin",
                "Hai",
                ""
            ]
        ]
    },
    {
        "id": "0911.0232",
        "submitter": "Bahman Gharesifard",
        "authors": "Bahman Gharesifard and Jorge Cortes",
        "title": "Distributed strategies for generating weight-balanced and doubly\n  stochastic digraphs",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Weight-balanced and doubly stochastic digraphs are two classes of digraphs\nthat play an essential role in a variety of cooperative control problems,\nincluding formation control, distributed averaging, and optimization. We refer\nto a digraph as doubly stochasticable (weight-balanceable) if it admits a\ndoubly stochastic (weight-balanced) adjacency matrix. This paper studies the\ncharacterization of both classes of digraphs, and introduces distributed\nalgorithms to compute the appropriate set of weights in each case.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 2 Nov 2009 04:24:59 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 13 Nov 2009 19:41:50 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 16 Apr 2010 05:07:29 GMT"
            },
            {
                "version": "v4",
                "created": "Thu, 10 Mar 2011 04:04:11 GMT"
            },
            {
                "version": "v5",
                "created": "Fri, 15 Jul 2011 01:45:39 GMT"
            },
            {
                "version": "v6",
                "created": "Mon, 17 Oct 2011 21:33:34 GMT"
            }
        ],
        "update_date": "2011-10-19",
        "authors_parsed": [
            [
                "Gharesifard",
                "Bahman",
                ""
            ],
            [
                "Cortes",
                "Jorge",
                ""
            ]
        ]
    },
    {
        "id": "0911.0492",
        "submitter": "Lek-Heng Lim",
        "authors": "Ming Gu, Lek-Heng Lim, Cinna Julie Wu",
        "title": "PARNES: A rapidly convergent algorithm for accurate recovery of sparse\n  and approximately sparse signals",
        "comments": "22 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY math.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article, we propose an algorithm, NESTA-LASSO, for the LASSO problem,\ni.e., an underdetermined linear least-squares problem with a 1-norm constraint\non the solution. We prove under the assumption of the restricted isometry\nproperty (RIP) and a sparsity condition on the solution, that NESTA-LASSO is\nguaranteed to be almost always locally linearly convergent. As in the case of\nthe algorithm NESTA proposed by Becker, Bobin, and Candes, we rely on\nNesterov's accelerated proximal gradient method, which takes O(e^{-1/2})\niterations to come within e > 0 of the optimal value. We introduce a\nmodification to Nesterov's method that regularly updates the prox-center in a\nprovably optimal manner, and the aforementioned linear convergence is in part\ndue to this modification.\n  In the second part of this article, we attempt to solve the basis pursuit\ndenoising BPDN problem (i.e., approximating the minimum 1-norm solution to an\nunderdetermined least squares problem) by using NESTA-LASSO in conjunction with\nthe Pareto root-finding method employed by van den Berg and Friedlander in\ntheir SPGL1 solver. The resulting algorithm is called PARNES. We provide\nnumerical evidence to show that it is comparable to currently available\nsolvers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 3 Nov 2009 04:44:00 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 5 Nov 2009 07:40:00 GMT"
            },
            {
                "version": "v3",
                "created": "Sat, 2 Jul 2011 09:19:39 GMT"
            },
            {
                "version": "v4",
                "created": "Sat, 31 Mar 2012 00:44:32 GMT"
            }
        ],
        "update_date": "2012-04-03",
        "authors_parsed": [
            [
                "Gu",
                "Ming",
                ""
            ],
            [
                "Lim",
                "Lek-Heng",
                ""
            ],
            [
                "Wu",
                "Cinna Julie",
                ""
            ]
        ]
    },
    {
        "id": "0911.1783",
        "submitter": "Anton Leykin",
        "authors": "Anton Leykin",
        "title": "Numerical Algebraic Geometry for Macaulay2",
        "comments": "7 pages",
        "journal-ref": "Numerical algebraic geometry. JSAG, 3:5-10, 2011",
        "doi": null,
        "report-no": "Mittag-Leffler-2011spring",
        "categories": "math.AG cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Numerical Algebraic Geometry uses numerical data to describe algebraic\nvarieties. It is based on the methods of numerical polynomial homotopy\ncontinuation, an alternative to the classical symbolic approaches of\ncomputational algebraic geometry. We present a package, the driving idea behind\nwhich is to interlink the existing symbolic methods of Macaulay2 and the\npowerful engine of numerical approximate computations. The core procedures of\nthe package exhibit performance competitive with the other homotopy\ncontinuation software.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 9 Nov 2009 21:17:43 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 22 May 2011 09:40:47 GMT"
            }
        ],
        "update_date": "2011-11-23",
        "authors_parsed": [
            [
                "Leykin",
                "Anton",
                ""
            ]
        ]
    },
    {
        "id": "0911.2258",
        "submitter": "Tomoki Ohsawa",
        "authors": "Tomoki Ohsawa, Anthony M. Bloch, Melvin Leok",
        "title": "Discrete Hamilton-Jacobi Theory",
        "comments": "26 pages, 2 figures",
        "journal-ref": "SIAM J. Control Optim. 49 (2011), pp. 1829-1856",
        "doi": "10.1137/090776822",
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We develop a discrete analogue of Hamilton-Jacobi theory in the framework of\ndiscrete Hamiltonian mechanics. The resulting discrete Hamilton-Jacobi equation\nis discrete only in time. We describe a discrete analogue of Jacobi's solution\nand also prove a discrete version of the geometric Hamilton-Jacobi theorem. The\ntheory applied to discrete linear Hamiltonian systems yields the discrete\nRiccati equation as a special case of the discrete Hamilton-Jacobi equation. We\nalso apply the theory to discrete optimal control problems, and recover some\nwell-known results, such as the Bellman equation (discrete-time HJB equation)\nof dynamic programming and its relation to the costate variable in the\nPontryagin maximum principle. This relationship between the discrete\nHamilton-Jacobi equation and Bellman equation is exploited to derive a\ngeneralized form of the Bellman equation that has controls at internal stages.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 11 Nov 2009 22:27:58 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 8 Jun 2011 18:46:31 GMT"
            }
        ],
        "update_date": "2011-08-15",
        "authors_parsed": [
            [
                "Ohsawa",
                "Tomoki",
                ""
            ],
            [
                "Bloch",
                "Anthony M.",
                ""
            ],
            [
                "Leok",
                "Melvin",
                ""
            ]
        ]
    },
    {
        "id": "0911.2889",
        "submitter": "Vladimir Karlin",
        "authors": "V. Karlin",
        "title": "Global communications in multiprocessor simulations of flames",
        "comments": "16 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.CE cs.MS cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we investigate performance of global communications in a\nparticular parallel code. The code simulates dynamics of expansion of premixed\nspherical flames using an asymptotic model of Sivashinsky type and a spectral\nnumerical algorithm. As a result, the code heavily relies on global all-to-all\ninterprocessor communications implementing transposition of the distributed\ndata array in which numerical solution to the problem is stored. This global\ndata interdependence makes interprocessor connectivity of the HPC system as\nimportant as the floating-point power of the processors of which the system is\nbuilt. Our experiments show that efficient numerical simulation of this\nparticular model, with global data interdependence, on modern HPC systems is\npossible. Prospects of performance of more sophisticated models of flame\ndynamics are analysed as well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 15 Nov 2009 17:10:31 GMT"
            }
        ],
        "update_date": "2009-11-17",
        "authors_parsed": [
            [
                "Karlin",
                "V.",
                ""
            ]
        ]
    },
    {
        "id": "0912.0161",
        "submitter": "Peter Csermely",
        "authors": "Istvan A. Kovacs, Robin Palotai, Mate S. Szalay and Peter Csermely",
        "title": "Community landscapes: an integrative approach to determine overlapping\n  network module hierarchy, identify key nodes and predict network dynamics",
        "comments": "25 pages with 6 figures and a Glossary + Supporting Information\n  containing pseudo-codes of all algorithms used, 14 Figures, 5 Tables (with 18\n  module definitions, 129 different modularization methods, 13 module\n  comparision methods) and 396 references. All algorithms can be downloaded\n  from this web-site: http://www.linkgroup.hu/modules.php",
        "journal-ref": "PLoS ONE 5, e12528 (2010)",
        "doi": "10.1371/journal.pone.0012528",
        "report-no": null,
        "categories": "physics.comp-ph cond-mat.dis-nn cs.MS physics.data-an physics.soc-ph q-bio.MN",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Background: Network communities help the functional organization and\nevolution of complex networks. However, the development of a method, which is\nboth fast and accurate, provides modular overlaps and partitions of a\nheterogeneous network, has proven to be rather difficult. Methodology/Principal\nFindings: Here we introduce the novel concept of ModuLand, an integrative\nmethod family determining overlapping network modules as hills of an influence\nfunction-based, centrality-type community landscape, and including several\nwidely used modularization methods as special cases. As various adaptations of\nthe method family, we developed several algorithms, which provide an efficient\nanalysis of weighted and directed networks, and (1) determine pervasively\noverlapping modules with high resolution; (2) uncover a detailed hierarchical\nnetwork structure allowing an efficient, zoom-in analysis of large networks;\n(3) allow the determination of key network nodes and (4) help to predict\nnetwork dynamics. Conclusions/Significance: The concept opens a wide range of\npossibilities to develop new approaches and applications including network\nrouting, classification, comparison and prediction.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 1 Dec 2009 10:36:18 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 2 Dec 2009 12:58:48 GMT"
            },
            {
                "version": "v3",
                "created": "Sat, 13 Mar 2010 11:44:46 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 3 Sep 2010 06:14:01 GMT"
            }
        ],
        "update_date": "2010-09-06",
        "authors_parsed": [
            [
                "Kovacs",
                "Istvan A.",
                ""
            ],
            [
                "Palotai",
                "Robin",
                ""
            ],
            [
                "Szalay",
                "Mate S.",
                ""
            ],
            [
                "Csermely",
                "Peter",
                ""
            ]
        ]
    },
    {
        "id": "0912.0606",
        "submitter": "Rdv Ijcsis",
        "authors": "C. Yaashuwanth, Dr. R. Ramesh",
        "title": "A New Scheduling Algorithms For Real Time Tasks",
        "comments": "6 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS November 2009, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/",
        "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 2, pp. 061-066, November 2009, USA",
        "doi": null,
        "report-no": "ISSN 1947 5500",
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main objective of this paper is to develop the two different ways in\nwhich round robin architecture is modified and made suitable to be implemented\nin real time and embedded systems. The scheduling algorithm plays a significant\nrole in the design of real time embedded systems. Simple round robin\narchitecture is not efficient to be implemented in embedded systems because of\nhigher context switch rate, larger waiting time and larger response time.\nMissing of deadlines will degrade the system performance in soft real time\nsystems. The main objective of this paper is to develop the scheduling\nalgorithm which removes the drawbacks in simple round robin architecture. A\ncomparison with round robin architecture to the proposed architectures has been\nmade. It is observed that the proposed architectures solves the problems\nencountered in round robin architecture in soft real time by decreasing the\nnumber of context switches waiting time and response time thereby increasing\nthe system throughput.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 3 Dec 2009 09:06:59 GMT"
            }
        ],
        "update_date": "2009-12-04",
        "authors_parsed": [
            [
                "Yaashuwanth",
                "C.",
                ""
            ],
            [
                "Ramesh",
                "Dr. R.",
                ""
            ]
        ]
    },
    {
        "id": "0912.0926",
        "submitter": "Bryan Ford",
        "authors": "Amittai Aviram and Bryan Ford (Yale University)",
        "title": "Deterministic Consistency: A Programming Model for Shared Memory\n  Parallelism",
        "comments": "7 pages, 3 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The difficulty of developing reliable parallel software is generating\ninterest in deterministic environments, where a given program and input can\nyield only one possible result. Languages or type systems can enforce\ndeterminism in new code, and runtime systems can impose synthetic schedules on\nlegacy parallel code. To parallelize existing serial code, however, we would\nlike a programming model that is naturally deterministic without language\nrestrictions or artificial scheduling. We propose \"deterministic consistency\",\na parallel programming model as easy to understand as the \"parallel assignment\"\nconstruct in sequential languages such as Perl and JavaScript, where concurrent\nthreads always read their inputs before writing shared outputs. DC supports\ncommon data- and task-parallel synchronization abstractions such as fork/join\nand barriers, as well as non-hierarchical structures such as producer/consumer\npipelines and futures. A preliminary prototype suggests that software-only\nimplementations of DC can run applications written for popular parallel\nenvironments such as OpenMP with low (<10%) overhead for some applications.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 4 Dec 2009 20:10:12 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 1 Feb 2010 16:27:48 GMT"
            }
        ],
        "update_date": "2010-02-01",
        "authors_parsed": [
            [
                "Aviram",
                "Amittai",
                "",
                "Yale University"
            ],
            [
                "Ford",
                "Bryan",
                "",
                "Yale University"
            ]
        ]
    },
    {
        "id": "0912.1883",
        "submitter": "Marcel Nutz",
        "authors": "Marcel Nutz",
        "title": "The Bellman equation for power utility maximization with semimartingales",
        "comments": "Published in at http://dx.doi.org/10.1214/11-AAP776 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)",
        "journal-ref": "Annals of Applied Probability 2012, Vol. 22, No. 1, 363-406",
        "doi": "10.1214/11-AAP776",
        "report-no": "IMS-AAP-AAP776",
        "categories": "q-fin.PM cs.SY math.OC math.PR q-fin.CP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study utility maximization for power utility random fields with and\nwithout intermediate consumption in a general semimartingale model with closed\nportfolio constraints. We show that any optimal strategy leads to a solution of\nthe corresponding Bellman equation. The optimal strategies are described\npointwise in terms of the opportunity process, which is characterized as the\nminimal solution of the Bellman equation. We also give verification theorems\nfor this equation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 Dec 2009 23:16:37 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 4 May 2011 07:37:27 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 8 Mar 2012 08:29:51 GMT"
            }
        ],
        "update_date": "2012-03-09",
        "authors_parsed": [
            [
                "Nutz",
                "Marcel",
                ""
            ]
        ]
    },
    {
        "id": "0912.2425",
        "submitter": "Wenlian Lu",
        "authors": "Wenlian Lu, Fatihcan M. Atay, Jurgen Jost",
        "title": "Consensus and synchronization in discrete-time networks of multi-agents\n  with stochastically switching topologies and time delays",
        "comments": "20 pages, 2 figures",
        "journal-ref": "Networks and Heterogeneous Media, 6:329-349, 2011",
        "doi": "10.3934/nhm.2011.6.329",
        "report-no": null,
        "categories": "math.DS cs.SY math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We analyze stability of consensus algorithms in networks of multi-agents with\ntime-varying topologies and delays. The topology and delays are modeled as\ninduced by an adapted process and are rather general, including i.i.d.\\\ntopology processes, asynchronous consensus algorithms, and Markovian jumping\nswitching. In case the self-links are instantaneous, we prove that the network\nreaches consensus for all bounded delays if the graph corresponding to the\nconditional expectation of the coupling matrix sum across a finite time\ninterval has a spanning tree almost surely. Moreover, when self-links are also\ndelayed and when the delays satisfy certain integer patterns, we observe and\nprove that the algorithm may not reach consensus but instead synchronize at a\nperiodic trajectory, whose period depends on the delay pattern. We also give a\nbrief discussion on the dynamics in the absence of self-links.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 12 Dec 2009 14:47:16 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 8 Aug 2010 11:19:06 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 25 Feb 2011 10:41:08 GMT"
            }
        ],
        "update_date": "2011-06-07",
        "authors_parsed": [
            [
                "Lu",
                "Wenlian",
                ""
            ],
            [
                "Atay",
                "Fatihcan M.",
                ""
            ],
            [
                "Jost",
                "Jurgen",
                ""
            ]
        ]
    },
    {
        "id": "0912.3398",
        "submitter": "Thomas Gorochowski",
        "authors": "Thomas E. Gorochowski, Mario di Bernardo, Claire S. Grierson",
        "title": "NetEvo: A computational framework for the evolution of dynamical complex\n  networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  NetEvo is a computational framework designed to help understand the evolution\nof dynamical complex networks. It provides flexible tools for the simulation of\ndynamical processes on networks and methods for the evolution of underlying\ntopological structures. The concept of a supervisor is used to bring together\nboth these aspects in a coherent way. It is the job of the supervisor to rewire\nthe network topology and alter model parameters such that a user specified\nperformance measure is minimised. This performance measure can make use of\ncurrent topological information and simulated dynamical output from the system.\nSuch an abstraction provides a suitable basis in which to study many\noutstanding questions related to complex system design and evolution.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 17 Dec 2009 13:49:59 GMT"
            }
        ],
        "update_date": "2009-12-18",
        "authors_parsed": [
            [
                "Gorochowski",
                "Thomas E.",
                ""
            ],
            [
                "di Bernardo",
                "Mario",
                ""
            ],
            [
                "Grierson",
                "Claire S.",
                ""
            ]
        ]
    },
    {
        "id": "0912.3852",
        "submitter": "Sathish Gopalakrishnan",
        "authors": "Sathish Gopalakrishnan",
        "title": "Sharp utilization thresholds for some real-time scheduling problems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.DM cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Scheduling policies for real-time systems exhibit threshold behavior that is\nrelated to the utilization of the task set they schedule, and in some cases\nthis threshold is sharp. For the rate monotonic scheduling policy, we show that\nperiodic workload with utilization less than a threshold $U_{RM}^{*}$ can be\nscheduled almost surely and that all workload with utilization greater than\n$U_{RM}^{*}$ is almost surely not schedulable. We study such sharp threshold\nbehavior in the context of processor scheduling using static task priorities,\nnot only for periodic real-time tasks but for aperiodic real-time tasks as\nwell. The notion of a utilization threshold provides a simple schedulability\ntest for most real-time applications. These results improve our understanding\nof scheduling policies and provide an interesting characterization of the\ntypical behavior of policies. The threshold is sharp (small deviations around\nthe threshold cause schedulability, as a property, to appear or disappear) for\nmost policies; this is a happy consequence that can be used to address the\nlimitations of existing utilization-based tests for schedulability. We\ndemonstrate the use of such an approach for balancing power consumption with\nthe need to meet deadlines in web servers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 19 Dec 2009 01:18:05 GMT"
            }
        ],
        "update_date": "2009-12-22",
        "authors_parsed": [
            [
                "Gopalakrishnan",
                "Sathish",
                ""
            ]
        ]
    },
    {
        "id": "0912.4062",
        "submitter": "Emil Vassev Dr.",
        "authors": "Emil Vassev",
        "title": "Process Description of COM Object Life Cycle",
        "comments": "15 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The objective of this article is to provide for the reader a basic\ndescription of all the steps involved in the COM object life-cycle process. COM\nis a software technology and process performer. The first section briefly\nintroduces the Component Object Model (COM), considering the process of the COM\nobject life cycle as the baseline of all COM issues. The second part describes\nin detail the basic steps of the process - client request, server location,\nobject creation, interaction, and disconnection. A brief description is given\nfor the components involved in each step. Finally, the third section provides a\nbrief conclusion summarizing all the process steps.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 20 Dec 2009 23:49:25 GMT"
            }
        ],
        "update_date": "2009-12-22",
        "authors_parsed": [
            [
                "Vassev",
                "Emil",
                ""
            ]
        ]
    },
    {
        "id": "1001.0196",
        "submitter": "Edward Walker",
        "authors": "Edward Walker",
        "title": "A distributed file system for a wide-area high performance computing\n  infrastructure",
        "comments": "6 pages, Proceedings of Third USENIX Workshop on Real, Large\n  Distributed Systems, Seattle, Nov 2006",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe our work in implementing a wide-area distributed file system for\nthe NSF TeraGrid. The system, called XUFS, allows private distributed name\nspaces to be created for transparent access to personal files across over 9000\ncomputer nodes. XUFS builds on many principles from prior distributed file\nsystems research, but extends key design goals to support the workflow of\ncomputational science researchers. Specifically, XUFS supports file access from\nthe desktop to the wide-area network seamlessly, survives transient\ndisconnected operations robustly, and demonstrates comparable or better\nthroughput than some current high performance file systems on the wide-area\nnetwork.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 1 Jan 2010 00:34:59 GMT"
            }
        ],
        "update_date": "2010-01-05",
        "authors_parsed": [
            [
                "Walker",
                "Edward",
                ""
            ]
        ]
    },
    {
        "id": "1001.1435",
        "submitter": "Arnaud Casteigts",
        "authors": "Arnaud Casteigts, R\\'emi Laplace",
        "title": "JBotSim, a Tool for Fast Prototyping of Distributed Algorithms in\n  Dynamic Networks",
        "comments": "A shorter version appeared in SIMUTOOLS 2015. For up to date\n  information and tutorials, visit http://jbotsim.io",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.DC cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  JBotSim is a java library that offers basic primitives for prototyping,\nrunning, and visualizing distributed algorithms in dynamic networks. With\nJBotSim, one can implement an idea in minutes and interact with it ({\\it e.g.\n}, add, move, or delete nodes) while it is running. JBotSim is well suited to\nprepare live demonstrations of your algorithms to colleagues or students; it\ncan also be used to evaluate performance at the algorithmic level (number of\nmessages, number of rounds, etc.). Unlike most tools, JBotSim is not an\nintegrated environment. It is a lightweight library to be used in your program.\nIn this paper, we present an overview of its distinctive features and\narchitecture.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 9 Jan 2010 18:26:00 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 1 Jul 2013 15:06:10 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 22 Jul 2013 15:42:34 GMT"
            },
            {
                "version": "v4",
                "created": "Wed, 25 Sep 2013 08:39:10 GMT"
            },
            {
                "version": "v5",
                "created": "Mon, 11 May 2015 16:14:03 GMT"
            },
            {
                "version": "v6",
                "created": "Mon, 18 May 2015 17:06:00 GMT"
            },
            {
                "version": "v7",
                "created": "Sat, 3 Mar 2018 15:27:04 GMT"
            },
            {
                "version": "v8",
                "created": "Mon, 15 Jul 2019 20:05:48 GMT"
            },
            {
                "version": "v9",
                "created": "Wed, 25 Sep 2019 13:56:45 GMT"
            }
        ],
        "update_date": "2019-09-26",
        "authors_parsed": [
            [
                "Casteigts",
                "Arnaud",
                ""
            ],
            [
                "Laplace",
                "R\u00e9mi",
                ""
            ]
        ]
    },
    {
        "id": "1001.1654",
        "submitter": "Sebastian F. Walter",
        "authors": "S.F. Walter, L. Lehmann",
        "title": "Algorithmic Differentiation of Linear Algebra Functions with Application\n  in Optimum Experimental Design (Extended Version)",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.MS math.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We derive algorithms for higher order derivative computation of the\nrectangular $QR$ and eigenvalue decomposition of symmetric matrices with\ndistinct eigenvalues in the forward and reverse mode of algorithmic\ndifferentiation (AD) using univariate Taylor propagation of matrices (UTPM).\nLinear algebra functions are regarded as elementary functions and not as\nalgorithms. The presented algorithms are implemented in the BSD licensed AD\ntool \\texttt{ALGOPY}. Numerical tests show that the UTPM algorithms derived in\nthis paper produce results close to machine precision accuracy. The theory\ndeveloped in this paper is applied to compute the gradient of an objective\nfunction motivated from optimum experimental design: $\\nabla_x\n\\Phi(C(J(F(x,y))))$, where $\\Phi = \\{\\lambda_1 : \\lambda_1 C\\}$, $C = (J^T\nJ)^{-1}$, $J = \\frac{\\dd F}{\\dd y}$ and $F = F(x,y)$.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 11 Jan 2010 13:37:38 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 19 Feb 2010 09:30:31 GMT"
            }
        ],
        "update_date": "2010-02-19",
        "authors_parsed": [
            [
                "Walter",
                "S. F.",
                ""
            ],
            [
                "Lehmann",
                "L.",
                ""
            ]
        ]
    },
    {
        "id": "1001.2612",
        "submitter": "Minghui Zhu",
        "authors": "Minghui Zhu and Sonia Martinez",
        "title": "On distributed convex optimization under inequality and equality\n  constraints via primal-dual subgradient methods",
        "comments": "44 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a general multi-agent convex optimization problem where the\nagents are to collectively minimize a global objective function subject to a\nglobal inequality constraint, a global equality constraint, and a global\nconstraint set. The objective function is defined by a sum of local objective\nfunctions, while the global constraint set is produced by the intersection of\nlocal constraint sets. In particular, we study two cases: one where the\nequality constraint is absent, and the other where the local constraint sets\nare identical. We devise two distributed primal-dual subgradient algorithms\nwhich are based on the characterization of the primal-dual optimal solutions as\nthe saddle points of the Lagrangian and penalty functions. These algorithms can\nbe implemented over networks with changing topologies but satisfying a standard\nconnectivity property, and allow the agents to asymptotically agree on optimal\nsolutions and optimal values of the optimization problem under the Slater's\ncondition.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 15 Jan 2010 05:12:48 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 11 May 2011 20:44:00 GMT"
            }
        ],
        "update_date": "2011-05-13",
        "authors_parsed": [
            [
                "Zhu",
                "Minghui",
                ""
            ],
            [
                "Martinez",
                "Sonia",
                ""
            ]
        ]
    },
    {
        "id": "1001.2620",
        "submitter": "Paolo Frasca",
        "authors": "Francesca Ceragioli, Claudio De Persis, Paolo Frasca",
        "title": "Discontinuities and hysteresis in quantized average consensus",
        "comments": "26 pages, 7 figures. Accepted for publication in Automatica. v4 is\n  minor revision of v3",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider continuous-time average consensus dynamics in which the agents'\nstates are communicated through uniform quantizers. Solutions to the resulting\nsystem are defined in the Krasowskii sense and are proven to converge to\nconditions of \"practical consensus\". To cope with undesired chattering\nphenomena we introduce a hysteretic quantizer, and we study the convergence\nproperties of the resulting dynamics by a hybrid system approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 15 Jan 2010 06:41:05 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 16 Feb 2010 16:03:40 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 12 Oct 2010 14:45:56 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 14 Mar 2011 13:56:52 GMT"
            }
        ],
        "update_date": "2011-03-16",
        "authors_parsed": [
            [
                "Ceragioli",
                "Francesca",
                ""
            ],
            [
                "De Persis",
                "Claudio",
                ""
            ],
            [
                "Frasca",
                "Paolo",
                ""
            ]
        ]
    },
    {
        "id": "1001.3213",
        "submitter": "Jean-Philippe Chancelier",
        "authors": "Jean-Philippe Chancelier (CERMICS), J\\'er\\^ome Lelong (LJK), Bernard\n  Lapeyre (CERMICS)",
        "title": "Using Premia and Nsp for Constructing a Risk Management Benchmark for\n  Testing Parallel Architecture",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CE cs.DC cs.MS cs.NA q-fin.CP q-fin.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Financial institutions have massive computations to carry out overnight which\nare very demanding in terms of the consumed CPU. The challenge is to price many\ndifferent products on a cluster-like architecture. We have used the Premia\nsoftware to valuate the financial derivatives. In this work, we explain how\nPremia can be embedded into Nsp, a scientific software like Matlab, to provide\na powerful tool to valuate a whole portfolio. Finally, we have integrated an\nMPI toolbox into Nsp to enable to use Premia to solve a bunch of pricing\nproblems on a cluster. This unified framework can then be used to test\ndifferent parallel architectures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 19 Jan 2010 07:54:16 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 21 May 2012 19:13:53 GMT"
            }
        ],
        "update_date": "2012-05-23",
        "authors_parsed": [
            [
                "Chancelier",
                "Jean-Philippe",
                "",
                "CERMICS"
            ],
            [
                "Lelong",
                "J\u00e9r\u00f4me",
                "",
                "LJK"
            ],
            [
                "Lapeyre",
                "Bernard",
                "",
                "CERMICS"
            ]
        ]
    },
    {
        "id": "1001.3727",
        "submitter": "T.R. Gopalakrishnan Nair",
        "authors": "A. Christy Persya, T.R. Gopalakrishnan Nair",
        "title": "Fault Tolerance in Real Time Multiprocessors - Embedded Systems",
        "comments": "4 pages",
        "journal-ref": "Fourth Innovative Conference on Embedded Systems, Mobile\n  Communication and Computing, 2009",
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  All real time tasks which are termed as critical tasks by nature have to\ncomplete its execution before its deadline, even in presence of faults. The\nmost popularly used real time task assignment algorithms are First Fit (FF),\nBest Fit (BF), Bin Packing (BP).The common task scheduling algorithms are Rate\nMonotonic (RM), Earliest Deadline First (EDF) etc.All the current approaches\ndeal with either fault tolerance or criticality in real time. In this paper we\nhave proposed an integrated approach with a new algorithm, called SASA (Sorting\nAnd Sequential Assignment) which maps the real time task assignment with task\nschedule and fault tolerance\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 21 Jan 2010 06:02:41 GMT"
            }
        ],
        "update_date": "2016-09-08",
        "authors_parsed": [
            [
                "Persya",
                "A. Christy",
                ""
            ],
            [
                "Nair",
                "T. R. Gopalakrishnan",
                ""
            ]
        ]
    },
    {
        "id": "1001.4115",
        "submitter": "Geoffrey Nelissen",
        "authors": "Shelby Funk, Vincent Nelis, Joel Goossens, Dragomir Milojevic,\n  Geoffrey Nelissen",
        "title": "On the Design of an Optimal Multiprocessor Real-Time Scheduling\n  Algorithm under Practical Considerations (Extended Version)",
        "comments": "This paper has been withdrawn by the authors",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This research addresses the multiprocessor scheduling problem of hard\nreal-time systems, and it especially focuses on optimal and global schedulers\nwhen practical constraints are taken into account. First, we propose an\nimprovement of the optimal algorithm BF. We formally prove that our adaptation\nis (i) optimal, i.e., it always generates a feasible schedule as long as such a\nschedule exists, and (ii) valid, i.e., it complies with the all the\nrequirements. We also show that it outperforms BF by providing a computing\ncomplexity of O(n), where n is the number of tasks to be scheduled. Next, we\npropose a schedulability analysis which indicates a priori whether the\nreal-time application can be scheduled by our improvement of BF without missing\nany deadline. This analysis is, to the best of our knowledge, the first such\ntest for multiprocessors that takes into account all the main overheads\ngenerated by the Operating System.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 25 Jan 2010 08:09:27 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 27 Jan 2010 10:09:41 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 24 Jan 2011 12:39:16 GMT"
            }
        ],
        "update_date": "2011-01-25",
        "authors_parsed": [
            [
                "Funk",
                "Shelby",
                ""
            ],
            [
                "Nelis",
                "Vincent",
                ""
            ],
            [
                "Goossens",
                "Joel",
                ""
            ],
            [
                "Milojevic",
                "Dragomir",
                ""
            ],
            [
                "Nelissen",
                "Geoffrey",
                ""
            ]
        ]
    },
    {
        "id": "1001.4475",
        "submitter": "Gilles Stoltz",
        "authors": "S\\'ebastien Bubeck (INRIA Futurs), R\\'emi Munos (INRIA Lille - Nord\n  Europe), Gilles Stoltz (DMA, GREGH, INRIA Paris - Rocquencourt), Csaba\n  Szepesvari",
        "title": "X-Armed Bandits",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.SY math.OC math.ST stat.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a generalization of stochastic bandits where the set of arms,\n$\\cX$, is allowed to be a generic measurable space and the mean-payoff function\nis \"locally Lipschitz\" with respect to a dissimilarity function that is known\nto the decision maker. Under this condition we construct an arm selection\npolicy, called HOO (hierarchical optimistic optimization), with improved regret\nbounds compared to previous results for a large class of problems. In\nparticular, our results imply that if $\\cX$ is the unit hypercube in a\nEuclidean space and the mean-payoff function has a finite number of global\nmaxima around which the behavior of the function is locally continuous with a\nknown smoothness degree, then the expected regret of HOO is bounded up to a\nlogarithmic factor by $\\sqrt{n}$, i.e., the rate of growth of the regret is\nindependent of the dimension of the space. We also prove the minimax optimality\nof our algorithm when the dissimilarity is a metric. Our basic strategy has\nquadratic computational complexity as a function of the number of time steps\nand does not rely on the doubling trick. We also introduce a modified strategy,\nwhich relies on the doubling trick but runs in linearithmic time. Both results\nare improvements with respect to previous approaches.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 25 Jan 2010 16:30:15 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 13 Apr 2011 07:03:48 GMT"
            }
        ],
        "update_date": "2011-04-15",
        "authors_parsed": [
            [
                "Bubeck",
                "S\u00e9bastien",
                "",
                "INRIA Futurs"
            ],
            [
                "Munos",
                "R\u00e9mi",
                "",
                "INRIA Lille - Nord\n  Europe"
            ],
            [
                "Stoltz",
                "Gilles",
                "",
                "DMA, GREGH, INRIA Paris - Rocquencourt"
            ],
            [
                "Szepesvari",
                "Csaba",
                ""
            ]
        ]
    },
    {
        "id": "1001.5272",
        "submitter": "Daniel Roche",
        "authors": "David Harvey (New York University) and Daniel S. Roche (University of\n  Waterloo)",
        "title": "An in-place truncated Fourier transform and applications to polynomial\n  multiplication",
        "comments": "5 pages, 1 figure, pdflatex",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.MS cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The truncated Fourier transform (TFT) was introduced by van der Hoeven in\n2004 as a means of smoothing the \"jumps\" in running time of the ordinary FFT\nalgorithm that occur at power-of-two input sizes. However, the TFT still\nintroduces these jumps in memory usage. We describe in-place variants of the\nforward and inverse TFT algorithms, achieving time complexity O(n log n) with\nonly O(1) auxiliary space. As an application, we extend the second author's\nresults on space-restricted FFT-based polynomial multiplication to polynomials\nof arbitrary degree.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 28 Jan 2010 21:10:41 GMT"
            }
        ],
        "update_date": "2010-02-01",
        "authors_parsed": [
            [
                "Harvey",
                "David",
                "",
                "New York University"
            ],
            [
                "Roche",
                "Daniel S.",
                "",
                "University of\n  Waterloo"
            ]
        ]
    },
    {
        "id": "1002.0012",
        "submitter": "Jacques Carette",
        "authors": "Jacques Carette, James H. Davenport",
        "title": "The Power of Vocabulary: The Case of Cyclotomic Polynomials",
        "comments": "7 pages. Submitted to ISSAC 2010",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.DS cs.MS math.AC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We observe that the vocabulary used to construct the \"answer\" to problems in\ncomputer algebra can have a dramatic effect on the computational complexity of\nsolving that problem. We recall a formalization of this observation and explain\nthe classic example of sparse polynomial arithmetic. For this case, we show\nthat it is possible to extend the vocabulary so as reap the benefits of\nconciseness whilst avoiding the obvious pitfall of repeating the problem\nstatement as the \"solution\".\n  It is possible to extend the vocabulary either by irreducible cyclotomics or\nby $x^n-1$: we look at the options and suggest that the pragmatist might opt\nfor both.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 29 Jan 2010 21:23:05 GMT"
            }
        ],
        "update_date": "2010-02-02",
        "authors_parsed": [
            [
                "Carette",
                "Jacques",
                ""
            ],
            [
                "Davenport",
                "James H.",
                ""
            ]
        ]
    },
    {
        "id": "1002.0298",
        "submitter": "Jayanthkumar Kannan",
        "authors": "Jayanthkumar Kannan, Petros Maniatis, Byung-Gon Chun",
        "title": "A Data Capsule Framework For Web Services: Providing Flexible Data\n  Access Control To Users",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CR cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper introduces the notion of a secure data capsule, which refers to an\nencapsulation of sensitive user information (such as a credit card number)\nalong with code that implements an interface suitable for the use of such\ninformation (such as charging for purchases) by a service (such as an online\nmerchant). In our capsule framework, users provide their data in the form of\nsuch capsules to web services rather than raw data. Capsules can be deployed in\na variety of ways, either on a trusted third party or the user's own computer\nor at the service itself, through the use of a variety of hardware or software\nmodules, such as a virtual machine monitor or trusted platform module: the only\nrequirement is that the deployment mechanism must ensure that the user's data\nis only accessed via the interface sanctioned by the user. The framework\nfurther allows an user to specify policies regarding which services or machines\nmay host her capsule, what parties are allowed to access the interface, and\nwith what parameters. The combination of interface restrictions and policy\ncontrol lets us bound the impact of an attacker who compromises the service to\ngain access to the user's capsule or a malicious insider at the service itself.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 1 Feb 2010 18:31:06 GMT"
            }
        ],
        "update_date": "2010-02-02",
        "authors_parsed": [
            [
                "Kannan",
                "Jayanthkumar",
                ""
            ],
            [
                "Maniatis",
                "Petros",
                ""
            ],
            [
                "Chun",
                "Byung-Gon",
                ""
            ]
        ]
    },
    {
        "id": "1002.2283",
        "submitter": "Choon Yik Tang",
        "authors": "Jie Lu, Choon Yik Tang, Paul R. Regier, Travis D. Bow",
        "title": "Gossip Algorithms for Convex Consensus Optimization over Networks",
        "comments": "15 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.DC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In many applications, nodes in a network desire not only a consensus, but an\noptimal one. To date, a family of subgradient algorithms have been proposed to\nsolve this problem under general convexity assumptions. This paper shows that,\nfor the scalar case and by assuming a bit more, novel non-gradient-based\nalgorithms with appealing features can be constructed. Specifically, we develop\nPairwise Equalizing (PE) and Pairwise Bisectioning (PB), two gossip algorithms\nthat solve unconstrained, separable, convex consensus optimization problems\nover undirected networks with time-varying topologies, where each local\nfunction is strictly convex, continuously differentiable, and has a minimizer.\nWe show that PE and PB are easy to implement, bypass limitations of the\nsubgradient algorithms, and produce switched, nonlinear, networked dynamical\nsystems that admit a common Lyapunov function and asymptotically converge.\nMoreover, PE generalizes the well-known Pairwise Averaging and Randomized\nGossip Algorithm, while PB relaxes a requirement of PE, allowing nodes to never\nshare their local functions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 11 Feb 2010 05:51:47 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 16 Feb 2010 23:22:30 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 10 Feb 2011 17:24:47 GMT"
            }
        ],
        "update_date": "2011-02-11",
        "authors_parsed": [
            [
                "Lu",
                "Jie",
                ""
            ],
            [
                "Tang",
                "Choon Yik",
                ""
            ],
            [
                "Regier",
                "Paul R.",
                ""
            ],
            [
                "Bow",
                "Travis D.",
                ""
            ]
        ]
    },
    {
        "id": "1002.2594",
        "submitter": "Luca De Feo",
        "authors": "Luca De Feo, \\'Eric Schost",
        "title": "Fast Arithmetics in Artin-Schreier Towers over Finite Fields",
        "comments": "28 pages, 4 figures, 3 tables, uses mathdots.sty, yjsco.sty Submitted\n  to J. Symb. Comput",
        "journal-ref": null,
        "doi": "10.1016/j.jsc.2011.12.008",
        "report-no": null,
        "categories": "cs.SC cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An Artin-Schreier tower over the finite field F_p is a tower of field\nextensions generated by polynomials of the form X^p - X - a. Following Cantor\nand Couveignes, we give algorithms with quasi-linear time complexity for\narithmetic operations in such towers. As an application, we present an\nimplementation of Couveignes' algorithm for computing isogenies between\nelliptic curves using the p-torsion.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 12 Feb 2010 16:44:18 GMT"
            }
        ],
        "update_date": "2020-01-07",
        "authors_parsed": [
            [
                "De Feo",
                "Luca",
                ""
            ],
            [
                "Schost",
                "\u00c9ric",
                ""
            ]
        ]
    },
    {
        "id": "1002.2654",
        "submitter": "Evgeny Norman D.",
        "authors": "Evgeny D. Norman",
        "title": "Assessment Of The Wind Farm Impact On The Radar",
        "comments": "Master's Thesis, 62 pages, LaTeX. Submitted to ENSIETA & Thales Air\n  Systems. Paris area, 2009",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.CV cs.MS",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  This study shows the means to evaluate the wind farm impact on the radar. It\nproposes the set of tools, which can be used to realise this objective. The big\npart of report covers the study of complex pattern propagation factor as the\ncritical issue of the Advanced Propagation Model (APM). Finally, the reader can\nfind here the implementation of this algorithm - the real scenario in Inverness\nairport (the United Kingdom), where the ATC radar STAR 2000, developed by\nThales Air Systems, operates in the presence of several wind farms. Basically,\nthe project is based on terms of the department \"Strategy Technology &\nInnovation\", where it has been done. Also you can find here how the radar\nindustry can act with the problem engendered by wind farms. The current\nstrategies in this area are presented, such as a wind turbine production,\nimprovements of air traffic handling procedures and the collaboration between\ndevelopers of radars and wind turbines. The possible strategy for Thales as a\nmain pioneer was given as well.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 13 Feb 2010 01:19:53 GMT"
            }
        ],
        "update_date": "2010-02-16",
        "authors_parsed": [
            [
                "Norman",
                "Evgeny D.",
                ""
            ]
        ]
    },
    {
        "id": "1002.3180",
        "submitter": "Fabrizio Caruso",
        "authors": "Fabrizio Caruso",
        "title": "Factorization of Non-Commutative Polynomials",
        "comments": "7 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe an algorithm for the factorization of non-commutative polynomials\nover a field. The first sketch of this algorithm appeared in an unpublished\nmanuscript (literally hand written notes) by James H. Davenport more than 20\nyears ago. This version of the algorithm contains some improvements with\nrespect to the original sketch. An improved version of the algorithm has been\nfully implemented in the Axiom computer algebra system.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 16 Feb 2010 22:17:56 GMT"
            }
        ],
        "update_date": "2010-02-18",
        "authors_parsed": [
            [
                "Caruso",
                "Fabrizio",
                ""
            ]
        ]
    },
    {
        "id": "1002.3312",
        "submitter": "Sugumar Murugesan",
        "authors": "Sugumar Murugesan, Philip Schniter, Ness B. Shroff",
        "title": "Multiuser Scheduling in a Markov-modeled Downlink using Randomly Delayed\n  ARQ Feedback",
        "comments": "Contains 22 pages, 6 figures and 8 tables; revised version including\n  additional analytical and numerical results; work submitted, Feb 2010, to\n  IEEE Transactions on Information Theory, revised April 2011; authors can be\n  reached at sugumar.murugesan@asu.edu/schniter@ece.osu.edu/shroff@ece.osu.edu",
        "journal-ref": null,
        "doi": "10.1109/TIT.2011.2173717",
        "report-no": null,
        "categories": "cs.IT cs.SY math.IT math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We focus on the downlink of a cellular system, which corresponds to the bulk\nof the data transfer in such wireless systems. We address the problem of\nopportunistic multiuser scheduling under imperfect channel state information,\nby exploiting the memory inherent in the channel. In our setting, the channel\nbetween the base station and each user is modeled by a two-state Markov chain\nand the scheduled user sends back an ARQ feedback signal that arrives at the\nscheduler with a random delay that is i.i.d across users and time. The\nscheduler indirectly estimates the channel via accumulated delayed-ARQ feedback\nand uses this information to make scheduling decisions. We formulate a\nthroughput maximization problem as a partially observable Markov decision\nprocess (POMDP). For the case of two users in the system, we show that a greedy\npolicy is sum throughput optimal for any distribution on the ARQ feedback\ndelay. For the case of more than two users, we prove that the greedy policy is\nsuboptimal and demonstrate, via numerical studies, that it has near optimal\nperformance. We show that the greedy policy can be implemented by a simple\nalgorithm that does not require the statistics of the underlying Markov channel\nor the ARQ feedback delay, thus making it robust against errors in system\nparameter estimation. Establishing an equivalence between the two-user system\nand a genie-aided system, we obtain a simple closed form expression for the sum\ncapacity of the Markov-modeled downlink. We further derive inner and outer\nbounds on the capacity region of the Markov-modeled downlink and tighten these\nbounds for special cases of the system parameters.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 17 Feb 2010 17:22:31 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 27 Apr 2011 04:59:34 GMT"
            }
        ],
        "update_date": "2016-11-15",
        "authors_parsed": [
            [
                "Murugesan",
                "Sugumar",
                ""
            ],
            [
                "Schniter",
                "Philip",
                ""
            ],
            [
                "Shroff",
                "Ness B.",
                ""
            ]
        ]
    },
    {
        "id": "1002.3329",
        "submitter": "Ashley Smith",
        "authors": "M.Tarighi, S.A.Motamedi and S.Sharifian",
        "title": "A new model for virtual machine migration in virtualized cluster server\n  based on Fuzzy Decision Making",
        "comments": "Journal of Telecommunications,Volume 1, Issue 1, pp40-51, February\n  2010",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we show that performance of the virtualized cluster servers\ncould be improved through intelligent decision over migration time of Virtual\nMachines across heterogeneous physical nodes of a cluster server. The cluster\nserves a variety range of services from Web Service to File Service. Some of\nthem are CPU-Intensive while others are RAM-Intensive and so on. Virtualization\nhas many advantages such as less hardware cost, cooling cost, more\nmanageability. One of the key benefits is better load balancing by using of VM\nmigration between hosts. To migrate, we must know which virtual machine needs\nto be migrated and when this relocation has to be done and, moreover, which\nhost must be destined. To relocate VMs from overloaded servers to underloaded\nones, we need to sort nodes from the highest volume to the lowest. There are\nsome models to finding the most overloaded node, but they have some\nshortcomings. The focus of this paper is to present a new method to migrate VMs\nbetween cluster nodes using TOPSIS algorithm - one of the most efficient Multi\nCriteria Decision Making techniques- to make more effective decision over whole\nactive servers of the Cluster and find the most loaded serversTo evaluate the\nperformance improvement resulted from this model, we used cluster Response time\nand Unbalanced Factor.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 17 Feb 2010 17:52:17 GMT"
            }
        ],
        "update_date": "2010-02-18",
        "authors_parsed": [
            [
                "Tarighi",
                "M.",
                ""
            ],
            [
                "Motamedi",
                "S. A.",
                ""
            ],
            [
                "Sharifian",
                "S.",
                ""
            ]
        ]
    },
    {
        "id": "1002.4057",
        "submitter": "Julien Langou",
        "authors": "Emmanuel Agullo, Henricus Bouwmeester, Jack Dongarra, Jakub Kurzak,\n  Julien Langou, and Lee Rosenberg",
        "title": "Towards an Efficient Tile Matrix Inversion of Symmetric Positive\n  Definite Matrices on Multicore Architectures",
        "comments": "8 pages, extended abstract submitted to VecPar10 on 12/11/09,\n  notification of acceptance received on 02/05/10. See:\n  http://vecpar.fe.up.pt/2010/",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS cs.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The algorithms in the current sequential numerical linear algebra libraries\n(e.g. LAPACK) do not parallelize well on multicore architectures. A new family\nof algorithms, the tile algorithms, has recently been introduced. Previous\nresearch has shown that it is possible to write efficient and scalable tile\nalgorithms for performing a Cholesky factorization, a (pseudo) LU\nfactorization, and a QR factorization. In this extended abstract, we attack the\nproblem of the computation of the inverse of a symmetric positive definite\nmatrix. We observe that, using a dynamic task scheduler, it is relatively\npainless to translate existing LAPACK code to obtain a ready-to-be-executed\ntile algorithm. However we demonstrate that non trivial compiler techniques\n(array renaming, loop reversal and pipelining) need then to be applied to\nfurther increase the parallelism of our application. We present preliminary\nexperimental results.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 22 Feb 2010 06:11:41 GMT"
            }
        ],
        "update_date": "2010-02-23",
        "authors_parsed": [
            [
                "Agullo",
                "Emmanuel",
                ""
            ],
            [
                "Bouwmeester",
                "Henricus",
                ""
            ],
            [
                "Dongarra",
                "Jack",
                ""
            ],
            [
                "Kurzak",
                "Jakub",
                ""
            ],
            [
                "Langou",
                "Julien",
                ""
            ],
            [
                "Rosenberg",
                "Lee",
                ""
            ]
        ]
    },
    {
        "id": "1002.4661",
        "submitter": "EPTCS",
        "authors": "Ozgur E. Akman (School of Engineering, Computing & Mathematics,\n  University of Exeter, UK), Maria Luisa Guerriero (Centre for Systems Biology\n  at Edinburgh, University of Edinburgh, UK), Laurence Loewe (Centre for\n  Systems Biology at Edinburgh, University of Edinburgh, UK), Carl Troein\n  (Centre for Systems Biology at Edinburgh and School of Biological Sciences,\n  University of Edinburgh, UK)",
        "title": "Complementary approaches to understanding the plant circadian clock",
        "comments": null,
        "journal-ref": "EPTCS 19, 2010, pp. 1-19",
        "doi": "10.4204/EPTCS.19.1",
        "report-no": null,
        "categories": "cs.CE cs.MS q-bio.MN",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Circadian clocks are oscillatory genetic networks that help organisms adapt\nto the 24-hour day/night cycle. The clock of the green alga Ostreococcus tauri\nis the simplest plant clock discovered so far. Its many advantages as an\nexperimental system facilitate the testing of computational predictions.\n  We present a model of the Ostreococcus clock in the stochastic process\nalgebra Bio-PEPA and exploit its mapping to different analysis techniques, such\nas ordinary differential equations, stochastic simulation algorithms and\nmodel-checking. The small number of molecules reported for this system tests\nthe limits of the continuous approximation underlying differential equations.\nWe investigate the difference between continuous-deterministic and\ndiscrete-stochastic approaches. Stochastic simulation and model-checking allow\nus to formulate new hypotheses on the system behaviour, such as the presence of\nself-sustained oscillations in single cells under constant light conditions.\n  We investigate how to model the timing of dawn and dusk in the context of\nmodel-checking, which we use to compute how the probability distributions of\nkey biochemical species change over time. These show that the relative\nvariation in expression level is smallest at the time of peak expression,\nmaking peak time an optimal experimental phase marker. Building on these\nanalyses, we use approaches from evolutionary systems biology to investigate\nhow changes in the rate of mRNA degradation impacts the phase of a key protein\nlikely to affect fitness. We explore how robust this circadian clock is towards\nsuch potential mutational changes in its underlying biochemistry. Our work\nshows that multiple approaches lead to a more complete understanding of the\nclock.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 24 Feb 2010 23:47:53 GMT"
            }
        ],
        "update_date": "2010-02-26",
        "authors_parsed": [
            [
                "Akman",
                "Ozgur E.",
                "",
                "School of Engineering, Computing & Mathematics,\n  University of Exeter, UK"
            ],
            [
                "Guerriero",
                "Maria Luisa",
                "",
                "Centre for Systems Biology\n  at Edinburgh, University of Edinburgh, UK"
            ],
            [
                "Loewe",
                "Laurence",
                "",
                "Centre for\n  Systems Biology at Edinburgh, University of Edinburgh, UK"
            ],
            [
                "Troein",
                "Carl",
                "",
                "Centre for Systems Biology at Edinburgh and School of Biological Sciences,\n  University of Edinburgh, UK"
            ]
        ]
    },
    {
        "id": "1002.4725",
        "submitter": "Antoine Bret",
        "authors": "A. Bret",
        "title": "Transferring a symbolic polynomial expression from \\emph{Mathematica} to\n  \\emph{Matlab}",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MS physics.comp-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A \\emph{Mathematica} Notebook is presented which allows for the transfer or\nany kind of polynomial expression to \\emph{Matlab}. The output is formatted in\nsuch a way that \\emph{Matlab} routines such as \"Root\" can be readily\nimplemented. Once the Notebook has been executed, only one copy-paste operation\nin necessary.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Feb 2010 15:42:39 GMT"
            }
        ],
        "update_date": "2010-02-26",
        "authors_parsed": [
            [
                "Bret",
                "A.",
                ""
            ]
        ]
    },
    {
        "id": "1002.4784",
        "submitter": "Rong Xiao",
        "authors": "Changbo Chen, James H. Davenport, John P. May, Marc Moreno Maza, Bican\n  Xia, and Rong Xiao",
        "title": "Triangular Decomposition of Semi-algebraic Systems",
        "comments": "8 pages, accepted by ISSAC 2010",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SC cs.CG cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Regular chains and triangular decompositions are fundamental and\nwell-developed tools for describing the complex solutions of polynomial\nsystems. This paper proposes adaptations of these tools focusing on solutions\nof the real analogue: semi-algebraic systems. We show that any such system can\nbe decomposed into finitely many {\\em regular semi-algebraic systems}. We\npropose two specifications of such a decomposition and present corresponding\nalgorithms. Under some assumptions, one type of decomposition can be computed\nin singly exponential time w.r.t.\\ the number of variables. We implement our\nalgorithms and the experimental results illustrate their effectiveness.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Feb 2010 13:39:09 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 13 May 2010 20:21:31 GMT"
            }
        ],
        "update_date": "2010-05-17",
        "authors_parsed": [
            [
                "Chen",
                "Changbo",
                ""
            ],
            [
                "Davenport",
                "James H.",
                ""
            ],
            [
                "May",
                "John P.",
                ""
            ],
            [
                "Maza",
                "Marc Moreno",
                ""
            ],
            [
                "Xia",
                "Bican",
                ""
            ],
            [
                "Xiao",
                "Rong",
                ""
            ]
        ]
    },
    {
        "id": "1003.1336",
        "submitter": "Zolt\\'an K\\'asa",
        "authors": "Peter Fornai, Antal Ivanyi",
        "title": "FIFO anomaly is unbounded",
        "comments": null,
        "journal-ref": "Acta Univ. Sapientiae, Informatica, 2,1 (2010) 80-89",
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Virtual memory of computers is usually implemented by demand paging. For some\npage replacement algorithms the number of page faults may increase as the\nnumber of page frames increases. Belady, Nelson and Shedler constructed\nreference strings for which page replacement algorithm FIFO produces near twice\nmore page faults in a larger memory than in a smaller one. They formulated the\nconjecture that 2 is a general bound. We prove that this ratio can be\narbitrarily large.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 5 Mar 2010 20:34:44 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 9 Mar 2010 13:02:01 GMT"
            }
        ],
        "update_date": "2010-03-13",
        "authors_parsed": [
            [
                "Fornai",
                "Peter",
                ""
            ],
            [
                "Ivanyi",
                "Antal",
                ""
            ]
        ]
    },
    {
        "id": "1003.1628",
        "submitter": "Darko Veberic",
        "authors": "Darko Veberic",
        "title": "Having Fun with Lambert W(x) Function",
        "comments": "15 pages, 11 figures, 4 tables, updated link to sources",
        "journal-ref": null,
        "doi": null,
        "report-no": "GAP-2009-114",
        "categories": "cs.MS cs.NA math.NA",
        "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
        "abstract": "  This short note presents the Lambert W(x) function and its possible\napplication in the framework of physics related to the Pierre Auger\nObservatory. The actual numerical implementation in C++ consists of Halley's\nand Fritsch's iteration with branch-point expansion, asymptotic series and\nrational fits as initial approximations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 8 Mar 2010 13:47:25 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 7 Jan 2018 15:18:25 GMT"
            }
        ],
        "update_date": "2018-01-09",
        "authors_parsed": [
            [
                "Veberic",
                "Darko",
                ""
            ]
        ]
    },
    {
        "id": "1003.2005",
        "submitter": "Taeyoung Lee",
        "authors": "Taeyoung Lee, Melvin Leok, N. Harris McClamroch",
        "title": "Control of Complex Maneuvers for a Quadrotor UAV using Geometric Methods\n  on SE(3)",
        "comments": "8 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper provides new results for control of complex flight maneuvers for a\nquadrotor unmanned aerial vehicle (UAV). The flight maneuvers are defined by a\nconcatenation of flight modes or primitives, each of which is achieved by a\nnonlinear controller that solves an output tracking problem. A mathematical\nmodel of the quadrotor UAV rigid body dynamics, defined on the configuration\nspace $\\SE$, is introduced as a basis for the analysis. The quadrotor UAV has\nfour input degrees of freedom, namely the magnitudes of the four rotor thrusts;\neach flight mode is defined by solving an asymptotic optimal tracking problem.\nAlthough many flight modes can be studied, we focus on three output tracking\nproblems, namely (1) outputs given by the vehicle attitude, (2) outputs given\nby the three position variables for the vehicle center of mass, and (3) output\ngiven by the three velocity variables for the vehicle center of mass. A\nnonlinear tracking controller is developed on the special Euclidean group $\\SE$\nfor each flight mode, and the closed loop is shown to have desirable closed\nloop properties that are almost global in each case. Several numerical\nexamples, including one example in which the quadrotor recovers from being\ninitially upside down and another example that includes switching and\ntransitions between different flight modes, illustrate the versatility and\ngenerality of the proposed approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 10 Mar 2010 01:18:08 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 3 Nov 2010 18:40:35 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 12 Nov 2010 21:38:37 GMT"
            },
            {
                "version": "v4",
                "created": "Fri, 9 Sep 2011 20:48:35 GMT"
            }
        ],
        "update_date": "2011-09-13",
        "authors_parsed": [
            [
                "Lee",
                "Taeyoung",
                ""
            ],
            [
                "Leok",
                "Melvin",
                ""
            ],
            [
                "McClamroch",
                "N. Harris",
                ""
            ]
        ]
    },
    {
        "id": "1003.3689",
        "submitter": "Murat Manguoglu",
        "authors": "Murat Manguoglu",
        "title": "A Highly Efficient Parallel Algorithm for Computing the Fiedler Vector",
        "comments": "This paper has been withdrawn by the author because it is under\n  revision",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NA cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper has been withdrawn by the author.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 18 Mar 2010 22:56:57 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 12 Feb 2013 19:44:27 GMT"
            }
        ],
        "update_date": "2015-03-13",
        "authors_parsed": [
            [
                "Manguoglu",
                "Murat",
                ""
            ]
        ]
    },
    {
        "id": "1003.4088",
        "submitter": "William Jackson",
        "authors": "Richa Gupta, Sanjiv Tokekar",
        "title": "Proficient Pair of Replacement Algorithms on L1 and L2 Cache for Merge\n  Sort",
        "comments": null,
        "journal-ref": "Journal of Computing, Volume 2, Issue 3, March 2010,\n  https://sites.google.com/site/journalofcomputing/",
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Memory hierarchy is used to compete the processors speed. Cache memory is the\nfast memory which is used to conduit the speed difference of memory and\nprocessor. The access patterns of Level 1 cache (L1) and Level 2 cache (L2) are\ndifferent, when CPU not gets the desired data in L1 then it accesses L2. Thus\nthe replacement algorithm which works efficiently on L1 may not be as efficient\non L2. Similarly various applications such as Matrix Multiplication, Web, Fast\nFourier Transform (FFT) etc will have varying access pattern. Thus same\nreplacement algorithm for all types of application may not be efficient. This\npaper works for getting an efficient pair of replacement algorithm on L1 and L2\nfor the algorithm Merge Sort. With the memory reference string of Merge Sort,\nwe have analyzed the behavior of various existing replacement algorithms on L1.\nThe existing replacement algorithms which are taken into consideration are:\nLeast Recently Used (LRU), Least Frequently Used (LFU) and First In First Out\n(FIFO). After Analyzing the memory reference pattern of Merge Sort, we have\nproposed a Partition Based Replacement algorithm (PBR_L1)) on L1 Cache.\nFurthermore we have analyzed various pairs of algorithms on L1 and L2\nrespectively, resulting in finding a suitable pair of replacement algorithms.\nSimulation on L1 shows, among the considered existing replacement algorithms\nFIFO is performing better than others. While the proposed replacement algorithm\nPBR_L1 is working about 1.7% to 44 % better than FIFO for various cache sizes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 22 Mar 2010 06:52:12 GMT"
            }
        ],
        "update_date": "2010-03-23",
        "authors_parsed": [
            [
                "Gupta",
                "Richa",
                ""
            ],
            [
                "Tokekar",
                "Sanjiv",
                ""
            ]
        ]
    },
    {
        "id": "1003.4216",
        "submitter": "Xueying Hu",
        "authors": "Erhan Bayraktar, Xueying Hu, Virginia R. Young",
        "title": "Minimizing the Probability of Lifetime Ruin under Stochastic Volatility",
        "comments": "Keywords: Optimal investment, minimizing the probability of lifetime\n  ruin, stochastic volatility",
        "journal-ref": null,
        "doi": "10.1016/j.insmatheco.2011.04.001",
        "report-no": null,
        "categories": "q-fin.PM cs.SY math.OC math.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We assume that an individual invests in a financial market with one riskless\nand one risky asset, with the latter's price following a diffusion with\nstochastic volatility. In the current financial market especially, it is\nimportant to include stochastic volatility in the risky asset's price process.\nGiven the rate of consumption, we find the optimal investment strategy for the\nindividual who wishes to minimize the probability of going bankrupt. To solve\nthis minimization problem, we use techniques from stochastic optimal control.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 19 Mar 2010 01:45:32 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 5 May 2011 14:34:45 GMT"
            }
        ],
        "update_date": "2011-05-06",
        "authors_parsed": [
            [
                "Bayraktar",
                "Erhan",
                ""
            ],
            [
                "Hu",
                "Xueying",
                ""
            ],
            [
                "Young",
                "Virginia R.",
                ""
            ]
        ]
    },
    {
        "id": "1003.4628",
        "submitter": "Pedro Gonnet",
        "authors": "Pedro Gonnet",
        "title": "Efficient Construction, Update and Downdate Of The Coefficients Of\n  Interpolants Based On Polynomials Satisfying A Three-Term Recurrence Relation",
        "comments": "18 pages, submitted to the Journal of Scientific Computing.",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NA cs.MS math.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider methods to compute the coefficients of\ninterpolants relative to a basis of polynomials satisfying a three-term\nrecurrence relation. Two new algorithms are presented: the first constructs the\ncoefficients of the interpolation incrementally and can be used to update the\ncoefficients whenever a nodes is added to or removed from the interpolation.\nThe second algorithm, which constructs the interpolation coefficients by\ndecomposing the Vandermonde-like matrix iteratively, can not be used to update\nor downdate an interpolation, yet is more numerically stable than the first\nalgorithm and is more efficient when the coefficients of multiple\ninterpolations are to be computed over the same set of nodes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 24 Mar 2010 12:46:52 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 30 Mar 2010 14:49:29 GMT"
            }
        ],
        "update_date": "2010-03-31",
        "authors_parsed": [
            [
                "Gonnet",
                "Pedro",
                ""
            ]
        ]
    },
    {
        "id": "1003.4629",
        "submitter": "Pedro Gonnet",
        "authors": "Pedro Gonnet",
        "title": "A Review of Error Estimation in Adaptive Quadrature",
        "comments": "Submitted to ACM Computing Surveys",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NA cs.MS math.NA",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The most critical component of any adaptive numerical quadrature routine is\nthe estimation of the integration error. Since the publication of the first\nalgorithms in the 1960s, many error estimation schemes have been presented,\nevaluated and discussed. This paper presents a review of existing error\nestimation techniques and discusses their differences and their common\nfeatures. Some common shortcomings of these algorithms are discussed and a new\ngeneral error estimation technique is presented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 24 Mar 2010 12:47:13 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 7 Nov 2010 14:01:25 GMT"
            }
        ],
        "update_date": "2010-11-09",
        "authors_parsed": [
            [
                "Gonnet",
                "Pedro",
                ""
            ]
        ]
    },
    {
        "id": "1003.4831",
        "submitter": "Yannick Aoustin",
        "authors": "Yannick Aoustin (IRCCyN), Alexander Formal'skii",
        "title": "Ball on a beam: stabilization under saturated input control with large\n  basin of attraction",
        "comments": null,
        "journal-ref": "Multibody System Dynamics 21 (2008) 71-89",
        "doi": "10.1007/s11044-008-9128-0",
        "report-no": null,
        "categories": "cs.RO cs.SY physics.med-ph",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This article is devoted to the stabilization of two underactuated planar\nsystems, the well-known straight beam-and-ball system and an original circular\nbeam-and-ball system. The feedback control for each system is designed, using\nthe Jordan form of its model, linearized near the unstable equilibrium. The\nlimits on the voltage, fed to the motor, are taken into account explicitly. The\nstraight beam-and-ball system has one unstable mode in the motion near the\nequilibrium point. The proposed control law ensures that the basin of\nattraction coincides with the controllability domain. The circular\nbeam-and-ball system has two unstable modes near the equilibrium point.\nTherefore, this device, never considered in the past, is much more difficult to\ncontrol than the straight beam-and-ball system. The main contribution is to\npropose a simple new control law, which ensures by adjusting its gain\nparameters that the basin of attraction arbitrarily can approach the\ncontrollability domain for the linear case. For both nonlinear systems,\nsimulation results are presented to illustrate the efficiency of the designed\nnonlinear control laws and to determine the basin of attraction.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 25 Mar 2010 09:53:22 GMT"
            }
        ],
        "update_date": "2010-12-01",
        "authors_parsed": [
            [
                "Aoustin",
                "Yannick",
                "",
                "IRCCyN"
            ],
            [
                "Formal'skii",
                "Alexander",
                ""
            ]
        ]
    },
    {
        "id": "1003.5192",
        "submitter": "Christoph Lange",
        "authors": "Christoph Lange",
        "title": "wiki.openmath.org - how it works, how you can participate",
        "comments": "OpenMath workshop 2009 (http://staff.bath.ac.uk/masjhd/OM2009.html)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DL cs.MS math.HO",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  At http://wiki.openmath.org, the OpenMath 2 and 3 Content Dictionaries are\naccessible via a semantic wiki interface, powered by the SWiM system. We\nshortly introduce the inner workings of the system, then describe how to use\nit, and conclude with first experiences gained from OpenMath society members\nworking with the system and an outlook to further development plans.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 26 Mar 2010 17:32:10 GMT"
            }
        ],
        "update_date": "2010-03-29",
        "authors_parsed": [
            [
                "Lange",
                "Christoph",
                ""
            ]
        ]
    },
    {
        "id": "1003.5196",
        "submitter": "Christoph Lange",
        "authors": "Christoph Lange",
        "title": "SWiM -- A Semantic Wiki for Mathematical Knowledge Management",
        "comments": null,
        "journal-ref": "S. Bechhofer, M. Hauswirth, J. Hoffmann, M. Koubarakis. The\n  Semantic Web: Research and Applications. ESWC 2008. LNCS 5021, Springer 2008",
        "doi": "10.1007/978-3-540-68234-9_68",
        "report-no": null,
        "categories": "cs.DL cs.MS math.HO",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  SWiM is a semantic wiki for collaboratively building, editing and browsing\nmathematical knowledge represented in the domain-specific structural semantic\nmarkup language OMDoc. It motivates users to contribute to collections of\nmathematical knowledge by instantly sharing the benefits of knowledge-powered\nservices with them. SWiM is currently being used for authoring content\ndictionaries, i. e. collections of uniquely identified mathematical symbols,\nand prepared for managing a large-scale proof formalisation effort.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 26 Mar 2010 18:17:01 GMT"
            }
        ],
        "update_date": "2010-03-29",
        "authors_parsed": [
            [
                "Lange",
                "Christoph",
                ""
            ]
        ]
    },
    {
        "id": "1003.5303",
        "submitter": "Bryan Ford",
        "authors": "Amittai Aviram, Sen Hu, Bryan Ford, and Ramakrishna Gummadi",
        "title": "Determinating Timing Channels in Compute Clouds",
        "comments": "6 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.CR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Timing side-channels represent an insidious security challenge for cloud\ncomputing, because: (a) massive parallelism in the cloud makes timing channels\npervasive and hard to control; (b) timing channels enable one customer to steal\ninformation from another without leaving a trail or raising alarms; (c) only\nthe cloud provider can feasibly detect and report such attacks, but the\nprovider's incentives are not to; and (d) resource partitioning schemes for\ntiming channel control undermine statistical sharing efficiency, and, with it,\nthe cloud computing business model. We propose a new approach to timing channel\ncontrol, using provider-enforced deterministic execution instead of resource\npartitioning to eliminate timing channels within a shared cloud domain.\nProvider-enforced determinism prevents execution timing from affecting the\nresults of a compute task, however large or parallel, ensuring that a task's\noutputs leak no timing information apart from explicit timing inputs and total\ncompute duration. Experiments with a prototype OS for deterministic cloud\ncomputing suggest that such an approach may be practical and efficient. The OS\nsupports deterministic versions of familiar APIs such as processes, threads,\nshared memory, and file systems, and runs coarse-grained parallel tasks as\nefficiently and scalably as current timing channel-ridden systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 27 Mar 2010 14:44:01 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 25 Jul 2010 15:40:38 GMT"
            }
        ],
        "update_date": "2010-07-27",
        "authors_parsed": [
            [
                "Aviram",
                "Amittai",
                ""
            ],
            [
                "Hu",
                "Sen",
                ""
            ],
            [
                "Ford",
                "Bryan",
                ""
            ],
            [
                "Gummadi",
                "Ramakrishna",
                ""
            ]
        ]
    },
    {
        "id": "1003.5525",
        "submitter": "Kees Middelburg",
        "authors": "C. A. Middelburg",
        "title": "Searching publications on operating systems",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This note concerns a search for publications in which one can find statements\nthat explain the concept of an operating system, reasons for introducing\noperating systems, a formalization of the concept of an operating system or\ntheory about operating systems based on such a formalization. It reports on the\nway in which the search has been carried out and the outcome of the search. The\noutcome includes not only what the search was meant for, but also some added\nbonuses.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 29 Mar 2010 12:51:23 GMT"
            }
        ],
        "update_date": "2010-03-30",
        "authors_parsed": [
            [
                "Middelburg",
                "C. A.",
                ""
            ]
        ]
    },
    {
        "id": "1003.6036",
        "submitter": "Christoph Spandl",
        "authors": "Christoph Spandl",
        "title": "Computational Complexity of Iterated Maps on the Interval",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NA cs.MS math.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The correct computation of orbits of discrete dynamical systems on the\ninterval is considered. Therefore, an arbitrary-precision floating-point\napproach based on automatic error analysis is chosen and a general algorithm is\npresented. The correctness of the algorithm is shown and the computational\ncomplexity is analyzed. There are two main results. First, the computational\ncomplexity measure considered here is related to the Lyapunov exponent of the\ndynamical system under consideration. Second, the presented algorithm is\noptimal with regard to that complexity measure.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 31 Mar 2010 12:26:10 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 3 Aug 2010 14:37:27 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 11 Oct 2011 13:36:18 GMT"
            }
        ],
        "update_date": "2015-03-13",
        "authors_parsed": [
            [
                "Spandl",
                "Christoph",
                ""
            ]
        ]
    },
    {
        "id": "1004.0382",
        "submitter": "Andrei Draganescu",
        "authors": "Andrei Draganescu, Cosmin Petra",
        "title": "Multigrid preconditioning of linear systems for interior point methods\n  applied to a class of box-constrained optimal control problems",
        "comments": "29 pages, 8 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.NA cs.SY math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this article we construct and analyze multigrid preconditioners for\ndiscretizations of operators of the form D+K* K, where D is the multiplication\nwith a relatively smooth positive function and K is a compact linear operator.\nThese systems arise when applying interior point methods to the minimization\nproblem min_u (||K u-f||^2 +b||u||^2) with box-constraints on the controls u.\nThe presented preconditioning technique is closely related to the one developed\nby Draganescu and Dupont in [11] for the associated unconstrained problem, and\nis intended for large-scale problems. As in [11], the quality of the resulting\npreconditioners is shown to increase with increasing resolution but decreases\nas the diagonal of D becomes less smooth. We test this algorithm first on a\nTikhonov-regularized backward parabolic equation with box-constraints on the\ncontrol, and then on a standard elliptic-constrained optimization problem. In\nboth cases it is shown that the number of linear iterations per optimization\nstep, as well as the total number of fine-scale matrix-vector multiplications\nis decreasing with increasing resolution, thus showing the method to be\npotentially very efficient for truly large-scale problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 2 Apr 2010 19:58:11 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 3 Apr 2011 23:18:46 GMT"
            }
        ],
        "update_date": "2011-04-05",
        "authors_parsed": [
            [
                "Draganescu",
                "Andrei",
                ""
            ],
            [
                "Petra",
                "Cosmin",
                ""
            ]
        ]
    },
    {
        "id": "1004.0477",
        "submitter": "Manuel Mazo Jr",
        "authors": "Manuel Mazo Jr. and Paulo Tabuada",
        "title": "Decentralized event-triggered control over wireless sensor/actuator\n  networks",
        "comments": "13 pages, 3 figures, journal submission",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In recent years we have witnessed a move of the major industrial automation\nproviders into the wireless domain. While most of these companies already offer\nwireless products for measurement and monitoring purposes, the ultimate goal is\nto be able to close feedback loops over wireless networks interconnecting\nsensors, computation devices, and actuators. In this paper we present a\ndecentralized event-triggered implementation, over sensor/actuator networks, of\ncentralized nonlinear controllers. Event-triggered control has been recently\nproposed as an alternative to the more traditional periodic execution of\ncontrol tasks. In a typical event-triggered implementation, the control signals\nare kept constant until the violation of a condition on the state of the plant\ntriggers the re-computation of the control signals. The possibility of reducing\nthe number of re-computations, and thus of transmissions, while guaranteeing\ndesired levels of performance makes event-triggered control very appealing in\nthe context of sensor/actuator networks. In these systems the communication\nnetwork is a shared resource and event-triggered implementations of control\nlaws offer a flexible way to reduce network utilization. Moreover reducing the\nnumber of times that a feedback control law is executed implies a reduction in\ntransmissions and thus a reduction in energy expenditures of battery powered\nwireless sensor nodes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 4 Apr 2010 00:57:27 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 3 Feb 2011 10:24:56 GMT"
            }
        ],
        "update_date": "2011-02-04",
        "authors_parsed": [
            [
                "Mazo",
                "Manuel",
                "Jr."
            ],
            [
                "Tabuada",
                "Paulo",
                ""
            ]
        ]
    },
    {
        "id": "1004.0763",
        "submitter": "Manuel Mazo Jr",
        "authors": "Manuel Mazo Jr. and Paulo Tabuada",
        "title": "Symbolic Approximate Time-Optimal Control",
        "comments": "17 pages, 2 figures, journal",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There is an increasing demand for controller design techniques capable of\naddressing the complex requirements of todays embedded applications. This\ndemand has sparked the interest in symbolic control where lower complexity\nmodels of control systems are used to cater for complex specifications given by\ntemporal logics, regular languages, or automata. These specification mechanisms\ncan be regarded as qualitative since they divide the trajectories of the plant\ninto bad trajectories (those that need to be avoided) and good trajectories.\nHowever, many applications require also the optimization of quantitative\nmeasures of the trajectories retained by the controller, as specified by a cost\nor utility function. As a first step towards the synthesis of controllers\nreconciling both qualitative and quantitative specifications, we investigate in\nthis paper the use of symbolic models for time-optimal controller synthesis. We\nconsider systems related by approximate (alternating) simulation relations and\nshow how such relations enable the transfer of time-optimality information\nbetween the systems. We then use this insight to synthesize approximately\ntime-optimal controllers for a control system by working with a lower\ncomplexity symbolic model. The resulting approximately time-optimal controllers\nare equipped with upper and lower bounds for the time to reach a target,\ndescribing the quality of the controller. The results described in this paper\nwere implemented in the Matlab Toolbox Pessoa which we used to workout several\nillustrative examples reported in this paper.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 6 Apr 2010 03:40:49 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 3 Feb 2011 10:11:03 GMT"
            }
        ],
        "update_date": "2015-03-14",
        "authors_parsed": [
            [
                "Mazo",
                "Manuel",
                "Jr."
            ],
            [
                "Tabuada",
                "Paulo",
                ""
            ]
        ]
    },
    {
        "id": "1004.2027",
        "submitter": "Mohammad Gheshlaghi Azar",
        "authors": "Mohammad Gheshlaghi Azar, Vicenc Gomez and Hilbert J. Kappen",
        "title": "Dynamic Policy Programming",
        "comments": "Submitted to Journal of Machine Learning Research",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.AI cs.SY math.OC stat.ML",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a novel policy iteration method, called dynamic\npolicy programming (DPP), to estimate the optimal policy in the\ninfinite-horizon Markov decision processes. We prove the finite-iteration and\nasymptotic l\\infty-norm performance-loss bounds for DPP in the presence of\napproximation/estimation error. The bounds are expressed in terms of the\nl\\infty-norm of the average accumulated error as opposed to the l\\infty-norm of\nthe error in the case of the standard approximate value iteration (AVI) and the\napproximate policy iteration (API). This suggests that DPP can achieve a better\nperformance than AVI and API since it averages out the simulation noise caused\nby Monte-Carlo sampling throughout the learning process. We examine this\ntheoretical results numerically by com- paring the performance of the\napproximate variants of DPP with existing reinforcement learning (RL) methods\non different problem domains. Our results show that, in all cases, DPP-based\nalgorithms outperform other RL methods by a wide margin.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 12 Apr 2010 19:09:43 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 6 Sep 2011 20:23:59 GMT"
            }
        ],
        "update_date": "2011-09-09",
        "authors_parsed": [
            [
                "Azar",
                "Mohammad Gheshlaghi",
                ""
            ],
            [
                "Gomez",
                "Vicenc",
                ""
            ],
            [
                "Kappen",
                "Hilbert J.",
                ""
            ]
        ]
    },
    {
        "id": "1004.2102",
        "submitter": "Alexander Olshevsky",
        "authors": "Julien M. Hendrickx, Alex Olshevsky, John N. Tsitsiklis",
        "title": "Distributed anonymous discrete function computation",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.DC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a model for deterministic distributed function computation by a\nnetwork of identical and anonymous nodes. In this model, each node has bounded\ncomputation and storage capabilities that do not grow with the network size.\nFurthermore, each node only knows its neighbors, not the entire graph. Our goal\nis to characterize the class of functions that can be computed within this\nmodel. In our main result, we provide a necessary condition for computability\nwhich we show to be nearly sufficient, in the sense that every function that\nsatisfies this condition can at least be approximated. The problem of computing\nsuitably rounded averages in a distributed manner plays a central role in our\ndevelopment; we provide an algorithm that solves it in time that grows\nquadratically with the size of the network.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 13 Apr 2010 03:23:53 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 21 Apr 2010 21:53:23 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 22 Apr 2011 02:07:30 GMT"
            },
            {
                "version": "v4",
                "created": "Sat, 25 Jun 2011 21:56:39 GMT"
            }
        ],
        "update_date": "2011-06-28",
        "authors_parsed": [
            [
                "Hendrickx",
                "Julien M.",
                ""
            ],
            [
                "Olshevsky",
                "Alex",
                ""
            ],
            [
                "Tsitsiklis",
                "John N.",
                ""
            ]
        ]
    },
    {
        "id": "1004.2342",
        "submitter": "Nicolas Gast",
        "authors": "Nicolas Gast (INRIA Grenoble Rh\\^one-Alpes / LIG laboratoire\n  d'Informatique de Grenoble, EPFL), Bruno Gaujal (INRIA Grenoble Rh\\^one-Alpes\n  / LIG laboratoire d'Informatique de Grenoble), Jean-Yves Le Boudec (EPFL)",
        "title": "Mean field for Markov Decision Processes: from Discrete to Continuous\n  Optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "RR-7239, RR-7239",
        "categories": "cs.AI cs.PF cs.SY math.OC math.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the convergence of Markov Decision Processes made of a large number\nof objects to optimization problems on ordinary differential equations (ODE).\nWe show that the optimal reward of such a Markov Decision Process, satisfying a\nBellman equation, converges to the solution of a continuous\nHamilton-Jacobi-Bellman (HJB) equation based on the mean field approximation of\nthe Markov Decision Process. We give bounds on the difference of the rewards,\nand a constructive algorithm for deriving an approximating solution to the\nMarkov Decision Process from a solution of the HJB equations. We illustrate the\nmethod on three examples pertaining respectively to investment strategies,\npopulation dynamics control and scheduling in queues are developed. They are\nused to illustrate and justify the construction of the controlled ODE and to\nshow the gain obtained by solving a continuous HJB equation rather than a large\ndiscrete Bellman equation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 14 Apr 2010 07:56:40 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 2 Jul 2010 14:26:32 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 19 May 2011 07:27:54 GMT"
            }
        ],
        "update_date": "2011-05-20",
        "authors_parsed": [
            [
                "Gast",
                "Nicolas",
                "",
                "INRIA Grenoble Rh\u00f4ne-Alpes / LIG laboratoire\n  d'Informatique de Grenoble, EPFL"
            ],
            [
                "Gaujal",
                "Bruno",
                "",
                "INRIA Grenoble Rh\u00f4ne-Alpes\n  / LIG laboratoire d'Informatique de Grenoble"
            ],
            [
                "Boudec",
                "Jean-Yves Le",
                "",
                "EPFL"
            ]
        ]
    },
    {
        "id": "1004.2519",
        "submitter": "Bernard Levy",
        "authors": "Bernard C. Levy and Ramine Nikoukhah",
        "title": "Robust State Space Filtering under Incremental Model Perturbations\n  Subject to a Relative Entropy Tolerance",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.IT cs.SY math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper considers robust filtering for a nominal Gaussian state-space\nmodel, when a relative entropy tolerance is applied to each time increment of a\ndynamical model. The problem is formulated as a dynamic minimax game where the\nmaximizer adopts a myopic strategy. This game is shown to admit a saddle point\nwhose structure is characterized by applying and extending results presented\nearlier in [1] for static least-squares estimation. The resulting minimax\nfilter takes the form of a risk-sensitive filter with a time varying risk\nsensitivity parameter, which depends on the tolerance bound applied to the\nmodel dynamics and observations at the corresponding time index. The\nleast-favorable model is constructed and used to evaluate the performance of\nalternative filters. Simulations comparing the proposed risk-sensitive filter\nto a standard Kalman filter show a significant performance advantage when\napplied to the least-favorable model, and only a small performance loss for the\nnominal model.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 14 Apr 2010 22:50:52 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 22 Sep 2011 18:57:22 GMT"
            }
        ],
        "update_date": "2011-09-26",
        "authors_parsed": [
            [
                "Levy",
                "Bernard C.",
                ""
            ],
            [
                "Nikoukhah",
                "Ramine",
                ""
            ]
        ]
    },
    {
        "id": "1004.3173",
        "submitter": "Richard Brent",
        "authors": "Richard P. Brent",
        "title": "MP users guide",
        "comments": "MP Users Guide (fourth edition), 73 pages. A technical report that\n  was not published elsewhere, submitted for archival purposes. For further\n  information see http://wwwmaths.anu.edu.au/~brent/pub/pub035.html",
        "journal-ref": null,
        "doi": null,
        "report-no": "TR-CS-81-08, Department of Computer Science, Australian National\n  University (June 1981)",
        "categories": "cs.MS math.NA math.NT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  MP is a package of ANSI Standard Fortran (ANS X3.9-1966) subroutines for\nperforming multiple-precision floating-point arithmetic and evaluating\nelementary and special functions. The subroutines are machine independent and\nthe precision is arbitrary, subject to storage limitations. The User's Guide\ndescribes the routines and their calling sequences, example and test programs,\nuse of the Augment precompiler, and gives installation instructions for the\npackage.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 19 Apr 2010 12:26:15 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 20 Apr 2010 00:56:02 GMT"
            }
        ],
        "update_date": "2010-04-21",
        "authors_parsed": [
            [
                "Brent",
                "Richard P.",
                ""
            ]
        ]
    },
    {
        "id": "1004.3687",
        "submitter": "Joel Goossens",
        "authors": "Patrick Meumeu Yomsi, Vincent Nelis and Jo\\\"el Goossens",
        "title": "Scheduling Multi-Mode Real-Time Systems upon Uniform Multiprocessor\n  Platforms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we address the scheduling problem of multi-mode real-time\nsystems upon uniform multiprocessor platforms. We propose two transition\nprotocols, specified together with their schedulability test, and provide the\nreader with two distinct upper bounds for the length of the transient phases\nduring mode transitions, respectively for the cases where jobs priorities are\nknown and unknown beforehand.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 21 Apr 2010 12:31:53 GMT"
            }
        ],
        "update_date": "2010-04-22",
        "authors_parsed": [
            [
                "Yomsi",
                "Patrick Meumeu",
                ""
            ],
            [
                "Nelis",
                "Vincent",
                ""
            ],
            [
                "Goossens",
                "Jo\u00ebl",
                ""
            ]
        ]
    },
    {
        "id": "1004.3715",
        "submitter": "Joel Goossens",
        "authors": "Irina Lupu (1), Pierre Courbin (2), Laurent George (2) and Jo\\\"el\n  Goossens (1) ((1) U.L.B., (2) ECE)",
        "title": "Multi-Criteria Evaluation of Partitioning Schemes for Real-Time Systems",
        "comments": null,
        "journal-ref": "IEEE International Conference on Emerging Technologies and Factory\n  Automation, 2010",
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the partitioning approach for multiprocessor real-time\nscheduling. This approach seems to be the easiest since, once the partitioning\nof the task set has been done, the problem reduces to well understood\nuniprocessor issues. Meanwhile, there is no optimal and polynomial solution to\npartition tasks on processors. In this paper we analyze partitioning algorithms\nfrom several points of view such that for a given task set and specific\nconstraints (processor number, task set type, etc.) we should be able to\nidentify the best heuristic and the best schedulability test. We also analyze\nthe influence of the heuristics on the performance of the uniprocessor tests\nand the impact of a specific task order on the schedulability. A study on\nperformance difference between Fixed Priority schedulers and EDF in the case of\npartitioning scheduling is also considered.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 21 Apr 2010 14:24:13 GMT"
            }
        ],
        "update_date": "2011-02-03",
        "authors_parsed": [
            [
                "Lupu",
                "Irina",
                "",
                "U.L.B"
            ],
            [
                "Courbin",
                "Pierre",
                "",
                "ECE"
            ],
            [
                "George",
                "Laurent",
                "",
                "ECE"
            ],
            [
                "Goossens",
                "Jo\u00ebl",
                "",
                "U.L.B"
            ]
        ]
    },
    {
        "id": "1004.3719",
        "submitter": "Jean-Guillaume Dumas",
        "authors": "Brice Boyer (LJK), Jean-Guillaume Dumas (LJK), Pascal Giorgi (LIRMM)",
        "title": "Exact Sparse Matrix-Vector Multiplication on GPU's and Multicore\n  Architectures",
        "comments": null,
        "journal-ref": "International Symposium on Parallel Symbolic Computation, Grenoble\n  : France (2010)",
        "doi": "10.1145/1837210.1837224",
        "report-no": null,
        "categories": "cs.DC cs.MS cs.SC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose different implementations of the sparse matrix--dense vector\nmultiplication (\\spmv{}) for finite fields and rings $\\Zb/m\\Zb$. We take\nadvantage of graphic card processors (GPU) and multi-core architectures. Our\naim is to improve the speed of \\spmv{} in the \\linbox library, and henceforth\nthe speed of its black box algorithms. Besides, we use this and a new\nparallelization of the sigma-basis algorithm in a parallel block Wiedemann rank\nimplementation over finite fields.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 21 Apr 2010 14:52:36 GMT"
            }
        ],
        "update_date": "2010-09-09",
        "authors_parsed": [
            [
                "Boyer",
                "Brice",
                "",
                "LJK"
            ],
            [
                "Dumas",
                "Jean-Guillaume",
                "",
                "LJK"
            ],
            [
                "Giorgi",
                "Pascal",
                "",
                "LIRMM"
            ]
        ]
    },
    {
        "id": "1004.5034",
        "submitter": "Franck Butelle",
        "authors": "Franck Butelle and Florent Hivert and Micaela Mayero and Fr\\'ed\\'eric\n  Toumazet",
        "title": "Formal Proof of SCHUR Conjugate Function",
        "comments": "To appear in CALCULEMUS 2010",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LO cs.MS cs.SC cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main goal of our work is to formally prove the correctness of the key\ncommands of the SCHUR software, an interactive program for calculating with\ncharacters of Lie groups and symmetric functions. The core of the computations\nrelies on enumeration and manipulation of combinatorial structures. As a first\n\"proof of concept\", we present a formal proof of the conjugate function,\nwritten in C. This function computes the conjugate of an integer partition. To\nformally prove this program, we use the Frama-C software. It allows us to\nannotate C functions and to generate proof obligations, which are proved using\nseveral automated theorem provers. In this paper, we also draw on methodology,\ndiscussing on how to formally prove this kind of program.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 28 Apr 2010 14:12:43 GMT"
            }
        ],
        "update_date": "2010-04-29",
        "authors_parsed": [
            [
                "Butelle",
                "Franck",
                ""
            ],
            [
                "Hivert",
                "Florent",
                ""
            ],
            [
                "Mayero",
                "Micaela",
                ""
            ],
            [
                "Toumazet",
                "Fr\u00e9d\u00e9ric",
                ""
            ]
        ]
    },
    {
        "id": "1005.0080",
        "submitter": "Xiaoyu Chen",
        "authors": "Xiaoyu Chen",
        "title": "Electronic Geometry Textbook: A Geometric Textbook Knowledge Management\n  System",
        "comments": "To appear in The 9th International Conference on Mathematical\n  Knowledge Management: MKM 2010",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.AI cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Electronic Geometry Textbook is a knowledge management system that manages\ngeometric textbook knowledge to enable users to construct and share dynamic\ngeometry textbooks interactively and efficiently. Based on a knowledge base\norganizing and storing the knowledge represented in specific languages, the\nsystem implements interfaces for maintaining the data representing that\nknowledge as well as relations among those data, for automatically generating\nreadable documents for viewing or printing, and for automatically discovering\nthe relations among knowledge data. An interface has been developed for users\nto create geometry textbooks with automatic checking, in real time, of the\nconsistency of the structure of each resulting textbook. By integrating an\nexternal geometric theorem prover and an external dynamic geometry software\npackage, the system offers the facilities for automatically proving theorems\nand generating dynamic figures in the created textbooks. This paper provides a\ncomprehensive account of the current version of Electronic Geometry Textbook.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 1 May 2010 15:06:32 GMT"
            }
        ],
        "update_date": "2010-05-04",
        "authors_parsed": [
            [
                "Chen",
                "Xiaoyu",
                ""
            ]
        ]
    },
    {
        "id": "1005.0146",
        "submitter": "Vyacheslav Levitsky",
        "authors": "Andriy Kovalchuk, Vyacheslav Levitsky, Igor Samolyuk and Valentyn\n  Yanchuk",
        "title": "The Formulator MathML Editor Project: User-Friendly Authoring of Content\n  Markup Documents",
        "comments": "To appear in The 9th International Conference on Mathematical\n  Knowledge Management: MKM 2010",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DL cs.HC cs.MS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Implementation of an editing process for Content MathML formulas in common\nvisual style is a real challenge for a software developer who does not really\nwant the user to have to understand the structure of Content MathML in order to\nedit an expression, since it is expected that users are often not that\ntechnically minded. In this paper, we demonstrate how this aim is achieved in\nthe context of the Formulator project and discuss features of this MathML\neditor, which provides a user with a WYSIWYG editing style while authoring\nMathML documents with Content or mixed markup. We also present the approach\ntaken to enhance availability of the MathML editor to end-users, demonstrating\nan online version of the editor that runs inside a Web browser.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 2 May 2010 16:12:32 GMT"
            }
        ],
        "update_date": "2010-05-04",
        "authors_parsed": [
            [
                "Kovalchuk",
                "Andriy",
                ""
            ],
            [
                "Levitsky",
                "Vyacheslav",
                ""
            ],
            [
                "Samolyuk",
                "Igor",
                ""
            ],
            [
                "Yanchuk",
                "Valentyn",
                ""
            ]
        ]
    },
    {
        "id": "1005.1252",
        "submitter": "Grigory Litvinov",
        "authors": "G. L. Litvinov, V. P. Maslov, A. Ya. Rodionov, and A. N. Sobolevski",
        "title": "Universal algorithms, mathematics of semirings and parallel computations",
        "comments": "36 pages, 1 figure. To appear in Springer Lecture Notes in\n  Computational Science and Engineering.",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.NA cs.DS cs.MS cs.NE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This is a survey paper on applications of mathematics of semirings to\nnumerical analysis and computing. Concepts of universal algorithm and generic\nprogram are discussed. Relations between these concepts and mathematics of\nsemirings are examined. A very brief introduction to mathematics of semirings\n(including idempotent and tropical mathematics) is presented. Concrete\napplications to optimization problems, idempotent linear algebra and interval\nanalysis are indicated. It is known that some nonlinear problems (and\nespecially optimization problems) become linear over appropriate semirings with\nidempotent addition (the so-called idempotent superposition principle). This\nlinearity over semirings is convenient for parallel computations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 May 2010 17:31:20 GMT"
            }
        ],
        "update_date": "2010-05-10",
        "authors_parsed": [
            [
                "Litvinov",
                "G. L.",
                ""
            ],
            [
                "Maslov",
                "V. P.",
                ""
            ],
            [
                "Rodionov",
                "A. Ya.",
                ""
            ],
            [
                "Sobolevski",
                "A. N.",
                ""
            ]
        ]
    },
    {
        "id": "1005.1292",
        "submitter": "Paolo Frasca",
        "authors": "Paolo Frasca, Fabio Fagnani",
        "title": "Broadcast gossip averaging algorithms: interference and asymptotical\n  error in large networks",
        "comments": "22 pages, 6 figures. Submitted",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study two related iterative randomized algorithms for\ndistributed computation of averages. The first one is the recently proposed\nBroadcast Gossip Algorithm, in which at each iteration one randomly selected\nnode broadcasts its own state to its neighbors. The second algorithm is a novel\nde-synchronized version of the previous one, in which at each iteration every\nnode is allowed to broadcast, with a given probability: hence this algorithm is\naffected by interference among messages. Both algorithms are proved to\nconverge, and their performance is evaluated in terms of rate of convergence\nand asymptotical error: focusing on the behavior for large networks, we\nhighlight the role of topology and design parameters on the performance.\nNamely, we show that on fully-connected graphs the rate is bounded away from\none, whereas the asymptotical error is bounded away from zero. On the contrary,\non a wide class of locally-connected graphs, the rate goes to one and the\nasymptotical error goes to zero, as the size of the network grows larger.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 7 May 2010 20:06:18 GMT"
            }
        ],
        "update_date": "2011-07-25",
        "authors_parsed": [
            [
                "Frasca",
                "Paolo",
                ""
            ],
            [
                "Fagnani",
                "Fabio",
                ""
            ]
        ]
    },
    {
        "id": "1005.2012",
        "submitter": "John Duchi",
        "authors": "John Duchi, Alekh Agarwal, Martin Wainwright",
        "title": "Dual Averaging for Distributed Optimization: Convergence Analysis and\n  Network Scaling",
        "comments": "40 pages, 4 figures",
        "journal-ref": "IEEE Transactions on Automatic Control 57(3), pp. 592 - 606. March\n  2012",
        "doi": "10.1109/TAC.2011.2161027",
        "report-no": null,
        "categories": "math.OC cs.SY stat.ML",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The goal of decentralized optimization over a network is to optimize a global\nobjective formed by a sum of local (possibly nonsmooth) convex functions using\nonly local computation and communication. It arises in various application\ndomains, including distributed tracking and localization, multi-agent\nco-ordination, estimation in sensor networks, and large-scale optimization in\nmachine learning. We develop and analyze distributed algorithms based on dual\naveraging of subgradients, and we provide sharp bounds on their convergence\nrates as a function of the network size and topology. Our method of analysis\nallows for a clear separation between the convergence of the optimization\nalgorithm itself and the effects of communication constraints arising from the\nnetwork structure. In particular, we show that the number of iterations\nrequired by our algorithm scales inversely in the spectral gap of the network.\nThe sharpness of this prediction is confirmed both by theoretical lower bounds\nand simulations for various networks. Our approach includes both the cases of\ndeterministic optimization and communication, as well as problems with\nstochastic optimization and/or communication.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 12 May 2010 07:44:49 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 14 May 2010 17:10:01 GMT"
            },
            {
                "version": "v3",
                "created": "Sun, 10 Apr 2011 22:21:08 GMT"
            }
        ],
        "update_date": "2015-03-17",
        "authors_parsed": [
            [
                "Duchi",
                "John",
                ""
            ],
            [
                "Agarwal",
                "Alekh",
                ""
            ],
            [
                "Wainwright",
                "Martin",
                ""
            ]
        ]
    },
    {
        "id": "1005.2633",
        "submitter": "Ermin Wei",
        "authors": "Ermin Wei, Asuman Ozdaglar and Ali Jadbabaie",
        "title": "A Distributed Newton Method for Network Utility Maximization",
        "comments": "27 pages, 4 figures, LIDS report, submitted to CDC 2010",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Most existing work uses dual decomposition and subgradient methods to solve\nNetwork Utility Maximization (NUM) problems in a distributed manner, which\nsuffer from slow rate of convergence properties. This work develops an\nalternative distributed Newton-type fast converging algorithm for solving\nnetwork utility maximization problems with self-concordant utility functions.\nBy using novel matrix splitting techniques, both primal and dual updates for\nthe Newton step can be computed using iterative schemes in a decentralized\nmanner with limited information exchange. Similarly, the stepsize can be\nobtained via an iterative consensus-based averaging scheme. We show that even\nwhen the Newton direction and the stepsize in our method are computed within\nsome error (due to finite truncation of the iterative schemes), the resulting\nobjective function value still converges superlinearly to an explicitly\ncharacterized error neighborhood. Simulation results demonstrate significant\nconvergence rate improvement of our algorithm relative to the existing\nsubgradient methods based on dual decomposition.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 14 May 2010 21:44:57 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 22 Apr 2011 16:32:15 GMT"
            }
        ],
        "update_date": "2015-03-17",
        "authors_parsed": [
            [
                "Wei",
                "Ermin",
                ""
            ],
            [
                "Ozdaglar",
                "Asuman",
                ""
            ],
            [
                "Jadbabaie",
                "Ali",
                ""
            ]
        ]
    },
    {
        "id": "1005.2967",
        "submitter": "Choon Yik Tang",
        "authors": "Choon Yik Tang and Jie Lu",
        "title": "Controlled Hopwise Averaging: Bandwidth/Energy-Efficient Asynchronous\n  Distributed Averaging for Wireless Networks",
        "comments": "33 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.DC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses the problem of averaging numbers across a wireless\nnetwork from an important, but largely neglected, viewpoint: bandwidth/energy\nefficiency. We show that existing distributed averaging schemes have several\ndrawbacks and are inefficient, producing networked dynamical systems that\nevolve with wasteful communications. Motivated by this, we develop Controlled\nHopwise Averaging (CHA), a distributed asynchronous algorithm that attempts to\n\"make the most\" out of each iteration by fully exploiting the broadcast nature\nof wireless medium and enabling control of when to initiate an iteration. We\nshow that CHA admits a common quadratic Lyapunov function for analysis, derive\nbounds on its exponential convergence rate, and show that they outperform the\nconvergence rate of Pairwise Averaging for some common graphs. We also\nintroduce a new way to apply Lyapunov stability theory, using the Lyapunov\nfunction to perform greedy, decentralized, feedback iteration control. Finally,\nthrough extensive simulation on random geometric graphs, we show that CHA is\nsubstantially more efficient than several existing schemes, requiring far fewer\ntransmissions to complete an averaging task.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 17 May 2010 16:33:41 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 26 Dec 2010 15:42:38 GMT"
            }
        ],
        "update_date": "2010-12-30",
        "authors_parsed": [
            [
                "Tang",
                "Choon Yik",
                ""
            ],
            [
                "Lu",
                "Jie",
                ""
            ]
        ]
    },
    {
        "id": "1005.3290",
        "submitter": "Serhiy Zhuk M.",
        "authors": "Sergiy Zhuk",
        "title": "Minimax state estimation for linear continuous differential-algebraic\n  equations",
        "comments": "9 pages, 1 figure",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes a minimax state estimation approach for linear\nDifferential-Algebraic Equations (DAE) with uncertain parameters. The approach\naddresses continuous-time DAE with non-stationary rectangular matrices and\nuncertain bounded deterministic input. An observation's noise is supposed to be\nrandom with zero mean and unknown bounded correlation function. Main results\nare a Generalized Kalman Duality (GKD) principle and sub-optimal minimax state\nestimation algorithm. GKD is derived by means of Young-Fenhel duality theorem.\nGKD proves that the minimax estimate coincides with a solution to a Dual\nControl Problem (DCP) with DAE constraints. The latter is ill-posed and,\ntherefore, the DCP is solved by means of Tikhonov regularization approach\nresulting a sub-optimal state estimation algorithm in the form of filter. We\nillustrate the approach by an synthetic example and we discuss connections with\nimpulse-observability.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 18 May 2010 19:26:46 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 5 Sep 2010 10:53:34 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 25 Feb 2011 15:24:09 GMT"
            }
        ],
        "update_date": "2011-02-28",
        "authors_parsed": [
            [
                "Zhuk",
                "Sergiy",
                ""
            ]
        ]
    },
    {
        "id": "1005.3450",
        "submitter": "Bryan Ford",
        "authors": "Amittai Aviram, Shu-Chun Weng, Sen Hu, Bryan Ford (Yale University)",
        "title": "Efficient System-Enforced Deterministic Parallelism",
        "comments": "14 pages, 12 figures, 3 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.DC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Deterministic execution offers many benefits for debugging, fault tolerance,\nand security. Running parallel programs deterministically is usually difficult\nand costly, however - especially if we desire system-enforced determinism,\nensuring precise repeatability of arbitrarily buggy or malicious software.\nDeterminator is a novel operating system that enforces determinism on both\nmultithreaded and multi-process computations. Determinator's kernel provides\nonly single-threaded, \"shared-nothing\" address spaces interacting via\ndeterministic synchronization. An untrusted user-level runtime uses distributed\ncomputing techniques to emulate familiar abstractions such as Unix processes,\nfile systems, and shared memory multithreading. The system runs parallel\napplications deterministically both on multicore PCs and across nodes in a\ncluster. Coarse-grained parallel benchmarks perform and scale comparably to -\nsometimes better than - conventional systems, though determinism is costly for\nfine-grained parallel applications.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 19 May 2010 14:17:56 GMT"
            }
        ],
        "update_date": "2010-05-20",
        "authors_parsed": [
            [
                "Aviram",
                "Amittai",
                "",
                "Yale University"
            ],
            [
                "Weng",
                "Shu-Chun",
                "",
                "Yale University"
            ],
            [
                "Hu",
                "Sen",
                "",
                "Yale University"
            ],
            [
                "Ford",
                "Bryan",
                "",
                "Yale University"
            ]
        ]
    },
    {
        "id": "1005.5045",
        "submitter": "Mario Bravetti",
        "authors": "Mario Bravetti",
        "title": "File Managing and Program Execution in Web Operating Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SE cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Web Operating Systems can be seen as an extension of traditional Operating\nSystems where the addresses used to manage files and execute programs (via the\nbasic load/execution mechanism) are extended from local filesystem path-names\nto URLs. A first consequence is that, similarly as in traditional web\ntechnologies, executing a program at a given URL, can be done in two\nmodalities: either the execution is performed client-side at the invoking\nmachine (and relative URL addressing in the executed program set to refer to\nthe invoked URL) or it is performed server-side at the machine addressed by the\ninvoked URL (as, e.g., for a web service). Moreover in this context, user\nidentification for access to programs and files and workflow-based composition\nof service programs is naturally based on token/session-like mechanisms. We\npropose a middleware based on client-server protocols and on a set primitives,\nfor managing files/resources and executing programs (in the form of\nclient-side/server-side components/services) in Web Operating Systems. We\nformally define the semantics of such middleware via a process algebraic\napproach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 27 May 2010 12:28:51 GMT"
            }
        ],
        "update_date": "2010-05-28",
        "authors_parsed": [
            [
                "Bravetti",
                "Mario",
                ""
            ]
        ]
    },
    {
        "id": "1005.5241",
        "submitter": "Jalil Boukhobza",
        "authors": "Jalil Boukhobza (LESTER), Timsit Claude (PRISM)",
        "title": "Simulation de traces r\\'eelles d'E/S disque de PC",
        "comments": null,
        "journal-ref": "RenPar'17 / SympA'2006 / CFSE'5 / JC'2006, Canet en Roussillon :\n  France (2006)",
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Under Windows operating system, existing I/O benchmarking tools does not\nallow a developer to efficiently define a file access strategy according to the\napplications' constraints. This is essentially due to the fact that the\nexisting tools do allow only a restricted set of I/O workloads that does not\ngenerally correspond to the target applications. To cope with this problem, we\ndesigned and implemented a precise I/O simulator allowing to simulate whatever\nreal I/O trace on a given defined architecture, and in which most of file and\ndisk cache strategies, their interactions and the detailed storage system\narchitecture are implemented. Simulation results on different workloads and\narchitectures show a very high degree of precision. In fact, the mean error\nrate as compared to real measures is of about 6% with a maximum of 10% on\nglobal throughput.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 28 May 2010 08:55:12 GMT"
            }
        ],
        "update_date": "2010-07-26",
        "authors_parsed": [
            [
                "Boukhobza",
                "Jalil",
                "",
                "LESTER"
            ],
            [
                "Claude",
                "Timsit",
                "",
                "PRISM"
            ]
        ]
    },
    {
        "id": "1005.5582",
        "submitter": "Necdet Serhat Aybat",
        "authors": "Necdet Serhat Aybat, Garud Iyengar",
        "title": "A First-order Augmented Lagrangian Method for Compressed Sensing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a first-order augmented Lagrangian algorithm (FAL) for solving the\nbasis pursuit problem. FAL computes a solution to this problem by inexactly\nsolving a sequence of L1-regularized least squares sub-problems. These\nsub-problems are solved using an infinite memory proximal gradient algorithm\nwherein each update reduces to \"shrinkage\" or constrained \"shrinkage\". We show\nthat FAL converges to an optimal solution of the basis pursuit problem whenever\nthe solution is unique, which is the case with very high probability for\ncompressed sensing problems. We construct a parameter sequence such that the\ncorresponding FAL iterates are eps-feasible and eps-optimal for all eps>0\nwithin O(log(1/eps)) FAL iterations. Moreover, FAL requires at most O(1/eps)\nmatrix-vector multiplications of the form Ax or A^Ty to compute an\neps-feasible, eps-optimal solution. We show that FAL can be easily extended to\nsolve the basis pursuit denoising problem when there is a non-trivial level of\nnoise on the measurements. We report the results of numerical experiments\ncomparing FAL with the state-of-the-art algorithms for both noisy and noiseless\ncompressed sensing problems. A striking property of FAL that we observed in the\nnumerical experiments with randomly generated instances when there is no\nmeasurement noise was that FAL always correctly identifies the support of the\ntarget signal without any thresholding or post-processing, for moderately small\nerror tolerance values.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 31 May 2010 04:20:49 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 25 Aug 2011 17:39:48 GMT"
            }
        ],
        "update_date": "2011-08-29",
        "authors_parsed": [
            [
                "Aybat",
                "Necdet Serhat",
                ""
            ],
            [
                "Iyengar",
                "Garud",
                ""
            ]
        ]
    },
    {
        "id": "1005.5638",
        "submitter": "Mazyar Mirrahimi",
        "authors": "Marianne Chapouly, Mazyar Mirrahimi",
        "title": "Distributed source identification for wave equations: an observer-based\n  approach (full paper)",
        "comments": "26 pages, 2 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY math.AP",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we consider the 1D wave equation where the spatial domain is a\nbounded interval. Assuming the initial conditions to be known, we are here\ninterested in identifying an unknown source term, while we take the Neumann\nderivative of the solution on one of the boundaries as the measurement output.\nApplying a back-and-forth iterative scheme and constructing well-chosen\nobservers, we retrieve the source term from the measurement output in the\nminimal observation time. We further provide an extension of the method to the\ncase of wave equations with N dimensional spatial domain.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 31 May 2010 09:32:13 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 29 Jun 2011 15:59:58 GMT"
            }
        ],
        "update_date": "2011-06-30",
        "authors_parsed": [
            [
                "Chapouly",
                "Marianne",
                ""
            ],
            [
                "Mirrahimi",
                "Mazyar",
                ""
            ]
        ]
    },
    {
        "id": "1006.0813",
        "submitter": "Kees Middelburg",
        "authors": "J. A. Bergstra, C. A. Middelburg",
        "title": "On the definition of a theoretical concept of an operating system",
        "comments": "8 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We dwell on how a definition of a theoretical concept of an operating system,\nsuitable to be incorporated in a mathematical theory of operating systems,\ncould look like. This is considered a valuable preparation for the development\nof a mathematical theory of operating systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 4 Jun 2010 09:03:39 GMT"
            }
        ],
        "update_date": "2010-06-07",
        "authors_parsed": [
            [
                "Bergstra",
                "J. A.",
                ""
            ],
            [
                "Middelburg",
                "C. A.",
                ""
            ]
        ]
    },
    {
        "id": "1006.1749",
        "submitter": "Mario Sigalotti",
        "authors": "Falk Hante (IECN, INRIA Lorraine / IECN / MMAS), Mario Sigalotti\n  (IECN, INRIA Lorraine / IECN / MMAS)",
        "title": "Converse Lyapunov Theorems for Switched Systems in Banach and Hilbert\n  Spaces",
        "comments": null,
        "journal-ref": "SIAM Journal on Control and Optimization, Vol. 49, Nr. 2, pp.\n  752--770, 2011",
        "doi": "10.1137/100801561",
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider switched systems on Banach and Hilbert spaces governed by\nstrongly continuous one-parameter semigroups of linear evolution operators. We\nprovide necessary and sufficient conditions for their global exponential\nstability, uniform with respect to the switching signal, in terms of the\nexistence of a Lyapunov function common to all modes.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 Jun 2010 09:10:57 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 29 Jun 2010 07:55:28 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 15 Feb 2011 15:56:21 GMT"
            }
        ],
        "update_date": "2012-11-26",
        "authors_parsed": [
            [
                "Hante",
                "Falk",
                "",
                "IECN, INRIA Lorraine / IECN / MMAS"
            ],
            [
                "Sigalotti",
                "Mario",
                "",
                "IECN, INRIA Lorraine / IECN / MMAS"
            ]
        ]
    },
    {
        "id": "1006.2104",
        "submitter": "Harco Leslie Hendric Spits Warnars",
        "authors": "Spits Warnars H.L.H",
        "title": "Perbandingan Shell Unix",
        "comments": "21 Pages",
        "journal-ref": "Widya, Vol 21,No. 230, pp. 9-15, November 2004",
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  Is it possible for an Information Technology [IT] product to be both mature\nand state-of-theart at the same time? In the case of the UNIX system, the\nanswer is an unqualified \"Yes.\" The UNIX system has continued to develop over\nthe past twenty-five years. In millions of installations running on nearly\nevery hardware platform made, the UNIX system has earned its reputation for\nstability and scalability. Over the years, UNIX system suppliers have steadily\nassimilated new technologies so that UNIX systems today provide more\nfunctionality as any other operating system.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 Jun 2010 18:13:36 GMT"
            }
        ],
        "update_date": "2010-06-11",
        "authors_parsed": [
            [
                "H",
                "Spits Warnars H. L.",
                ""
            ]
        ]
    },
    {
        "id": "1006.2165",
        "submitter": "Marc Deisenroth",
        "authors": "Marc Peter Deisenroth and Henrik Ohlsson",
        "title": "A Probabilistic Perspective on Gaussian Filtering and Smoothing",
        "comments": "14 pages. Extended version of conference paper (ACC 2011)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "stat.ME cs.AI cs.RO cs.SY math.OC stat.ML",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present a general probabilistic perspective on Gaussian filtering and\nsmoothing. This allows us to show that common approaches to Gaussian\nfiltering/smoothing can be distinguished solely by their methods of\ncomputing/approximating the means and covariances of joint probabilities. This\nimplies that novel filters and smoothers can be derived straightforwardly by\nproviding methods for computing these moments. Based on this insight, we derive\nthe cubature Kalman smoother and propose a novel robust filtering and smoothing\nalgorithm based on Gibbs sampling.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 Jun 2010 22:23:23 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 29 Jun 2010 00:14:05 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 9 Aug 2010 01:36:08 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 4 Apr 2011 20:54:13 GMT"
            },
            {
                "version": "v5",
                "created": "Wed, 8 Jun 2011 06:15:34 GMT"
            }
        ],
        "update_date": "2011-06-09",
        "authors_parsed": [
            [
                "Deisenroth",
                "Marc Peter",
                ""
            ],
            [
                "Ohlsson",
                "Henrik",
                ""
            ]
        ]
    },
    {
        "id": "1006.2617",
        "submitter": "Joel Goossens",
        "authors": "Jo\\\"el Goossens and Vandy Berten",
        "title": "Gang FTP scheduling of periodic and parallel rigid real-time tasks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider the scheduling of periodic and parallel rigid\ntasks. We provide (and prove correct) an exact schedulability test for Fixed\nTask Priority (FTP) Gang scheduler sub-classes: Parallelism Monotonic, Idling,\nLimited Gang, and Limited Slack Reclaiming. Additionally, we study the\npredictability of our schedulers: we show that Gang FJP schedulers are not\npredictable and we identify several sub-classes which are actually predictable.\nMoreover, we extend the definition of rigid, moldable and malleable jobs to\nrecurrent tasks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 Jun 2010 07:17:14 GMT"
            }
        ],
        "update_date": "2010-06-15",
        "authors_parsed": [
            [
                "Goossens",
                "Jo\u00ebl",
                ""
            ],
            [
                "Berten",
                "Vandy",
                ""
            ]
        ]
    },
    {
        "id": "1006.2637",
        "submitter": "Joel Goossens",
        "authors": "Fran\\c{c}ois Dorin, Patrick Meumeu Yomsi, Jo\\\"el Goossens and Pascal\n  Richard",
        "title": "Semi-Partitioned Hard Real-Time Scheduling with Restricted Migrations\n  upon Identical Multiprocessor Platforms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Algorithms based on semi-partitioned scheduling have been proposed as a\nviable alternative between the two extreme ones based on global and partitioned\nscheduling. In particular, allowing migration to occur only for few tasks which\ncannot be assigned to any individual processor, while most tasks are assigned\nto specific processors, considerably reduces the runtime overhead compared to\nglobal scheduling on the one hand, and improve both the schedulability and the\nsystem utilization factor compared to partitioned scheduling on the other hand.\nIn this paper, we address the preemptive scheduling problem of hard real-time\nsystems composed of sporadic constrained-deadline tasks upon identical\nmultiprocessor platforms. We propose a new algorithm and a scheduling paradigm\nbased on the concept of semi-partitioned scheduling with restricted migrations\nin which jobs are not allowed to migrate, but two subsequent jobs of a task can\nbe assigned to different processors by following a periodic strategy.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 14 Jun 2010 08:51:05 GMT"
            }
        ],
        "update_date": "2010-06-15",
        "authors_parsed": [
            [
                "Dorin",
                "Fran\u00e7ois",
                ""
            ],
            [
                "Yomsi",
                "Patrick Meumeu",
                ""
            ],
            [
                "Goossens",
                "Jo\u00ebl",
                ""
            ],
            [
                "Richard",
                "Pascal",
                ""
            ]
        ]
    },
    {
        "id": "1006.4046",
        "submitter": "Benjamin Recht",
        "authors": "Laura Balzano and Robert Nowak and Benjamin Recht",
        "title": "Online Identification and Tracking of Subspaces from Highly Incomplete\n  Information",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.SY math.IT math.OC stat.ML",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work presents GROUSE (Grassmanian Rank-One Update Subspace Estimation),\nan efficient online algorithm for tracking subspaces from highly incomplete\nobservations. GROUSE requires only basic linear algebraic manipulations at each\niteration, and each subspace update can be performed in linear time in the\ndimension of the subspace. The algorithm is derived by analyzing incremental\ngradient descent on the Grassmannian manifold of subspaces. With a slight\nmodification, GROUSE can also be used as an online incremental algorithm for\nthe matrix completion problem of imputing missing entries of a low-rank matrix.\nGROUSE performs exceptionally well in practice both in tracking subspaces and\nas an online algorithm for matrix completion.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 21 Jun 2010 12:12:27 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 12 Jul 2011 20:19:54 GMT"
            }
        ],
        "update_date": "2011-07-14",
        "authors_parsed": [
            [
                "Balzano",
                "Laura",
                ""
            ],
            [
                "Nowak",
                "Robert",
                ""
            ],
            [
                "Recht",
                "Benjamin",
                ""
            ]
        ]
    },
    {
        "id": "1006.4804",
        "submitter": "Yimin Yan",
        "authors": "Yimin Yan",
        "title": "The General Solutions of Linear ODE and Riccati Equation",
        "comments": "9 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.CA cs.SY math-ph math.AP math.MP math.OC nlin.SI",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  This paper gives out the general solutions of variable coefficients ODE and\nRiccati equation by way of integral series E(X) and F(X). Such kinds of\nintegral series are the generalized form of exponential function, and keep the\nproperties of convergent and reversible.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 24 Jun 2010 14:40:47 GMT"
            },
            {
                "version": "v2",
                "created": "Sat, 17 Jul 2010 14:09:53 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 10 Nov 2010 15:39:21 GMT"
            },
            {
                "version": "v4",
                "created": "Mon, 15 Aug 2011 06:53:05 GMT"
            }
        ],
        "update_date": "2011-08-16",
        "authors_parsed": [
            [
                "Yan",
                "Yimin",
                ""
            ]
        ]
    },
    {
        "id": "1006.5845",
        "submitter": "Mattia Monga",
        "authors": "Aristide Fattori, Roberto Paleari, Lorenzo Martignoni, Mattia Monga",
        "title": "Dynamic and Transparent Analysis of Commodity Production Systems",
        "comments": "10 pages, To appear in the 25th IEEE/ACM International Conference on\n  Automated Software Engineering, Antwerp, Belgium, 20-24 September 2010",
        "journal-ref": null,
        "doi": "10.1145/1858996.1859085",
        "report-no": null,
        "categories": "cs.OS cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We propose a framework that provides a programming interface to perform\ncomplex dynamic system-level analyses of deployed production systems. By\nleveraging hardware support for virtualization available nowadays on all\ncommodity machines, our framework is completely transparent to the system under\nanalysis and it guarantees isolation of the analysis tools running on its top.\nThus, the internals of the kernel of the running system needs not to be\nmodified and the whole platform runs unaware of the framework. Moreover, errors\nin the analysis tools do not affect the running system and the framework. This\nis accomplished by installing a minimalistic virtual machine monitor and\nmigrating the system, as it runs, into a virtual machine. In order to\ndemonstrate the potentials of our framework we developed an interactive kernel\ndebugger, nicknamed HyperDbg. HyperDbg can be used to debug any critical kernel\ncomponent, and even to single step the execution of exception and interrupt\nhandlers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 30 Jun 2010 13:01:58 GMT"
            }
        ],
        "update_date": "2015-03-04",
        "authors_parsed": [
            [
                "Fattori",
                "Aristide",
                ""
            ],
            [
                "Paleari",
                "Roberto",
                ""
            ],
            [
                "Martignoni",
                "Lorenzo",
                ""
            ],
            [
                "Monga",
                "Mattia",
                ""
            ]
        ]
    },
    {
        "id": "1007.0199",
        "submitter": "Mauricio Junca",
        "authors": "Mauricio Junca",
        "title": "Optimal execution strategy in the presence of permanent price impact and\n  fixed transaction cost",
        "comments": null,
        "journal-ref": "Optim. Control Appl. and Meth. 33 (6):713-738.(2012)",
        "doi": null,
        "report-no": null,
        "categories": "q-fin.TR cs.SY math.OC math.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a single risky financial asset model subject to price impact and\ntransaction cost over an infinite horizon. An investor needs to execute a long\nposition in the asset affecting the price of the asset and possibly incurring\nin fixed transaction cost. The objective is to maximize the discounted revenue\nobtained by this transaction. This problem is formulated first as an impulse\ncontrol problem and we characterize the value function using the viscosity\nsolutions framework. We also analyze the case where there is no transaction\ncost and how this formulation relates with a singular control problem. A\nviscosity solution characterization is provided in this case as well. We also\nestablish a connection between both formulations with zero fixed transaction\ncost. Numerical examples with different types of price impact conclude the\ndiscussion.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 1 Jul 2010 15:45:58 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 23 Sep 2010 22:12:24 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 28 Dec 2010 03:11:18 GMT"
            },
            {
                "version": "v4",
                "created": "Wed, 9 Feb 2011 23:25:31 GMT"
            },
            {
                "version": "v5",
                "created": "Wed, 24 Aug 2011 22:01:22 GMT"
            }
        ],
        "update_date": "2014-09-19",
        "authors_parsed": [
            [
                "Junca",
                "Mauricio",
                ""
            ]
        ]
    },
    {
        "id": "1007.1234",
        "submitter": "Georgi Medvedev S.",
        "authors": "Georgi S. Medvedev",
        "title": "Stochastic stability of continuous time consensus protocols",
        "comments": "SIAM Journal on Control and Optimization, to appear",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY nlin.AO q-bio.NC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  A unified approach to studying convergence and stochastic stability of\ncontinuous time consensus protocols (CPs) is presented in this work. Our method\napplies to networks with directed information flow; both cooperative and\nnoncooperative interactions; networks under weak stochastic forcing; and those\nwhose topology and strength of connections may vary in time. The graph\ntheoretic interpretation of the analytical results is emphasized. We show how\nthe spectral properties, such as algebraic connectivity and total effective\nresistance, as well as the geometric properties, such the dimension and the\nstructure of the cycle subspace of the underlying graph, shape stability of the\ncorresponding CPs. In addition, we explore certain implications of the spectral\ngraph theory to CP design. In particular, we point out that expanders, sparse\nhighly connected graphs, generate CPs whose performance remains uniformly high\nwhen the size of the network grows unboundedly. Similarly, we highlight the\nbenefits of using random versus regular network topologies for CP design. We\nillustrate these observations with numerical examples and refer to the relevant\ngraph-theoretic results.\n  Keywords: consensus protocol, dynamical network, synchronization, robustness\nto noise, algebraic connectivity, effective resistance, expander, random graph\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 7 Jul 2010 19:59:16 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 1 Jun 2011 00:06:59 GMT"
            },
            {
                "version": "v3",
                "created": "Sat, 2 Jun 2012 14:02:26 GMT"
            }
        ],
        "update_date": "2012-06-05",
        "authors_parsed": [
            [
                "Medvedev",
                "Georgi S.",
                ""
            ]
        ]
    },
    {
        "id": "1007.2738",
        "submitter": "Fabio Pasqualetti",
        "authors": "Fabio Pasqualetti, Antonio Bicchi, Francesco Bullo",
        "title": "Consensus Computation in Unreliable Networks: A System Theoretic\n  Approach",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work addresses the problem of ensuring trustworthy computation in a\nlinear consensus network. A solution to this problem is relevant for several\ntasks in multi-agent systems including motion coordination, clock\nsynchronization, and cooperative estimation. In a linear consensus network, we\nallow for the presence of misbehaving agents, whose behavior deviate from the\nnominal consensus evolution. We model misbehaviors as unknown and unmeasurable\ninputs affecting the network, and we cast the misbehavior detection and\nidentification problem into an unknown-input system theoretic framework. We\nconsider two extreme cases of misbehaving agents, namely faulty (non-colluding)\nand malicious (Byzantine) agents. First, we characterize the set of inputs that\nallow misbehaving agents to affect the consensus network while remaining\nundetected and/or unidentified from certain observing agents. Second, we\nprovide worst-case bounds for the number of concurrent faulty or malicious\nagents that can be detected and identified. Precisely, the consensus network\nneeds to be 2k+1 (resp. k+1) connected for k malicious (resp. faulty) agents to\nbe generically detectable and identifiable by every well behaving agent. Third,\nwe quantify the effect of undetectable inputs on the final consensus value.\nFourth, we design three algorithms to detect and identify misbehaving agents.\nThe first and the second algorithm apply fault detection techniques, and\naffords complete detection and identification if global knowledge of the\nnetwork is available to each agent, at a high computational cost. The third\nalgorithm is designed to exploit the presence in the network of weakly\ninterconnected subparts, and provides local detection and identification of\nmisbehaving agents whose behavior deviates more than a threshold, which is\nquantified in terms of the interconnection structure.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 16 Jul 2010 10:53:17 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 10 Nov 2010 07:10:22 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 14 Feb 2011 05:32:15 GMT"
            },
            {
                "version": "v4",
                "created": "Sat, 16 Apr 2011 16:59:53 GMT"
            }
        ],
        "update_date": "2011-04-19",
        "authors_parsed": [
            [
                "Pasqualetti",
                "Fabio",
                ""
            ],
            [
                "Bicchi",
                "Antonio",
                ""
            ],
            [
                "Bullo",
                "Francesco",
                ""
            ]
        ]
    },
    {
        "id": "1008.0775",
        "submitter": "Armen Bagdasaryan",
        "authors": "Armen Bagdasaryan",
        "title": "Systems Theoretic Techniques for Modeling, Control, and Decision Support\n  in Complex Dynamic Systems",
        "comments": "58 pages, 24 figures, 1 table; a book chapter published by Bentham\n  Science",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SY cs.AI cs.MA math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss the problems of modeling, control, and decision support in complex\ndynamic systems from a general system theoretic point of view. The main\ncharacteristics of complex systems and of system approach to complex system\nstudy are considered. We provide an overview and analysis of known existing\nparadigms and methods of mathematical modeling and simulation of complex\nsystems, which support the processes of control and decision making. Then we\ncontinue with the general dynamic modeling and simulation technique for complex\nhierarchical systems functioning in control loop. Architectural and structural\nmodels of computer information system intended for simulation and decision\nsupport in complex systems are presented.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 4 Aug 2010 13:29:57 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 25 Dec 2013 15:31:39 GMT"
            }
        ],
        "update_date": "2013-12-30",
        "authors_parsed": [
            [
                "Bagdasaryan",
                "Armen",
                ""
            ]
        ]
    },
    {
        "id": "1008.1571",
        "submitter": "Ananth Narayan Sankaranarayanan",
        "authors": "Ananth Narayan S and Somsubhra Sharangi and Alexandra Fedorova",
        "title": "Scaling Turbo Boost to a 1000 cores",
        "comments": "7 pages, short paper",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The Intel Core i7 processor code named Nehalem provides a feature named Turbo\nBoost which opportunistically varies the frequencies of the processor's cores.\nThe frequency of a core is determined by core temperature, the number of active\ncores, the estimated power consumption, the estimated current consumption, and\noperating system frequency scaling requests. For a chip multi-processor(CMP)\nthat has a small number of physical cores and a small set of performance\nstates, deciding the Turbo Boost frequency to use on a given core might not be\ndifficult. However, we do not know the complexity of this decision making\nprocess in the context of a large number of cores, scaling to the 100s, as\npredicted by researchers in the field.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 9 Aug 2010 19:23:10 GMT"
            }
        ],
        "update_date": "2010-08-10",
        "authors_parsed": [
            [
                "S",
                "Ananth Narayan",
                ""
            ],
            [
                "Sharangi",
                "Somsubhra",
                ""
            ],
            [
                "Fedorova",
                "Alexandra",
                ""
            ]
        ]
    },
    {
        "id": "1008.3222",
        "submitter": "Christoffer Sloth Sloth",
        "authors": "Christoffer Sloth and Rafael Wisniewski",
        "title": "Proofs for an Abstraction of Continuous Dynamical Systems Utilizing\n  Lyapunov Functions",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this report proofs are presented for a method for abstracting continuous\ndynamical systems by timed automata. The method is based on partitioning the\nstate space of dynamical systems with invariant sets, which form cells\nrepresenting locations of the timed automata.\n  To enable verification of the dynamical system based on the abstraction,\nconditions for obtaining sound, complete, and refinable abstractions are set\nup.\n  It is proposed to partition the state space utilizing sub-level sets of\nLyapunov functions, since they are positive invariant sets. The existence of\nsound abstractions for Morse-Smale systems and complete and refinable\nabstractions for linear systems are proved.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 19 Aug 2010 06:14:47 GMT"
            }
        ],
        "update_date": "2010-08-20",
        "authors_parsed": [
            [
                "Sloth",
                "Christoffer",
                ""
            ],
            [
                "Wisniewski",
                "Rafael",
                ""
            ]
        ]
    },
    {
        "id": "1008.3614",
        "submitter": "Iordanis  Koutsopoulos",
        "authors": "Iordanis Koutsopoulos and Leandros Tassiulas",
        "title": "Control and Optimization Meet the Smart Power Grid - Scheduling of Power\n  Demands for Optimal Energy Management",
        "comments": "submitted to INFOCOM 2011; 9 pages, 4 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The smart power grid aims at harnessing information and communication\ntechnologies to enhance reliability and enforce sensible use of energy. Its\nrealization is geared by the fundamental goal of effective management of demand\nload. In this work, we envision a scenario with real-time communication between\nthe operator and consumers. The grid operator controller receives requests for\npower demands from consumers, with different power requirement, duration, and a\ndeadline by which it is to be completed. The objective is to devise a power\ndemand task scheduling policy that minimizes the grid operational cost over a\ntime horizon. The operational cost is a convex function of instantaneous power\nconsumption and reflects the fact that each additional unit of power needed to\nserve demands is more expensive as demand load increases.First, we study the\noff-line demand scheduling problem, where parameters are fixed and known. Next,\nwe devise a stochastic model for the case when demands are generated\ncontinually and scheduling decisions are taken online and focus on long-term\naverage cost. We present two instances of power consumption control based on\nobserving current consumption. First, the controller may choose to serve a new\ndemand request upon arrival or to postpone it to the end of its deadline.\nSecond, the additional option exists to activate one of the postponed demands\nwhen an active demand terminates. For both instances, the optimal policies are\nthreshold based. We derive a lower performance bound over all policies, which\nis asymptotically tight as deadlines increase. We propose the Controlled\nRelease threshold policy and prove it is asymptotically optimal. The policy\nactivates a new demand request if the current power consumption is less than a\nthreshold, otherwise it is queued. Queued demands are scheduled when their\ndeadline expires or when the consumption drops below the threshold.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 21 Aug 2010 08:23:48 GMT"
            }
        ],
        "update_date": "2010-08-24",
        "authors_parsed": [
            [
                "Koutsopoulos",
                "Iordanis",
                ""
            ],
            [
                "Tassiulas",
                "Leandros",
                ""
            ]
        ]
    },
    {
        "id": "1008.3651",
        "submitter": "Anatoli Iouditski",
        "authors": "Anatoli Iouditski (LJK), Arkadii S. Nemirovski (ISyE)",
        "title": "Accuracy guarantees for L1-recovery",
        "comments": null,
        "journal-ref": "IEEE Transactions on Information Theory 57, 12 (2011) 7818 - 7839",
        "doi": "10.1109/TIT.2011.2162569",
        "report-no": null,
        "categories": "math.ST cs.SY math.OC stat.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We discuss two new methods of recovery of sparse signals from noisy\nobservation based on $\\ell_1$- minimization. They are closely related to the\nwell-known techniques such as Lasso and Dantzig Selector. However, these\nestimators come with efficiently verifiable guaranties of performance. By\noptimizing these bounds with respect to the method parameters we are able to\nconstruct the estimators which possess better statistical properties than the\ncommonly used ones. We also show how these techniques allow to provide\nefficiently computable accuracy bounds for Lasso and Dantzig Selector. We link\nour performance estimations to the well known results of Compressive Sensing\nand justify our proposed approach with an oracle inequality which links the\nproperties of the recovery algorithms and the best estimation performance when\nthe signal support is known. We demonstrate how the estimates can be computed\nusing the Non-Euclidean Basis Pursuit algorithm.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 21 Aug 2010 18:32:03 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 29 Oct 2010 15:03:42 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 27 May 2011 13:43:50 GMT"
            }
        ],
        "update_date": "2014-04-11",
        "authors_parsed": [
            [
                "Iouditski",
                "Anatoli",
                "",
                "LJK"
            ],
            [
                "Nemirovski",
                "Arkadii S.",
                "",
                "ISyE"
            ]
        ]
    },
    {
        "id": "1008.3760",
        "submitter": "Ishanu Chattopadhyay",
        "authors": "Ishanu Chattopadhyay and Anthony Cascone and Asok Ray",
        "title": "Formal-language-theoretic Optimal Path Planning For Accommodation of\n  Amortized Uncertainties and Dynamic Effects",
        "comments": "Submitted for review for possible publication elsewhere; journal\n  reference will be added when available",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.RO cs.SY math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We report a globally-optimal approach to robotic path planning under\nuncertainty, based on the theory of quantitative measures of formal languages.\nA significant generalization to the language-measure-theoretic path planning\nalgorithm $\\nustar$ is presented that explicitly accounts for average dynamic\nuncertainties and estimation errors in plan execution. The notion of the\nnavigation automaton is generalized to include probabilistic uncontrollable\ntransitions, which account for uncertainties by modeling and planning for\nprobabilistic deviations from the computed policy in the course of execution.\nThe planning problem is solved by casting it in the form of a performance\nmaximization problem for probabilistic finite state automata. In essence we\nsolve the following optimization problem: Compute the navigation policy which\nmaximizes the probability of reaching the goal, while simultaneously minimizing\nthe probability of hitting an obstacle. Key novelties of the proposed approach\ninclude the modeling of uncertainties using the concept of uncontrollable\ntransitions, and the solution of the ensuing optimization problem using a\nhighly efficient search-free combinatorial approach to maximize quantitative\nmeasures of probabilistic regular languages. Applicability of the algorithm in\nvarious models of robot navigation has been shown with experimental validation\non a two-wheeled mobile robotic platform (SEGWAY RMP 200) in a laboratory\nenvironment.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Aug 2010 07:16:51 GMT"
            }
        ],
        "update_date": "2010-08-24",
        "authors_parsed": [
            [
                "Chattopadhyay",
                "Ishanu",
                ""
            ],
            [
                "Cascone",
                "Anthony",
                ""
            ],
            [
                "Ray",
                "Asok",
                ""
            ]
        ]
    },
    {
        "id": "1008.3932",
        "submitter": "Sugumar Murugesan",
        "authors": "Miao He, Sugumar Murugesan, Junshan Zhang",
        "title": "Multiple Timescale Dispatch and Scheduling for Stochastic Reliability in\n  Smart Grids with Wind Generation Integration",
        "comments": "Submitted to IEEE Infocom 2011. Contains 10 pages and 4 figures.\n  Replaces the previous arXiv submission (dated Aug-23-2010) with the same\n  title",
        "journal-ref": null,
        "doi": "10.1109/INFCOM.2011.5935204",
        "report-no": null,
        "categories": "cs.SY cs.PF",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Integrating volatile renewable energy resources into the bulk power grid is\nchallenging, due to the reliability requirement that at each instant the load\nand generation in the system remain balanced. In this study, we tackle this\nchallenge for smart grid with integrated wind generation, by leveraging\nmulti-timescale dispatch and scheduling. Specifically, we consider smart grids\nwith two classes of energy users - traditional energy users and opportunistic\nenergy users (e.g., smart meters or smart appliances), and investigate pricing\nand dispatch at two timescales, via day-ahead scheduling and realtime\nscheduling. In day-ahead scheduling, with the statistical information on wind\ngeneration and energy demands, we characterize the optimal procurement of the\nenergy supply and the day-ahead retail price for the traditional energy users;\nin realtime scheduling, with the realization of wind generation and the load of\ntraditional energy users, we optimize real-time prices to manage the\nopportunistic energy users so as to achieve systemwide reliability. More\nspecifically, when the opportunistic users are non-persistent, i.e., a subset\nof them leave the power market when the real-time price is not acceptable, we\nobtain closedform solutions to the two-level scheduling problem. For the\npersistent case, we treat the scheduling problem as a multitimescale Markov\ndecision process. We show that it can be recast, explicitly, as a classic\nMarkov decision process with continuous state and action spaces, the solution\nto which can be found via standard techniques. We conclude that the proposed\nmulti-scale dispatch and scheduling with real-time pricing can effectively\naddress the volatility and uncertainty of wind generation and energy demand,\nand has the potential to improve the penetration of renewable energy into smart\ngrids.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 23 Aug 2010 22:35:47 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 13 Sep 2010 18:42:37 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "He",
                "Miao",
                ""
            ],
            [
                "Murugesan",
                "Sugumar",
                ""
            ],
            [
                "Zhang",
                "Junshan",
                ""
            ]
        ]
    },
    {
        "id": "1008.4406",
        "submitter": "Fangwen Fu",
        "authors": "Fangwen Fu, and Mihaela van der Schaar",
        "title": "Structural Solutions to Dynamic Scheduling for Multimedia Transmission\n  in Unknown Wireless Environments",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MM cs.LG cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a systematic solution to the problem of scheduling\ndelay-sensitive media data for transmission over time-varying wireless\nchannels. We first formulate the dynamic scheduling problem as a Markov\ndecision process (MDP) that explicitly considers the users' heterogeneous\nmultimedia data characteristics (e.g. delay deadlines, distortion impacts and\ndependencies etc.) and time-varying channel conditions, which are not\nsimultaneously considered in state-of-the-art packet scheduling algorithms.\nThis formulation allows us to perform foresighted decisions to schedule\nmultiple data units for transmission at each time in order to optimize the\nlong-term utilities of the multimedia applications. The heterogeneity of the\nmedia data enables us to express the transmission priorities between the\ndifferent data units as a priority graph, which is a directed acyclic graph\n(DAG). This priority graph provides us with an elegant structure to decompose\nthe multi-data unit foresighted decision at each time into multiple single-data\nunit foresighted decisions which can be performed sequentially, from the high\npriority data units to the low priority data units, thereby significantly\nreducing the computation complexity. When the statistical knowledge of the\nmultimedia data characteristics and channel conditions is unknown a priori, we\ndevelop a low-complexity online learning algorithm to update the value\nfunctions which capture the impact of the current decision on the future\nutility. The simulation results show that the proposed solution significantly\noutperforms existing state-of-the-art scheduling solutions.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 Aug 2010 23:06:39 GMT"
            }
        ],
        "update_date": "2010-08-27",
        "authors_parsed": [
            [
                "Fu",
                "Fangwen",
                ""
            ],
            [
                "van der Schaar",
                "Mihaela",
                ""
            ]
        ]
    },
    {
        "id": "1008.4895",
        "submitter": "Longbo Huang",
        "authors": "Longbo Huang, Scott Moeller, Michael J. Neely, Bhaskar Krishnamachari",
        "title": "LIFO-Backpressure Achieves Near Optimal Utility-Delay Tradeoff",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  There has been considerable recent work developing a new stochastic network\nutility maximization framework using Backpressure algorithms, also known as\nMaxWeight. A key open problem has been the development of utility-optimal\nalgorithms that are also delay efficient. In this paper, we show that the\nBackpressure algorithm, when combined with the LIFO queueing discipline (called\nLIFO-Backpressure), is able to achieve a utility that is within $O(1/V)$ of the\noptimal value, while maintaining an average delay of $O([\\log(V)]^2)$ for all\nbut a tiny fraction of the network traffic. This result holds for general\nstochastic network optimization problems and general Markovian dynamics.\nRemarkably, the performance of LIFO-Backpressure can be achieved by simply\nchanging the queueing discipline; it requires no other modifications of the\noriginal Backpressure algorithm. We validate the results through empirical\nmeasurements from a sensor network testbed, which show good match between\ntheory and practice.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 29 Aug 2010 00:12:32 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 3 Apr 2011 18:20:03 GMT"
            }
        ],
        "update_date": "2011-04-05",
        "authors_parsed": [
            [
                "Huang",
                "Longbo",
                ""
            ],
            [
                "Moeller",
                "Scott",
                ""
            ],
            [
                "Neely",
                "Michael J.",
                ""
            ],
            [
                "Krishnamachari",
                "Bhaskar",
                ""
            ]
        ]
    },
    {
        "id": "1008.5373",
        "submitter": "Yong Zhang",
        "authors": "Zhaosong Lu and Yong Zhang",
        "title": "Penalty Decomposition Methods for Rank Minimization",
        "comments": "This paper has been withdrawn by the author",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.LG cs.NA cs.SY q-fin.CP q-fin.ST",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider general rank minimization problems with rank\nappearing in either objective function or constraint. We first establish that a\nclass of special rank minimization problems has closed-form solutions. Using\nthis result, we then propose penalty decomposition methods for general rank\nminimization problems in which each subproblem is solved by a block coordinate\ndescend method. Under some suitable assumptions, we show that any accumulation\npoint of the sequence generated by the penalty decomposition methods satisfies\nthe first-order optimality conditions of a nonlinear reformulation of the\nproblems. Finally, we test the performance of our methods by applying them to\nthe matrix completion and nearest low-rank correlation matrix problems. The\ncomputational results demonstrate that our methods are generally comparable or\nsuperior to the existing methods in terms of solution quality.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 31 Aug 2010 17:25:01 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 22 Nov 2010 22:29:44 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 17 May 2012 16:23:59 GMT"
            },
            {
                "version": "v4",
                "created": "Tue, 29 May 2012 16:08:51 GMT"
            }
        ],
        "update_date": "2012-05-30",
        "authors_parsed": [
            [
                "Lu",
                "Zhaosong",
                ""
            ],
            [
                "Zhang",
                "Yong",
                ""
            ]
        ]
    },
    {
        "id": "1009.0498",
        "submitter": "Haddouchi Faouzi",
        "authors": "Faouzi Haddouchi",
        "title": "One side invertibility for implicit hyperbolic systems with delays",
        "comments": "Paper presented at the conference\" The 3rd International IEEE\n  Scientific Conference on Physics and Control (PhysCon 2007), September\n  3rd-7th 2007 at the University of Potsdam, Germany. Abstract available at\n  http://opus.kobv.de/ubp/volltexte/2007/1522/ ; Full paper available at IPACS\n  Electronic library http://lib.physcon.ru/",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper deals with left invertibility problem of implicit hyperbolic\nsystems with delays in infinite dimensional Hilbert spaces. From a\ndecomposition procedure, invertibility for this class of systems is shown to be\nequivalent to the left invertibility of a subsystem without delays.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 2 Sep 2010 18:26:09 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 5 Jul 2011 16:05:06 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 8 Jul 2011 10:31:23 GMT"
            },
            {
                "version": "v4",
                "created": "Thu, 2 Jan 2014 22:18:40 GMT"
            }
        ],
        "update_date": "2014-01-06",
        "authors_parsed": [
            [
                "Haddouchi",
                "Faouzi",
                ""
            ]
        ]
    },
    {
        "id": "1009.0558",
        "submitter": "Daoyi Dong",
        "authors": "Daoyi Dong, Ian R. Petersen",
        "title": "Sliding Mode Control of Two-Level Quantum Systems",
        "comments": "29 pages, 4 figures, accepted by Automatica",
        "journal-ref": "Automatica 48 (2012) 725-735",
        "doi": "10.1016/j.automatica.2012.02.003",
        "report-no": null,
        "categories": "quant-ph cs.SY math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a robust control method based on sliding mode design for\ntwo-level quantum systems with bounded uncertainties. An eigenstate of the\ntwo-level quantum system is identified as a sliding mode. The objective is to\ndesign a control law to steer the system's state into the sliding mode domain\nand then maintain it in that domain when bounded uncertainties exist in the\nsystem Hamiltonian. We propose a controller design method using the Lyapunov\nmethodology and periodic projective measurements. In particular, we give\nconditions for designing such a control law, which can guarantee the desired\nrobustness in the presence of the uncertainties. The sliding mode control\nmethod has potential applications to quantum information processing with\nuncertainties.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 2 Sep 2010 23:15:59 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 22 Sep 2011 11:14:31 GMT"
            }
        ],
        "update_date": "2012-04-26",
        "authors_parsed": [
            [
                "Dong",
                "Daoyi",
                ""
            ],
            [
                "Petersen",
                "Ian R.",
                ""
            ]
        ]
    },
    {
        "id": "1009.0571",
        "submitter": "Alekh Agarwal",
        "authors": "Alekh Agarwal, Peter L. Bartlett, Pradeep Ravikumar, Martin J.\n  Wainwright",
        "title": "Information-theoretic lower bounds on the oracle complexity of\n  stochastic convex optimization",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "stat.ML cs.SY math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Relative to the large literature on upper bounds on complexity of convex\noptimization, lesser attention has been paid to the fundamental hardness of\nthese problems. Given the extensive use of convex optimization in machine\nlearning and statistics, gaining an understanding of these complexity-theoretic\nissues is important. In this paper, we study the complexity of stochastic\nconvex optimization in an oracle model of computation. We improve upon known\nresults and obtain tight minimax complexity estimates for various function\nclasses.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 3 Sep 2010 02:49:20 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 2 Aug 2011 02:51:27 GMT"
            },
            {
                "version": "v3",
                "created": "Sun, 20 Nov 2011 07:11:57 GMT"
            }
        ],
        "update_date": "2011-11-22",
        "authors_parsed": [
            [
                "Agarwal",
                "Alekh",
                ""
            ],
            [
                "Bartlett",
                "Peter L.",
                ""
            ],
            [
                "Ravikumar",
                "Pradeep",
                ""
            ],
            [
                "Wainwright",
                "Martin J.",
                ""
            ]
        ]
    },
    {
        "id": "1009.0870",
        "submitter": "Bo Tan",
        "authors": "Bo Tan and R. Srikant",
        "title": "Online Advertisement, Optimization and Stochastic Networks",
        "comments": "32 pages (single-column, double-spaced), 8 figures, 1 table",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DS cs.PF cs.SY math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a stochastic model to describe how search service\nproviders charge client companies based on users' queries for the keywords\nrelated to these companies' ads by using certain advertisement assignment\nstrategies. We formulate an optimization problem to maximize the long-term\naverage revenue for the service provider under each client's long-term average\nbudget constraint, and design an online algorithm which captures the stochastic\nproperties of users' queries and click-through behaviors. We solve the\noptimization problem by making connections to scheduling problems in wireless\nnetworks, queueing theory and stochastic networks. Unlike prior models, we do\nnot assume that the number of query arrivals is known. Due to the stochastic\nnature of the arrival process considered here, either temporary \"free\" service,\ni.e., service above the specified budget or under-utilization of the budget is\nunavoidable. We prove that our online algorithm can achieve a revenue that is\nwithin $O(\\epsilon)$ of the optimal revenue while ensuring that the overdraft\nor underdraft is $O(1/\\epsilon)$, where $\\epsilon$ can be arbitrarily small.\nWith a view towards practice, we can show that one can always operate strictly\nunder the budget. In addition, we extend our results to a click-through rate\nmaximization model, and also show how our algorithm can be modified to handle\nnon-stationary query arrival processes and clients with short-term contracts.\n  Our algorithm allows us to quantify the effect of errors in click-through\nrate estimation on the achieved revenue. We also show that in the long run, an\nexpected overdraft level of $\\Omega(\\log(1/\\epsilon))$ is unavoidable (a\nuniversal lower bound) under any stationary ad assignment algorithm which\nachieves a long-term average revenue within $O(\\epsilon)$ of the offline\noptimum.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 4 Sep 2010 20:54:58 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 29 Mar 2011 01:31:39 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 5 Apr 2011 15:50:05 GMT"
            },
            {
                "version": "v4",
                "created": "Thu, 2 Jun 2011 18:50:03 GMT"
            },
            {
                "version": "v5",
                "created": "Wed, 20 Jul 2011 23:26:41 GMT"
            },
            {
                "version": "v6",
                "created": "Fri, 7 Sep 2012 02:45:49 GMT"
            }
        ],
        "update_date": "2012-09-10",
        "authors_parsed": [
            [
                "Tan",
                "Bo",
                ""
            ],
            [
                "Srikant",
                "R.",
                ""
            ]
        ]
    },
    {
        "id": "1009.0932",
        "submitter": "Erhan Bayraktar",
        "authors": "Erhan Bayraktar, Yu-Jui Huang",
        "title": "On the Multi-Dimensional Controller and Stopper Games",
        "comments": "Key words: Controller-stopper games, weak dynamic programming\n  principle, viscosity solutions, robust optimal stopping, stopping strategies.\n  35 pages. Final version. To appear in the SIAM Journal on Control and\n  Optimization",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY math.PR q-fin.GN",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider a zero-sum stochastic differential controller-and-stopper game in\nwhich the state process is a controlled diffusion evolving in a\nmulti-dimensional Euclidean space. In this game, the controller affects both\nthe drift and the volatility terms of the state process. Under appropriate\nconditions, we show that the game has a value and the value function is the\nunique viscosity solution to an obstacle problem for a Hamilton-Jacobi-Bellman\nequation.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 5 Sep 2010 16:59:59 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 24 Sep 2010 21:00:04 GMT"
            },
            {
                "version": "v3",
                "created": "Sun, 3 Jul 2011 06:22:22 GMT"
            },
            {
                "version": "v4",
                "created": "Wed, 13 Jul 2011 19:24:29 GMT"
            },
            {
                "version": "v5",
                "created": "Tue, 6 Sep 2011 21:05:31 GMT"
            },
            {
                "version": "v6",
                "created": "Thu, 8 Sep 2011 01:37:32 GMT"
            },
            {
                "version": "v7",
                "created": "Wed, 1 Aug 2012 15:25:01 GMT"
            },
            {
                "version": "v8",
                "created": "Mon, 14 Jan 2013 00:31:16 GMT"
            }
        ],
        "update_date": "2013-01-15",
        "authors_parsed": [
            [
                "Bayraktar",
                "Erhan",
                ""
            ],
            [
                "Huang",
                "Yu-Jui",
                ""
            ]
        ]
    },
    {
        "id": "1009.1128",
        "submitter": "Joao Mota",
        "authors": "Jo\\~ao F. C. Mota, Jo\\~ao M. F. Xavier, Pedro M. Q. Aguiar, Markus\n  P\\\"uschel",
        "title": "Distributed Basis Pursuit",
        "comments": "Preprint of the journal version of the paper; IEEE Transactions on\n  Signal Processing, Vol. 60, Issue 4, April, 2012",
        "journal-ref": null,
        "doi": "10.1109/TSP.2011.2182347",
        "report-no": null,
        "categories": "math.OC cs.IT cs.SY math.IT",
        "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "abstract": "  We propose a distributed algorithm for solving the optimization problem Basis\nPursuit (BP). BP finds the least L1-norm solution of the underdetermined linear\nsystem Ax = b and is used, for example, in compressed sensing for\nreconstruction. Our algorithm solves BP on a distributed platform such as a\nsensor network, and is designed to minimize the communication between nodes.\nThe algorithm only requires the network to be connected, has no notion of a\ncentral processing node, and no node has access to the entire matrix A at any\ntime. We consider two scenarios in which either the columns or the rows of A\nare distributed among the compute nodes. Our algorithm, named D-ADMM, is a\ndecentralized implementation of the alternating direction method of\nmultipliers. We show through numerical simulation that our algorithm requires\nconsiderably less communications between the nodes than the state-of-the-art\nalgorithms.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 6 Sep 2010 19:09:50 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 15 Jul 2011 22:36:07 GMT"
            },
            {
                "version": "v3",
                "created": "Wed, 14 Mar 2012 15:17:51 GMT"
            }
        ],
        "update_date": "2012-03-15",
        "authors_parsed": [
            [
                "Mota",
                "Jo\u00e3o F. C.",
                ""
            ],
            [
                "Xavier",
                "Jo\u00e3o M. F.",
                ""
            ],
            [
                "Aguiar",
                "Pedro M. Q.",
                ""
            ],
            [
                "P\u00fcschel",
                "Markus",
                ""
            ]
        ]
    },
    {
        "id": "1009.2032",
        "submitter": "Julio Braslavsky",
        "authors": "Hernan Haimovich and Julio H. Braslavsky",
        "title": "Feedback stabilisation of switched systems via iterative approximate\n  eigenvector assignment",
        "comments": "Extended version of a paper to appear in the 49th IEEE Conference on\n  Decision and Control, Atlanta, Georgia, USA, 2010",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SY math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents and implements an iterative feedback design algorithm for\nstabilisation of discrete-time switched systems under arbitrary switching\nregimes. The algorithm seeks state feedback gains so that the closed-loop\nswitching system admits a common quadratic Lyapunov function (CQLF) and hence\nis uniformly globally exponentially stable. Although the feedback design\nproblem considered can be solved directly via linear matrix inequalities\n(LMIs), direct application of LMIs for feedback design does not provide\ninformation on closed-loop system structure. In contrast, the feedback matrices\ncomputed by the proposed algorithm assign closed-loop structure approximating\nthat required to satisfy Lie-algebraic conditions that guarantee existence of a\nCQLF. The main contribution of the paper is to provide, for single-input\nsystems, a numerical implementation of the algorithm based on iterative\napproximate common eigenvector assignment, and to establish cases where such\nalgorithm is guaranteed to succeed. We include pseudocode and a few numerical\nexamples to illustrate advantages and limitations of the proposed technique.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 10 Sep 2010 15:22:03 GMT"
            }
        ],
        "update_date": "2010-09-13",
        "authors_parsed": [
            [
                "Haimovich",
                "Hernan",
                ""
            ],
            [
                "Braslavsky",
                "Julio H.",
                ""
            ]
        ]
    },
    {
        "id": "1009.2528",
        "submitter": "Pulkit Grover",
        "authors": "Pulkit Grover and Anant Sahai",
        "title": "Is Witsenhausen's counterexample a relevant toy?",
        "comments": "preprint for paper that will appear in proceedings of 49th IEEE\n  Conference on Decision and Control (CDC) 2010, Atlanta, Georgia",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.SY math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper answers a question raised by Doyle on the relevance of the\nWitsenhausen counterexample as a toy decentralized control problem. The\nquestion has two sides, the first of which focuses on the lack of an external\nchannel in the counterexample. Using existing results, we argue that the core\ndifficulty in the counterexample is retained even in the presence of such a\nchannel. The second side questions the LQG formulation of the counterexample.\nWe consider alternative formulations and show that the understanding developed\nfor the LQG case guides the investigation for these other cases as well.\nSpecifically, we consider 1) a variation on the original counterexample with\ngeneral, but bounded, noise distributions, and 2) an adversarial extension with\nbounded disturbance and quadratic costs. For each of these formulations, we\nshow that quantization-based nonlinear strategies outperform linear strategies\nby an arbitrarily large factor. Further, these nonlinear strategies also\nperform within a constant factor of the optimal, uniformly over all possible\nparameter choices (for fixed noise distributions in the Bayesian case).\n  Fortuitously, the assumption of bounded noise results in a significant\nsimplification of proofs as compared to those for the LQG formulation.\nTherefore, the results in this paper are also of pedagogical interest.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 13 Sep 2010 21:48:22 GMT"
            }
        ],
        "update_date": "2010-09-15",
        "authors_parsed": [
            [
                "Grover",
                "Pulkit",
                ""
            ],
            [
                "Sahai",
                "Anant",
                ""
            ]
        ]
    },
    {
        "id": "1009.2653",
        "submitter": "Giacomo Como",
        "authors": "Daron Acemoglu, Giacomo Como, Fabio Fagnani, Asuman Ozdaglar",
        "title": "Opinion fluctuations and disagreement in social networks",
        "comments": "33 pages, accepted for publication in Mathematics of Operation\n  Research",
        "journal-ref": null,
        "doi": "10.1287/moor.1120.0570",
        "report-no": null,
        "categories": "cs.SI cs.SY math.OC math.PR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study a tractable opinion dynamics model that generates long-run\ndisagreements and persistent opinion fluctuations. Our model involves an\ninhomogeneous stochastic gossip process of continuous opinion dynamics in a\nsociety consisting of two types of agents: regular agents, who update their\nbeliefs according to information that they receive from their social neighbors;\nand stubborn agents, who never update their opinions. When the society contains\nstubborn agents with different opinions, the belief dynamics never lead to a\nconsensus (among the regular agents). Instead, beliefs in the society fail to\nconverge almost surely, the belief profile keeps on fluctuating in an ergodic\nfashion, and it converges in law to a non-degenerate random vector. The\nstructure of the network and the location of the stubborn agents within it\nshape the opinion dynamics. The expected belief vector evolves according to an\nordinary differential equation coinciding with the Kolmogorov backward equation\nof a continuous-time Markov chain with absorbing states corresponding to the\nstubborn agents and converges to a harmonic vector, with every regular agent's\nvalue being the weighted average of its neighbors' values, and boundary\nconditions corresponding to the stubborn agents'. Expected cross-products of\nthe agents' beliefs allow for a similar characterization in terms of coupled\nMarkov chains on the network. We prove that, in large-scale societies which are\nhighly fluid, meaning that the product of the mixing time of the Markov chain\non the graph describing the social network and the relative size of the\nlinkages to stubborn agents vanishes as the population size grows large, a\ncondition of \\emph{homogeneous influence} emerges, whereby the stationary\nbeliefs' marginal distributions of most of the regular agents have\napproximately equal first and second moments.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 14 Sep 2010 13:18:22 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 27 Jul 2011 09:41:47 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 28 Jul 2011 12:08:30 GMT"
            },
            {
                "version": "v4",
                "created": "Sat, 13 Oct 2012 09:32:12 GMT"
            }
        ],
        "update_date": "2012-12-03",
        "authors_parsed": [
            [
                "Acemoglu",
                "Daron",
                ""
            ],
            [
                "Como",
                "Giacomo",
                ""
            ],
            [
                "Fagnani",
                "Fabio",
                ""
            ],
            [
                "Ozdaglar",
                "Asuman",
                ""
            ]
        ]
    },
    {
        "id": "1009.3088",
        "submitter": "Byung-Gon Chun",
        "authors": "Byung-Gon Chun, Sunghwan Ihm, Petros Maniatis, Mayur Naik",
        "title": "CloneCloud: Boosting Mobile Device Applications Through Cloud Clone\n  Execution",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Mobile applications are becoming increasingly ubiquitous and provide ever\nricher functionality on mobile devices. At the same time, such devices often\nenjoy strong connectivity with more powerful machines ranging from laptops and\ndesktops to commercial clouds. This paper presents the design and\nimplementation of CloneCloud, a system that automatically transforms mobile\napplications to benefit from the cloud. The system is a flexible application\npartitioner and execution runtime that enables unmodified mobile applications\nrunning in an application-level virtual machine to seamlessly off-load part of\ntheir execution from mobile devices onto device clones operating in a\ncomputational cloud. CloneCloud uses a combination of static analysis and\ndynamic profiling to optimally and automatically partition an application so\nthat it migrates, executes in the cloud, and re-integrates computation in a\nfine-grained manner that makes efficient use of resources. Our evaluation shows\nthat CloneCloud can achieve up to 21.2x speedup of smartphone applications we\ntested and it allows different partitioning for different inputs and networks.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 16 Sep 2010 04:43:46 GMT"
            },
            {
                "version": "v2",
                "created": "Sun, 26 Sep 2010 01:40:36 GMT"
            }
        ],
        "update_date": "2010-09-28",
        "authors_parsed": [
            [
                "Chun",
                "Byung-Gon",
                ""
            ],
            [
                "Ihm",
                "Sunghwan",
                ""
            ],
            [
                "Maniatis",
                "Petros",
                ""
            ],
            [
                "Naik",
                "Mayur",
                ""
            ]
        ]
    },
    {
        "id": "1009.3455",
        "submitter": "Carlo Alberto Furia",
        "authors": "Carlo A. Furia, Alberto Leva, Martina Maggio, Paola Spoletini",
        "title": "A control-theoretical methodology for the scheduling problem",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper presents a novel methodology to develop scheduling algorithms. The\nscheduling problem is phrased as a control problem, and control-theoretical\ntechniques are used to design a scheduling algorithm that meets specific\nrequirements. Unlike most approaches to feedback scheduling, where a controller\nintegrates a \"basic\" scheduling algorithm and dynamically tunes its parameters\nand hence its performances, our methodology essentially reduces the design of a\nscheduling algorithm to the synthesis of a controller that closes the feedback\nloop. This approach allows the re-use of control-theoretical techniques to\ndesign efficient scheduling algorithms; it frames and solves the scheduling\nproblem in a general setting; and it can naturally tackle certain peculiar\nrequirements such as robustness and dynamic performance tuning. A few\nexperiments demonstrate the feasibility of the approach on a real-time\nbenchmark.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 17 Sep 2010 15:44:58 GMT"
            }
        ],
        "update_date": "2010-09-20",
        "authors_parsed": [
            [
                "Furia",
                "Carlo A.",
                ""
            ],
            [
                "Leva",
                "Alberto",
                ""
            ],
            [
                "Maggio",
                "Martina",
                ""
            ],
            [
                "Spoletini",
                "Paola",
                ""
            ]
        ]
    },
    {
        "id": "1009.3888",
        "submitter": "Chang-Ching Chen",
        "authors": "Chang-Ching Chen and Chia-Shiang Tseng and Che Lin",
        "title": "A General Proof of Convergence for Adaptive Distributed Beamforming\n  Schemes",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This work focuses on the convergence analysis of adaptive distributed\nbeamforming schemes that can be reformulated as local random search algorithms\nvia a random search framework. Once reformulated as local random search\nalgorithms, it is proved that under two sufficient conditions: a) the objective\nfunction of the algorithm is continuous and all its local maxima are global\nmaxima, and b) the origin is an interior point within the range of the\nconsidered transformation of the random perturbation, the corresponding\nadaptive distributed beamforming schemes converge both in probability and in\nmean. This proof of convergence is general since it can be applied to analyze\nrandomized adaptive distributed beamforming schemes with any type of objective\nfunctions and probability measures as long as both the sufficient conditions\nare satisfied. Further, this framework can be generalized to analyze an\nasynchronous scheme where distributed transmitters can only update their\nbeamforming coefficients asynchronously. Simulation results are also provided\nto validate our analyses.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 20 Sep 2010 17:07:57 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 24 Sep 2010 15:36:48 GMT"
            },
            {
                "version": "v3",
                "created": "Thu, 30 Sep 2010 08:27:28 GMT"
            },
            {
                "version": "v4",
                "created": "Wed, 6 Oct 2010 03:59:19 GMT"
            },
            {
                "version": "v5",
                "created": "Fri, 15 Oct 2010 04:49:48 GMT"
            },
            {
                "version": "v6",
                "created": "Fri, 22 Oct 2010 05:45:33 GMT"
            },
            {
                "version": "v7",
                "created": "Wed, 9 Feb 2011 09:09:43 GMT"
            }
        ],
        "update_date": "2011-02-10",
        "authors_parsed": [
            [
                "Chen",
                "Chang-Ching",
                ""
            ],
            [
                "Tseng",
                "Chia-Shiang",
                ""
            ],
            [
                "Lin",
                "Che",
                ""
            ]
        ]
    },
    {
        "id": "1009.3961",
        "submitter": "Marco Levorato",
        "authors": "Marco Levorato, Daniel O'Neill, Andrea Goldsmith and Urbashi Mitra",
        "title": "Optimization of ARQ Protocols in Interference Networks with QoS\n  Constraints",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SY cs.NI",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study optimal transmission strategies in interfering wireless networks,\nunder Quality of Service constraints. A buffered, dynamic network with multiple\nsources is considered, and sources use a retransmission strategy in order to\nimprove packet delivery probability. The optimization problem is formulated as\na Markov Decision Process, where constraints and objective functions are ratios\nof time-averaged cost functions. The optimal strategy is found as the solution\nof a Linear Fractional Program, where the optimization variables are the\nsteady-state probability of state-action pairs. Numerical results illustrate\nthe dependence of optimal transmission/interference strategies on the\nconstraints imposed on the network.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 20 Sep 2010 22:30:57 GMT"
            }
        ],
        "update_date": "2010-09-22",
        "authors_parsed": [
            [
                "Levorato",
                "Marco",
                ""
            ],
            [
                "O'Neill",
                "Daniel",
                ""
            ],
            [
                "Goldsmith",
                "Andrea",
                ""
            ],
            [
                "Mitra",
                "Urbashi",
                ""
            ]
        ]
    },
    {
        "id": "1009.4219",
        "submitter": "Tarek Rabbani",
        "authors": "Laurent El Ghaoui, Vivian Viallon, Tarek Rabbani",
        "title": "Safe Feature Elimination for the LASSO and Sparse Supervised Learning\n  Problems",
        "comments": "Submitted to JMLR in April 2011",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.LG cs.SY math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We describe a fast method to eliminate features (variables) in l1 -penalized\nleast-square regression (or LASSO) problems. The elimination of features leads\nto a potentially substantial reduction in running time, specially for large\nvalues of the penalty parameter. Our method is not heuristic: it only\neliminates features that are guaranteed to be absent after solving the LASSO\nproblem. The feature elimination step is easy to parallelize and can test each\nfeature for elimination independently. Moreover, the computational effort of\nour method is negligible compared to that of solving the LASSO problem -\nroughly it is the same as single gradient step. Our method extends the scope of\nexisting LASSO algorithms to treat larger data sets, previously out of their\nreach. We show how our method can be extended to general l1 -penalized convex\nproblems and present preliminary results for the Sparse Support Vector Machine\nand Logistic Regression problems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 21 Sep 2010 21:13:15 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 18 May 2011 16:38:10 GMT"
            }
        ],
        "update_date": "2011-05-19",
        "authors_parsed": [
            [
                "Ghaoui",
                "Laurent El",
                ""
            ],
            [
                "Viallon",
                "Vivian",
                ""
            ],
            [
                "Rabbani",
                "Tarek",
                ""
            ]
        ]
    },
    {
        "id": "1009.5055",
        "submitter": "Zhouchen Lin",
        "authors": "Zhouchen Lin, Minming Chen, Yi Ma",
        "title": "The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted\n  Low-Rank Matrices",
        "comments": "Please cite \"Zhouchen Lin, Risheng Liu, and Zhixun Su, Linearized\n  Alternating Direction Method with Adaptive Penalty for Low Rank\n  Representation, NIPS 2011.\" (available at arXiv:1109.0367) instead for a more\n  general method called Linearized Alternating Direction Method This manuscript\n  first appeared as University of Illinois at Urbana-Champaign technical report\n  #UILU-ENG-09-2215 in October 2009 Zhouchen Lin, Risheng Liu, and Zhixun Su,\n  Linearized Alternating Direction Method with Adaptive Penalty for Low Rank\n  Representation, NIPS 2011. (available at http://arxiv.org/abs/1109.0367)",
        "journal-ref": null,
        "doi": "10.1016/j.jsb.2012.10.010",
        "report-no": null,
        "categories": "math.OC cs.NA cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes scalable and fast algorithms for solving the Robust PCA\nproblem, namely recovering a low-rank matrix with an unknown fraction of its\nentries being arbitrarily corrupted. This problem arises in many applications,\nsuch as image processing, web data ranking, and bioinformatic data analysis. It\nwas recently shown that under surprisingly broad conditions, the Robust PCA\nproblem can be exactly solved via convex optimization that minimizes a\ncombination of the nuclear norm and the $\\ell^1$-norm . In this paper, we apply\nthe method of augmented Lagrange multipliers (ALM) to solve this convex\nprogram. As the objective function is non-smooth, we show how to extend the\nclassical analysis of ALM to such new objective functions and prove the\noptimality of the proposed algorithms and characterize their convergence rate.\nEmpirically, the proposed new algorithms can be more than five times faster\nthan the previous state-of-the-art algorithms for Robust PCA, such as the\naccelerated proximal gradient (APG) algorithm. Moreover, the new algorithms\nachieve higher precision, yet being less storage/memory demanding. We also show\nthat the ALM technique can be used to solve the (related but somewhat simpler)\nmatrix completion problem and obtain rather promising results too. We further\nprove the necessary and sufficient condition for the inexact ALM to converge\nglobally. Matlab code of all algorithms discussed are available at\nhttp://perception.csl.illinois.edu/matrix-rank/home.html\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 26 Sep 2010 03:42:27 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 9 Mar 2011 04:19:52 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 18 Oct 2013 13:42:57 GMT"
            }
        ],
        "update_date": "2013-10-21",
        "authors_parsed": [
            [
                "Lin",
                "Zhouchen",
                ""
            ],
            [
                "Chen",
                "Minming",
                ""
            ],
            [
                "Ma",
                "Yi",
                ""
            ]
        ]
    },
    {
        "id": "1009.5094",
        "submitter": "Alain Rapaport",
        "authors": "Pedro Gajardo, J\\'er\\^ome Harmand (MISTEA, Lbe), Hector Ramirez\n  Cabrera (CMM), Alain Rapaport (MISTEA)",
        "title": "Minimal-time bioremediation of natural water resources",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY math.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study minimal time strategies for the treatment of pollution of large\nvolumes, such as lakes or natural reservoirs, with the help of an autonomous\nbioreactor. The control consists in feeding the bioreactor from the resource,\nthe clean output returning to the resource with the same flow rate. We first\ncharacterize the optimal policies among constant and feedback controls, under\nthe assumption of a uniform concentration in the resource. In a second part, we\nstudy the influence of an inhomogeneity in the resource, considering two\nmeasurements points. With the help of the Maximum Principle, we show that the\noptimal control law is non-monotonic and terminates with a constant phase,\ncontrary to the homogeneous case for which the optimal flow rate is decreasing\nwith time. This study allows the decision makers to identify situations for\nwhich the benefit of using non-constant flow rates is significant.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 26 Sep 2010 14:07:07 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 23 May 2011 07:37:49 GMT"
            }
        ],
        "update_date": "2011-05-26",
        "authors_parsed": [
            [
                "Gajardo",
                "Pedro",
                "",
                "MISTEA, Lbe"
            ],
            [
                "Harmand",
                "J\u00e9r\u00f4me",
                "",
                "MISTEA, Lbe"
            ],
            [
                "Cabrera",
                "Hector Ramirez",
                "",
                "CMM"
            ],
            [
                "Rapaport",
                "Alain",
                "",
                "MISTEA"
            ]
        ]
    },
    {
        "id": "1009.5208",
        "submitter": "Adolfo Anta",
        "authors": "Adolfo Anta, Paulo Tabuada",
        "title": "Exploiting isochrony in self-triggered control",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY math.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Event-triggered control and self-triggered control have been recently\nproposed as new implementation paradigms that reduce resource usage for control\nsystems. In self-triggered control, the controller is augmented with the\ncomputation of the next time instant at which the feedback control law is to be\nrecomputed. Since these execution instants are obtained as a function of the\nplant state, we effectively close the loop only when it is required to maintain\nthe desired performance, thereby greatly reducing the resources required for\ncontrol. In this paper we present a new technique for the computation of the\nexecution instants by exploiting the concept of isochronous manifolds, also\nintroduced in this paper. While our previous results showed how homogeneity can\nbe used to compute the execution instants along some directions in the state\nspace, the concept of isochrony allows us to compute the executions instants\nalong every direction in the state space. Moreover, we also show in this paper\nhow to homogenize smooth control systems thus making our results applicable to\nany smooth control system. The benefits of the proposed approach with respect\nto existing techniques are analyzed in two examples.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 27 Sep 2010 10:01:58 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 26 Apr 2011 13:32:26 GMT"
            }
        ],
        "update_date": "2011-04-27",
        "authors_parsed": [
            [
                "Anta",
                "Adolfo",
                ""
            ],
            [
                "Tabuada",
                "Paulo",
                ""
            ]
        ]
    },
    {
        "id": "1009.5614",
        "submitter": "Ian Manchester",
        "authors": "Ian R. Manchester",
        "title": "Input Design for System Identification via Convex Relaxation",
        "comments": "Preprint submitted for journal publication, extended version of a\n  paper at 2010 IEEE Conference on Decision and Control",
        "journal-ref": null,
        "doi": "10.1109/CDC.2010.5717097",
        "report-no": null,
        "categories": "math.OC cs.SY math.ST stat.TH",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper proposes a new framework for the optimization of excitation inputs\nfor system identification. The optimization problem considered is to maximize a\nreduced Fisher information matrix in any of the classical D-, E-, or A-optimal\nsenses. In contrast to the majority of published work on this topic, we\nconsider the problem in the time domain and subject to constraints on the\namplitude of the input signal. This optimization problem is nonconvex. The main\nresult of the paper is a convex relaxation that gives an upper bound accurate\nto within $2/\\pi$ of the true maximum. A randomized algorithm is presented for\nfinding a feasible solution which, in a certain sense is expected to be at\nleast $2/\\pi$ as informative as the globally optimal input signal. In the case\nof a single constraint on input power, the proposed approach recovers the true\nglobal optimum exactly. Extensions to situations with both power and amplitude\nconstraints on both inputs and outputs are given. A simple simulation example\nillustrates the technique.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 28 Sep 2010 16:07:15 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Manchester",
                "Ian R.",
                ""
            ]
        ]
    },
    {
        "id": "1010.0034",
        "submitter": "Victor M. Preciado",
        "authors": "Michael M. Zavlanos, Victor M. Preciado and Ali Jadbabaie",
        "title": "Spectral Control of Mobile Robot Networks",
        "comments": "http://alum.mit.edu/www/vmp",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.MA cs.SY math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The eigenvalue spectrum of the adjacency matrix of a network is closely\nrelated to the behavior of many dynamical processes run over the network. In\nthe field of robotics, this spectrum has important implications in many\nproblems that require some form of distributed coordination within a team of\nrobots. In this paper, we propose a continuous-time control scheme that\nmodifies the structure of a position-dependent network of mobile robots so that\nit achieves a desired set of adjacency eigenvalues. For this, we employ a novel\nabstraction of the eigenvalue spectrum by means of the adjacency matrix\nspectral moments. Since the eigenvalue spectrum is uniquely determined by its\nspectral moments, this abstraction provides a way to indirectly control the\neigenvalues of the network. Our construction is based on artificial potentials\nthat capture the distance of the network's spectral moments to their desired\nvalues. Minimization of these potentials is via a gradient descent closed-loop\nsystem that, under certain convexity assumptions, ensures convergence of the\nnetwork topology to one with the desired set of moments and, therefore,\neigenvalues. We illustrate our approach in nontrivial computer simulations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 30 Sep 2010 23:03:07 GMT"
            }
        ],
        "update_date": "2010-10-04",
        "authors_parsed": [
            [
                "Zavlanos",
                "Michael M.",
                ""
            ],
            [
                "Preciado",
                "Victor M.",
                ""
            ],
            [
                "Jadbabaie",
                "Ali",
                ""
            ]
        ]
    },
    {
        "id": "1010.0066",
        "submitter": "Paolo Frasca",
        "authors": "Francesca Ceragioli and Paolo Frasca",
        "title": "Continuous-time Discontinuous Equations in Bounded Confidence Opinion\n  Dynamics",
        "comments": "16 pages, no figures - v4: revised proof of Theorem 2",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SI cs.SY math.DS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This report studies a continuous-time version of the well-known\nHegselmann-Krause model of opinion dynamics with bounded confidence. As the\nequations of this model have discontinuous right-hand side, we study their\nKrasovskii solutions. We present results about existence and completeness of\nsolutions, and asymptotical convergence to equilibria featuring a\n\"clusterization\" of opinions. The robustness of such equilibria to small\nperturbations is also studied.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 1 Oct 2010 05:34:35 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 12 Oct 2010 03:48:56 GMT"
            },
            {
                "version": "v3",
                "created": "Tue, 4 Oct 2011 13:30:55 GMT"
            },
            {
                "version": "v4",
                "created": "Thu, 17 Nov 2011 12:55:34 GMT"
            }
        ],
        "update_date": "2011-11-18",
        "authors_parsed": [
            [
                "Ceragioli",
                "Francesca",
                ""
            ],
            [
                "Frasca",
                "Paolo",
                ""
            ]
        ]
    },
    {
        "id": "1010.0609",
        "submitter": "George Theodorakopoulos",
        "authors": "George Theodorakopoulos, Jean-Yves Le Boudec, John S. Baras",
        "title": "Selfish Response to Epidemic Propagation",
        "comments": "19 pages, 5 figures, submitted to the IEEE Transactions on Automatic\n  Control",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SY cs.MA nlin.AO",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An epidemic spreading in a network calls for a decision on the part of the\nnetwork members: They should decide whether to protect themselves or not. Their\ndecision depends on the trade-off between their perceived risk of being\ninfected and the cost of being protected. The network members can make\ndecisions repeatedly, based on information that they receive about the changing\ninfection level in the network.\n  We study the equilibrium states reached by a network whose members increase\n(resp. decrease) their security deployment when learning that the network\ninfection is widespread (resp. limited). Our main finding is that the\nequilibrium level of infection increases as the learning rate of the members\nincreases. We confirm this result in three scenarios for the behavior of the\nmembers: strictly rational cost minimizers, not strictly rational, and strictly\nrational but split into two response classes. In the first two cases, we\ncompletely characterize the stability and the domains of attraction of the\nequilibrium points, even though the first case leads to a differential\ninclusion. We validate our conclusions with simulations on human mobility\ntraces.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 4 Oct 2010 14:51:58 GMT"
            }
        ],
        "update_date": "2015-03-17",
        "authors_parsed": [
            [
                "Theodorakopoulos",
                "George",
                ""
            ],
            [
                "Boudec",
                "Jean-Yves Le",
                ""
            ],
            [
                "Baras",
                "John S.",
                ""
            ]
        ]
    },
    {
        "id": "1010.0696",
        "submitter": "Masoud Abbaszadeh",
        "authors": "Masoud Abbaszadeh and Horacio J. Marquez",
        "title": "Robust H_infinity Filter Design for Lipschitz Nonlinear Systems via\n  Multiobjective Optimization",
        "comments": "20 pages, 5 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SY math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, a new method of H_infinity observer design for Lipschitz\nnonlinear systems is proposed in the form of an LMI optimization problem. The\nproposed observer has guaranteed decay rate (exponential convergence) and is\nrobust against unknown exogenous disturbance. In addition, thanks to the\nlinearity of the proposed LMIs in the admissible Lipschitz constant, it can be\nmaximized via LMI optimization. This adds an extra important feature to the\nobserver, robustness against nonlinear uncertainty. Explicit bound on the\ntolerable nonlinear uncertainty is derived. The new LMI formulation also allows\noptimizations over the disturbance attenuation level (H_infinity cost). Then,\nthe admissible Lipschitz constant and the disturbance attenuation level of the\nH_infinity filter are simultaneously optimized through LMI multiobjective\noptimization.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 4 Oct 2010 20:25:53 GMT"
            }
        ],
        "update_date": "2010-10-06",
        "authors_parsed": [
            [
                "Abbaszadeh",
                "Masoud",
                ""
            ],
            [
                "Marquez",
                "Horacio J.",
                ""
            ]
        ]
    },
    {
        "id": "1010.1149",
        "submitter": "Marco Spadini",
        "authors": "Laura Poggiolini and Marco Spadini",
        "title": "Bang--bang trajectories with a double switching time: sufficient strong\n  local optimality conditions",
        "comments": "43 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper gives sufficient conditions for a class of bang-bang extremals\nwith multiple switches to be locally optimal in the strong topology. The\nconditions are the natural generalizations of the ones considered in previous\npapers for more specific cases. We require both the strict bang-bang Legendre\ncondition, and the second order conditions for the finite dimensional problem\nobtained by moving the switching times of the reference trajectory.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 6 Oct 2010 13:29:54 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 4 Mar 2011 17:36:11 GMT"
            }
        ],
        "update_date": "2011-03-07",
        "authors_parsed": [
            [
                "Poggiolini",
                "Laura",
                ""
            ],
            [
                "Spadini",
                "Marco",
                ""
            ]
        ]
    },
    {
        "id": "1010.1438",
        "submitter": "Walid Saad",
        "authors": "Walid Saad and Zhu Han and Are Hj{\\o}rungnes and Dusit Niyato and\n  Ekram Hossain",
        "title": "Coalition Formation Games for Distributed Cooperation Among Roadside\n  Units in Vehicular Networks",
        "comments": "accepted and to appear in IEEE Journal on Selected Areas in\n  Communications (JSAC), Special issue on Vehicular Communications and Networks",
        "journal-ref": "IEEE Journal on Selected Areas in Communications (JSAC), 2010",
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.GT cs.SY math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Vehicle-to-roadside (V2R) communications enable vehicular networks to support\na wide range of applications for enhancing the efficiency of road\ntransportation. While existing work focused on non-cooperative techniques for\nV2R communications between vehicles and roadside units (RSUs), this paper\ninvestigates novel cooperative strategies among the RSUs in a vehicular\nnetwork. We propose a scheme whereby, through cooperation, the RSUs in a\nvehicular network can coordinate the classes of data being transmitted through\nV2R communications links to the vehicles. This scheme improves the diversity of\nthe information circulating in the network while exploiting the underlying\ncontent-sharing vehicle-to-vehicle communication network. We model the problem\nas a coalition formation game with transferable utility and we propose an\nalgorithm for forming coalitions among the RSUs. For coalition formation, each\nRSU can take an individual decision to join or leave a coalition, depending on\nits utility which accounts for the generated revenues and the costs for\ncoalition coordination. We show that the RSUs can self-organize into a\nNash-stable partition and adapt this partition to environmental changes.\nSimulation results show that, depending on different scenarios, coalition\nformation presents a performance improvement, in terms of the average payoff\nper RSU, ranging between 20.5% and 33.2%, relative to the non-cooperative case.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 7 Oct 2010 14:18:26 GMT"
            }
        ],
        "update_date": "2010-10-08",
        "authors_parsed": [
            [
                "Saad",
                "Walid",
                ""
            ],
            [
                "Han",
                "Zhu",
                ""
            ],
            [
                "Hj\u00f8rungnes",
                "Are",
                ""
            ],
            [
                "Niyato",
                "Dusit",
                ""
            ],
            [
                "Hossain",
                "Ekram",
                ""
            ]
        ]
    },
    {
        "id": "1010.1622",
        "submitter": "Ming Zhang nudt",
        "authors": "Ming Zhang, Zairong Xi, Jia-Hua Wei",
        "title": "Manipulating quantum information on the controllable systems or\n  subspaces",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "quant-ph cs.SY math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we explore how to constructively manipulate qubits by rotating\nBloch spheres. It is revealed that three-rotation and one-rotation Hamiltonian\ncontrols can be constructed to steer qubits when two tunable Hamiltonian\ncontrols are available. It is demonstrated in this research that local-wave\nfunction controls such as Bang-Bang, triangle-function and quadratic function\ncontrols can be utilized to manipulate quantum states on the Bloch sphere. A\nnew kind of time-energy performance index is proposed to trade-off time and\nenergy resource cost, in which control magnitudes are optimized in terms of\nthis kind of performance. It is further exemplified that this idea can be\ngeneralized to manipulate encoded qubits on the controllable subspace.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 8 Oct 2010 08:00:05 GMT"
            },
            {
                "version": "v2",
                "created": "Thu, 16 Jun 2011 14:07:09 GMT"
            }
        ],
        "update_date": "2011-06-17",
        "authors_parsed": [
            [
                "Zhang",
                "Ming",
                ""
            ],
            [
                "Xi",
                "Zairong",
                ""
            ],
            [
                "Wei",
                "Jia-Hua",
                ""
            ]
        ]
    },
    {
        "id": "1010.1862",
        "submitter": "Longbo Huang",
        "authors": "Longbo Huang, Michael J. Neely",
        "title": "Utility Optimal Scheduling in Processing Networks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We consider the problem of utility optimal scheduling in general\n\\emph{processing networks} with random arrivals and network conditions. These\nare generalizations of traditional data networks where commodities in one or\nmore queues can be combined to produce new commodities that are delivered to\nother parts of the network. This can be used to model problems such as\nin-network data fusion, stream processing, and grid computing. Scheduling\nactions are complicated by the \\emph{underflow problem} that arises when some\nqueues with required components go empty. In this paper, we develop the\nPerturbed Max-Weight algorithm (PMW) to achieve optimal utility. The idea of\nPMW is to perturb the weights used by the usual Max-Weight algorithm to\n``push'' queue levels towards non-zero values (avoiding underflows). We show\nthat when the perturbations are carefully chosen, PMW is able to achieve a\nutility that is within $O(1/V)$ of the optimal value for any $V\\geq1$, while\nensuring an average network backlog of $O(V)$.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 9 Oct 2010 18:07:55 GMT"
            }
        ],
        "update_date": "2010-10-12",
        "authors_parsed": [
            [
                "Huang",
                "Longbo",
                ""
            ],
            [
                "Neely",
                "Michael J.",
                ""
            ]
        ]
    },
    {
        "id": "1010.1973",
        "submitter": "Stefano Galli",
        "authors": "Stefano Galli, Anna Scaglione, Zhifang Wang",
        "title": "For the Grid and Through the Grid: The Role of Power Line Communications\n  in the Smart Grid",
        "comments": "26 pages, 6 figures, 2 tables",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.IT cs.SY math.IT",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Is Power Line Communications (PLC) a good candidate for Smart Grid\napplications? The objective of this paper is to address this important\nquestion. To do so we provide an overview of what PLC can deliver today by\nsurveying its history and describing the most recent technological advances in\nthe area. We then address Smart Grid applications as instances of sensor\nnetworking and network control problems and discuss the main conclusion one can\ndraw from the literature on these subjects. The application scenario of PLC\nwithin the Smart Grid is then analyzed in detail. Since a necessary ingredient\nof network planning is modeling, we also discuss two aspects of engineering\nmodeling that relate to our question. The first aspect is modeling the PLC\nchannel through fading models. The second aspect we review is the Smart Grid\ncontrol and traffic modeling problem which allows us to achieve a better\nunderstanding of the communications requirements. Finally, this paper reports\nrecent studies on the electrical and topological properties of a sample power\ndistribution network. Power grid topological studies are very important for PLC\nnetworking as the power grid is not only the information source \\textit{but\nalso} the information delivery system - a unique feature when PLC is used for\nthe Smart Grid.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 10 Oct 2010 23:32:11 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 12 Jan 2011 19:33:27 GMT"
            }
        ],
        "update_date": "2011-01-13",
        "authors_parsed": [
            [
                "Galli",
                "Stefano",
                ""
            ],
            [
                "Scaglione",
                "Anna",
                ""
            ],
            [
                "Wang",
                "Zhifang",
                ""
            ]
        ]
    },
    {
        "id": "1010.2128",
        "submitter": "Moslem Rashidi  Avendi",
        "authors": "Moslem Rashidi, Sara Mansouri",
        "title": "Parameter Selection in Periodic Nonuniform Sampling of Multiband Signals",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Periodic nonuniform sampling has been considered in literature as an\neffective approach to reduce the sampling rate far below the Nyquist rate for\nsparse spectrum multiband signals. In the presence of non-ideality the sampling\nparameters play an important role on the quality of reconstructed signal. Also\nthe average sampling ratio is directly dependent on the sampling parameters\nthat they should be chosen for a minimum rate and complexity. In this paper we\nconsider the effect of sampling parameters on the reconstruction error and the\nsampling ratio and suggest feasible approaches for achieving an optimal\nsampling and reconstruction.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 11 Oct 2010 15:07:07 GMT"
            }
        ],
        "update_date": "2010-10-12",
        "authors_parsed": [
            [
                "Rashidi",
                "Moslem",
                ""
            ],
            [
                "Mansouri",
                "Sara",
                ""
            ]
        ]
    },
    {
        "id": "1010.2247",
        "submitter": "Ian Manchester",
        "authors": "Ian R. Manchester, Mark M. Tobenkin, Michael Levashov, Russ Tedrake",
        "title": "Regions of Attraction for Hybrid Limit Cycles of Walking Robots",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "math.OC cs.RO cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper illustrates the application of recent research in\nregion-of-attraction analysis for nonlinear hybrid limit cycles. Three example\nsystems are analyzed in detail: the van der Pol oscillator, the \"rimless\nwheel\", and the \"compass gait\", the latter two being simplified models of\nunderactuated walking robots. The method used involves decomposition of the\ndynamics about the target cycle into tangential and transverse components, and\na search for a Lyapunov function in the transverse dynamics using\nsum-of-squares analysis (semidefinite programming). Each example illuminates\ndifferent aspects of the procedure, including optimization of transversal\nsurfaces, the handling of impact maps, optimization of the Lyapunov function,\nand orbitally-stabilizing control design.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 11 Oct 2010 21:36:52 GMT"
            }
        ],
        "update_date": "2010-10-13",
        "authors_parsed": [
            [
                "Manchester",
                "Ian R.",
                ""
            ],
            [
                "Tobenkin",
                "Mark M.",
                ""
            ],
            [
                "Levashov",
                "Michael",
                ""
            ],
            [
                "Tedrake",
                "Russ",
                ""
            ]
        ]
    },
    {
        "id": "1010.2285",
        "submitter": "Maxim Raginsky",
        "authors": "Maxim Raginsky and Alexander Rakhlin",
        "title": "Information-based complexity, feedback and dynamics in convex\n  programming",
        "comments": "final version; to appear in IEEE Transactions on Information Theory",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.IT cs.SY math.IT math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We study the intrinsic limitations of sequential convex optimization through\nthe lens of feedback information theory. In the oracle model of optimization,\nan algorithm queries an {\\em oracle} for noisy information about the unknown\nobjective function, and the goal is to (approximately) minimize every function\nin a given class using as few queries as possible. We show that, in order for a\nfunction to be optimized, the algorithm must be able to accumulate enough\ninformation about the objective. This, in turn, puts limits on the speed of\noptimization under specific assumptions on the oracle and the type of feedback.\nOur techniques are akin to the ones used in statistical literature to obtain\nminimax lower bounds on the risks of estimation procedures; the notable\ndifference is that, unlike in the case of i.i.d. data, a sequential\noptimization algorithm can gather observations in a {\\em controlled} manner, so\nthat the amount of information at each step is allowed to change in time. In\nparticular, we show that optimization algorithms often obey the law of\ndiminishing returns: the signal-to-noise ratio drops as the optimization\nalgorithm approaches the optimum. To underscore the generality of the tools, we\nuse our approach to derive fundamental lower bounds for a certain active\nlearning problem. Overall, the present work connects the intuitive notions of\ninformation in optimization, experimental design, estimation, and active\nlearning to the quantitative notion of Shannon information.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 12 Oct 2010 02:19:43 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 26 Apr 2011 15:08:35 GMT"
            },
            {
                "version": "v3",
                "created": "Fri, 9 Sep 2011 15:57:54 GMT"
            }
        ],
        "update_date": "2011-09-12",
        "authors_parsed": [
            [
                "Raginsky",
                "Maxim",
                ""
            ],
            [
                "Rakhlin",
                "Alexander",
                ""
            ]
        ]
    },
    {
        "id": "1010.3125",
        "submitter": "Igor Vladimirov",
        "authors": "Igor G. Vladimirov and Ian R. Petersen",
        "title": "A Quasi-separation Principle and Newton-like Scheme for Coherent Quantum\n  LQG Control",
        "comments": "24 pages, 2 figures, accepted for publication, 18th IFAC World\n  Congress, Milan, Italy, 28 August - 2 September, 2011",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "quant-ph cs.SY math.DS math.OC",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper is concerned with constructing an optimal controller in the\ncoherent quantum Linear Quadratic Gaussian problem. A coherent quantum\ncontroller is itself a quantum system and is required to be physically\nrealizable. The use of coherent control avoids the need for classical\nmeasurements, which inherently entail the loss of quantum information. Physical\nrealizability corresponds to the equivalence of the controller to an open\nquantum harmonic oscillator and relates its state-space matrices to the\nHamiltonian, coupling and scattering operators of the oscillator. The\nHamiltonian parameterization of the controller is combined with Frechet\ndifferentiation of the LQG cost with respect to the state-space matrices to\nobtain equations for the optimal controller. A quasi-separation principle for\nthe gain matrices of the quantum controller is established, and a Newton-like\niterative scheme for numerical solution of the equations is outlined.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 15 Oct 2010 11:18:35 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 15 Apr 2011 15:15:39 GMT"
            },
            {
                "version": "v3",
                "created": "Mon, 1 Aug 2011 06:06:24 GMT"
            }
        ],
        "update_date": "2012-05-21",
        "authors_parsed": [
            [
                "Vladimirov",
                "Igor G.",
                ""
            ],
            [
                "Petersen",
                "Ian R.",
                ""
            ]
        ]
    },
    {
        "id": "1010.4411",
        "submitter": "Valmir Barbosa",
        "authors": "Fabiano de S. Oliveira, Valmir C. Barbosa",
        "title": "Revisiting deadlock prevention: a probabilistic approach",
        "comments": null,
        "journal-ref": "Networks 63 (2014), 203-210",
        "doi": "10.1002/net.21537",
        "report-no": null,
        "categories": "cs.DC cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We revisit the deadlock-prevention problem by focusing on priority digraphs\ninstead of the traditional wait-for digraphs. This has allowed us to formulate\ndeadlock prevention in terms of prohibiting the occurrence of directed cycles\neven in the most general of wait models (the so-called AND-OR model, in which\nprohibiting wait-for directed cycles is generally overly restrictive). For a\nparticular case in which the priority digraphs are somewhat simplified, we\nintroduce a Las Vegas probabilistic mechanism for resource granting and analyze\nits key aspects in detail.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 21 Oct 2010 10:12:59 GMT"
            }
        ],
        "update_date": "2014-01-07",
        "authors_parsed": [
            [
                "Oliveira",
                "Fabiano de S.",
                ""
            ],
            [
                "Barbosa",
                "Valmir C.",
                ""
            ]
        ]
    },
    {
        "id": "1010.5571",
        "submitter": "EPTCS",
        "authors": "Matthieu Lemerre (CEA LIST), Vincent David (CEA LIST), Christophe\n  Aussagu\\`es (CEA LIST), Guy Vidal-Naquet (SUPELEC)",
        "title": "An Introduction to Time-Constrained Automata",
        "comments": "In Proceedings ICE 2010, arXiv:1010.5308",
        "journal-ref": "EPTCS 38, 2010, pp. 83-98",
        "doi": "10.4204/EPTCS.38.9",
        "report-no": null,
        "categories": "cs.LO cs.FL cs.OS cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  We present time-constrained automata (TCA), a model for hard real-time\ncomputation in which agents behaviors are modeled by automata and constrained\nby time intervals.\n  TCA actions can have multiple start time and deadlines, can be aperiodic, and\nare selected dynamically following a graph, the time-constrained automaton.\nThis allows expressing much more precise time constraints than classical\nperiodic or sporadic model, while preserving the ease of scheduling and\nanalysis.\n  We provide some properties of this model as well as their scheduling\nsemantics. We show that TCA can be automatically derived from source-code, and\noptimally scheduled on single processors using a variant of EDF. We explain how\ntime constraints can be used to guarantee communication determinism by\nconstruction, and to study when possible agent interactions happen.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 27 Oct 2010 05:04:38 GMT"
            }
        ],
        "update_date": "2010-10-28",
        "authors_parsed": [
            [
                "Lemerre",
                "Matthieu",
                "",
                "CEA LIST"
            ],
            [
                "David",
                "Vincent",
                "",
                "CEA LIST"
            ],
            [
                "Aussagu\u00e8s",
                "Christophe",
                "",
                "CEA LIST"
            ],
            [
                "Vidal-Naquet",
                "Guy",
                "",
                "SUPELEC"
            ]
        ]
    },
    {
        "id": "1011.1735",
        "submitter": "Tshilidzi Marwala",
        "authors": "George Anderson, Tshilidzi Marwala, and Fulufhelo V. Nelwamondo",
        "title": "Use of Data Mining in Scheduler Optimization",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The operating system's role in a computer system is to manage the various\nresources. One of these resources is the Central Processing Unit. It is managed\nby a component of the operating system called the CPU scheduler. Schedulers are\noptimized for typical workloads expected to run on the platform. However, a\nsingle scheduler may not be appropriate for all workloads. That is, a scheduler\nmay schedule a workload such that the completion time is minimized, but when\nanother type of workload is run on the platform, scheduling and therefore\ncompletion time will not be optimal; a different scheduling algorithm, or a\ndifferent set of parameters, may work better. Several approaches to solving\nthis problem have been proposed. The objective of this survey is to summarize\nthe approaches based on data mining, which are available in the literature. In\naddition to solutions that can be directly utilized for solving this problem,\nwe are interested in data mining research in related areas that have potential\nfor use in operating system scheduling. We also explain general technical\nissues involved in scheduling in modern computers, including parallel\nscheduling issues related to multi-core CPUs. We propose a taxonomy that\nclassifies the scheduling approaches we discuss into different categories.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 8 Nov 2010 09:07:54 GMT"
            }
        ],
        "update_date": "2010-11-09",
        "authors_parsed": [
            [
                "Anderson",
                "George",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ],
            [
                "Nelwamondo",
                "Fulufhelo V.",
                ""
            ]
        ]
    },
    {
        "id": "1011.3087",
        "submitter": "Feng Xia",
        "authors": "Hongtao Huang, Feng Xia, Jijie Wang, Siyu Lei, Guowei Wu",
        "title": "Leakage-Aware Reallocation for Periodic Real-Time Tasks on Multicore\n  Processors",
        "comments": "The 5th International Conference on Frontier of Computer Science and\n  Technology (FCST), IEEE, Changchun, China, August 2010",
        "journal-ref": null,
        "doi": "10.1109/FCST.2010.105",
        "report-no": null,
        "categories": "cs.DC cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is an increasingly important issue to reduce the energy consumption of\ncomputing systems. In this paper, we consider partition based energy-aware\nscheduling of periodic real-time tasks on multicore processors. The scheduling\nexploits dynamic voltage scaling (DVS) and core sleep scheduling to reduce both\ndynamic and leakage energy consumption. If the overhead of core state switching\nis non-negligible, however, the performance of this scheduling strategy in\nterms of energy efficiency might degrade. To achieve further energy saving, we\nextend the static task scheduling with run-time task reallocation. The basic\nidea is to aggregate idle time among cores so that as many cores as possible\ncould be put into sleep in a way that the overall energy consumption is\nreduced. Simulation results show that the proposed approach results in up to\n20% energy saving over traditional leakage-aware DVS.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 13 Nov 2010 02:17:29 GMT"
            }
        ],
        "update_date": "2016-11-17",
        "authors_parsed": [
            [
                "Huang",
                "Hongtao",
                ""
            ],
            [
                "Xia",
                "Feng",
                ""
            ],
            [
                "Wang",
                "Jijie",
                ""
            ],
            [
                "Lei",
                "Siyu",
                ""
            ],
            [
                "Wu",
                "Guowei",
                ""
            ]
        ]
    },
    {
        "id": "1012.2831",
        "submitter": "Lin Zhong",
        "authors": "Mian Dong and Lin Zhong",
        "title": "Sesame: Self-Constructive System Energy Modeling for Battery-Powered\n  Mobile Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  System energy models are important for energy optimization and management in\nmobile systems. However, existing system energy models are built in lab with\nthe help from a second computer. Not only are they labor-intensive; but also\nthey will not adequately account for the great diversity in the hardware and\nusage of mobile systems. Moreover, existing system energy models are intended\nfor energy estimation for time intervals of one second or longer; they do not\nprovide the required rate for fine-grain use such as per-application energy\naccounting.\n  In this work, we study a self-modeling paradigm in which a mobile system\nautomatically generates its energy model without any external assistance. Our\nsolution, Se-same, leverages the possibility of self power measurement through\nthe smart battery interface and employs a suite of novel techniques to achieve\naccuracy and rate much higher than that of the smart battery interface.\n  We report the implementation and evaluation of Se-same on a laptop and a\nsmartphone. The experiment results show that Sesame generates system energy\nmodels of 95% accuracy at one estimation per second and 88% accuracy at one\nestimation per 10ms, without any external assistance. A five-day field studies\nwith four laptop and four smartphones users further demonstrate the\neffectiveness, efficiency, and noninvasiveness of Sesame.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 13 Dec 2010 18:47:10 GMT"
            },
            {
                "version": "v2",
                "created": "Mon, 20 Dec 2010 22:16:29 GMT"
            }
        ],
        "update_date": "2010-12-22",
        "authors_parsed": [
            [
                "Dong",
                "Mian",
                ""
            ],
            [
                "Zhong",
                "Lin",
                ""
            ]
        ]
    },
    {
        "id": "1012.3071",
        "submitter": "Lin Zhong",
        "authors": "Ahmad Rahmati, Clay Shepard, Chad Tossell, Angela Nicoara, Lin Zhong,\n  Phil Kortum, Jatinder Singh",
        "title": "Seamless Flow Migration on Smartphones without Network Support",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.NI cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper addresses the following question: Is it possible to migrate TCP/IP\nflows between different networks on modern mobile devices, without\ninfrastructure support or protocol changes? To answer this question, we make\nthree research contributions. (i) We report a comprehensive characterization of\nIP traffic on smartphones using traces collected from 27 iPhone 3GS users for\nthree months. (ii) Driven by the findings from the characterization, we devise\ntwo novel system mechanisms for mobile devices to sup-port seamless flow\nmigration without network support, and extensively evaluate their effectiveness\nusing our field collected traces of real-life usage. Wait-n-Migrate leverages\nthe fact that most flows are short lived. It establishes new flows on newly\navailable networks but allows pre-existing flows on the old network to\nterminate naturally, effectively decreasing, or even eliminating, connectivity\ngaps during network switches. Resumption Agent takes advantage of the\nfunctionality integrated into many modern protocols to securely resume flows\nwithout application intervention. When combined, Wait-n-Migrate and Resumption\nAgent provide an unprecedented opportunity to immediately deploy performance\nand efficiency-enhancing policies that leverage multiple networks to improve\nthe performance, efficiency, and connectivity of mobile devices. (iii) Finally,\nwe report an iPhone 3GS based implementation of these two system mechanisms and\nshow that their overhead is negligible. Furthermore, we employ an example\nnetwork switching policy, called AutoSwitch, to demonstrate their performance.\nAutoSwitch improves the Wi-Fi user experience by intelligently migrating TCP\nflows between Wi-Fi and cellular networks. Through traces and field\nmeasurements, we show that AutoSwitch reduces the number of user disruptions by\nan order of magnitude.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 14 Dec 2010 16:10:08 GMT"
            }
        ],
        "update_date": "2010-12-15",
        "authors_parsed": [
            [
                "Rahmati",
                "Ahmad",
                ""
            ],
            [
                "Shepard",
                "Clay",
                ""
            ],
            [
                "Tossell",
                "Chad",
                ""
            ],
            [
                "Nicoara",
                "Angela",
                ""
            ],
            [
                "Zhong",
                "Lin",
                ""
            ],
            [
                "Kortum",
                "Phil",
                ""
            ],
            [
                "Singh",
                "Jatinder",
                ""
            ]
        ]
    },
    {
        "id": "1012.3452",
        "submitter": "Mohammad Nikseresht",
        "authors": "Mohammad R Nikseresht, Anil Somayaji, Anil Maheshwari",
        "title": "Customer Appeasement Scheduling",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": "TR-10-18",
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Almost all of the current process scheduling algorithms which are used in\nmodern operating systems (OS) have their roots in the classical scheduling\nparadigms which were developed during the 1970's. But modern computers have\ndifferent types of software loads and user demands. We think it is important to\nrun what the user wants at the current moment. A user can be a human, sitting\nin front of a desktop machine, or it can be another machine sending a request\nto a server through a network connection. We think that OS should become\nintelligent to distinguish between different processes and allocate resources,\nincluding CPU, to those processes which need them most. In this work, as a\nfirst step to make the OS aware of the current state of the system, we consider\nprocess dependencies and interprocess communications. We are developing a\nmodel, which considers the need to satisfy interactive users and other possible\nremote users or customers, by making scheduling decisions based on process\ndependencies and interprocess communications. Our simple proof of concept\nimplementation and experiments show the effectiveness of this approach in the\nreal world applications. Our implementation does not require any change in the\nsoftware applications nor any special kind of configuration in the system,\nMoreover, it does not require any additional information about CPU needs of\napplications nor other resource requirements. Our experiments show significant\nperformance improvement for real world applications. For example, almost\nconstant average response time for Mysql data base server and constant frame\nrate for mplayer under different simulated load values.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 15 Dec 2010 20:38:40 GMT"
            }
        ],
        "update_date": "2010-12-16",
        "authors_parsed": [
            [
                "Nikseresht",
                "Mohammad R",
                ""
            ],
            [
                "Somayaji",
                "Anil",
                ""
            ],
            [
                "Maheshwari",
                "Anil",
                ""
            ]
        ]
    },
    {
        "id": "1012.4045",
        "submitter": "Tshilidzi Marwala",
        "authors": "George Anderson, Tshilidzi Marwala and Fulufhelo Vincent Nelwamondo",
        "title": "Application of Global and One-Dimensional Local Optimization to\n  Operating System Scheduler Tuning",
        "comments": "Proceedings of the Twenty-First Annual Symposium of the Pattern\n  Recognition Association of South Africa 22-23 November 2010 Stellenbosch,\n  South Africa, pp. 7-11",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes a study of comparison of global and one-dimensional\nlocal optimization methods to operating system scheduler tuning. The operating\nsystem scheduler we use is the Linux 2.6.23 Completely Fair Scheduler (CFS)\nrunning in simulator (LinSched). We have ported the Hackbench scheduler\nbenchmark to this simulator and use this as the workload. The global\noptimization approach we use is Particle Swarm Optimization (PSO). We make use\nof Response Surface Methodology (RSM) to specify optimal parameters for our PSO\nimplementation. The one-dimensional local optimization approach we use is the\nGolden Section method. In order to use this approach, we convert the scheduler\ntuning problem from one involving setting of three parameters to one involving\nthe manipulation of one parameter. Our results show that the global\noptimization approach yields better response but the one- dimensional\noptimization approach converges to a solution faster than the global\noptimization approach.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sat, 18 Dec 2010 00:51:57 GMT"
            }
        ],
        "update_date": "2010-12-21",
        "authors_parsed": [
            [
                "Anderson",
                "George",
                ""
            ],
            [
                "Marwala",
                "Tshilidzi",
                ""
            ],
            [
                "Nelwamondo",
                "Fulufhelo Vincent",
                ""
            ]
        ]
    },
    {
        "id": "1012.5695",
        "submitter": "Santhi Baskaran",
        "authors": "Santhi Baskaran and P. Thambidurai",
        "title": "Dynamic Scheduling of Skippable Periodic Tasks with Energy Efficiency in\n  Weakly Hard Real-Time System",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Energy consumption is a critical design issue in real-time systems,\nespecially in battery- operated systems. Maintaining high performance, while\nextending the battery life between charges is an interesting challenge for\nsystem designers. Dynamic Voltage Scaling (DVS) allows a processor to\ndynamically change speed and voltage at run time, thereby saving energy by\nspreading run cycles into idle time. Knowing when to use full power and when\nnot, requires the cooperation of the operating system scheduler. Usually,\nhigher processor voltage and frequency leads to higher system throughput while\nenergy reduction can be obtained using lower voltage and frequency. Instead of\nlowering processor voltage and frequency as much as possible, energy efficient\nreal-time scheduling adjusts voltage and frequency according to some\noptimization criteria, such as low energy consumption or high throughput, while\nit meets the timing constraints of the real-time tasks. As the quantity and\nfunctional complexity of battery powered portable devices continues to raise,\nenergy efficient design of such devices has become increasingly important. Many\nreal-time scheduling algorithms have been developed recently to reduce energy\nconsumption in the portable devices that use DVS capable processors. Three\nalgorithms namely Red Tasks Only (RTO), Blue When Possible (BWP) and Red as\nLate as Possible (RLP) are proposed in the literature to schedule the real-time\ntasks in Weakly-hard real-time systems. This paper proposes optimal slack\nmanagement algorithms to make the above existing weakly hard real-time\nscheduling algorithms energy efficient using DVS and DPD techniques.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 28 Dec 2010 04:35:02 GMT"
            }
        ],
        "update_date": "2010-12-30",
        "authors_parsed": [
            [
                "Baskaran",
                "Santhi",
                ""
            ],
            [
                "Thambidurai",
                "P.",
                ""
            ]
        ]
    },
    {
        "id": "1012.5929",
        "submitter": "Joel Goossens",
        "authors": "Jo\\\"el Goossens (1), Patrick Meumeu Yomsi (2) ((1) Brussels\n  University, U.L.B., Brussels, Belgium., (2) F.N.R.S, Belgium.)",
        "title": "Exact Schedulability Test for global-EDF Scheduling of Periodic Hard\n  Real-Time Tasks on Identical Multiprocessors",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we consider the scheduling problem of hard real-time systems\ncomposed of periodic constrained-deadline tasks upon identical multiprocessor\nplatforms. We assume that tasks are scheduled by using the global-EDF\nscheduler. We establish an exact schedulability test for this scheduler by\nexploiting on the one hand its predictability property and by providing on the\nother hand a feasibility interval so that if it is possible to find a valid\nschedule for all the jobs contained in this interval, then the whole system\nwill be stamped feasible. In addition, we show by means of a counterexample\nthat the feasibility interval, and thus the schedulability test, proposed by\nLeung [Leung 1989] is incorrect and we show which arguments are actually\nincorrect.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 29 Dec 2010 12:41:13 GMT"
            }
        ],
        "update_date": "2010-12-30",
        "authors_parsed": [
            [
                "Goossens",
                "Jo\u00ebl",
                ""
            ],
            [
                "Yomsi",
                "Patrick Meumeu",
                ""
            ]
        ]
    },
    {
        "id": "1101.1466",
        "submitter": "Sudipta Das",
        "authors": "Sudipta Das, Lawrence Jenkins and Debasis Sengupta",
        "title": "Comparison of Loss ratios of different scheduling algorithms",
        "comments": "8 pages, 10 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  It is well known that in a firm real time system with a renewal arrival\nprocess, exponential service times and independent and identically distributed\ndeadlines till the end of service of a job, the earliest deadline first (EDF)\nscheduling policy has smaller loss ratio (expected fraction of jobs, not\ncompleted) than any other service time independent scheduling policy, including\nthe first come first served (FCFS). Various modifications to the EDF and FCFS\npolicies have been proposed in the literature, with a view to improving\nperformance. In this article, we compare the loss ratios of these two policies\nalong with some of the said modifications, as well as their counterparts with\ndeterministic deadlines. The results include some formal inequalities and some\ncounter-examples to establish non-existence of an order. A few relations\ninvolving loss ratios are posed as conjectures, and simulation results in\nsupport of these are reported. These results lead to a complete picture of\ndominance and non-dominance relations between pairs of scheduling policies, in\nterms of loss ratios.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 6 Jan 2011 10:28:58 GMT"
            }
        ],
        "update_date": "2015-03-17",
        "authors_parsed": [
            [
                "Das",
                "Sudipta",
                ""
            ],
            [
                "Jenkins",
                "Lawrence",
                ""
            ],
            [
                "Sengupta",
                "Debasis",
                ""
            ]
        ]
    },
    {
        "id": "1102.2094",
        "submitter": "Jo\\\"el Goossens",
        "authors": "Vincent Nelis (1), Patrick Meumeu Yomsi (2), Bj\\\"orn Andersson (1) and\n  Jo\\\"el Goossens (3) ((1) CISTER Research unit Polytechnic Institute of Porto\n  (2) LORIA (3) U.L.B.)",
        "title": "Global Scheduling of Multi-Mode Real-Time Applications upon\n  Multiprocessor Platforms",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Multi-mode real-time systems are those which support applications with\ndifferent modes of operation, where each mode is characterized by a specific\nset of tasks. At run-time, such systems can, at any time, be requested to\nswitch from its current operating mode to another mode (called \"new mode\") by\nreplacing the current set of tasks with that of the new-mode. Thereby, ensuring\nthat all the timing requirements are met not only requires that a\nschedulability test is performed on the tasks of each mode but also that (i) a\nprotocol for transitioning from one mode to another is specified and (ii) a\nschedulability test for each transition is performed. We propose two distinct\nprotocols that manage the mode transitions upon uniform and identical\nmultiprocessor platforms at run-time, each specific to distinct task\nrequirements. For each protocol, we formally establish schedulability analyses\nthat indicate beforehand whether all the timing requirements will be met during\nany mode transition of the system. This is performed assuming both\nFixed-Task-Priority and Fixed-Job-Priority schedulers.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 10 Feb 2011 12:15:58 GMT"
            }
        ],
        "update_date": "2015-03-18",
        "authors_parsed": [
            [
                "Nelis",
                "Vincent",
                ""
            ],
            [
                "Yomsi",
                "Patrick Meumeu",
                ""
            ],
            [
                "Andersson",
                "Bj\u00f6rn",
                ""
            ],
            [
                "Goossens",
                "Jo\u00ebl",
                ""
            ]
        ]
    },
    {
        "id": "1103.1717",
        "submitter": "Matthieu Moy",
        "authors": "Matthieu Moy (VERIMAG - IMAG)",
        "title": "Efficient and Playful Tools to Teach Unix to New Students",
        "comments": "ITiCSE, Darmstadt : Germany (2011)",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Teaching Unix to new students is a common tasks in many higher schools. This\npaper presents an approach to such course where the students progress\nautonomously with the help of the teacher. The traditional textbook is\ncomplemented with a wiki, and the main thread of the course is a game, in the\nform of a treasure hunt. The course finishes with a lab exam, where students\nhave to perform practical manipulations similar to the ones performed during\nthe treasure hunt. The exam is graded fully automatically. This paper discusses\nthe motivations and advantages of the approach, and gives an overall view of\nthe tools we developed. The tools are available from the web, and open-source,\nhence re-usable outside the Ensimag.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 9 Mar 2011 07:25:00 GMT"
            }
        ],
        "update_date": "2011-03-10",
        "authors_parsed": [
            [
                "Moy",
                "Matthieu",
                "",
                "VERIMAG - IMAG"
            ]
        ]
    },
    {
        "id": "1103.2336",
        "submitter": "Raul",
        "authors": "Nabil Litayem, Ahmed Ben Achballah, Slim Ben Saoud",
        "title": "Building XenoBuntu Linux Distribution for Teaching and Prototyping\n  Real-Time Operating Systems",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper describes the realization of a new Linux distribution based on\nUbuntu Linux and Xenomai Real-Time framework. This realization is motivated by\nthe eminent need of real-time systems in modern computer science courses. The\nmajority of the technical choices are made after qualitative comparison. The\nmain goal of this distribution is to offer standard Operating Systems (OS) that\ninclude Xenomai infrastructure and the essential tools to begin hard real-time\napplication development inside a convivial desktop environment. The released\nlive/installable DVD can be adopted to emulate several classic RTOS Application\nProgram Interfaces (APIs), directly use and understand real-time Linux in\nconvivial desktop environment and prototyping real-time embedded applications.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 Mar 2011 18:36:38 GMT"
            }
        ],
        "update_date": "2011-03-14",
        "authors_parsed": [
            [
                "Litayem",
                "Nabil",
                ""
            ],
            [
                "Achballah",
                "Ahmed Ben",
                ""
            ],
            [
                "Saoud",
                "Slim Ben",
                ""
            ]
        ]
    },
    {
        "id": "1103.2348",
        "submitter": "Lin Zhong",
        "authors": "Felix Xiaozhu Lin, Zhen Wang, Robert LiKamWa, and Lin Zhong",
        "title": "Transparent Programming of Heterogeneous Smartphones for Sensing",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.PL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Sensing on smartphones is known to be power-hungry. It has been shown that\nthis problem can be solved by adding an ultra low-power processor to execute\nsimple, frequent sensor data processing. While very effective in saving energy,\nthis resulting heterogeneous, distributed architecture poses a significant\nchallenge to application development.\n  We present Reflex, a suite of runtime and compilation techniques to conceal\nthe heterogeneous, distributed nature from developers. The Reflex automatically\ntransforms the developer's code for distributed execution with the help of the\nReflex runtime. To create a unified system illusion, Reflex features a novel\nsoftware distributed shared memory (DSM) design that leverages the extreme\narchitectural asymmetry between the low-power processor and the powerful\ncentral processor to achieve both energy efficiency and performance.\n  We report a complete realization of Reflex for heterogeneous smartphones with\nMaemo/Linux as the central kernel. Using a tri-processor hardware prototype and\nsensing applications reported in recent literature, we evaluate the Reflex\nrealization for programming transparency, energy efficiency, and performance.\nWe show that Reflex supports a programming style that is very close to\ncontemporary smartphone programming. It allows existing sensing applications to\nbe ported with minor source code changes. Reflex reduces the system power in\nsensing by up to 83%, and its runtime system only consumes 10% local memory on\na typical ultra-low power processor.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 11 Mar 2011 19:12:15 GMT"
            }
        ],
        "update_date": "2011-03-14",
        "authors_parsed": [
            [
                "Lin",
                "Felix Xiaozhu",
                ""
            ],
            [
                "Wang",
                "Zhen",
                ""
            ],
            [
                "LiKamWa",
                "Robert",
                ""
            ],
            [
                "Zhong",
                "Lin",
                ""
            ]
        ]
    },
    {
        "id": "1103.3831",
        "submitter": "Himansu Sekhar Behera",
        "authors": "H. S. Behera, Rakesh Mohanty, Debashree Nayak",
        "title": "A New Proposed Dynamic Quantum with Re-Adjusted Round Robin Scheduling\n  Algorithm and Its Performance Analysis",
        "comments": "06 pages; International Journal of Computer Applications, Vol. 5, No.\n  5, August 2010",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Scheduling is the central concept used frequently in Operating System. It\nhelps in choosing the processes for execution. Round Robin (RR) is one of the\nmost widely used CPU scheduling algorithm. But, its performance degrades with\nrespect to context switching, which is an overhead and it occurs during each\nscheduling. Overall performance of the system depends on choice of an optimal\ntime quantum, so that context switching can be reduced. In this paper, we have\nproposed a new variant of RR scheduling algorithm, known as Dynamic Quantum\nwith Readjusted Round Robin (DQRRR) algorithm. We have experimentally shown\nthat performance of DQRRR is better than RR by reducing number of context\nswitching, average waiting time and average turn around time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 20 Mar 2011 06:45:16 GMT"
            }
        ],
        "update_date": "2011-03-22",
        "authors_parsed": [
            [
                "Behera",
                "H. S.",
                ""
            ],
            [
                "Mohanty",
                "Rakesh",
                ""
            ],
            [
                "Nayak",
                "Debashree",
                ""
            ]
        ]
    },
    {
        "id": "1103.3832",
        "submitter": "Himansu Sekhar Behera",
        "authors": "H.S. Behera, Simpi Patel, Bijayalakshmi Panda",
        "title": "A New Dynamic Round Robin and SRTN Algorithm with Variable Original Time\n  Slice and Intelligent Time Slice for Soft Real Time Systems",
        "comments": "07 pages; International Journal of Computer Applications, Vol 16, No.\n  1(9) February 2011",
        "journal-ref": null,
        "doi": "10.5120/2037-2648",
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  The main objective of the paper is to improve the Round Robin (RR) algorithm\nusing dynamic ITS by coalescing it with Shortest Remaining Time Next (SRTN)\nalgorithm thus reducing the average waiting time, average turnaround time and\nthe number of context switches. The original time slice has been calculated for\neach process based on its burst time.This is mostly suited for soft real time\nsystems where meeting of deadlines is desirable to increase its performance.\nThe advantage is that processes that are closer to their remaining completion\ntime will get more chances to execute and leave the ready queue. This will\nreduce the number of processes in the ready queue by knocking out short jobs\nrelatively faster in a hope to reduce the average waiting time, turn around\ntime and number of context switches. This paper improves the algorithm [8] and\nthe experimental analysis shows that the proposed algorithm performs better\nthan algorithm [6] and [8] when the processes are having an increasing order,\ndecreasing order and random order of burst time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 20 Mar 2011 06:56:27 GMT"
            }
        ],
        "update_date": "2011-03-22",
        "authors_parsed": [
            [
                "Behera",
                "H. S.",
                ""
            ],
            [
                "Patel",
                "Simpi",
                ""
            ],
            [
                "Panda",
                "Bijayalakshmi",
                ""
            ]
        ]
    },
    {
        "id": "1104.2110",
        "submitter": "Heechul Yun",
        "authors": "Heechul Yun, Cheolgi Kim and Lui Sha",
        "title": "Deterministic Real-time Thread Scheduling",
        "comments": "RTAS11 Work-In-Progress",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Race condition is a timing sensitive problem. A significant source of timing\nvariation comes from nondeterministic hardware interactions such as cache\nmisses. While data race detectors and model checkers can check races, the\nenormous state space of complex software makes it difficult to identify all of\nthe races and those residual implementation errors still remain a big\nchallenge. In this paper, we propose deterministic real-time scheduling methods\nto address scheduling nondeterminism in uniprocessor systems. The main idea is\nto use timing insensitive deterministic events, e.g, an instruction counter, in\nconjunction with a real-time clock to schedule threads. By introducing the\nconcept of Worst Case Executable Instructions (WCEI), we guarantee both\ndeterminism and real-time performance.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 12 Apr 2011 04:06:34 GMT"
            }
        ],
        "update_date": "2011-04-13",
        "authors_parsed": [
            [
                "Yun",
                "Heechul",
                ""
            ],
            [
                "Kim",
                "Cheolgi",
                ""
            ],
            [
                "Sha",
                "Lui",
                ""
            ]
        ]
    },
    {
        "id": "1104.3523",
        "submitter": "Paul Regnier M.",
        "authors": "Paul Regnier and George Lima and Ernesto Massa",
        "title": "An Optimal Real-Time Scheduling Approach: From Multiprocessor to\n  Uniprocessor",
        "comments": "10 pages - rejected for publication by ECRTS 2011",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  An optimal solution to the problem of scheduling real-time tasks on a set of\nidentical processors is derived. The described approach is based on solving an\nequivalent uniprocessor real-time scheduling problem. Although there are other\nscheduling algorithms that achieve optimality, they usually impose prohibitive\npreemption costs. Unlike these algorithms, it is observed through simulation\nthat the proposed approach produces no more than three preemptions points per\njob.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 18 Apr 2011 15:39:21 GMT"
            }
        ],
        "update_date": "2011-04-19",
        "authors_parsed": [
            [
                "Regnier",
                "Paul",
                ""
            ],
            [
                "Lima",
                "George",
                ""
            ],
            [
                "Massa",
                "Ernesto",
                ""
            ]
        ]
    },
    {
        "id": "1105.1736",
        "submitter": "Rakesh Mohanty",
        "authors": "Rakesh Mohanty, H. S. Behera, Khusbu Patwari, Monisha Dash, M. Lakshmi\n  Prasanna",
        "title": "Priority Based Dynamic Round Robin (PBDRR) Algorithm with Intelligent\n  Time Slice for Soft Real Time Systems",
        "comments": "5 pages",
        "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), Vol. 2 No. 2, February 2011 2011, 46-50",
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, a new variant of Round Robin (RR) algorithm is proposed which\nis suitable for soft real time systems. RR algorithm performs optimally in\ntimeshared systems, but it is not suitable for soft real time systems. Because\nit gives more number of context switches, larger waiting time and larger\nresponse time. We have proposed a novel algorithm, known as Priority Based\nDynamic Round Robin Algorithm(PBDRR),which calculates intelligent time slice\nfor individual processes and changes after every round of execution. The\nproposed scheduling algorithm is developed by taking dynamic time quantum\nconcept into account. Our experimental results show that our proposed algorithm\nperforms better than algorithm in [8] in terms of reducing the number of\ncontext switches, average waiting time and average turnaround time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 9 May 2011 17:29:03 GMT"
            }
        ],
        "update_date": "2011-05-10",
        "authors_parsed": [
            [
                "Mohanty",
                "Rakesh",
                ""
            ],
            [
                "Behera",
                "H. S.",
                ""
            ],
            [
                "Patwari",
                "Khusbu",
                ""
            ],
            [
                "Dash",
                "Monisha",
                ""
            ],
            [
                "Prasanna",
                "M. Lakshmi",
                ""
            ]
        ]
    },
    {
        "id": "1105.1811",
        "submitter": "Niall Douglas",
        "authors": "Niall Douglas",
        "title": "User Mode Memory Page Allocation: A Silver Bullet For Memory Allocation?",
        "comments": "10 pages. Rejected from ISMM11",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.PF",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  This paper proposes a novel solution: the elimination of paged virtual memory\nand partial outsourcing of memory page allocation and manipulation from the\noperating system kernel into the individual process' user space - a user mode\npage allocator - which allows an application to have direct, bare metal access\nto the page mappings used by the hardware Memory Management Unit (MMU) for its\npart of the overall address space. A user mode page allocator based emulation\nof the mmap() abstraction layer of dlmalloc is then benchmarked against the\ntraditional kernel mode implemented mmap() in a series of synthetic Monte-Carlo\nand real world application settings. Given the superb synthetic and positive\nreal world results from the profiling conducted, this paper proposes that with\nproper operating system and API support one could gain a further order higher\nperformance again while keeping allocator performance invariant to the amount\nof memory being allocated or freed i.e. a 100x performance improvement or more\nin some common use cases. It is rare that through a simple and easy to\nimplement API and operating system structure change one can gain a Silver\nBullet with the potential for a second one.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 9 May 2011 22:26:15 GMT"
            }
        ],
        "update_date": "2011-05-11",
        "authors_parsed": [
            [
                "Douglas",
                "Niall",
                ""
            ]
        ]
    },
    {
        "id": "1105.1815",
        "submitter": "Niall Douglas",
        "authors": "Niall Douglas",
        "title": "User Mode Memory Page Management: An old idea applied anew to the memory\n  wall problem",
        "comments": "6 pages. Rejected from MSPC11",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.PF",
        "license": "http://creativecommons.org/licenses/by/3.0/",
        "abstract": "  It is often said that one of the biggest limitations on computer performance\nis memory bandwidth (i.e.\"the memory wall problem\"). In this position paper, I\nargue that if historical trends in computing evolution (where growth in\navailable capacity is exponential and reduction in its access latencies is\nlinear) continue as they have, then this view is wrong - in fact we ought to be\nconcentrating on reducing whole system memory access latencies wherever\npossible, and by \"whole system\" I mean that we ought to look at how software\ncan be unnecessarily wasteful with memory bandwidth due to legacy design\ndecisions. To this end I conduct a feasibility study to determine whether we\nought to virtualise the MMU for each application process such that it has\ndirect access to its own MMU page tables and the memory allocated to a process\nis managed exclusively by the process and not the kernel. I find under typical\nconditions that nearly scale invariant performance to memory allocation size is\npossible such that hundreds of megabytes of memory can be allocated, relocated,\nswapped and deallocated in almost the same time as kilobytes (e.g. allocating\n8Mb is 10x quicker under this experimental allocator than a conventional\nallocator, and resizing a 128Kb block to 256Kb block is 4.5x faster). I find\nthat first time page access latencies are improved tenfold; moreover, because\nthe kernel page fault handler is never called, the lack of cache pollution\nimproves whole application memory access latencies increasing performance by up\nto 2x. Finally, I try binary patching existing applications to use the\nexperimental allocation technique, finding almost universal performance\nimprovements without having to recompile these applications to make better use\nof the new facilities.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 9 May 2011 22:39:46 GMT"
            }
        ],
        "update_date": "2011-05-11",
        "authors_parsed": [
            [
                "Douglas",
                "Niall",
                ""
            ]
        ]
    },
    {
        "id": "1105.3232",
        "submitter": "Sokol Kosta",
        "authors": "Sokol Kosta, Andrius Aucinas, Pan Hui, Richard Mortier, Xinwen Zhang",
        "title": "Unleashing the Power of Mobile Cloud Computing using ThinkAir",
        "comments": "17 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.NI cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Smartphones have exploded in popularity in recent years, becoming ever more\nsophisticated and capable. As a result, developers worldwide are building\nincreasingly complex applications that require ever increasing amounts of\ncomputational power and energy. In this paper we propose ThinkAir, a framework\nthat makes it simple for developers to migrate their smartphone applications to\nthe cloud. ThinkAir exploits the concept of smartphone virtualization in the\ncloud and provides method level computation offloading. Advancing on previous\nworks, it focuses on the elasticity and scalability of the server side and\nenhances the power of mobile cloud computing by parallelizing method execution\nusing multiple Virtual Machine (VM) images. We evaluate the system using a\nrange of benchmarks starting from simple micro-benchmarks to more complex\napplications. First, we show that the execution time and energy consumption\ndecrease two orders of magnitude for the N-queens puzzle and one order of\nmagnitude for a face detection and a virus scan application, using cloud\noffloading. We then show that if a task is parallelizable, the user can request\nmore than one VM to execute it, and these VMs will be provided dynamically. In\nfact, by exploiting parallelization, we achieve a greater reduction on the\nexecution time and energy consumption for the previous applications. Finally,\nwe use a memory-hungry image combiner tool to demonstrate that applications can\ndynamically request VMs with more computational power in order to meet their\ncomputational requirements.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 16 May 2011 21:45:54 GMT"
            }
        ],
        "update_date": "2015-03-19",
        "authors_parsed": [
            [
                "Kosta",
                "Sokol",
                ""
            ],
            [
                "Aucinas",
                "Andrius",
                ""
            ],
            [
                "Hui",
                "Pan",
                ""
            ],
            [
                "Mortier",
                "Richard",
                ""
            ],
            [
                "Zhang",
                "Xinwen",
                ""
            ]
        ]
    },
    {
        "id": "1105.5055",
        "submitter": "Markus Lindstr\\\"om",
        "authors": "Markus Lindstr\\\"om, Gilles Geeraerts, Jo\\\"el Goossens",
        "title": "A faster exact multiprocessor schedulability test for sporadic tasks",
        "comments": "10 pages",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.FL",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Baker and Cirinei introduced an exact but naive algorithm, based on solving a\nstate reachability problem in a finite automaton, to check whether sets of\nsporadic hard real-time tasks are schedulable on identical multiprocessor\nplatforms. However, the algorithm suffered from poor performance due to the\nexponential size of the automaton relative to the size of the task set. In this\npaper, we successfully apply techniques developed by the formal verification\ncommunity, specifically antichain algorithms, by defining and proving the\ncorrectness of a simulation relation on Baker and Cirinei's automaton. We show\nour improved algorithm yields dramatically improved performance for the\nschedulability test and opens for many further improvements.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 May 2011 15:02:01 GMT"
            },
            {
                "version": "v2",
                "created": "Wed, 7 Sep 2011 13:42:51 GMT"
            }
        ],
        "update_date": "2011-09-08",
        "authors_parsed": [
            [
                "Lindstr\u00f6m",
                "Markus",
                ""
            ],
            [
                "Geeraerts",
                "Gilles",
                ""
            ],
            [
                "Goossens",
                "Jo\u00ebl",
                ""
            ]
        ]
    },
    {
        "id": "1105.5080",
        "submitter": "Jo\\\"el Goossens",
        "authors": "Irina Iulia Lupu, Jo\\\"el Goossens (U.L.B.)",
        "title": "Scheduling of Hard Real-Time Multi-Thread Periodic Tasks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper we study the scheduling of parallel and real-time recurrent\ntasks. Firstly, we propose a new parallel task model which allows recurrent\ntasks to be composed of several threads, each thread requires a single\nprocessor for execution and can be scheduled simultaneously. Secondly, we\ndefine several kinds of real-time schedulers that can be applied to our\nparallel task model. We distinguish between two scheduling classes:\nhierarchical schedulers and global thread schedulers. We present and prove\ncorrect an exact schedulability test for each class. Lastly, we also evaluate\nthe performance of our scheduling paradigm in comparison with Gang scheduling\nby means of simulations.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 25 May 2011 16:15:35 GMT"
            }
        ],
        "update_date": "2015-03-19",
        "authors_parsed": [
            [
                "Lupu",
                "Irina Iulia",
                "",
                "U.L.B."
            ],
            [
                "Goossens",
                "Jo\u00ebl",
                "",
                "U.L.B."
            ]
        ]
    },
    {
        "id": "1106.2766",
        "submitter": "Luis Nogueira PhD",
        "authors": "Lu\\'is Nogueira, Lu\\'is Miguel Pinho",
        "title": "Supporting Parallelism in Server-based Multiprocessor Systems",
        "comments": "WiP Session of the 31st IEEE Real-Time Systems Symposium",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.DC cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Developing an efficient server-based real-time scheduling solution that\nsupports dynamic task-level parallelism is now relevant to even the desktop and\nembedded domains and no longer only to the high performance computing market\nniche. This paper proposes a novel approach that combines the constant\nbandwidth server abstraction with a work-stealing load balancing scheme which,\nwhile ensuring isolation among tasks, enables a task to be executed on more\nthan one processor at a given time instant.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 14 Jun 2011 17:26:21 GMT"
            }
        ],
        "update_date": "2011-06-15",
        "authors_parsed": [
            [
                "Nogueira",
                "Lu\u00eds",
                ""
            ],
            [
                "Pinho",
                "Lu\u00eds Miguel",
                ""
            ]
        ]
    },
    {
        "id": "1106.2992",
        "submitter": "Michiel W. van Tol",
        "authors": "Michiel W. van Tol",
        "title": "A Characterization of the SPARC T3-4 System",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PF cs.DC cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This technical report covers a set of experiments on the 64-core SPARC T3-4\nsystem, comparing it to two similar AMD and Intel systems. Key characteristics\nas maximum integer and floating point arithmetic throughput are measured as\nwell as memory throughput, showing the scalability of the SPARC T3-4 system.\nThe performance of POSIX threads primitives is characterized and compared in\ndetail, such as thread creation and mutex synchronization. Scalability tests\nwith a fine grained multithreaded runtime are performed, showing problems with\natomic CAS operations on such physically highly parallel systems.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 15 Jun 2011 15:18:27 GMT"
            }
        ],
        "update_date": "2011-06-17",
        "authors_parsed": [
            [
                "van Tol",
                "Michiel W.",
                ""
            ]
        ]
    },
    {
        "id": "1107.2003",
        "submitter": "Qi Guo",
        "authors": "Qi Guo, Yunji Chen, Tianshi chen, and Ling Li",
        "title": "Efficient Deterministic Replay Using Complete Race Detection",
        "comments": "18 pages, 7 figures",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.PL cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Data races can significantly affect the executions of multi-threaded\nprograms. Hence, one has to recur the results of data races to\ndeterministically replay a multi-threaded program. However, data races are\nconcealed in enormous number of memory operations in a program. Due to the\ndifficulty of accurately identifying data races, previous multi-threaded\ndeterministic record/replay schemes for commodity multi-processor system give\nup to record data races directly. Consequently, they either record all shared\nmemory operations, which brings remarkable slowdown to the production run, or\nrecord the synchronization only, which introduces significant efforts to\nreplay.\n  Inspired by the advances in data race detection, we propose an efficient\nsoftware-only deterministic replay scheme for commodity multi-processor\nsystems, which is named RacX. The key insight of RacX is as follows: although\nit is NP-hard to accurately identify the existence of data races between a pair\nof memory operations, we can find out all potential data races in a\nmulti-threaded program, in which the false positives can be reduced to a small\namount with our automatic false positive reduction techniques. As a result,\nRacX can efficiently monitor all potential data races to deterministically\nreplay a multi-threaded program.\n  To evaluate RacX, we have carried out experiments over a number of well-known\nmulti-threaded programs from SPLASH-2 benchmark suite and large-scale\ncommercial programs. RacX can precisely recur production runs of these programs\nwith value determinism. Averagely, RacX causes only about 1.21%, 1.89%, 2.20%,\nand 8.41% slowdown to the original run during recording (for 2-, 4-, 8- and\n16-thread programs, respectively). The soundness, efficiency, scalability, and\nportability of RacX well demonstrate its superiority.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 11 Jul 2011 11:51:06 GMT"
            },
            {
                "version": "v2",
                "created": "Tue, 12 Jul 2011 08:46:58 GMT"
            }
        ],
        "update_date": "2011-07-13",
        "authors_parsed": [
            [
                "Guo",
                "Qi",
                ""
            ],
            [
                "Chen",
                "Yunji",
                ""
            ],
            [
                "chen",
                "Tianshi",
                ""
            ],
            [
                "Li",
                "Ling",
                ""
            ]
        ]
    },
    {
        "id": "1107.4786",
        "submitter": "Frederic Le Mouel",
        "authors": "Roya Golchay (CITI Insa Lyon / Inria Grenoble Rh\\^one-Alpes),\n  Fr\\'ed\\'eric Le Mou\\\"el (CITI Insa Lyon / Inria Grenoble Rh\\^one-Alpes),\n  St\\'ephane Fr\\'enot (CITI Insa Lyon / Inria Grenoble Rh\\^one-Alpes), Julien\n  Ponge (CITI Insa Lyon / Inria Grenoble Rh\\^one-Alpes)",
        "title": "Towards Bridging IoT and Cloud Services: Proposing Smartphones as Mobile\n  and Autonomic Service Gateways",
        "comments": "Position Paper",
        "journal-ref": "UbiMob'2011 (2011) 45--48",
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Computing is currently getting at the same time incredibly in the small with\nsensors/actuators embedded in our every- day objects and also greatly in the\nlarge with data and ser- vice clouds accessible anytime, anywhere. This\nInternet of Things is physically closed to the user but suffers from weak\nrun-time execution environments. Cloud Environments provide powerful data\nstorage and computing power but can not be easily accessed and integrate the\nfinal-user context- awareness. We consider smartphones are set to become the\nuniversal interface between these two worlds. In this position paper, we\npropose a middleware approach where smartphones provide service gateways to\nbridge the gap between IoT services and Cloud services. Since smartphones are\nmobile gateways, they should be able to (re)configure themself according to\ntheir place, things discovered around, and their own resources such battery.\nSeveral issues are discussed: collaborative event-based context management,\nadaptive and opportunistic service deployment and invocation, multi-criteria\n(user- and performance-oriented) optimization decision algorithm.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Sun, 24 Jul 2011 19:39:02 GMT"
            }
        ],
        "update_date": "2011-07-26",
        "authors_parsed": [
            [
                "Golchay",
                "Roya",
                "",
                "CITI Insa Lyon / Inria Grenoble Rh\u00f4ne-Alpes"
            ],
            [
                "Mou\u00ebl",
                "Fr\u00e9d\u00e9ric Le",
                "",
                "CITI Insa Lyon / Inria Grenoble Rh\u00f4ne-Alpes"
            ],
            [
                "Fr\u00e9not",
                "St\u00e9phane",
                "",
                "CITI Insa Lyon / Inria Grenoble Rh\u00f4ne-Alpes"
            ],
            [
                "Ponge",
                "Julien",
                "",
                "CITI Insa Lyon / Inria Grenoble Rh\u00f4ne-Alpes"
            ]
        ]
    },
    {
        "id": "1109.2638",
        "submitter": "Fabiano Botelho Dr.",
        "authors": "Nitin Garg and Ed Zhu and Fabiano C. Botelho",
        "title": "Light-weight Locks",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we propose a new approach to building synchronization\nprimitives, dubbed \"lwlocks\" (short for light-weight locks). The primitives are\noptimized for small memory footprint while maintaining efficient performance in\nlow contention scenarios. A read-write lwlock occupies 4 bytes, a mutex\noccupies 4 bytes (2 if deadlock detection is not required), and a condition\nvariable occupies 4 bytes. The corresponding primitives of the popular pthread\nlibrary occupy 56 bytes, 40 bytes and 48 bytes respectively on the x86-64\nplatform. The API for lwlocks is similar to that of the pthread library but\ncovering only the most common use cases. Lwlocks allow explicit control of\nqueuing and scheduling decisions in contention situations and support\n\"asynchronous\" or \"deferred blocking\" acquisition of locks. Asynchronous\nlocking helps in working around the constraints of lock-ordering which\notherwise limits concurrency. The small footprint of lwlocks enables the\nconstruction of data structures with very fine-grained locking, which in turn\nis crucial for lowering contention and supporting highly concurrent access to a\ndata structure. Currently, the Data Domain File System uses lwlocks for its\nin-memory inode cache as well as in a generic doubly-linked concurrent list\nwhich forms the building block for more sophisticated structures.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 12 Sep 2011 21:49:38 GMT"
            }
        ],
        "update_date": "2011-09-14",
        "authors_parsed": [
            [
                "Garg",
                "Nitin",
                ""
            ],
            [
                "Zhu",
                "Ed",
                ""
            ],
            [
                "Botelho",
                "Fabiano C.",
                ""
            ]
        ]
    },
    {
        "id": "1109.3075",
        "submitter": "Rakesh Mohanty",
        "authors": "Rakesh Mohanty, Manas Das, M. Lakshmi Prasanna, Sudhashree",
        "title": "Design and Performance Evaluation of A New Proposed Fittest Job First\n  Dynamic Round Robin(FJFDRR) Scheduling Algorithm",
        "comments": "05 Pages, 12 Figures, International Journal of Computer Information\n  Systems Vol. 2, No. 2, February 2011",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we have proposed a new variant of Round Robin scheduling\nalgorithm by executing the processes according to the new calculated Fit Factor\nf and using the concept of dynamic time quantum. We have compared the\nperformance of our proposed Fittest Job First Dynamic Round Robin(FJFDRR)\nalgorithm with the Priority Based Static Round Robin(PBSRR) algorithm.\nExperimental results show that our proposed algorithm performs better than\nPBSRR in terms of reducing the number of context switches, average waiting time\nand average turnaround time.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 14 Sep 2011 13:26:07 GMT"
            }
        ],
        "update_date": "2011-09-15",
        "authors_parsed": [
            [
                "Mohanty",
                "Rakesh",
                ""
            ],
            [
                "Das",
                "Manas",
                ""
            ],
            [
                "Prasanna",
                "M. Lakshmi",
                ""
            ],
            [
                "Sudhashree",
                "",
                ""
            ]
        ]
    },
    {
        "id": "1109.3076",
        "submitter": "Rakesh Mohanty",
        "authors": "H. S. Behera, Rakesh Mohanty, Sabyasachi Sahu, Sourav Kumar Bhoi",
        "title": "Comparative performance analysis of multi dynamic time quantum Round\n  Robin(MDTQRR) algorithm with arrival time",
        "comments": "10 pages, 18 Figures, Indian Journal of Computer Science and\n  Engineering vol. 2 no. 2 April-May 2011",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  CPU being considered a primary computer resource, its scheduling is central\nto operating-system design. A thorough performance evaluation of various\nscheduling algorithms manifests that Round Robin Algorithm is considered as\noptimal in time shared environment because the static time is equally shared\namong the processes. We have proposed an efficient technique in the process\nscheduling algorithm by using dynamic time quantum in Round Robin. Our approach\nis based on the calculation of time quantum twice in single round robin cycle.\nTaking into consideration the arrival time, we implement the algorithm.\nExperimental analysis shows better performance of this improved algorithm over\nthe Round Robin algorithm and the Shortest Remaining Burst Round Robin\nalgorithm. It minimizes the overall number of context switches, average waiting\ntime and average turn-around time. Consequently the throughput and CPU\nutilization is better.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 14 Sep 2011 13:32:55 GMT"
            }
        ],
        "update_date": "2011-09-15",
        "authors_parsed": [
            [
                "Behera",
                "H. S.",
                ""
            ],
            [
                "Mohanty",
                "Rakesh",
                ""
            ],
            [
                "Sahu",
                "Sabyasachi",
                ""
            ],
            [
                "Bhoi",
                "Sourav Kumar",
                ""
            ]
        ]
    },
    {
        "id": "1110.4623",
        "submitter": "Jeff A Stuart",
        "authors": "Jeff A. Stuart and John D. Owens",
        "title": "Efficient Synchronization Primitives for GPUs",
        "comments": "13 pages with appendix, several figures, plans to submit to CompSci\n  conference in early 2012",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS cs.DC cs.DS cs.GR",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  In this paper, we revisit the design of synchronization\nprimitives---specifically barriers, mutexes, and semaphores---and how they\napply to the GPU. Previous implementations are insufficient due to the\ndiscrepancies in hardware and programming model of the GPU and CPU. We create\nnew implementations in CUDA and analyze the performance of spinning on the GPU,\nas well as a method of sleeping on the GPU, by running a set of memory-system\nbenchmarks on two of the most common GPUs in use, the Tesla- and Fermi-class\nGPUs from NVIDIA. From our results we define higher-level principles that are\nvalid for generic many-core processors, the most important of which is to limit\nthe number of atomic accesses required for a synchronization operation because\natomic accesses are slower than regular memory accesses. We use the results of\nthe benchmarks to critique existing synchronization algorithms and guide our\nnew implementations, and then define an abstraction of GPUs to classify any GPU\nbased on the behavior of the memory system. We use this abstraction to create\nsuitable implementations of the primitives specifically targeting the GPU, and\nanalyze the performance of these algorithms on Tesla and Fermi. We then predict\nperformance on future GPUs based on characteristics of the abstraction. We also\nexamine the roles of spin waiting and sleep waiting in each primitive and how\ntheir performance varies based on the machine abstraction, then give a set of\nguidelines for when each strategy is useful based on the characteristics of the\nGPU and expected contention.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Thu, 20 Oct 2011 19:43:58 GMT"
            }
        ],
        "update_date": "2011-10-21",
        "authors_parsed": [
            [
                "Stuart",
                "Jeff A.",
                ""
            ],
            [
                "Owens",
                "John D.",
                ""
            ]
        ]
    },
    {
        "id": "1110.5793",
        "submitter": "Jo\\\"el Goossens",
        "authors": "Vandy Berten (1), Jo\\\"el Goossens (1) ((1) U.L.B.)",
        "title": "Sufficient FTP Schedulability Test for the Non-Cyclic Generalized\n  Multiframe Task Model",
        "comments": null,
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Our goal is to provide a sufficient schedulability test -ideally polynomial-\nfor the scheduling of Non-Cyclic Generalized Multiframe Task Model using\nFixed-Task-Priority schedulers. We report two first results: (i) we present and\nprove correct the critical instant for the Non-Cyclic Generalized Multiframe\nTask Model then (ii) we propose an algorithm which provides a sufficient (but\npseudo-polynomial) schedulability test.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Wed, 26 Oct 2011 13:53:41 GMT"
            }
        ],
        "update_date": "2011-10-27",
        "authors_parsed": [
            [
                "Berten",
                "Vandy",
                "",
                "U.L.B"
            ],
            [
                "Goossens",
                "Jo\u00ebl",
                "",
                "U.L.B"
            ]
        ]
    },
    {
        "id": "1111.1930",
        "submitter": "Boudhir Anouar Abdelhakim",
        "authors": "A. A. Boudhir, M. Bouhorma, M. Ben Ahmed and Elbrak Said",
        "title": "The UWB Solution for Multimedia Traffic in Wireless Sensor Networks",
        "comments": "8 pages, 11 figures, IJWMN Journal",
        "journal-ref": "International Journal of Wireless & Mobile Networks October Issue\n  2011",
        "doi": "10.5121/ijwmn",
        "report-no": null,
        "categories": "cs.NI cs.MM cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Several researches are focused on the QoS (Quality of Service) and Energy\nconsumption in wireless Multimedia Sensor Networks. Those research projects\ninvest in theory and practice in order to extend the spectrum of use of norms,\nstandards and technologies which are emerged in wireless communications. The\nperformance of these technologies is strongly related to domains of use and\nlimitations of their characteristics. In this paper, we give a comparison of\nZigBee technology, most widely used in sensor networks, and UWB (Ultra Wide\nBand) which presents itself as competitor that present in these work better\nresults for audiovisual applications with medium-range and high throughput.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 8 Nov 2011 15:03:26 GMT"
            }
        ],
        "update_date": "2011-11-09",
        "authors_parsed": [
            [
                "Boudhir",
                "A. A.",
                ""
            ],
            [
                "Bouhorma",
                "M.",
                ""
            ],
            [
                "Ahmed",
                "M. Ben",
                ""
            ],
            [
                "Said",
                "Elbrak",
                ""
            ]
        ]
    },
    {
        "id": "1111.5251",
        "submitter": "Miguel A. Fortuna",
        "authors": "Miguel A. Fortuna, Juan A. Bonachela and Simon A. Levin",
        "title": "Evolution of a Modular Software Network",
        "comments": "To appear in PNAS",
        "journal-ref": null,
        "doi": "10.1073/pnas.1115960108",
        "report-no": null,
        "categories": "cs.OS cs.SE",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  \"Evolution behaves like a tinkerer\" (Francois Jacob, Science, 1977). Software\nsystems provide a unique opportunity to understand biological processes using\nconcepts from network theory. The Debian GNU/Linux operating system allows us\nto explore the evolution of a complex network in a novel way. The modular\ndesign detected during its growth is based on the reuse of existing code in\norder to minimize costs during programming. The increase of modularity\nexperienced by the system over time has not counterbalanced the increase in\nincompatibilities between software packages within modules. This negative\neffect is far from being a failure of design. A random process of package\ninstallation shows that the higher the modularity the larger the fraction of\npackages working properly in a local computer. The decrease in the relative\nnumber of conflicts between packages from different modules avoids a failure in\nthe functionality of one package spreading throughout the entire system. Some\npotential analogies with the evolutionary and ecological processes determining\nthe structure of ecological networks of interacting species are discussed.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 22 Nov 2011 17:00:50 GMT"
            }
        ],
        "update_date": "2015-06-03",
        "authors_parsed": [
            [
                "Fortuna",
                "Miguel A.",
                ""
            ],
            [
                "Bonachela",
                "Juan A.",
                ""
            ],
            [
                "Levin",
                "Simon A.",
                ""
            ]
        ]
    },
    {
        "id": "1111.5348",
        "submitter": "Seifedine Kadry Seifedine Kadry",
        "authors": "Abbas Noon, Ali Kalakech, Seifedine Kadry",
        "title": "A New Round Robin Based Scheduling Algorithm for Operating Systems:\n  Dynamic Quantum Using the Mean Average",
        "comments": "6 pages, 4 figures",
        "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 3, No. 1, 2011, 224-229",
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  Round Robin, considered as the most widely adopted CPU scheduling algorithm,\nundergoes severe problems directly related to quantum size. If time quantum\nchosen is too large, the response time of the processes is considered too high.\nOn the other hand, if this quantum is too small, it increases the overhead of\nthe CPU. In this paper, we propose a new algorithm, called AN, based on a new\napproach called dynamic-time-quantum; the idea of this approach is to make the\noperating systems adjusts the time quantum according to the burst time of the\nset of waiting processes in the ready queue. Based on the simulations and\nexperiments, we show that the new proposed algorithm solves the fixed time\nquantum problem and increases the performance of Round Robin.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Tue, 22 Nov 2011 21:44:25 GMT"
            }
        ],
        "update_date": "2011-11-24",
        "authors_parsed": [
            [
                "Noon",
                "Abbas",
                ""
            ],
            [
                "Kalakech",
                "Ali",
                ""
            ],
            [
                "Kadry",
                "Seifedine",
                ""
            ]
        ]
    },
    {
        "id": "1111.5880",
        "submitter": "Zhenwu Shi",
        "authors": "Fumin Zhang, Zhenwu Shi, and Shayok Mukhopadhyay",
        "title": "Robustness Analysis for Battery Supported Cyber-Physical Systems",
        "comments": "This paper has been accepted by ACM Transactions in Embedded\n  Computing Systems (TECS) in October, 2011",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.ET cs.OS cs.SY",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  This paper establishes a novel analytical approach to quantify robustness of\nscheduling and battery management for battery supported cyber-physical systems.\nA dynamic schedulability test is introduced to determine whether tasks are\nschedulable within a finite time window. The test is used to measure robustness\nof a real-time scheduling algorithm by evaluating the strength of computing\ntime perturbations that break schedulability at runtime. Robustness of battery\nmanagement is quantified analytically by an adaptive threshold on the state of\ncharge. The adaptive threshold significantly reduces the false alarm rate for\nbattery management algorithms to decide when a battery needs to be replaced.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Fri, 25 Nov 2011 02:10:11 GMT"
            }
        ],
        "update_date": "2011-11-28",
        "authors_parsed": [
            [
                "Zhang",
                "Fumin",
                ""
            ],
            [
                "Shi",
                "Zhenwu",
                ""
            ],
            [
                "Mukhopadhyay",
                "Shayok",
                ""
            ]
        ]
    },
    {
        "id": "1112.4451",
        "submitter": "Abhijat Vichare",
        "authors": "Abhijat Vichare",
        "title": "What is an OS?",
        "comments": "Major changes: Improvised the discussion of the implicit assumptions,\n  added a sketch of a theory of an OS, and added a figure. Comments welcome. 32\n  pages, 5 figures. Submitted",
        "journal-ref": null,
        "doi": null,
        "report-no": null,
        "categories": "cs.OS",
        "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
        "abstract": "  While the engineering of operating systems is well understood, their formal\nstructure and properties are not. The latter needs a clear definition of the\npurpose of an OS and an identification of the core. In this paper I offer\ndefinitions of the OS, processes and files, and present a few useful\nprinciples. The principles allow us to identify work like closure and\ncontinuation algorithms, in programming languages that is useful for the OS\nproblem. The definitions and principles should yield a symbolic, albeit\nsemiquantitative, framework that encompasses practice. Towards that end I\nspecialise the definitions to describe conventional OSes and identify the core\noperations for a single computer OS that can be used to express their\nalgorithms. The assumptions underlying the algorithms offer the design space\nframework. The paging and segmentation algorithms for conventional OSes are\nextracted from the framework as a check. Among the insights the emerge is that\nan OS is a constructive proof of equivalence between models of computation.\nClear and useful definitions and principles are the first step towards a fully\nquantitative structure of an OS.\n",
        "versions": [
            {
                "version": "v1",
                "created": "Mon, 19 Dec 2011 20:07:54 GMT"
            },
            {
                "version": "v2",
                "created": "Fri, 17 Feb 2012 20:40:00 GMT"
            }
        ],
        "update_date": "2012-02-20",
        "authors_parsed": [
            [
                "Vichare",
                "Abhijat",
                ""
            ]
        ]
    }
]